; ModuleID = 'bench/jemalloc/original/ctl.ll'
source_filename = "bench/jemalloc/original/ctl.ll"
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

%struct.ctl_named_node_s = type { %struct.ctl_node_s, ptr, i64, ptr, ptr }
%struct.ctl_node_s = type { i8 }
%struct.malloc_mutex_s = type { %union.anon }
%union.anon = type { %struct.anon }
%struct.anon = type { %struct.mutex_prof_data_t, %struct.atomic_b_t, %union.pthread_mutex_t }
%struct.mutex_prof_data_t = type { %struct.nstime_t, %struct.nstime_t, i64, i64, i32, %struct.atomic_u32_t, i64, ptr, i64 }
%struct.nstime_t = type { i64 }
%struct.atomic_u32_t = type { i32 }
%struct.atomic_b_t = type { i8 }
%union.pthread_mutex_t = type { %struct.__pthread_mutex_s }
%struct.__pthread_mutex_s = type { i32, i32, i32, i32, i32, i16, i16, %struct.__pthread_internal_list }
%struct.__pthread_internal_list = type { ptr, ptr }
%struct.tsd_s = type { i8, i8, i8, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, ptr, i64, i64, i64, ptr, ptr, %struct.ticker_geom_s, i8, %struct.tsd_binshards_s, %struct.tsd_link_t, i8, %struct.peak_s, %struct.activity_callback_thunk_s, %struct.tcache_slow_s, %struct.rtree_ctx_s, %struct.atomic_u8_t, i64, i64, i64, i64, %struct.tcache_s, %struct.witness_tsd_s }
%struct.ticker_geom_s = type { i32, i32 }
%struct.tsd_binshards_s = type { [36 x i8] }
%struct.tsd_link_t = type { ptr, ptr }
%struct.peak_s = type { i64, i64 }
%struct.activity_callback_thunk_s = type { ptr, ptr }
%struct.tcache_slow_s = type { %struct.anon.3, %struct.cache_bin_array_descriptor_s, ptr, i32, i32, [36 x i8], [36 x i8], [36 x i8], ptr, ptr }
%struct.anon.3 = type { ptr, ptr }
%struct.cache_bin_array_descriptor_s = type { %struct.anon.4, ptr }
%struct.anon.4 = type { ptr, ptr }
%struct.rtree_ctx_s = type { [16 x %struct.rtree_ctx_cache_elm_s], [8 x %struct.rtree_ctx_cache_elm_s] }
%struct.rtree_ctx_cache_elm_s = type { i64, ptr }
%struct.atomic_u8_t = type { i8 }
%struct.tcache_s = type { ptr, [73 x %struct.cache_bin_s] }
%struct.cache_bin_s = type { ptr, %struct.cache_bin_stats_s, i16, i16, i16, %struct.cache_bin_info_s }
%struct.cache_bin_stats_s = type { i64 }
%struct.cache_bin_info_s = type { i16 }
%struct.witness_tsd_s = type { %struct.witness_list_t, i8 }
%struct.witness_list_t = type { ptr }
%struct.atomic_p_t = type { ptr }
%struct.arena_config_s = type { ptr, i8 }
%struct.ctl_indexed_node_s = type { %struct.ctl_node_s, ptr }
%struct.hpa_shard_opts_s = type { i64, i64, i32, i8, i64, i64 }
%struct.sec_opts_s = type { i64, i64, i64, i64, i64 }
%struct.extent_hooks_s = type { ptr, ptr, ptr, ptr, ptr, ptr, ptr, ptr, ptr }
%struct.bin_info_s = type { i64, i64, i32, i32, %struct.bitmap_info_s }
%struct.bitmap_info_s = type { i64, i64 }
%struct.emap_s = type { %struct.rtree_s }
%struct.rtree_s = type { ptr, %struct.malloc_mutex_s, [262144 x %struct.rtree_node_elm_s] }
%struct.rtree_node_elm_s = type { %struct.atomic_p_t }
%struct.atomic_zu_t = type { i64 }
%struct.bin_stats_data_s = type { %struct.bin_stats_s, %struct.mutex_prof_data_t }
%struct.bin_stats_s = type { i64, i64, i64, i64, i64, i64, i64, i64, i64, i64 }
%struct.arena_stats_large_s = type { %struct.locked_u64_s, %struct.locked_u64_s, %struct.locked_u64_s, %struct.locked_u64_s, %struct.locked_u64_s, i64 }
%struct.locked_u64_s = type { %struct.atomic_u64_t }
%struct.atomic_u64_t = type { i64 }
%struct.pac_estats_s = type { i64, i64, i64, i64, i64, i64 }
%struct.background_thread_info_s = type { i64, %union.pthread_cond_t, %struct.malloc_mutex_s, i32, %struct.atomic_b_t, %struct.nstime_t, i64, i64, %struct.nstime_t }
%union.pthread_cond_t = type { %struct.__pthread_cond_s }
%struct.__pthread_cond_s = type { %union.__atomic_wide_counter, %union.__atomic_wide_counter, [2 x i32], [2 x i32], i32, i32, [2 x i32] }
%union.__atomic_wide_counter = type { i64 }
%struct.rtree_contents_s = type { ptr, %struct.rtree_metadata_s }
%struct.rtree_metadata_s = type { i32, i32, i8, i8 }
%struct.rtree_leaf_elm_s = type { %struct.atomic_p_t }
%struct.bin_s = type { %struct.malloc_mutex_s, %struct.bin_stats_s, ptr, %struct.edata_heap_t, %struct.edata_list_active_t }
%struct.edata_heap_t = type { %struct.ph_s }
%struct.ph_s = type { ptr, i64 }
%struct.edata_list_active_t = type { %struct.anon.7 }
%struct.anon.7 = type { ptr }
%struct.psset_bin_stats_s = type { i64, i64, i64 }
%struct.hooks_s = type { ptr, ptr, ptr, ptr }
%struct.inspect_extent_util_stats_s = type { i64, i64, i64 }

@ctl_initialized = internal unnamed_addr global i1 false, align 1
@super_root_node = internal constant [1 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.1, i64 13, ptr @root_node, ptr null }], align 16
@ctl_mtx = internal global %struct.malloc_mutex_s zeroinitializer, align 8
@.str = private unnamed_addr constant [4 x i8] c"ctl\00", align 1
@ctl_arenas = internal unnamed_addr global ptr null, align 8
@ctl_stats = internal unnamed_addr global ptr null, align 8
@dss_prec_names = external local_unnamed_addr constant [0 x ptr], align 8
@background_thread_lock = external global %struct.malloc_mutex_s, align 8
@tsd_tls = external thread_local(initialexec) global %struct.tsd_s, align 8
@arenas = external local_unnamed_addr global [0 x %struct.atomic_p_t], align 8
@arena_config_default = external constant %struct.arena_config_s, align 8
@sz_index2size_tab = external local_unnamed_addr global [232 x i64], align 16
@nstime_zero = internal constant %struct.nstime_t zeroinitializer, align 8
@.str.1 = private unnamed_addr constant [1 x i8] zeroinitializer, align 1
@root_node = internal constant [13 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.2, i64 0, ptr null, ptr @version_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.3, i64 0, ptr null, ptr @epoch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.4, i64 0, ptr null, ptr @background_thread_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.5, i64 0, ptr null, ptr @max_background_threads_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.6, i64 9, ptr @thread_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.7, i64 12, ptr @config_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.8, i64 67, ptr @opt_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.9, i64 3, ptr @tcache_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.10, i64 1, ptr @arena_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.11, i64 13, ptr @arenas_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.12, i64 11, ptr @prof_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.13, i64 13, ptr @stats_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.14, i64 7, ptr @experimental_node, ptr null }], align 16
@.str.2 = private unnamed_addr constant [8 x i8] c"version\00", align 1
@.str.3 = private unnamed_addr constant [6 x i8] c"epoch\00", align 1
@.str.4 = private unnamed_addr constant [18 x i8] c"background_thread\00", align 1
@.str.5 = private unnamed_addr constant [23 x i8] c"max_background_threads\00", align 1
@.str.6 = private unnamed_addr constant [7 x i8] c"thread\00", align 1
@thread_node = internal constant [9 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.10, i64 0, ptr null, ptr @thread_arena_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.16, i64 0, ptr null, ptr @thread_allocated_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.17, i64 0, ptr null, ptr @thread_allocatedp_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.18, i64 0, ptr null, ptr @thread_deallocated_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.19, i64 0, ptr null, ptr @thread_deallocatedp_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.9, i64 4, ptr @thread_tcache_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.20, i64 2, ptr @thread_peak_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.12, i64 2, ptr @thread_prof_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.21, i64 0, ptr null, ptr @thread_idle_ctl }], align 16
@.str.7 = private unnamed_addr constant [7 x i8] c"config\00", align 1
@config_node = internal constant [12 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.32, i64 0, ptr null, ptr @config_cache_oblivious_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.33, i64 0, ptr null, ptr @config_debug_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.34, i64 0, ptr null, ptr @config_fill_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.35, i64 0, ptr null, ptr @config_lazy_lock_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.36, i64 0, ptr null, ptr @config_malloc_conf_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.37, i64 0, ptr null, ptr @config_opt_safety_checks_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.12, i64 0, ptr null, ptr @config_prof_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.38, i64 0, ptr null, ptr @config_prof_libgcc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.39, i64 0, ptr null, ptr @config_prof_libunwind_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.13, i64 0, ptr null, ptr @config_stats_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.40, i64 0, ptr null, ptr @config_utrace_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.41, i64 0, ptr null, ptr @config_xmalloc_ctl }], align 16
@.str.8 = private unnamed_addr constant [4 x i8] c"opt\00", align 1
@opt_node = internal constant [67 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.42, i64 0, ptr null, ptr @opt_abort_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.43, i64 0, ptr null, ptr @opt_abort_conf_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.32, i64 0, ptr null, ptr @opt_cache_oblivious_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.44, i64 0, ptr null, ptr @opt_trust_madvise_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.45, i64 0, ptr null, ptr @opt_confirm_conf_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.46, i64 0, ptr null, ptr @opt_hpa_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.47, i64 0, ptr null, ptr @opt_hpa_slab_max_alloc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.48, i64 0, ptr null, ptr @opt_hpa_hugification_threshold_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.49, i64 0, ptr null, ptr @opt_hpa_hugify_delay_ms_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.50, i64 0, ptr null, ptr @opt_hpa_min_purge_interval_ms_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.51, i64 0, ptr null, ptr @opt_hpa_dirty_mult_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.52, i64 0, ptr null, ptr @opt_hpa_sec_nshards_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.53, i64 0, ptr null, ptr @opt_hpa_sec_max_alloc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.54, i64 0, ptr null, ptr @opt_hpa_sec_max_bytes_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.55, i64 0, ptr null, ptr @opt_hpa_sec_bytes_after_flush_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.56, i64 0, ptr null, ptr @opt_hpa_sec_batch_fill_extra_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.57, i64 0, ptr null, ptr @opt_metadata_thp_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.58, i64 0, ptr null, ptr @opt_retain_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.59, i64 0, ptr null, ptr @opt_dss_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.60, i64 0, ptr null, ptr @opt_narenas_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.61, i64 0, ptr null, ptr @opt_percpu_arena_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.62, i64 0, ptr null, ptr @opt_oversize_threshold_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.63, i64 0, ptr null, ptr @opt_mutex_max_spin_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.4, i64 0, ptr null, ptr @opt_background_thread_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.5, i64 0, ptr null, ptr @opt_max_background_threads_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.64, i64 0, ptr null, ptr @opt_dirty_decay_ms_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.65, i64 0, ptr null, ptr @opt_muzzy_decay_ms_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.66, i64 0, ptr null, ptr @opt_stats_print_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.67, i64 0, ptr null, ptr @opt_stats_print_opts_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.68, i64 0, ptr null, ptr @opt_stats_interval_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.69, i64 0, ptr null, ptr @opt_stats_interval_opts_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.70, i64 0, ptr null, ptr @opt_junk_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.71, i64 0, ptr null, ptr @opt_zero_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.40, i64 0, ptr null, ptr @opt_utrace_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.41, i64 0, ptr null, ptr @opt_xmalloc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.72, i64 0, ptr null, ptr @opt_experimental_infallible_new_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.9, i64 0, ptr null, ptr @opt_tcache_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.73, i64 0, ptr null, ptr @opt_tcache_max_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.74, i64 0, ptr null, ptr @opt_tcache_nslots_small_min_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.75, i64 0, ptr null, ptr @opt_tcache_nslots_small_max_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.76, i64 0, ptr null, ptr @opt_tcache_nslots_large_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.77, i64 0, ptr null, ptr @opt_lg_tcache_nslots_mul_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.78, i64 0, ptr null, ptr @opt_tcache_gc_incr_bytes_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.79, i64 0, ptr null, ptr @opt_tcache_gc_delay_bytes_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.80, i64 0, ptr null, ptr @opt_lg_tcache_flush_small_div_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.81, i64 0, ptr null, ptr @opt_lg_tcache_flush_large_div_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.82, i64 0, ptr null, ptr @opt_thp_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.83, i64 0, ptr null, ptr @opt_lg_extent_max_active_fit_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.12, i64 0, ptr null, ptr @opt_prof_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.84, i64 0, ptr null, ptr @opt_prof_prefix_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.85, i64 0, ptr null, ptr @opt_prof_active_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.86, i64 0, ptr null, ptr @opt_prof_thread_active_init_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.87, i64 0, ptr null, ptr @opt_prof_bt_max_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.88, i64 0, ptr null, ptr @opt_lg_prof_sample_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.89, i64 0, ptr null, ptr @opt_lg_prof_interval_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.90, i64 0, ptr null, ptr @opt_prof_gdump_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.91, i64 0, ptr null, ptr @opt_prof_final_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.92, i64 0, ptr null, ptr @opt_prof_leak_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.93, i64 0, ptr null, ptr @opt_prof_leak_error_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.94, i64 0, ptr null, ptr @opt_prof_accum_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.95, i64 0, ptr null, ptr @opt_prof_recent_alloc_max_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.96, i64 0, ptr null, ptr @opt_prof_stats_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.97, i64 0, ptr null, ptr @opt_prof_sys_thread_name_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.98, i64 0, ptr null, ptr @opt_prof_time_res_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.99, i64 0, ptr null, ptr @opt_lg_san_uaf_align_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.100, i64 0, ptr null, ptr @opt_zero_realloc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.101, i64 0, ptr null, ptr @opt_debug_double_free_max_scan_ctl }], align 16
@.str.9 = private unnamed_addr constant [7 x i8] c"tcache\00", align 1
@tcache_node = internal constant [3 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.102, i64 0, ptr null, ptr @tcache_create_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.24, i64 0, ptr null, ptr @tcache_flush_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.103, i64 0, ptr null, ptr @tcache_destroy_ctl }], align 16
@.str.10 = private unnamed_addr constant [6 x i8] c"arena\00", align 1
@arena_node = internal constant [1 x %struct.ctl_indexed_node_s] [%struct.ctl_indexed_node_s { %struct.ctl_node_s zeroinitializer, ptr @arena_i_index }], align 16
@.str.11 = private unnamed_addr constant [7 x i8] c"arenas\00", align 1
@arenas_node = internal constant [13 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.60, i64 0, ptr null, ptr @arenas_narenas_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.64, i64 0, ptr null, ptr @arenas_dirty_decay_ms_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.65, i64 0, ptr null, ptr @arenas_muzzy_decay_ms_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.109, i64 0, ptr null, ptr @arenas_quantum_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.110, i64 0, ptr null, ptr @arenas_page_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.73, i64 0, ptr null, ptr @arenas_tcache_max_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.111, i64 0, ptr null, ptr @arenas_nbins_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.112, i64 0, ptr null, ptr @arenas_nhbins_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.113, i64 1, ptr @arenas_bin_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.114, i64 0, ptr null, ptr @arenas_nlextents_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.115, i64 1, ptr @arenas_lextent_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.102, i64 0, ptr null, ptr @arenas_create_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.116, i64 0, ptr null, ptr @arenas_lookup_ctl }], align 16
@.str.12 = private unnamed_addr constant [5 x i8] c"prof\00", align 1
@prof_node = internal constant [11 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.121, i64 0, ptr null, ptr @prof_thread_active_init_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.31, i64 0, ptr null, ptr @prof_active_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.122, i64 0, ptr null, ptr @prof_dump_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.123, i64 0, ptr null, ptr @prof_gdump_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.124, i64 0, ptr null, ptr @prof_prefix_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.29, i64 0, ptr null, ptr @prof_reset_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.125, i64 0, ptr null, ptr @prof_interval_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.126, i64 0, ptr null, ptr @lg_prof_sample_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.127, i64 0, ptr null, ptr @prof_log_start_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.128, i64 0, ptr null, ptr @prof_log_stop_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.13, i64 2, ptr @prof_stats_node, ptr null }], align 16
@.str.13 = private unnamed_addr constant [6 x i8] c"stats\00", align 1
@stats_node = internal constant [13 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.16, i64 0, ptr null, ptr @stats_allocated_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.31, i64 0, ptr null, ptr @stats_active_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.131, i64 0, ptr null, ptr @stats_metadata_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.132, i64 0, ptr null, ptr @stats_metadata_edata_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.133, i64 0, ptr null, ptr @stats_metadata_rtree_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.57, i64 0, ptr null, ptr @stats_metadata_thp_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.134, i64 0, ptr null, ptr @stats_resident_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.135, i64 0, ptr null, ptr @stats_mapped_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.136, i64 0, ptr null, ptr @stats_retained_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.4, i64 3, ptr @stats_background_thread_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.137, i64 10, ptr @stats_mutexes_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.11, i64 1, ptr @stats_arenas_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.138, i64 0, ptr null, ptr @stats_zero_reallocs_ctl }], align 16
@.str.14 = private unnamed_addr constant [13 x i8] c"experimental\00", align 1
@experimental_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.214, i64 7, ptr @experimental_hooks_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.215, i64 2, ptr @experimental_utilization_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.11, i64 1, ptr @experimental_arenas_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.216, i64 0, ptr null, ptr @experimental_arenas_create_ext_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.217, i64 2, ptr @experimental_prof_recent_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.218, i64 0, ptr null, ptr @experimental_batch_alloc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.6, i64 1, ptr @experimental_thread_node, ptr null }], align 16
@.str.15 = private unnamed_addr constant [52 x i8] c"5.3.0-147-ge4817c8d89a2a413e835c4adeab5c5c4412f9235\00", align 1
@background_thread_enabled_state = external local_unnamed_addr global %struct.atomic_b_t, align 1
@max_background_threads = external local_unnamed_addr global i64, align 8
@opt_max_background_threads = external local_unnamed_addr global i64, align 8
@.str.16 = private unnamed_addr constant [10 x i8] c"allocated\00", align 1
@.str.17 = private unnamed_addr constant [11 x i8] c"allocatedp\00", align 1
@.str.18 = private unnamed_addr constant [12 x i8] c"deallocated\00", align 1
@.str.19 = private unnamed_addr constant [13 x i8] c"deallocatedp\00", align 1
@thread_tcache_node = internal constant [4 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.22, i64 0, ptr null, ptr @thread_tcache_enabled_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.23, i64 0, ptr null, ptr @thread_tcache_max_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.24, i64 0, ptr null, ptr @thread_tcache_flush_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.25, i64 2, ptr @thread_tcache_ncached_max_node, ptr null }], align 16
@.str.20 = private unnamed_addr constant [5 x i8] c"peak\00", align 1
@thread_peak_node = internal constant [2 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.28, i64 0, ptr null, ptr @thread_peak_read_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.29, i64 0, ptr null, ptr @thread_peak_reset_ctl }], align 16
@thread_prof_node = internal constant [2 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.30, i64 0, ptr null, ptr @thread_prof_name_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.31, i64 0, ptr null, ptr @thread_prof_active_ctl }], align 16
@.str.21 = private unnamed_addr constant [5 x i8] c"idle\00", align 1
@opt_percpu_arena = external local_unnamed_addr global i32, align 4
@ncpus = external local_unnamed_addr global i32, align 4
@.str.22 = private unnamed_addr constant [8 x i8] c"enabled\00", align 1
@.str.23 = private unnamed_addr constant [4 x i8] c"max\00", align 1
@.str.24 = private unnamed_addr constant [6 x i8] c"flush\00", align 1
@.str.25 = private unnamed_addr constant [12 x i8] c"ncached_max\00", align 1
@thread_tcache_ncached_max_node = internal constant [2 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.26, i64 0, ptr null, ptr @thread_tcache_ncached_max_read_sizeclass_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.27, i64 0, ptr null, ptr @thread_tcache_ncached_max_write_ctl }], align 16
@sz_size2index_tab = external local_unnamed_addr global [0 x i8], align 1
@.str.26 = private unnamed_addr constant [15 x i8] c"read_sizeclass\00", align 1
@.str.27 = private unnamed_addr constant [6 x i8] c"write\00", align 1
@.str.28 = private unnamed_addr constant [5 x i8] c"read\00", align 1
@.str.29 = private unnamed_addr constant [6 x i8] c"reset\00", align 1
@.str.30 = private unnamed_addr constant [5 x i8] c"name\00", align 1
@.str.31 = private unnamed_addr constant [7 x i8] c"active\00", align 1
@opt_narenas = external local_unnamed_addr global i32, align 4
@.str.32 = private unnamed_addr constant [16 x i8] c"cache_oblivious\00", align 1
@.str.33 = private unnamed_addr constant [6 x i8] c"debug\00", align 1
@.str.34 = private unnamed_addr constant [5 x i8] c"fill\00", align 1
@.str.35 = private unnamed_addr constant [10 x i8] c"lazy_lock\00", align 1
@.str.36 = private unnamed_addr constant [12 x i8] c"malloc_conf\00", align 1
@.str.37 = private unnamed_addr constant [18 x i8] c"opt_safety_checks\00", align 1
@.str.38 = private unnamed_addr constant [12 x i8] c"prof_libgcc\00", align 1
@.str.39 = private unnamed_addr constant [15 x i8] c"prof_libunwind\00", align 1
@.str.40 = private unnamed_addr constant [7 x i8] c"utrace\00", align 1
@.str.41 = private unnamed_addr constant [8 x i8] c"xmalloc\00", align 1
@.str.42 = private unnamed_addr constant [6 x i8] c"abort\00", align 1
@.str.43 = private unnamed_addr constant [11 x i8] c"abort_conf\00", align 1
@.str.44 = private unnamed_addr constant [14 x i8] c"trust_madvise\00", align 1
@.str.45 = private unnamed_addr constant [13 x i8] c"confirm_conf\00", align 1
@.str.46 = private unnamed_addr constant [4 x i8] c"hpa\00", align 1
@.str.47 = private unnamed_addr constant [19 x i8] c"hpa_slab_max_alloc\00", align 1
@.str.48 = private unnamed_addr constant [27 x i8] c"hpa_hugification_threshold\00", align 1
@.str.49 = private unnamed_addr constant [20 x i8] c"hpa_hugify_delay_ms\00", align 1
@.str.50 = private unnamed_addr constant [26 x i8] c"hpa_min_purge_interval_ms\00", align 1
@.str.51 = private unnamed_addr constant [15 x i8] c"hpa_dirty_mult\00", align 1
@.str.52 = private unnamed_addr constant [16 x i8] c"hpa_sec_nshards\00", align 1
@.str.53 = private unnamed_addr constant [18 x i8] c"hpa_sec_max_alloc\00", align 1
@.str.54 = private unnamed_addr constant [18 x i8] c"hpa_sec_max_bytes\00", align 1
@.str.55 = private unnamed_addr constant [26 x i8] c"hpa_sec_bytes_after_flush\00", align 1
@.str.56 = private unnamed_addr constant [25 x i8] c"hpa_sec_batch_fill_extra\00", align 1
@.str.57 = private unnamed_addr constant [13 x i8] c"metadata_thp\00", align 1
@.str.58 = private unnamed_addr constant [7 x i8] c"retain\00", align 1
@.str.59 = private unnamed_addr constant [4 x i8] c"dss\00", align 1
@.str.60 = private unnamed_addr constant [8 x i8] c"narenas\00", align 1
@.str.61 = private unnamed_addr constant [13 x i8] c"percpu_arena\00", align 1
@.str.62 = private unnamed_addr constant [19 x i8] c"oversize_threshold\00", align 1
@.str.63 = private unnamed_addr constant [15 x i8] c"mutex_max_spin\00", align 1
@.str.64 = private unnamed_addr constant [15 x i8] c"dirty_decay_ms\00", align 1
@.str.65 = private unnamed_addr constant [15 x i8] c"muzzy_decay_ms\00", align 1
@.str.66 = private unnamed_addr constant [12 x i8] c"stats_print\00", align 1
@.str.67 = private unnamed_addr constant [17 x i8] c"stats_print_opts\00", align 1
@.str.68 = private unnamed_addr constant [15 x i8] c"stats_interval\00", align 1
@.str.69 = private unnamed_addr constant [20 x i8] c"stats_interval_opts\00", align 1
@.str.70 = private unnamed_addr constant [5 x i8] c"junk\00", align 1
@.str.71 = private unnamed_addr constant [5 x i8] c"zero\00", align 1
@.str.72 = private unnamed_addr constant [28 x i8] c"experimental_infallible_new\00", align 1
@.str.73 = private unnamed_addr constant [11 x i8] c"tcache_max\00", align 1
@.str.74 = private unnamed_addr constant [24 x i8] c"tcache_nslots_small_min\00", align 1
@.str.75 = private unnamed_addr constant [24 x i8] c"tcache_nslots_small_max\00", align 1
@.str.76 = private unnamed_addr constant [20 x i8] c"tcache_nslots_large\00", align 1
@.str.77 = private unnamed_addr constant [21 x i8] c"lg_tcache_nslots_mul\00", align 1
@.str.78 = private unnamed_addr constant [21 x i8] c"tcache_gc_incr_bytes\00", align 1
@.str.79 = private unnamed_addr constant [22 x i8] c"tcache_gc_delay_bytes\00", align 1
@.str.80 = private unnamed_addr constant [26 x i8] c"lg_tcache_flush_small_div\00", align 1
@.str.81 = private unnamed_addr constant [26 x i8] c"lg_tcache_flush_large_div\00", align 1
@.str.82 = private unnamed_addr constant [4 x i8] c"thp\00", align 1
@.str.83 = private unnamed_addr constant [25 x i8] c"lg_extent_max_active_fit\00", align 1
@.str.84 = private unnamed_addr constant [12 x i8] c"prof_prefix\00", align 1
@.str.85 = private unnamed_addr constant [12 x i8] c"prof_active\00", align 1
@.str.86 = private unnamed_addr constant [24 x i8] c"prof_thread_active_init\00", align 1
@.str.87 = private unnamed_addr constant [12 x i8] c"prof_bt_max\00", align 1
@.str.88 = private unnamed_addr constant [15 x i8] c"lg_prof_sample\00", align 1
@.str.89 = private unnamed_addr constant [17 x i8] c"lg_prof_interval\00", align 1
@.str.90 = private unnamed_addr constant [11 x i8] c"prof_gdump\00", align 1
@.str.91 = private unnamed_addr constant [11 x i8] c"prof_final\00", align 1
@.str.92 = private unnamed_addr constant [10 x i8] c"prof_leak\00", align 1
@.str.93 = private unnamed_addr constant [16 x i8] c"prof_leak_error\00", align 1
@.str.94 = private unnamed_addr constant [11 x i8] c"prof_accum\00", align 1
@.str.95 = private unnamed_addr constant [22 x i8] c"prof_recent_alloc_max\00", align 1
@.str.96 = private unnamed_addr constant [11 x i8] c"prof_stats\00", align 1
@.str.97 = private unnamed_addr constant [21 x i8] c"prof_sys_thread_name\00", align 1
@.str.98 = private unnamed_addr constant [21 x i8] c"prof_time_resolution\00", align 1
@.str.99 = private unnamed_addr constant [17 x i8] c"lg_san_uaf_align\00", align 1
@.str.100 = private unnamed_addr constant [13 x i8] c"zero_realloc\00", align 1
@.str.101 = private unnamed_addr constant [27 x i8] c"debug_double_free_max_scan\00", align 1
@opt_abort = external local_unnamed_addr global i8, align 1
@opt_abort_conf = external local_unnamed_addr global i8, align 1
@opt_cache_oblivious = external local_unnamed_addr global i8, align 1
@opt_trust_madvise = external local_unnamed_addr global i8, align 1
@opt_confirm_conf = external local_unnamed_addr global i8, align 1
@opt_hpa = external local_unnamed_addr global i8, align 1
@opt_hpa_opts = external local_unnamed_addr global %struct.hpa_shard_opts_s, align 8
@opt_hpa_sec_opts = external local_unnamed_addr global %struct.sec_opts_s, align 8
@metadata_thp_mode_names = external local_unnamed_addr constant [0 x ptr], align 8
@opt_metadata_thp = external local_unnamed_addr global i32, align 4
@opt_retain = external local_unnamed_addr global i8, align 1
@opt_dss = external local_unnamed_addr global ptr, align 8
@percpu_arena_mode_names = external local_unnamed_addr constant [0 x ptr], align 8
@opt_oversize_threshold = external local_unnamed_addr global i64, align 8
@opt_mutex_max_spin = external local_unnamed_addr global i64, align 8
@opt_background_thread = external local_unnamed_addr global i8, align 1
@opt_dirty_decay_ms = external local_unnamed_addr global i64, align 8
@opt_muzzy_decay_ms = external local_unnamed_addr global i64, align 8
@opt_stats_print = external local_unnamed_addr global i8, align 1
@opt_stats_print_opts = external global [11 x i8], align 1
@opt_stats_interval = external local_unnamed_addr global i64, align 8
@opt_stats_interval_opts = external global [11 x i8], align 1
@opt_junk = external local_unnamed_addr global ptr, align 8
@opt_zero = external local_unnamed_addr global i8, align 1
@opt_experimental_infallible_new = external local_unnamed_addr global i8, align 1
@opt_tcache = external local_unnamed_addr global i8, align 1
@opt_tcache_max = external local_unnamed_addr global i64, align 8
@opt_tcache_nslots_small_min = external local_unnamed_addr global i32, align 4
@opt_tcache_nslots_small_max = external local_unnamed_addr global i32, align 4
@opt_tcache_nslots_large = external local_unnamed_addr global i32, align 4
@opt_lg_tcache_nslots_mul = external local_unnamed_addr global i64, align 8
@opt_tcache_gc_incr_bytes = external local_unnamed_addr global i64, align 8
@opt_tcache_gc_delay_bytes = external local_unnamed_addr global i64, align 8
@opt_lg_tcache_flush_small_div = external local_unnamed_addr global i32, align 4
@opt_lg_tcache_flush_large_div = external local_unnamed_addr global i32, align 4
@thp_mode_names = external local_unnamed_addr constant [0 x ptr], align 8
@opt_thp = external local_unnamed_addr global i32, align 4
@opt_lg_extent_max_active_fit = external local_unnamed_addr global i64, align 8
@zero_realloc_mode_names = external local_unnamed_addr constant [0 x ptr], align 8
@opt_zero_realloc_action = external local_unnamed_addr global i32, align 4
@opt_debug_double_free_max_scan = external local_unnamed_addr global i32, align 4
@.str.102 = private unnamed_addr constant [7 x i8] c"create\00", align 1
@.str.103 = private unnamed_addr constant [8 x i8] c"destroy\00", align 1
@super_arena_i_node = internal constant [1 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.1, i64 12, ptr @arena_i_node, ptr null }], align 16
@arena_i_node = internal constant [12 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.104, i64 0, ptr null, ptr @arena_i_initialized_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.105, i64 0, ptr null, ptr @arena_i_decay_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.106, i64 0, ptr null, ptr @arena_i_purge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.29, i64 0, ptr null, ptr @arena_i_reset_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.103, i64 0, ptr null, ptr @arena_i_destroy_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.59, i64 0, ptr null, ptr @arena_i_dss_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.62, i64 0, ptr null, ptr @arena_i_oversize_threshold_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.64, i64 0, ptr null, ptr @arena_i_dirty_decay_ms_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.65, i64 0, ptr null, ptr @arena_i_muzzy_decay_ms_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.107, i64 0, ptr null, ptr @arena_i_extent_hooks_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.108, i64 0, ptr null, ptr @arena_i_retain_grow_limit_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.30, i64 0, ptr null, ptr @arena_i_name_ctl }], align 16
@.str.104 = private unnamed_addr constant [12 x i8] c"initialized\00", align 1
@.str.105 = private unnamed_addr constant [6 x i8] c"decay\00", align 1
@.str.106 = private unnamed_addr constant [6 x i8] c"purge\00", align 1
@.str.107 = private unnamed_addr constant [13 x i8] c"extent_hooks\00", align 1
@.str.108 = private unnamed_addr constant [18 x i8] c"retain_grow_limit\00", align 1
@manual_arena_base = external local_unnamed_addr global i32, align 4
@background_thread_info = external local_unnamed_addr global ptr, align 8
@narenas_auto = external local_unnamed_addr global i32, align 4
@ehooks_default_extent_hooks = external constant %struct.extent_hooks_s, align 8
@.str.109 = private unnamed_addr constant [8 x i8] c"quantum\00", align 1
@.str.110 = private unnamed_addr constant [5 x i8] c"page\00", align 1
@.str.111 = private unnamed_addr constant [6 x i8] c"nbins\00", align 1
@.str.112 = private unnamed_addr constant [7 x i8] c"nhbins\00", align 1
@.str.113 = private unnamed_addr constant [4 x i8] c"bin\00", align 1
@arenas_bin_node = internal constant [1 x %struct.ctl_indexed_node_s] [%struct.ctl_indexed_node_s { %struct.ctl_node_s zeroinitializer, ptr @arenas_bin_i_index }], align 16
@.str.114 = private unnamed_addr constant [10 x i8] c"nlextents\00", align 1
@.str.115 = private unnamed_addr constant [8 x i8] c"lextent\00", align 1
@arenas_lextent_node = internal constant [1 x %struct.ctl_indexed_node_s] [%struct.ctl_indexed_node_s { %struct.ctl_node_s zeroinitializer, ptr @arenas_lextent_i_index }], align 16
@.str.116 = private unnamed_addr constant [7 x i8] c"lookup\00", align 1
@global_do_not_change_tcache_maxclass = external local_unnamed_addr global i64, align 8
@global_do_not_change_tcache_nbins = external local_unnamed_addr global i32, align 4
@super_arenas_bin_i_node = internal constant [1 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.1, i64 4, ptr @arenas_bin_i_node, ptr null }], align 16
@arenas_bin_i_node = internal constant [4 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.117, i64 0, ptr null, ptr @arenas_bin_i_size_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.118, i64 0, ptr null, ptr @arenas_bin_i_nregs_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.119, i64 0, ptr null, ptr @arenas_bin_i_slab_size_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.120, i64 0, ptr null, ptr @arenas_bin_i_nshards_ctl }], align 16
@.str.117 = private unnamed_addr constant [5 x i8] c"size\00", align 1
@.str.118 = private unnamed_addr constant [6 x i8] c"nregs\00", align 1
@.str.119 = private unnamed_addr constant [10 x i8] c"slab_size\00", align 1
@.str.120 = private unnamed_addr constant [8 x i8] c"nshards\00", align 1
@bin_infos = external local_unnamed_addr global [36 x %struct.bin_info_s], align 16
@super_arenas_lextent_i_node = internal constant [1 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.1, i64 1, ptr @arenas_lextent_i_node, ptr null }], align 16
@arenas_lextent_i_node = internal constant [1 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.117, i64 0, ptr null, ptr @arenas_lextent_i_size_ctl }], align 16
@arena_emap_global = external global %struct.emap_s, align 8
@.str.121 = private unnamed_addr constant [19 x i8] c"thread_active_init\00", align 1
@.str.122 = private unnamed_addr constant [5 x i8] c"dump\00", align 1
@.str.123 = private unnamed_addr constant [6 x i8] c"gdump\00", align 1
@.str.124 = private unnamed_addr constant [7 x i8] c"prefix\00", align 1
@.str.125 = private unnamed_addr constant [9 x i8] c"interval\00", align 1
@.str.126 = private unnamed_addr constant [10 x i8] c"lg_sample\00", align 1
@.str.127 = private unnamed_addr constant [10 x i8] c"log_start\00", align 1
@.str.128 = private unnamed_addr constant [9 x i8] c"log_stop\00", align 1
@prof_stats_node = internal constant [2 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.129, i64 1, ptr @prof_stats_bins_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.130, i64 1, ptr @prof_stats_lextents_node, ptr null }], align 16
@.str.129 = private unnamed_addr constant [5 x i8] c"bins\00", align 1
@prof_stats_bins_node = internal constant [1 x %struct.ctl_indexed_node_s] [%struct.ctl_indexed_node_s { %struct.ctl_node_s zeroinitializer, ptr @prof_stats_bins_i_index }], align 16
@.str.130 = private unnamed_addr constant [9 x i8] c"lextents\00", align 1
@prof_stats_lextents_node = internal constant [1 x %struct.ctl_indexed_node_s] [%struct.ctl_indexed_node_s { %struct.ctl_node_s zeroinitializer, ptr @prof_stats_lextents_i_index }], align 16
@.str.131 = private unnamed_addr constant [9 x i8] c"metadata\00", align 1
@.str.132 = private unnamed_addr constant [15 x i8] c"metadata_edata\00", align 1
@.str.133 = private unnamed_addr constant [15 x i8] c"metadata_rtree\00", align 1
@.str.134 = private unnamed_addr constant [9 x i8] c"resident\00", align 1
@.str.135 = private unnamed_addr constant [7 x i8] c"mapped\00", align 1
@.str.136 = private unnamed_addr constant [9 x i8] c"retained\00", align 1
@stats_background_thread_node = internal constant [3 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.139, i64 0, ptr null, ptr @stats_background_thread_num_threads_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.140, i64 0, ptr null, ptr @stats_background_thread_num_runs_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.141, i64 0, ptr null, ptr @stats_background_thread_run_interval_ctl }], align 16
@.str.137 = private unnamed_addr constant [8 x i8] c"mutexes\00", align 1
@stats_mutexes_node = internal constant [10 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.4, i64 7, ptr @stats_mutexes_background_thread_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.142, i64 7, ptr @stats_mutexes_max_per_bg_thd_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str, i64 7, ptr @stats_mutexes_ctl_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.12, i64 7, ptr @stats_mutexes_prof_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.143, i64 7, ptr @stats_mutexes_prof_thds_data_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.144, i64 7, ptr @stats_mutexes_prof_dump_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.145, i64 7, ptr @stats_mutexes_prof_recent_alloc_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.146, i64 7, ptr @stats_mutexes_prof_recent_dump_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.96, i64 7, ptr @stats_mutexes_prof_stats_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.29, i64 0, ptr null, ptr @stats_mutexes_reset_ctl }], align 16
@stats_arenas_node = internal constant [1 x %struct.ctl_indexed_node_s] [%struct.ctl_indexed_node_s { %struct.ctl_node_s zeroinitializer, ptr @stats_arenas_i_index }], align 16
@.str.138 = private unnamed_addr constant [14 x i8] c"zero_reallocs\00", align 1
@.str.139 = private unnamed_addr constant [12 x i8] c"num_threads\00", align 1
@.str.140 = private unnamed_addr constant [9 x i8] c"num_runs\00", align 1
@.str.141 = private unnamed_addr constant [13 x i8] c"run_interval\00", align 1
@stats_mutexes_background_thread_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.147, i64 0, ptr null, ptr @stats_mutexes_background_thread_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.148, i64 0, ptr null, ptr @stats_mutexes_background_thread_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.149, i64 0, ptr null, ptr @stats_mutexes_background_thread_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.150, i64 0, ptr null, ptr @stats_mutexes_background_thread_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.151, i64 0, ptr null, ptr @stats_mutexes_background_thread_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.152, i64 0, ptr null, ptr @stats_mutexes_background_thread_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.153, i64 0, ptr null, ptr @stats_mutexes_background_thread_max_num_thds_ctl }], align 16
@.str.142 = private unnamed_addr constant [15 x i8] c"max_per_bg_thd\00", align 1
@stats_mutexes_max_per_bg_thd_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.147, i64 0, ptr null, ptr @stats_mutexes_max_per_bg_thd_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.148, i64 0, ptr null, ptr @stats_mutexes_max_per_bg_thd_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.149, i64 0, ptr null, ptr @stats_mutexes_max_per_bg_thd_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.150, i64 0, ptr null, ptr @stats_mutexes_max_per_bg_thd_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.151, i64 0, ptr null, ptr @stats_mutexes_max_per_bg_thd_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.152, i64 0, ptr null, ptr @stats_mutexes_max_per_bg_thd_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.153, i64 0, ptr null, ptr @stats_mutexes_max_per_bg_thd_max_num_thds_ctl }], align 16
@stats_mutexes_ctl_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.147, i64 0, ptr null, ptr @stats_mutexes_ctl_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.148, i64 0, ptr null, ptr @stats_mutexes_ctl_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.149, i64 0, ptr null, ptr @stats_mutexes_ctl_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.150, i64 0, ptr null, ptr @stats_mutexes_ctl_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.151, i64 0, ptr null, ptr @stats_mutexes_ctl_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.152, i64 0, ptr null, ptr @stats_mutexes_ctl_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.153, i64 0, ptr null, ptr @stats_mutexes_ctl_max_num_thds_ctl }], align 16
@stats_mutexes_prof_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.147, i64 0, ptr null, ptr @stats_mutexes_prof_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.148, i64 0, ptr null, ptr @stats_mutexes_prof_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.149, i64 0, ptr null, ptr @stats_mutexes_prof_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.150, i64 0, ptr null, ptr @stats_mutexes_prof_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.151, i64 0, ptr null, ptr @stats_mutexes_prof_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.152, i64 0, ptr null, ptr @stats_mutexes_prof_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.153, i64 0, ptr null, ptr @stats_mutexes_prof_max_num_thds_ctl }], align 16
@.str.143 = private unnamed_addr constant [15 x i8] c"prof_thds_data\00", align 1
@stats_mutexes_prof_thds_data_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.147, i64 0, ptr null, ptr @stats_mutexes_prof_thds_data_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.148, i64 0, ptr null, ptr @stats_mutexes_prof_thds_data_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.149, i64 0, ptr null, ptr @stats_mutexes_prof_thds_data_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.150, i64 0, ptr null, ptr @stats_mutexes_prof_thds_data_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.151, i64 0, ptr null, ptr @stats_mutexes_prof_thds_data_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.152, i64 0, ptr null, ptr @stats_mutexes_prof_thds_data_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.153, i64 0, ptr null, ptr @stats_mutexes_prof_thds_data_max_num_thds_ctl }], align 16
@.str.144 = private unnamed_addr constant [10 x i8] c"prof_dump\00", align 1
@stats_mutexes_prof_dump_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.147, i64 0, ptr null, ptr @stats_mutexes_prof_dump_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.148, i64 0, ptr null, ptr @stats_mutexes_prof_dump_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.149, i64 0, ptr null, ptr @stats_mutexes_prof_dump_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.150, i64 0, ptr null, ptr @stats_mutexes_prof_dump_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.151, i64 0, ptr null, ptr @stats_mutexes_prof_dump_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.152, i64 0, ptr null, ptr @stats_mutexes_prof_dump_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.153, i64 0, ptr null, ptr @stats_mutexes_prof_dump_max_num_thds_ctl }], align 16
@.str.145 = private unnamed_addr constant [18 x i8] c"prof_recent_alloc\00", align 1
@stats_mutexes_prof_recent_alloc_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.147, i64 0, ptr null, ptr @stats_mutexes_prof_recent_alloc_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.148, i64 0, ptr null, ptr @stats_mutexes_prof_recent_alloc_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.149, i64 0, ptr null, ptr @stats_mutexes_prof_recent_alloc_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.150, i64 0, ptr null, ptr @stats_mutexes_prof_recent_alloc_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.151, i64 0, ptr null, ptr @stats_mutexes_prof_recent_alloc_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.152, i64 0, ptr null, ptr @stats_mutexes_prof_recent_alloc_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.153, i64 0, ptr null, ptr @stats_mutexes_prof_recent_alloc_max_num_thds_ctl }], align 16
@.str.146 = private unnamed_addr constant [17 x i8] c"prof_recent_dump\00", align 1
@stats_mutexes_prof_recent_dump_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.147, i64 0, ptr null, ptr @stats_mutexes_prof_recent_dump_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.148, i64 0, ptr null, ptr @stats_mutexes_prof_recent_dump_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.149, i64 0, ptr null, ptr @stats_mutexes_prof_recent_dump_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.150, i64 0, ptr null, ptr @stats_mutexes_prof_recent_dump_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.151, i64 0, ptr null, ptr @stats_mutexes_prof_recent_dump_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.152, i64 0, ptr null, ptr @stats_mutexes_prof_recent_dump_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.153, i64 0, ptr null, ptr @stats_mutexes_prof_recent_dump_max_num_thds_ctl }], align 16
@stats_mutexes_prof_stats_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.147, i64 0, ptr null, ptr @stats_mutexes_prof_stats_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.148, i64 0, ptr null, ptr @stats_mutexes_prof_stats_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.149, i64 0, ptr null, ptr @stats_mutexes_prof_stats_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.150, i64 0, ptr null, ptr @stats_mutexes_prof_stats_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.151, i64 0, ptr null, ptr @stats_mutexes_prof_stats_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.152, i64 0, ptr null, ptr @stats_mutexes_prof_stats_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.153, i64 0, ptr null, ptr @stats_mutexes_prof_stats_max_num_thds_ctl }], align 16
@.str.147 = private unnamed_addr constant [8 x i8] c"num_ops\00", align 1
@.str.148 = private unnamed_addr constant [9 x i8] c"num_wait\00", align 1
@.str.149 = private unnamed_addr constant [13 x i8] c"num_spin_acq\00", align 1
@.str.150 = private unnamed_addr constant [17 x i8] c"num_owner_switch\00", align 1
@.str.151 = private unnamed_addr constant [16 x i8] c"total_wait_time\00", align 1
@.str.152 = private unnamed_addr constant [14 x i8] c"max_wait_time\00", align 1
@.str.153 = private unnamed_addr constant [13 x i8] c"max_num_thds\00", align 1
@arena_bin_offsets = external local_unnamed_addr global [36 x i32], align 16
@super_stats_arenas_i_node = internal constant [1 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.1, i64 34, ptr @stats_arenas_i_node, ptr null }], align 16
@stats_arenas_i_node = internal constant [34 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.154, i64 0, ptr null, ptr @stats_arenas_i_nthreads_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.155, i64 0, ptr null, ptr @stats_arenas_i_uptime_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.59, i64 0, ptr null, ptr @stats_arenas_i_dss_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.64, i64 0, ptr null, ptr @stats_arenas_i_dirty_decay_ms_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.65, i64 0, ptr null, ptr @stats_arenas_i_muzzy_decay_ms_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.156, i64 0, ptr null, ptr @stats_arenas_i_pactive_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.157, i64 0, ptr null, ptr @stats_arenas_i_pdirty_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.158, i64 0, ptr null, ptr @stats_arenas_i_pmuzzy_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.135, i64 0, ptr null, ptr @stats_arenas_i_mapped_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.136, i64 0, ptr null, ptr @stats_arenas_i_retained_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.159, i64 0, ptr null, ptr @stats_arenas_i_extent_avail_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.160, i64 0, ptr null, ptr @stats_arenas_i_dirty_npurge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.161, i64 0, ptr null, ptr @stats_arenas_i_dirty_nmadvise_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.162, i64 0, ptr null, ptr @stats_arenas_i_dirty_purged_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.163, i64 0, ptr null, ptr @stats_arenas_i_muzzy_npurge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.164, i64 0, ptr null, ptr @stats_arenas_i_muzzy_nmadvise_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.165, i64 0, ptr null, ptr @stats_arenas_i_muzzy_purged_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.166, i64 0, ptr null, ptr @stats_arenas_i_base_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.167, i64 0, ptr null, ptr @stats_arenas_i_internal_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.132, i64 0, ptr null, ptr @stats_arenas_i_metadata_edata_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.133, i64 0, ptr null, ptr @stats_arenas_i_metadata_rtree_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.57, i64 0, ptr null, ptr @stats_arenas_i_metadata_thp_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.168, i64 0, ptr null, ptr @stats_arenas_i_tcache_bytes_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.169, i64 0, ptr null, ptr @stats_arenas_i_tcache_stashed_bytes_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.134, i64 0, ptr null, ptr @stats_arenas_i_resident_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.170, i64 0, ptr null, ptr @stats_arenas_i_abandoned_vm_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.171, i64 0, ptr null, ptr @stats_arenas_i_hpa_sec_bytes_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.172, i64 6, ptr @stats_arenas_i_small_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.173, i64 6, ptr @stats_arenas_i_large_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.129, i64 1, ptr @stats_arenas_i_bins_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.130, i64 1, ptr @stats_arenas_i_lextents_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.174, i64 1, ptr @stats_arenas_i_extents_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.137, i64 12, ptr @stats_arenas_i_mutexes_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.175, i64 7, ptr @stats_arenas_i_hpa_shard_node, ptr null }], align 16
@.str.154 = private unnamed_addr constant [9 x i8] c"nthreads\00", align 1
@.str.155 = private unnamed_addr constant [7 x i8] c"uptime\00", align 1
@.str.156 = private unnamed_addr constant [8 x i8] c"pactive\00", align 1
@.str.157 = private unnamed_addr constant [7 x i8] c"pdirty\00", align 1
@.str.158 = private unnamed_addr constant [7 x i8] c"pmuzzy\00", align 1
@.str.159 = private unnamed_addr constant [13 x i8] c"extent_avail\00", align 1
@.str.160 = private unnamed_addr constant [13 x i8] c"dirty_npurge\00", align 1
@.str.161 = private unnamed_addr constant [15 x i8] c"dirty_nmadvise\00", align 1
@.str.162 = private unnamed_addr constant [13 x i8] c"dirty_purged\00", align 1
@.str.163 = private unnamed_addr constant [13 x i8] c"muzzy_npurge\00", align 1
@.str.164 = private unnamed_addr constant [15 x i8] c"muzzy_nmadvise\00", align 1
@.str.165 = private unnamed_addr constant [13 x i8] c"muzzy_purged\00", align 1
@.str.166 = private unnamed_addr constant [5 x i8] c"base\00", align 1
@.str.167 = private unnamed_addr constant [9 x i8] c"internal\00", align 1
@.str.168 = private unnamed_addr constant [13 x i8] c"tcache_bytes\00", align 1
@.str.169 = private unnamed_addr constant [21 x i8] c"tcache_stashed_bytes\00", align 1
@.str.170 = private unnamed_addr constant [13 x i8] c"abandoned_vm\00", align 1
@.str.171 = private unnamed_addr constant [14 x i8] c"hpa_sec_bytes\00", align 1
@.str.172 = private unnamed_addr constant [6 x i8] c"small\00", align 1
@stats_arenas_i_small_node = internal constant [6 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.16, i64 0, ptr null, ptr @stats_arenas_i_small_allocated_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.176, i64 0, ptr null, ptr @stats_arenas_i_small_nmalloc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.177, i64 0, ptr null, ptr @stats_arenas_i_small_ndalloc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.178, i64 0, ptr null, ptr @stats_arenas_i_small_nrequests_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.179, i64 0, ptr null, ptr @stats_arenas_i_small_nfills_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.180, i64 0, ptr null, ptr @stats_arenas_i_small_nflushes_ctl }], align 16
@.str.173 = private unnamed_addr constant [6 x i8] c"large\00", align 1
@stats_arenas_i_large_node = internal constant [6 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.16, i64 0, ptr null, ptr @stats_arenas_i_large_allocated_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.176, i64 0, ptr null, ptr @stats_arenas_i_large_nmalloc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.177, i64 0, ptr null, ptr @stats_arenas_i_large_ndalloc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.178, i64 0, ptr null, ptr @stats_arenas_i_large_nrequests_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.179, i64 0, ptr null, ptr @stats_arenas_i_large_nfills_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.180, i64 0, ptr null, ptr @stats_arenas_i_large_nflushes_ctl }], align 16
@stats_arenas_i_bins_node = internal constant [1 x %struct.ctl_indexed_node_s] [%struct.ctl_indexed_node_s { %struct.ctl_node_s zeroinitializer, ptr @stats_arenas_i_bins_j_index }], align 16
@stats_arenas_i_lextents_node = internal constant [1 x %struct.ctl_indexed_node_s] [%struct.ctl_indexed_node_s { %struct.ctl_node_s zeroinitializer, ptr @stats_arenas_i_lextents_j_index }], align 16
@.str.174 = private unnamed_addr constant [8 x i8] c"extents\00", align 1
@stats_arenas_i_extents_node = internal constant [1 x %struct.ctl_indexed_node_s] [%struct.ctl_indexed_node_s { %struct.ctl_node_s zeroinitializer, ptr @stats_arenas_i_extents_j_index }], align 16
@stats_arenas_i_mutexes_node = internal constant [12 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.173, i64 7, ptr @stats_arenas_i_mutexes_large_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.159, i64 7, ptr @stats_arenas_i_mutexes_extent_avail_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.194, i64 7, ptr @stats_arenas_i_mutexes_extents_dirty_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.195, i64 7, ptr @stats_arenas_i_mutexes_extents_muzzy_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.196, i64 7, ptr @stats_arenas_i_mutexes_extents_retained_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.197, i64 7, ptr @stats_arenas_i_mutexes_decay_dirty_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.198, i64 7, ptr @stats_arenas_i_mutexes_decay_muzzy_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.166, i64 7, ptr @stats_arenas_i_mutexes_base_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.199, i64 7, ptr @stats_arenas_i_mutexes_tcache_list_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.175, i64 7, ptr @stats_arenas_i_mutexes_hpa_shard_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.200, i64 7, ptr @stats_arenas_i_mutexes_hpa_shard_grow_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.201, i64 7, ptr @stats_arenas_i_mutexes_hpa_sec_node, ptr null }], align 16
@.str.175 = private unnamed_addr constant [10 x i8] c"hpa_shard\00", align 1
@stats_arenas_i_hpa_shard_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.202, i64 6, ptr @stats_arenas_i_hpa_shard_full_slabs_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.203, i64 6, ptr @stats_arenas_i_hpa_shard_empty_slabs_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.185, i64 1, ptr @stats_arenas_i_hpa_shard_nonfull_slabs_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.204, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_npurge_passes_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.205, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_npurges_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.206, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_nhugifies_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.207, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_ndehugifies_ctl }], align 16
@.str.176 = private unnamed_addr constant [8 x i8] c"nmalloc\00", align 1
@.str.177 = private unnamed_addr constant [8 x i8] c"ndalloc\00", align 1
@.str.178 = private unnamed_addr constant [10 x i8] c"nrequests\00", align 1
@.str.179 = private unnamed_addr constant [7 x i8] c"nfills\00", align 1
@.str.180 = private unnamed_addr constant [9 x i8] c"nflushes\00", align 1
@super_stats_arenas_i_bins_j_node = internal constant [1 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.1, i64 11, ptr @stats_arenas_i_bins_j_node, ptr null }], align 16
@stats_arenas_i_bins_j_node = internal constant [11 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.176, i64 0, ptr null, ptr @stats_arenas_i_bins_j_nmalloc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.177, i64 0, ptr null, ptr @stats_arenas_i_bins_j_ndalloc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.178, i64 0, ptr null, ptr @stats_arenas_i_bins_j_nrequests_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.181, i64 0, ptr null, ptr @stats_arenas_i_bins_j_curregs_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.179, i64 0, ptr null, ptr @stats_arenas_i_bins_j_nfills_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.180, i64 0, ptr null, ptr @stats_arenas_i_bins_j_nflushes_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.182, i64 0, ptr null, ptr @stats_arenas_i_bins_j_nslabs_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.183, i64 0, ptr null, ptr @stats_arenas_i_bins_j_nreslabs_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.184, i64 0, ptr null, ptr @stats_arenas_i_bins_j_curslabs_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.185, i64 0, ptr null, ptr @stats_arenas_i_bins_j_nonfull_slabs_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.186, i64 7, ptr @stats_arenas_i_bins_j_mutex_node, ptr null }], align 16
@.str.181 = private unnamed_addr constant [8 x i8] c"curregs\00", align 1
@.str.182 = private unnamed_addr constant [7 x i8] c"nslabs\00", align 1
@.str.183 = private unnamed_addr constant [9 x i8] c"nreslabs\00", align 1
@.str.184 = private unnamed_addr constant [9 x i8] c"curslabs\00", align 1
@.str.185 = private unnamed_addr constant [14 x i8] c"nonfull_slabs\00", align 1
@.str.186 = private unnamed_addr constant [6 x i8] c"mutex\00", align 1
@stats_arenas_i_bins_j_mutex_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.147, i64 0, ptr null, ptr @stats_arenas_i_bins_j_mutex_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.148, i64 0, ptr null, ptr @stats_arenas_i_bins_j_mutex_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.149, i64 0, ptr null, ptr @stats_arenas_i_bins_j_mutex_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.150, i64 0, ptr null, ptr @stats_arenas_i_bins_j_mutex_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.151, i64 0, ptr null, ptr @stats_arenas_i_bins_j_mutex_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.152, i64 0, ptr null, ptr @stats_arenas_i_bins_j_mutex_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.153, i64 0, ptr null, ptr @stats_arenas_i_bins_j_mutex_max_num_thds_ctl }], align 16
@super_stats_arenas_i_lextents_j_node = internal constant [1 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.1, i64 4, ptr @stats_arenas_i_lextents_j_node, ptr null }], align 16
@stats_arenas_i_lextents_j_node = internal constant [4 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.176, i64 0, ptr null, ptr @stats_arenas_i_lextents_j_nmalloc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.177, i64 0, ptr null, ptr @stats_arenas_i_lextents_j_ndalloc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.178, i64 0, ptr null, ptr @stats_arenas_i_lextents_j_nrequests_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.187, i64 0, ptr null, ptr @stats_arenas_i_lextents_j_curlextents_ctl }], align 16
@.str.187 = private unnamed_addr constant [12 x i8] c"curlextents\00", align 1
@super_stats_arenas_i_extents_j_node = internal constant [1 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.1, i64 6, ptr @stats_arenas_i_extents_j_node, ptr null }], align 16
@stats_arenas_i_extents_j_node = internal constant [6 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.188, i64 0, ptr null, ptr @stats_arenas_i_extents_j_ndirty_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.189, i64 0, ptr null, ptr @stats_arenas_i_extents_j_nmuzzy_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.190, i64 0, ptr null, ptr @stats_arenas_i_extents_j_nretained_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.191, i64 0, ptr null, ptr @stats_arenas_i_extents_j_dirty_bytes_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.192, i64 0, ptr null, ptr @stats_arenas_i_extents_j_muzzy_bytes_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.193, i64 0, ptr null, ptr @stats_arenas_i_extents_j_retained_bytes_ctl }], align 16
@.str.188 = private unnamed_addr constant [7 x i8] c"ndirty\00", align 1
@.str.189 = private unnamed_addr constant [7 x i8] c"nmuzzy\00", align 1
@.str.190 = private unnamed_addr constant [10 x i8] c"nretained\00", align 1
@.str.191 = private unnamed_addr constant [12 x i8] c"dirty_bytes\00", align 1
@.str.192 = private unnamed_addr constant [12 x i8] c"muzzy_bytes\00", align 1
@.str.193 = private unnamed_addr constant [15 x i8] c"retained_bytes\00", align 1
@stats_arenas_i_mutexes_large_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.147, i64 0, ptr null, ptr @stats_arenas_i_mutexes_large_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.148, i64 0, ptr null, ptr @stats_arenas_i_mutexes_large_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.149, i64 0, ptr null, ptr @stats_arenas_i_mutexes_large_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.150, i64 0, ptr null, ptr @stats_arenas_i_mutexes_large_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.151, i64 0, ptr null, ptr @stats_arenas_i_mutexes_large_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.152, i64 0, ptr null, ptr @stats_arenas_i_mutexes_large_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.153, i64 0, ptr null, ptr @stats_arenas_i_mutexes_large_max_num_thds_ctl }], align 16
@stats_arenas_i_mutexes_extent_avail_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.147, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extent_avail_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.148, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extent_avail_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.149, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extent_avail_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.150, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extent_avail_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.151, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extent_avail_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.152, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extent_avail_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.153, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extent_avail_max_num_thds_ctl }], align 16
@.str.194 = private unnamed_addr constant [14 x i8] c"extents_dirty\00", align 1
@stats_arenas_i_mutexes_extents_dirty_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.147, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_dirty_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.148, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_dirty_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.149, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_dirty_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.150, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_dirty_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.151, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_dirty_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.152, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_dirty_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.153, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_dirty_max_num_thds_ctl }], align 16
@.str.195 = private unnamed_addr constant [14 x i8] c"extents_muzzy\00", align 1
@stats_arenas_i_mutexes_extents_muzzy_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.147, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_muzzy_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.148, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_muzzy_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.149, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_muzzy_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.150, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_muzzy_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.151, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_muzzy_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.152, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_muzzy_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.153, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_muzzy_max_num_thds_ctl }], align 16
@.str.196 = private unnamed_addr constant [17 x i8] c"extents_retained\00", align 1
@stats_arenas_i_mutexes_extents_retained_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.147, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_retained_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.148, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_retained_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.149, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_retained_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.150, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_retained_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.151, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_retained_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.152, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_retained_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.153, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_retained_max_num_thds_ctl }], align 16
@.str.197 = private unnamed_addr constant [12 x i8] c"decay_dirty\00", align 1
@stats_arenas_i_mutexes_decay_dirty_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.147, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_dirty_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.148, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_dirty_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.149, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_dirty_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.150, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_dirty_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.151, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_dirty_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.152, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_dirty_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.153, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_dirty_max_num_thds_ctl }], align 16
@.str.198 = private unnamed_addr constant [12 x i8] c"decay_muzzy\00", align 1
@stats_arenas_i_mutexes_decay_muzzy_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.147, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_muzzy_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.148, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_muzzy_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.149, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_muzzy_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.150, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_muzzy_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.151, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_muzzy_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.152, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_muzzy_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.153, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_muzzy_max_num_thds_ctl }], align 16
@stats_arenas_i_mutexes_base_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.147, i64 0, ptr null, ptr @stats_arenas_i_mutexes_base_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.148, i64 0, ptr null, ptr @stats_arenas_i_mutexes_base_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.149, i64 0, ptr null, ptr @stats_arenas_i_mutexes_base_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.150, i64 0, ptr null, ptr @stats_arenas_i_mutexes_base_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.151, i64 0, ptr null, ptr @stats_arenas_i_mutexes_base_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.152, i64 0, ptr null, ptr @stats_arenas_i_mutexes_base_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.153, i64 0, ptr null, ptr @stats_arenas_i_mutexes_base_max_num_thds_ctl }], align 16
@.str.199 = private unnamed_addr constant [12 x i8] c"tcache_list\00", align 1
@stats_arenas_i_mutexes_tcache_list_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.147, i64 0, ptr null, ptr @stats_arenas_i_mutexes_tcache_list_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.148, i64 0, ptr null, ptr @stats_arenas_i_mutexes_tcache_list_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.149, i64 0, ptr null, ptr @stats_arenas_i_mutexes_tcache_list_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.150, i64 0, ptr null, ptr @stats_arenas_i_mutexes_tcache_list_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.151, i64 0, ptr null, ptr @stats_arenas_i_mutexes_tcache_list_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.152, i64 0, ptr null, ptr @stats_arenas_i_mutexes_tcache_list_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.153, i64 0, ptr null, ptr @stats_arenas_i_mutexes_tcache_list_max_num_thds_ctl }], align 16
@stats_arenas_i_mutexes_hpa_shard_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.147, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.148, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.149, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.150, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.151, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.152, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.153, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_max_num_thds_ctl }], align 16
@.str.200 = private unnamed_addr constant [15 x i8] c"hpa_shard_grow\00", align 1
@stats_arenas_i_mutexes_hpa_shard_grow_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.147, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_grow_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.148, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_grow_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.149, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_grow_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.150, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_grow_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.151, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_grow_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.152, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_grow_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.153, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_grow_max_num_thds_ctl }], align 16
@.str.201 = private unnamed_addr constant [8 x i8] c"hpa_sec\00", align 1
@stats_arenas_i_mutexes_hpa_sec_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.147, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_sec_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.148, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_sec_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.149, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_sec_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.150, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_sec_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.151, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_sec_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.152, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_sec_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.153, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_sec_max_num_thds_ctl }], align 16
@.str.202 = private unnamed_addr constant [11 x i8] c"full_slabs\00", align 1
@stats_arenas_i_hpa_shard_full_slabs_node = internal constant [6 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.208, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_full_slabs_npageslabs_nonhuge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.209, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_full_slabs_npageslabs_huge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.210, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_full_slabs_nactive_nonhuge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.211, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_full_slabs_nactive_huge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.212, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_full_slabs_ndirty_nonhuge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.213, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_full_slabs_ndirty_huge_ctl }], align 16
@.str.203 = private unnamed_addr constant [12 x i8] c"empty_slabs\00", align 1
@stats_arenas_i_hpa_shard_empty_slabs_node = internal constant [6 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.208, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_empty_slabs_npageslabs_nonhuge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.209, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_empty_slabs_npageslabs_huge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.210, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_empty_slabs_nactive_nonhuge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.211, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_empty_slabs_nactive_huge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.212, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_empty_slabs_ndirty_nonhuge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.213, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_empty_slabs_ndirty_huge_ctl }], align 16
@stats_arenas_i_hpa_shard_nonfull_slabs_node = internal constant [1 x %struct.ctl_indexed_node_s] [%struct.ctl_indexed_node_s { %struct.ctl_node_s zeroinitializer, ptr @stats_arenas_i_hpa_shard_nonfull_slabs_j_index }], align 16
@.str.204 = private unnamed_addr constant [14 x i8] c"npurge_passes\00", align 1
@.str.205 = private unnamed_addr constant [8 x i8] c"npurges\00", align 1
@.str.206 = private unnamed_addr constant [10 x i8] c"nhugifies\00", align 1
@.str.207 = private unnamed_addr constant [12 x i8] c"ndehugifies\00", align 1
@.str.208 = private unnamed_addr constant [19 x i8] c"npageslabs_nonhuge\00", align 1
@.str.209 = private unnamed_addr constant [16 x i8] c"npageslabs_huge\00", align 1
@.str.210 = private unnamed_addr constant [16 x i8] c"nactive_nonhuge\00", align 1
@.str.211 = private unnamed_addr constant [13 x i8] c"nactive_huge\00", align 1
@.str.212 = private unnamed_addr constant [15 x i8] c"ndirty_nonhuge\00", align 1
@.str.213 = private unnamed_addr constant [12 x i8] c"ndirty_huge\00", align 1
@super_stats_arenas_i_hpa_shard_nonfull_slabs_j_node = internal constant [1 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.1, i64 6, ptr @stats_arenas_i_hpa_shard_nonfull_slabs_j_node, ptr null }], align 16
@stats_arenas_i_hpa_shard_nonfull_slabs_j_node = internal constant [6 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.208, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_nonfull_slabs_j_npageslabs_nonhuge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.209, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_nonfull_slabs_j_npageslabs_huge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.210, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_nonfull_slabs_j_nactive_nonhuge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.211, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_nonfull_slabs_j_nactive_huge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.212, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_nonfull_slabs_j_ndirty_nonhuge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.213, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_nonfull_slabs_j_ndirty_huge_ctl }], align 16
@zero_realloc_count = external local_unnamed_addr global %struct.atomic_zu_t, align 8
@.str.214 = private unnamed_addr constant [6 x i8] c"hooks\00", align 1
@experimental_hooks_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.219, i64 0, ptr null, ptr @experimental_hooks_install_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.220, i64 0, ptr null, ptr @experimental_hooks_remove_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.221, i64 0, ptr null, ptr @experimental_hooks_prof_backtrace_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.144, i64 0, ptr null, ptr @experimental_hooks_prof_dump_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.222, i64 0, ptr null, ptr @experimental_hooks_prof_sample_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.223, i64 0, ptr null, ptr @experimental_hooks_prof_sample_free_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.224, i64 0, ptr null, ptr @experimental_hooks_safety_check_abort_ctl }], align 16
@.str.215 = private unnamed_addr constant [12 x i8] c"utilization\00", align 1
@experimental_utilization_node = internal constant [2 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.225, i64 0, ptr null, ptr @experimental_utilization_query_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.226, i64 0, ptr null, ptr @experimental_utilization_batch_query_ctl }], align 16
@experimental_arenas_node = internal constant [1 x %struct.ctl_indexed_node_s] [%struct.ctl_indexed_node_s { %struct.ctl_node_s zeroinitializer, ptr @experimental_arenas_i_index }], align 16
@.str.216 = private unnamed_addr constant [18 x i8] c"arenas_create_ext\00", align 1
@.str.217 = private unnamed_addr constant [12 x i8] c"prof_recent\00", align 1
@experimental_prof_recent_node = internal constant [2 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.228, i64 0, ptr null, ptr @experimental_prof_recent_alloc_max_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.229, i64 0, ptr null, ptr @experimental_prof_recent_alloc_dump_ctl }], align 16
@.str.218 = private unnamed_addr constant [12 x i8] c"batch_alloc\00", align 1
@experimental_thread_node = internal constant [1 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.230, i64 0, ptr null, ptr @experimental_thread_activity_callback_ctl }], align 16
@.str.219 = private unnamed_addr constant [8 x i8] c"install\00", align 1
@.str.220 = private unnamed_addr constant [7 x i8] c"remove\00", align 1
@.str.221 = private unnamed_addr constant [15 x i8] c"prof_backtrace\00", align 1
@.str.222 = private unnamed_addr constant [12 x i8] c"prof_sample\00", align 1
@.str.223 = private unnamed_addr constant [17 x i8] c"prof_sample_free\00", align 1
@.str.224 = private unnamed_addr constant [19 x i8] c"safety_check_abort\00", align 1
@opt_prof = external local_unnamed_addr global i8, align 1
@.str.225 = private unnamed_addr constant [6 x i8] c"query\00", align 1
@.str.226 = private unnamed_addr constant [12 x i8] c"batch_query\00", align 1
@super_experimental_arenas_i_node = internal constant [1 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.1, i64 1, ptr @experimental_arenas_i_node, ptr null }], align 16
@experimental_arenas_i_node = internal constant [1 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.227, i64 0, ptr null, ptr @experimental_arenas_i_pactivep_ctl }], align 16
@.str.227 = private unnamed_addr constant [9 x i8] c"pactivep\00", align 1
@.str.228 = private unnamed_addr constant [10 x i8] c"alloc_max\00", align 1
@.str.229 = private unnamed_addr constant [11 x i8] c"alloc_dump\00", align 1
@.str.230 = private unnamed_addr constant [18 x i8] c"activity_callback\00", align 1

; Function Attrs: nounwind uwtable
define hidden i32 @ctl_byname(ptr noundef %tsd, ptr noundef %name, ptr noundef %oldp, ptr noundef %oldlenp, ptr noundef %newp, i64 noundef %newlen) local_unnamed_addr #0 {
entry:
  %depth = alloca i64, align 8
  %mib = alloca [7 x i64], align 16
  %node = alloca ptr, align 8
  %.b5 = load i1, ptr @ctl_initialized, align 1
  br i1 %.b5, label %if.end, label %land.lhs.true

land.lhs.true:                                    ; preds = %entry
  %call = tail call fastcc zeroext i1 @ctl_init(ptr noundef %tsd)
  br i1 %call, label %label_return, label %if.end

if.end:                                           ; preds = %land.lhs.true, %entry
  store i64 7, ptr %depth, align 8
  %call2 = call fastcc i32 @ctl_lookup(ptr noundef %tsd, ptr noundef nonnull @super_root_node, ptr noundef %name, ptr noundef nonnull %node, ptr noundef nonnull %mib, ptr noundef nonnull %depth), !range !5
  %cmp.not = icmp eq i32 %call2, 0
  br i1 %cmp.not, label %if.end4, label %label_return

if.end4:                                          ; preds = %if.end
  %0 = load ptr, ptr %node, align 8
  %cmp5.not = icmp eq ptr %0, null
  br i1 %cmp5.not, label %label_return, label %land.lhs.true6

land.lhs.true6:                                   ; preds = %if.end4
  %ctl = getelementptr inbounds i8, ptr %0, i64 32
  %1 = load ptr, ptr %ctl, align 8
  %tobool7.not = icmp eq ptr %1, null
  br i1 %tobool7.not, label %label_return, label %if.then8

if.then8:                                         ; preds = %land.lhs.true6
  %2 = load i64, ptr %depth, align 8
  %call11 = call i32 %1(ptr noundef %tsd, ptr noundef nonnull %mib, i64 noundef %2, ptr noundef %oldp, ptr noundef %oldlenp, ptr noundef %newp, i64 noundef %newlen) #14
  br label %label_return

label_return:                                     ; preds = %if.end4, %land.lhs.true6, %land.lhs.true, %if.then8, %if.end
  %ret.0 = phi i32 [ %call2, %if.end ], [ %call11, %if.then8 ], [ 11, %land.lhs.true ], [ 2, %land.lhs.true6 ], [ 2, %if.end4 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal fastcc zeroext i1 @ctl_init(ptr noundef %tsd) unnamed_addr #0 {
entry:
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %.b10 = load i1, ptr @ctl_initialized, align 1
  br i1 %.b10, label %label_return, label %if.then

if.then:                                          ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_arenas, align 8
  %cmp = icmp eq ptr %3, null
  br i1 %cmp, label %if.then1, label %if.end6

if.then1:                                         ; preds = %if.then
  %call2 = tail call ptr @b0get() #14
  %call3 = tail call ptr @base_alloc(ptr noundef %tsd, ptr noundef %call2, i64 noundef 32800, i64 noundef 16) #14
  store ptr %call3, ptr @ctl_arenas, align 8
  %cmp4 = icmp eq ptr %call3, null
  br i1 %cmp4, label %label_return, label %if.end6

if.end6:                                          ; preds = %if.then1, %if.then
  %4 = phi ptr [ %call3, %if.then1 ], [ %3, %if.then ]
  %5 = load ptr, ptr @ctl_stats, align 8
  %cmp7 = icmp eq ptr %5, null
  br i1 %cmp7, label %if.then8, label %if.end14

if.then8:                                         ; preds = %if.end6
  %call9 = tail call ptr @b0get() #14
  %call10 = tail call ptr @base_alloc(ptr noundef %tsd, ptr noundef %call9, i64 noundef 736, i64 noundef 16) #14
  store ptr %call10, ptr @ctl_stats, align 8
  %cmp11 = icmp eq ptr %call10, null
  br i1 %cmp11, label %label_return, label %if.then8.if.end14_crit_edge

if.then8.if.end14_crit_edge:                      ; preds = %if.then8
  %.pre = load ptr, ptr @ctl_arenas, align 8
  br label %if.end14

if.end14:                                         ; preds = %if.then8.if.end14_crit_edge, %if.end6
  %6 = phi ptr [ %.pre, %if.then8.if.end14_crit_edge ], [ %4, %if.end6 ]
  %arenas.i = getelementptr inbounds i8, ptr %6, i64 24
  %7 = load ptr, ptr %arenas.i, align 8
  %cmp.i = icmp eq ptr %7, null
  br i1 %cmp.i, label %if.then.i11, label %if.end18

if.then.i11:                                      ; preds = %if.end14
  %call4.i = tail call ptr @b0get() #14
  %call5.i = tail call ptr @base_alloc(ptr noundef %tsd, ptr noundef %call4.i, i64 noundef 37872, i64 noundef 16) #14
  %cmp6.i = icmp eq ptr %call5.i, null
  br i1 %cmp6.i, label %label_return, label %if.end.i12

if.end.i12:                                       ; preds = %if.then.i11
  %astats.i = getelementptr inbounds i8, ptr %call5.i, i64 88
  %astats8.i = getelementptr inbounds i8, ptr %call5.i, i64 80
  store ptr %astats.i, ptr %astats8.i, align 8
  store i32 4096, ptr %call5.i, align 8
  %8 = load ptr, ptr @ctl_arenas, align 8
  %arenas9.i = getelementptr inbounds i8, ptr %8, i64 24
  store ptr %call5.i, ptr %arenas9.i, align 8
  br label %if.end18

if.end18:                                         ; preds = %if.end.i12, %if.end14
  %retval.0.i.ph = phi ptr [ %7, %if.end14 ], [ %call5.i, %if.end.i12 ]
  %initialized = getelementptr inbounds i8, ptr %retval.0.i.ph, i64 4
  store i8 1, ptr %initialized, align 4
  %9 = load ptr, ptr @ctl_arenas, align 8
  %arrayidx.i = getelementptr inbounds i8, ptr %9, i64 32
  %10 = load ptr, ptr %arrayidx.i, align 8
  %cmp.i14 = icmp eq ptr %10, null
  br i1 %cmp.i14, label %if.then.i16, label %if.end22

if.then.i16:                                      ; preds = %if.end18
  %call4.i17 = tail call ptr @b0get() #14
  %call5.i18 = tail call ptr @base_alloc(ptr noundef %tsd, ptr noundef %call4.i17, i64 noundef 37872, i64 noundef 16) #14
  %cmp6.i19 = icmp eq ptr %call5.i18, null
  br i1 %cmp6.i19, label %label_return, label %if.end.i20

if.end.i20:                                       ; preds = %if.then.i16
  %astats.i21 = getelementptr inbounds i8, ptr %call5.i18, i64 88
  %astats8.i22 = getelementptr inbounds i8, ptr %call5.i18, i64 80
  store ptr %astats.i21, ptr %astats8.i22, align 8
  store i32 4097, ptr %call5.i18, align 8
  %11 = load ptr, ptr @ctl_arenas, align 8
  %arrayidx13.i = getelementptr inbounds i8, ptr %11, i64 32
  store ptr %call5.i18, ptr %arrayidx13.i, align 8
  br label %if.end22

if.end22:                                         ; preds = %if.end.i20, %if.end18
  %retval.0.i15.ph = phi ptr [ %10, %if.end18 ], [ %call5.i18, %if.end.i20 ]
  %nthreads.i = getelementptr inbounds i8, ptr %retval.0.i15.ph, i64 24
  store i32 0, ptr %nthreads.i, align 8
  %12 = load ptr, ptr getelementptr inbounds ([0 x ptr], ptr @dss_prec_names, i64 0, i64 3), align 8
  %dss.i = getelementptr inbounds i8, ptr %retval.0.i15.ph, i64 32
  store ptr %12, ptr %dss.i, align 8
  %dirty_decay_ms.i = getelementptr inbounds i8, ptr %retval.0.i15.ph, i64 40
  %pactive.i = getelementptr inbounds i8, ptr %retval.0.i15.ph, i64 56
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) %dirty_decay_ms.i, i8 -1, i64 16, i1 false)
  %astats.i25 = getelementptr inbounds i8, ptr %retval.0.i15.ph, i64 80
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) %pactive.i, i8 0, i64 24, i1 false)
  %13 = load ptr, ptr %astats.i25, align 8
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(37784) %13, i8 0, i64 37784, i1 false)
  %call23 = tail call i32 @narenas_total_get() #14
  %14 = load ptr, ptr @ctl_arenas, align 8
  %narenas = getelementptr inbounds i8, ptr %14, i64 8
  store i32 %call23, ptr %narenas, align 8
  %cmp2550.not = icmp eq i32 %call23, 0
  br i1 %cmp2550.not, label %do.body, label %for.body

for.body:                                         ; preds = %if.end22, %for.inc
  %15 = phi ptr [ %21, %for.inc ], [ %14, %if.end22 ]
  %indvars.iv = phi i64 [ %indvars.iv.next, %for.inc ], [ 0, %if.end22 ]
  %arenas.i26 = getelementptr inbounds i8, ptr %15, i64 24
  %16 = trunc i64 %indvars.iv to i32
  switch i32 %16, label %sw.default.i.i [
    i32 4096, label %arenas_i2a_impl.exit.i
    i32 4097, label %sw.bb2.i.i
  ]

sw.bb2.i.i:                                       ; preds = %for.body
  br label %arenas_i2a_impl.exit.i

sw.default.i.i:                                   ; preds = %for.body
  %add.i.i = add nuw nsw i64 %indvars.iv, 2
  %17 = and i64 %add.i.i, 4294967295
  br label %arenas_i2a_impl.exit.i

arenas_i2a_impl.exit.i:                           ; preds = %sw.default.i.i, %sw.bb2.i.i, %for.body
  %a.0.i.i = phi i64 [ %17, %sw.default.i.i ], [ 1, %sw.bb2.i.i ], [ 0, %for.body ]
  %arrayidx.i27 = getelementptr inbounds [4097 x ptr], ptr %arenas.i26, i64 0, i64 %a.0.i.i
  %18 = load ptr, ptr %arrayidx.i27, align 8
  %cmp.i28 = icmp eq ptr %18, null
  br i1 %cmp.i28, label %if.then.i30, label %for.inc

if.then.i30:                                      ; preds = %arenas_i2a_impl.exit.i
  %call4.i31 = tail call ptr @b0get() #14
  %call5.i32 = tail call ptr @base_alloc(ptr noundef %tsd, ptr noundef %call4.i31, i64 noundef 37872, i64 noundef 16) #14
  %cmp6.i33 = icmp eq ptr %call5.i32, null
  br i1 %cmp6.i33, label %label_return, label %if.end.i34

if.end.i34:                                       ; preds = %if.then.i30
  %astats.i35 = getelementptr inbounds i8, ptr %call5.i32, i64 88
  %astats8.i36 = getelementptr inbounds i8, ptr %call5.i32, i64 80
  store ptr %astats.i35, ptr %astats8.i36, align 8
  store i32 %16, ptr %call5.i32, align 8
  %19 = load ptr, ptr @ctl_arenas, align 8
  %arenas9.i37 = getelementptr inbounds i8, ptr %19, i64 24
  switch i32 %16, label %sw.default.i12.i [
    i32 4096, label %arenas_i2a_impl.exit20.i
    i32 4097, label %sw.bb2.i10.i
  ]

sw.bb2.i10.i:                                     ; preds = %if.end.i34
  br label %arenas_i2a_impl.exit20.i

sw.default.i12.i:                                 ; preds = %if.end.i34
  %add.i15.i = add nuw nsw i64 %indvars.iv, 2
  %20 = and i64 %add.i15.i, 4294967295
  br label %arenas_i2a_impl.exit20.i

arenas_i2a_impl.exit20.i:                         ; preds = %sw.default.i12.i, %sw.bb2.i10.i, %if.end.i34
  %a.0.i11.i = phi i64 [ %20, %sw.default.i12.i ], [ 1, %sw.bb2.i10.i ], [ 0, %if.end.i34 ]
  %arrayidx13.i38 = getelementptr inbounds [4097 x ptr], ptr %arenas9.i37, i64 0, i64 %a.0.i11.i
  store ptr %call5.i32, ptr %arrayidx13.i38, align 8
  br label %for.inc

for.inc:                                          ; preds = %arenas_i2a_impl.exit20.i, %arenas_i2a_impl.exit.i
  %21 = phi ptr [ %19, %arenas_i2a_impl.exit20.i ], [ %15, %arenas_i2a_impl.exit.i ]
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %narenas24 = getelementptr inbounds i8, ptr %21, i64 8
  %22 = load i32, ptr %narenas24, align 8
  %23 = zext i32 %22 to i64
  %cmp25 = icmp ult i64 %indvars.iv.next, %23
  br i1 %cmp25, label %for.body, label %do.body, !llvm.loop !6

do.body:                                          ; preds = %for.inc, %if.end22
  %.lcssa = phi ptr [ %14, %if.end22 ], [ %21, %for.inc ]
  %destroyed = getelementptr inbounds i8, ptr %.lcssa, i64 16
  store ptr null, ptr %destroyed, align 8
  tail call fastcc void @ctl_refresh(ptr noundef %tsd)
  store i1 true, ptr @ctl_initialized, align 1
  br label %label_return

label_return:                                     ; preds = %if.then.i30, %if.then.i16, %if.then.i11, %malloc_mutex_lock.exit, %do.body, %if.then8, %if.then1
  %ret.0 = phi i1 [ true, %if.then1 ], [ true, %if.then8 ], [ false, %do.body ], [ false, %malloc_mutex_lock.exit ], [ true, %if.then.i11 ], [ true, %if.then.i16 ], [ true, %if.then.i30 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i1 %ret.0
}

; Function Attrs: nounwind uwtable
define internal fastcc i32 @ctl_lookup(ptr noundef %tsdn, ptr noundef %starting_node, ptr noundef %name, ptr noundef writeonly %ending_nodep, ptr noundef %mibp, ptr nocapture noundef %depthp) unnamed_addr #0 {
entry:
  %call = tail call ptr @strchr(ptr noundef nonnull dereferenceable(1) %name, i32 noundef 46) #15
  %cmp.not = icmp eq ptr %call, null
  br i1 %cmp.not, label %cond.false, label %cond.end

cond.false:                                       ; preds = %entry
  %strlen = tail call i64 @strlen(ptr nonnull dereferenceable(1) %name)
  %strchr = getelementptr inbounds i8, ptr %name, i64 %strlen
  br label %cond.end

cond.end:                                         ; preds = %entry, %cond.false
  %cond = phi ptr [ %strchr, %cond.false ], [ %call, %entry ]
  %0 = ptrtoint ptr %cond to i64
  %1 = ptrtoint ptr %name to i64
  %sub = sub i64 %0, %1
  %cmp2 = icmp eq i64 %sub, 0
  br i1 %cmp2, label %label_return, label %for.cond.preheader

for.cond.preheader:                               ; preds = %cond.end
  %2 = load i64, ptr %depthp, align 8
  %cmp367.not = icmp eq i64 %2, 0
  br i1 %cmp367.not, label %for.end61, label %do.end5

do.end5:                                          ; preds = %for.cond.preheader, %cond.end56
  %elm.072 = phi ptr [ %arrayidx49, %cond.end56 ], [ %name, %for.cond.preheader ]
  %dot.071 = phi ptr [ %cond57, %cond.end56 ], [ %cond, %for.cond.preheader ]
  %node.070 = phi ptr [ %node.2, %cond.end56 ], [ %starting_node, %for.cond.preheader ]
  %i.069 = phi i64 [ %inc60, %cond.end56 ], [ 0, %for.cond.preheader ]
  %elen.068 = phi i64 [ %sub58, %cond.end56 ], [ %sub, %for.cond.preheader ]
  %children = getelementptr inbounds i8, ptr %node.070, i64 24
  %3 = load ptr, ptr %children, align 8
  %4 = load i8, ptr %3, align 1
  %5 = and i8 %4, 1
  %tobool.not.i = icmp eq i8 %5, 0
  br i1 %tobool.not.i, label %if.else, label %for.cond9.preheader

for.cond9.preheader:                              ; preds = %do.end5
  %nchildren = getelementptr inbounds i8, ptr %node.070, i64 16
  %6 = load i64, ptr %nchildren, align 8
  %cmp1065.not = icmp eq i64 %6, 0
  br i1 %cmp1065.not, label %label_return, label %for.body11

for.body11:                                       ; preds = %for.cond9.preheader, %for.inc
  %j.066 = phi i64 [ %inc, %for.inc ], [ 0, %for.cond9.preheader ]
  %arrayidx.i = getelementptr inbounds %struct.ctl_named_node_s, ptr %3, i64 %j.066
  %name13 = getelementptr inbounds i8, ptr %arrayidx.i, i64 8
  %7 = load ptr, ptr %name13, align 8
  %call14 = tail call i64 @strlen(ptr noundef nonnull dereferenceable(1) %7) #15
  %cmp15 = icmp eq i64 %call14, %elen.068
  br i1 %cmp15, label %land.lhs.true, label %for.inc

land.lhs.true:                                    ; preds = %for.body11
  %call17 = tail call i32 @strncmp(ptr noundef %elm.072, ptr noundef %7, i64 noundef %elen.068) #15
  %cmp18 = icmp eq i32 %call17, 0
  br i1 %cmp18, label %for.end, label %for.inc

for.inc:                                          ; preds = %for.body11, %land.lhs.true
  %inc = add nuw i64 %j.066, 1
  %exitcond.not = icmp eq i64 %inc, %6
  br i1 %exitcond.not, label %label_return, label %for.body11, !llvm.loop !8

for.end:                                          ; preds = %land.lhs.true
  %arrayidx = getelementptr inbounds i64, ptr %mibp, i64 %i.069
  store i64 %j.066, ptr %arrayidx, align 8
  %cmp21 = icmp eq ptr %arrayidx.i, %node.070
  br i1 %cmp21, label %label_return, label %if.end37

if.else:                                          ; preds = %do.end5
  %call24 = tail call i64 @malloc_strtoumax(ptr noundef %elm.072, ptr noundef null, i32 noundef 10) #14
  %cmp25 = icmp eq i64 %call24, -1
  br i1 %cmp25, label %label_return, label %if.end28

if.end28:                                         ; preds = %if.else
  %8 = load ptr, ptr %children, align 8
  %9 = load i8, ptr %8, align 1
  %10 = and i8 %9, 1
  %tobool.not.i43 = icmp eq i8 %10, 0
  %cond.i44 = select i1 %tobool.not.i43, ptr %8, ptr null
  %index31 = getelementptr inbounds i8, ptr %cond.i44, i64 8
  %11 = load ptr, ptr %index31, align 8
  %12 = load i64, ptr %depthp, align 8
  %call32 = tail call ptr %11(ptr noundef %tsdn, ptr noundef %mibp, i64 noundef %12, i64 noundef %call24) #14
  %cmp33 = icmp eq ptr %call32, null
  br i1 %cmp33, label %label_return, label %if.end35

if.end35:                                         ; preds = %if.end28
  %arrayidx36 = getelementptr inbounds i64, ptr %mibp, i64 %i.069
  store i64 %call24, ptr %arrayidx36, align 8
  br label %if.end37

if.end37:                                         ; preds = %for.end, %if.end35
  %node.2 = phi ptr [ %arrayidx.i, %for.end ], [ %call32, %if.end35 ]
  %ctl = getelementptr inbounds i8, ptr %node.2, i64 32
  %13 = load ptr, ptr %ctl, align 8
  %cmp38.not = icmp eq ptr %13, null
  %14 = load i8, ptr %dot.071, align 1
  %cmp40 = icmp eq i8 %14, 0
  br i1 %cmp38.not, label %lor.lhs.false39, label %if.then42

lor.lhs.false39:                                  ; preds = %if.end37
  br i1 %cmp40, label %if.end47, label %if.end48

if.then42:                                        ; preds = %if.end37
  br i1 %cmp40, label %if.end47, label %label_return

if.end47:                                         ; preds = %lor.lhs.false39, %if.then42
  %add = add i64 %i.069, 1
  store i64 %add, ptr %depthp, align 8
  br label %for.end61

if.end48:                                         ; preds = %lor.lhs.false39
  %arrayidx49 = getelementptr inbounds i8, ptr %dot.071, i64 1
  %call50 = tail call ptr @strchr(ptr noundef nonnull dereferenceable(1) %arrayidx49, i32 noundef 46) #15
  %cmp51.not = icmp eq ptr %call50, null
  br i1 %cmp51.not, label %cond.false54, label %cond.end56

cond.false54:                                     ; preds = %if.end48
  %strlen40 = tail call i64 @strlen(ptr nonnull dereferenceable(1) %arrayidx49)
  %strchr41 = getelementptr inbounds i8, ptr %arrayidx49, i64 %strlen40
  br label %cond.end56

cond.end56:                                       ; preds = %if.end48, %cond.false54
  %cond57 = phi ptr [ %strchr41, %cond.false54 ], [ %call50, %if.end48 ]
  %15 = ptrtoint ptr %cond57 to i64
  %16 = ptrtoint ptr %arrayidx49 to i64
  %sub58 = sub i64 %15, %16
  %inc60 = add nuw i64 %i.069, 1
  %17 = load i64, ptr %depthp, align 8
  %cmp3 = icmp ult i64 %inc60, %17
  br i1 %cmp3, label %do.end5, label %for.end61, !llvm.loop !9

for.end61:                                        ; preds = %cond.end56, %for.cond.preheader, %if.end47
  %node.3 = phi ptr [ %node.2, %if.end47 ], [ %starting_node, %for.cond.preheader ], [ %node.2, %cond.end56 ]
  %cmp62.not = icmp eq ptr %ending_nodep, null
  br i1 %cmp62.not, label %label_return, label %if.then64

if.then64:                                        ; preds = %for.end61
  store ptr %node.3, ptr %ending_nodep, align 8
  br label %label_return

label_return:                                     ; preds = %if.end28, %if.else, %for.end, %for.cond9.preheader, %for.inc, %for.end61, %if.then64, %if.then42, %cond.end
  %ret.0 = phi i32 [ 2, %cond.end ], [ 2, %if.then42 ], [ 0, %if.then64 ], [ 0, %for.end61 ], [ 2, %for.inc ], [ 2, %for.cond9.preheader ], [ 2, %for.end ], [ 2, %if.else ], [ 2, %if.end28 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define hidden i32 @ctl_nametomib(ptr noundef %tsd, ptr noundef %name, ptr noundef %mibp, ptr nocapture noundef %miblenp) local_unnamed_addr #0 {
entry:
  %.b2 = load i1, ptr @ctl_initialized, align 1
  br i1 %.b2, label %if.end, label %land.lhs.true

land.lhs.true:                                    ; preds = %entry
  %call = tail call fastcc zeroext i1 @ctl_init(ptr noundef %tsd)
  br i1 %call, label %label_return, label %if.end

if.end:                                           ; preds = %land.lhs.true, %entry
  %call2 = tail call fastcc i32 @ctl_lookup(ptr noundef %tsd, ptr noundef nonnull @super_root_node, ptr noundef %name, ptr noundef null, ptr noundef %mibp, ptr noundef %miblenp), !range !5
  br label %label_return

label_return:                                     ; preds = %land.lhs.true, %if.end
  %ret.0 = phi i32 [ %call2, %if.end ], [ 11, %land.lhs.true ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define hidden i32 @ctl_bymib(ptr noundef %tsd, ptr noundef %mib, i64 noundef %miblen, ptr noundef %oldp, ptr noundef %oldlenp, ptr noundef %newp, i64 noundef %newlen) local_unnamed_addr #0 {
entry:
  %.b7 = load i1, ptr @ctl_initialized, align 1
  br i1 %.b7, label %if.end, label %land.lhs.true

land.lhs.true:                                    ; preds = %entry
  %call = tail call fastcc zeroext i1 @ctl_init(ptr noundef %tsd)
  br i1 %call, label %label_return, label %if.end

if.end:                                           ; preds = %land.lhs.true, %entry
  %cmp18.not.i = icmp eq i64 %miblen, 0
  br i1 %cmp18.not.i, label %land.lhs.true6, label %do.end2.i

do.end2.i:                                        ; preds = %if.end, %for.inc.i
  %i.020.i = phi i64 [ %inc.i, %for.inc.i ], [ 0, %if.end ]
  %node.019.i = phi ptr [ %node.1.i, %for.inc.i ], [ @super_root_node, %if.end ]
  %children.i = getelementptr inbounds i8, ptr %node.019.i, i64 24
  %0 = load ptr, ptr %children.i, align 8
  %1 = load i8, ptr %0, align 1
  %2 = and i8 %1, 1
  %tobool.not.i.i = icmp eq i8 %2, 0
  %arrayidx10.i = getelementptr inbounds i64, ptr %mib, i64 %i.020.i
  %3 = load i64, ptr %arrayidx10.i, align 8
  br i1 %tobool.not.i.i, label %if.else.i, label %if.then.i

if.then.i:                                        ; preds = %do.end2.i
  %nchildren.i = getelementptr inbounds i8, ptr %node.019.i, i64 16
  %4 = load i64, ptr %nchildren.i, align 8
  %cmp4.not.i = icmp ugt i64 %4, %3
  br i1 %cmp4.not.i, label %if.end.i, label %label_return

if.end.i:                                         ; preds = %if.then.i
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_named_node_s, ptr %0, i64 %3
  br label %for.inc.i

if.else.i:                                        ; preds = %do.end2.i
  %index.i = getelementptr inbounds i8, ptr %0, i64 8
  %5 = load ptr, ptr %index.i, align 8
  %call11.i = tail call ptr %5(ptr noundef %tsd, ptr noundef nonnull %mib, i64 noundef %miblen, i64 noundef %3) #14
  %cmp12.i = icmp eq ptr %call11.i, null
  br i1 %cmp12.i, label %label_return, label %for.inc.i

for.inc.i:                                        ; preds = %if.else.i, %if.end.i
  %node.1.i = phi ptr [ %arrayidx.i.i, %if.end.i ], [ %call11.i, %if.else.i ]
  %inc.i = add nuw i64 %i.020.i, 1
  %exitcond.not.i = icmp eq i64 %inc.i, %miblen
  br i1 %exitcond.not.i, label %if.end4, label %do.end2.i, !llvm.loop !10

if.end4:                                          ; preds = %for.inc.i
  %tobool5.not = icmp eq ptr %node.1.i, null
  br i1 %tobool5.not, label %label_return, label %land.lhs.true6

land.lhs.true6:                                   ; preds = %if.end, %if.end4
  %node.0.ph14 = phi ptr [ %node.1.i, %if.end4 ], [ @super_root_node, %if.end ]
  %ctl = getelementptr inbounds i8, ptr %node.0.ph14, i64 32
  %6 = load ptr, ptr %ctl, align 8
  %tobool7.not = icmp eq ptr %6, null
  br i1 %tobool7.not, label %label_return, label %if.then8

if.then8:                                         ; preds = %land.lhs.true6
  %call10 = tail call i32 %6(ptr noundef %tsd, ptr noundef %mib, i64 noundef %miblen, ptr noundef %oldp, ptr noundef %oldlenp, ptr noundef %newp, i64 noundef %newlen) #14
  br label %label_return

label_return:                                     ; preds = %if.else.i, %if.then.i, %if.end4, %land.lhs.true6, %land.lhs.true, %if.then8
  %ret.0 = phi i32 [ %call10, %if.then8 ], [ 11, %land.lhs.true ], [ 2, %land.lhs.true6 ], [ 2, %if.end4 ], [ 2, %if.then.i ], [ 2, %if.else.i ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define hidden i32 @ctl_mibnametomib(ptr noundef %tsd, ptr noundef %mib, i64 noundef %miblen, ptr noundef %name, ptr nocapture noundef %miblenp) local_unnamed_addr #0 {
entry:
  %.b10 = load i1, ptr @ctl_initialized, align 1
  br i1 %.b10, label %if.end, label %land.lhs.true

land.lhs.true:                                    ; preds = %entry
  %call = tail call fastcc zeroext i1 @ctl_init(ptr noundef %tsd)
  br i1 %call, label %label_return, label %if.end

if.end:                                           ; preds = %land.lhs.true, %entry
  %cmp18.not.i = icmp eq i64 %miblen, 0
  br i1 %cmp18.not.i, label %lor.lhs.false, label %do.end2.i

do.end2.i:                                        ; preds = %if.end, %for.inc.i
  %i.020.i = phi i64 [ %inc.i, %for.inc.i ], [ 0, %if.end ]
  %node.019.i = phi ptr [ %node.1.i, %for.inc.i ], [ @super_root_node, %if.end ]
  %children.i = getelementptr inbounds i8, ptr %node.019.i, i64 24
  %0 = load ptr, ptr %children.i, align 8
  %1 = load i8, ptr %0, align 1
  %2 = and i8 %1, 1
  %tobool.not.i.i = icmp eq i8 %2, 0
  %arrayidx10.i = getelementptr inbounds i64, ptr %mib, i64 %i.020.i
  %3 = load i64, ptr %arrayidx10.i, align 8
  br i1 %tobool.not.i.i, label %if.else.i, label %if.then.i

if.then.i:                                        ; preds = %do.end2.i
  %nchildren.i = getelementptr inbounds i8, ptr %node.019.i, i64 16
  %4 = load i64, ptr %nchildren.i, align 8
  %cmp4.not.i = icmp ugt i64 %4, %3
  br i1 %cmp4.not.i, label %if.end.i, label %label_return

if.end.i:                                         ; preds = %if.then.i
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_named_node_s, ptr %0, i64 %3
  br label %for.inc.i

if.else.i:                                        ; preds = %do.end2.i
  %index.i = getelementptr inbounds i8, ptr %0, i64 8
  %5 = load ptr, ptr %index.i, align 8
  %call11.i = tail call ptr %5(ptr noundef %tsd, ptr noundef nonnull %mib, i64 noundef %miblen, i64 noundef %3) #14
  %cmp12.i = icmp eq ptr %call11.i, null
  br i1 %cmp12.i, label %label_return, label %for.inc.i

for.inc.i:                                        ; preds = %if.else.i, %if.end.i
  %node.1.i = phi ptr [ %arrayidx.i.i, %if.end.i ], [ %call11.i, %if.else.i ]
  %inc.i = add nuw i64 %i.020.i, 1
  %exitcond.not.i = icmp eq i64 %inc.i, %miblen
  br i1 %exitcond.not.i, label %if.end4, label %do.end2.i, !llvm.loop !10

if.end4:                                          ; preds = %for.inc.i
  %cmp5 = icmp eq ptr %node.1.i, null
  br i1 %cmp5, label %label_return, label %lor.lhs.false

lor.lhs.false:                                    ; preds = %if.end, %if.end4
  %node.0.ph18 = phi ptr [ %node.1.i, %if.end4 ], [ @super_root_node, %if.end ]
  %ctl = getelementptr inbounds i8, ptr %node.0.ph18, i64 32
  %6 = load ptr, ptr %ctl, align 8
  %cmp6.not = icmp eq ptr %6, null
  br i1 %cmp6.not, label %do.end10, label %label_return

do.end10:                                         ; preds = %lor.lhs.false
  %7 = load i64, ptr %miblenp, align 8
  %sub = sub i64 %7, %miblen
  store i64 %sub, ptr %miblenp, align 8
  %add.ptr = getelementptr inbounds i64, ptr %mib, i64 %miblen
  %call12 = tail call fastcc i32 @ctl_lookup(ptr noundef %tsd, ptr noundef nonnull %node.0.ph18, ptr noundef %name, ptr noundef null, ptr noundef %add.ptr, ptr noundef nonnull %miblenp), !range !5
  %8 = load i64, ptr %miblenp, align 8
  %add = add i64 %8, %miblen
  store i64 %add, ptr %miblenp, align 8
  br label %label_return

label_return:                                     ; preds = %if.else.i, %if.then.i, %if.end4, %lor.lhs.false, %land.lhs.true, %do.end10
  %ret.0 = phi i32 [ %call12, %do.end10 ], [ 11, %land.lhs.true ], [ 2, %lor.lhs.false ], [ 2, %if.end4 ], [ 2, %if.then.i ], [ 2, %if.else.i ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define hidden i32 @ctl_bymibname(ptr noundef %tsd, ptr noundef %mib, i64 noundef %miblen, ptr noundef %name, ptr nocapture noundef %miblenp, ptr noundef %oldp, ptr noundef %oldlenp, ptr noundef %newp, i64 noundef %newlen) local_unnamed_addr #0 {
entry:
  %node = alloca ptr, align 8
  %.b16 = load i1, ptr @ctl_initialized, align 1
  br i1 %.b16, label %if.end, label %land.lhs.true

land.lhs.true:                                    ; preds = %entry
  %call = tail call fastcc zeroext i1 @ctl_init(ptr noundef %tsd)
  br i1 %call, label %label_return, label %if.end

if.end:                                           ; preds = %land.lhs.true, %entry
  %cmp18.not.i = icmp eq i64 %miblen, 0
  br i1 %cmp18.not.i, label %if.end4.thread, label %do.end2.i

if.end4.thread:                                   ; preds = %if.end
  store ptr @super_root_node, ptr %node, align 8
  br label %lor.lhs.false

do.end2.i:                                        ; preds = %if.end, %for.inc.i
  %i.020.i = phi i64 [ %inc.i, %for.inc.i ], [ 0, %if.end ]
  %node.019.i = phi ptr [ %node.1.i, %for.inc.i ], [ @super_root_node, %if.end ]
  %children.i = getelementptr inbounds i8, ptr %node.019.i, i64 24
  %0 = load ptr, ptr %children.i, align 8
  %1 = load i8, ptr %0, align 1
  %2 = and i8 %1, 1
  %tobool.not.i.i = icmp eq i8 %2, 0
  %arrayidx10.i = getelementptr inbounds i64, ptr %mib, i64 %i.020.i
  %3 = load i64, ptr %arrayidx10.i, align 8
  br i1 %tobool.not.i.i, label %if.else.i, label %if.then.i

if.then.i:                                        ; preds = %do.end2.i
  %nchildren.i = getelementptr inbounds i8, ptr %node.019.i, i64 16
  %4 = load i64, ptr %nchildren.i, align 8
  %cmp4.not.i = icmp ugt i64 %4, %3
  br i1 %cmp4.not.i, label %if.end.i, label %label_return

if.end.i:                                         ; preds = %if.then.i
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_named_node_s, ptr %0, i64 %3
  br label %for.inc.i

if.else.i:                                        ; preds = %do.end2.i
  %index.i = getelementptr inbounds i8, ptr %0, i64 8
  %5 = load ptr, ptr %index.i, align 8
  %call11.i = tail call ptr %5(ptr noundef %tsd, ptr noundef nonnull %mib, i64 noundef %miblen, i64 noundef %3) #14
  %cmp12.i = icmp eq ptr %call11.i, null
  br i1 %cmp12.i, label %label_return, label %for.inc.i

for.inc.i:                                        ; preds = %if.else.i, %if.end.i
  %node.1.i = phi ptr [ %arrayidx.i.i, %if.end.i ], [ %call11.i, %if.else.i ]
  %inc.i = add nuw i64 %i.020.i, 1
  %exitcond.not.i = icmp eq i64 %inc.i, %miblen
  br i1 %exitcond.not.i, label %if.end4, label %do.end2.i, !llvm.loop !10

if.end4:                                          ; preds = %for.inc.i
  store ptr %node.1.i, ptr %node, align 8
  %cmp5 = icmp eq ptr %node.1.i, null
  br i1 %cmp5, label %label_return, label %lor.lhs.false

lor.lhs.false:                                    ; preds = %if.end4.thread, %if.end4
  %node.0.lcssa.i21 = phi ptr [ @super_root_node, %if.end4.thread ], [ %node.1.i, %if.end4 ]
  %ctl = getelementptr inbounds i8, ptr %node.0.lcssa.i21, i64 32
  %6 = load ptr, ptr %ctl, align 8
  %cmp6.not = icmp eq ptr %6, null
  br i1 %cmp6.not, label %do.end10, label %label_return

do.end10:                                         ; preds = %lor.lhs.false
  %7 = load i64, ptr %miblenp, align 8
  %sub = sub i64 %7, %miblen
  store i64 %sub, ptr %miblenp, align 8
  %add.ptr = getelementptr inbounds i64, ptr %mib, i64 %miblen
  %call12 = call fastcc i32 @ctl_lookup(ptr noundef %tsd, ptr noundef nonnull %node.0.lcssa.i21, ptr noundef %name, ptr noundef nonnull %node, ptr noundef %add.ptr, ptr noundef nonnull %miblenp), !range !5
  %8 = load i64, ptr %miblenp, align 8
  %add = add i64 %8, %miblen
  store i64 %add, ptr %miblenp, align 8
  %cmp13.not = icmp eq i32 %call12, 0
  br i1 %cmp13.not, label %if.end15, label %label_return

if.end15:                                         ; preds = %do.end10
  %9 = load ptr, ptr %node, align 8
  %cmp16.not = icmp eq ptr %9, null
  br i1 %cmp16.not, label %label_return, label %land.lhs.true17

land.lhs.true17:                                  ; preds = %if.end15
  %ctl18 = getelementptr inbounds i8, ptr %9, i64 32
  %10 = load ptr, ptr %ctl18, align 8
  %tobool19.not = icmp eq ptr %10, null
  br i1 %tobool19.not, label %label_return, label %if.then20

if.then20:                                        ; preds = %land.lhs.true17
  %call22 = call i32 %10(ptr noundef %tsd, ptr noundef %mib, i64 noundef %add, ptr noundef %oldp, ptr noundef %oldlenp, ptr noundef %newp, i64 noundef %newlen) #14
  br label %label_return

label_return:                                     ; preds = %if.else.i, %if.then.i, %if.end15, %land.lhs.true17, %if.end4, %lor.lhs.false, %land.lhs.true, %if.then20, %do.end10
  %ret.0 = phi i32 [ %call12, %do.end10 ], [ %call22, %if.then20 ], [ 11, %land.lhs.true ], [ 2, %lor.lhs.false ], [ 2, %if.end4 ], [ 2, %land.lhs.true17 ], [ 2, %if.end15 ], [ 2, %if.then.i ], [ 2, %if.else.i ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define hidden zeroext i1 @ctl_boot() local_unnamed_addr #0 {
entry:
  %call = tail call zeroext i1 @malloc_mutex_init(ptr noundef nonnull @ctl_mtx, ptr noundef nonnull @.str, i32 noundef 2, i32 noundef 0) #14
  br i1 %call, label %return, label %if.end

if.end:                                           ; preds = %entry
  store i1 false, ptr @ctl_initialized, align 1
  br label %return

return:                                           ; preds = %entry, %if.end
  ret i1 %call
}

declare zeroext i1 @malloc_mutex_init(ptr noundef, ptr noundef, i32 noundef, i32 noundef) local_unnamed_addr #1

; Function Attrs: nounwind uwtable
define hidden void @ctl_prefork(ptr noundef %tsdn) local_unnamed_addr #0 {
entry:
  tail call void @malloc_mutex_prefork(ptr noundef %tsdn, ptr noundef nonnull @ctl_mtx) #14
  ret void
}

declare void @malloc_mutex_prefork(ptr noundef, ptr noundef) local_unnamed_addr #1

; Function Attrs: nounwind uwtable
define hidden void @ctl_postfork_parent(ptr noundef %tsdn) local_unnamed_addr #0 {
entry:
  tail call void @malloc_mutex_postfork_parent(ptr noundef %tsdn, ptr noundef nonnull @ctl_mtx) #14
  ret void
}

declare void @malloc_mutex_postfork_parent(ptr noundef, ptr noundef) local_unnamed_addr #1

; Function Attrs: nounwind uwtable
define hidden void @ctl_postfork_child(ptr noundef %tsdn) local_unnamed_addr #0 {
entry:
  tail call void @malloc_mutex_postfork_child(ptr noundef %tsdn, ptr noundef nonnull @ctl_mtx) #14
  ret void
}

declare void @malloc_mutex_postfork_child(ptr noundef, ptr noundef) local_unnamed_addr #1

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define hidden void @ctl_mtx_assert_held(ptr nocapture noundef readnone %tsdn) local_unnamed_addr #2 {
entry:
  ret void
}

declare ptr @base_alloc(ptr noundef, ptr noundef, i64 noundef, i64 noundef) local_unnamed_addr #1

declare ptr @b0get() local_unnamed_addr #1

declare i32 @narenas_total_get() local_unnamed_addr #1

; Function Attrs: nounwind uwtable
define internal fastcc void @ctl_refresh(ptr noundef %tsdn) unnamed_addr #0 {
entry:
  %0 = load ptr, ptr @ctl_arenas, align 8
  %narenas1 = getelementptr inbounds i8, ptr %0, i64 8
  %1 = load i32, ptr %narenas1, align 8
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %2, i64 824
  %3 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %3, 0
  br i1 %cmp6.i.not.i, label %arenas_i.exit, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %entry
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %2, i1 noundef zeroext false) #14
  %.pre = load ptr, ptr @ctl_arenas, align 8
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %entry, %if.then11.i.i
  %4 = phi ptr [ %0, %entry ], [ %.pre, %if.then11.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %4, i64 24
  %5 = load ptr, ptr %arenas.i.i, align 8
  %6 = zext i32 %1 to i64
  %vla = alloca ptr, i64 %6, align 16
  %nthreads.i = getelementptr inbounds i8, ptr %5, i64 24
  store i32 0, ptr %nthreads.i, align 8
  %7 = load ptr, ptr getelementptr inbounds ([0 x ptr], ptr @dss_prec_names, i64 0, i64 3), align 8
  %dss.i = getelementptr inbounds i8, ptr %5, i64 32
  store ptr %7, ptr %dss.i, align 8
  %dirty_decay_ms.i = getelementptr inbounds i8, ptr %5, i64 40
  %pactive.i = getelementptr inbounds i8, ptr %5, i64 56
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) %dirty_decay_ms.i, i8 -1, i64 16, i1 false)
  %astats.i = getelementptr inbounds i8, ptr %5, i64 80
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) %pactive.i, i8 0, i64 24, i1 false)
  %8 = load ptr, ptr %astats.i, align 8
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(37784) %8, i8 0, i64 37784, i1 false)
  %cmp41.not = icmp eq i32 %1, 0
  br i1 %cmp41.not, label %for.end19, label %for.body

for.cond4.preheader:                              ; preds = %for.body
  br i1 %cmp41.not, label %for.end19, label %for.body6

for.body:                                         ; preds = %arenas_i.exit, %for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %for.body ], [ 0, %arenas_i.exit ]
  %arrayidx.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %indvars.iv
  %9 = load atomic i64, ptr %arrayidx.i acquire, align 8
  %10 = inttoptr i64 %9 to ptr
  %arrayidx = getelementptr inbounds ptr, ptr %vla, i64 %indvars.iv
  store ptr %10, ptr %arrayidx, align 8
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, %6
  br i1 %exitcond.not, label %for.cond4.preheader, label %for.body, !llvm.loop !11

for.body6:                                        ; preds = %for.cond4.preheader, %for.inc17
  %indvars.iv46 = phi i64 [ %indvars.iv.next47, %for.inc17 ], [ 0, %for.cond4.preheader ]
  %11 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i32 = icmp eq i8 %11, 0
  br i1 %cmp6.i.not.i32, label %tsd_fetch_impl.exit.i, label %if.then11.i.i33

if.then11.i.i33:                                  ; preds = %for.body6
  %call13.i.i34 = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %2, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i33, %for.body6
  %12 = load ptr, ptr @ctl_arenas, align 8
  %13 = trunc i64 %indvars.iv46 to i32
  switch i32 %13, label %sw.default.i.i.i [
    i32 4096, label %arenas_i.exit36
    i32 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit36

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %12, i64 8
  %14 = load i32, ptr %narenas.i.i.i, align 8
  %15 = zext i32 %14 to i64
  %cmp.i.i.i = icmp eq i64 %indvars.iv46, %15
  br i1 %cmp.i.i.i, label %arenas_i.exit36, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add nuw nsw i64 %indvars.iv46, 2
  %16 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit36

arenas_i.exit36:                                  ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %16, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i35 = getelementptr inbounds i8, ptr %12, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i35, i64 0, i64 %a.0.i.i.i
  %17 = load ptr, ptr %arrayidx.i.i, align 8
  %arrayidx9 = getelementptr inbounds ptr, ptr %vla, i64 %indvars.iv46
  %18 = load ptr, ptr %arrayidx9, align 8
  %cmp10 = icmp ne ptr %18, null
  %frombool = zext i1 %cmp10 to i8
  %initialized12 = getelementptr inbounds i8, ptr %17, i64 4
  store i8 %frombool, ptr %initialized12, align 4
  br i1 %cmp10, label %if.then, label %for.inc17

if.then:                                          ; preds = %arenas_i.exit36
  tail call fastcc void @ctl_arena_refresh(ptr noundef %tsdn, ptr noundef nonnull %18, ptr noundef %5, i32 noundef %13, i1 noundef zeroext false)
  br label %for.inc17

for.inc17:                                        ; preds = %arenas_i.exit36, %if.then
  %indvars.iv.next47 = add nuw nsw i64 %indvars.iv46, 1
  %exitcond50.not = icmp eq i64 %indvars.iv.next47, %6
  br i1 %exitcond50.not, label %for.end19, label %for.body6, !llvm.loop !12

for.end19:                                        ; preds = %for.inc17, %arenas_i.exit, %for.cond4.preheader
  %19 = load ptr, ptr %astats.i, align 8
  %allocated_small = getelementptr inbounds i8, ptr %19, i64 10384
  %20 = load i64, ptr %allocated_small, align 8
  %allocated_large = getelementptr inbounds i8, ptr %19, i64 56
  %21 = load i64, ptr %allocated_large, align 8
  %add = add i64 %21, %20
  %22 = load ptr, ptr @ctl_stats, align 8
  store i64 %add, ptr %22, align 8
  %23 = load i64, ptr %pactive.i, align 8
  %shl = shl i64 %23, 12
  %active = getelementptr inbounds i8, ptr %22, i64 8
  store i64 %shl, ptr %active, align 8
  %24 = load ptr, ptr %astats.i, align 8
  %25 = load i64, ptr %24, align 8
  %internal = getelementptr inbounds i8, ptr %24, i64 48
  %26 = load atomic i64, ptr %internal monotonic, align 8
  %add27 = add i64 %26, %25
  %metadata = getelementptr inbounds i8, ptr %22, i64 16
  store i64 %add27, ptr %metadata, align 8
  %27 = load ptr, ptr %astats.i, align 8
  %metadata_edata = getelementptr inbounds i8, ptr %27, i64 8
  %28 = load i64, ptr %metadata_edata, align 8
  %metadata_edata30 = getelementptr inbounds i8, ptr %22, i64 24
  store i64 %28, ptr %metadata_edata30, align 8
  %29 = load ptr, ptr %astats.i, align 8
  %metadata_rtree = getelementptr inbounds i8, ptr %29, i64 16
  %30 = load i64, ptr %metadata_rtree, align 8
  %metadata_rtree33 = getelementptr inbounds i8, ptr %22, i64 32
  store i64 %30, ptr %metadata_rtree33, align 8
  %31 = load ptr, ptr %astats.i, align 8
  %resident = getelementptr inbounds i8, ptr %31, i64 24
  %32 = load i64, ptr %resident, align 8
  %resident36 = getelementptr inbounds i8, ptr %22, i64 48
  store i64 %32, ptr %resident36, align 8
  %33 = load ptr, ptr %astats.i, align 8
  %metadata_thp = getelementptr inbounds i8, ptr %33, i64 32
  %34 = load i64, ptr %metadata_thp, align 8
  %metadata_thp39 = getelementptr inbounds i8, ptr %22, i64 40
  store i64 %34, ptr %metadata_thp39, align 8
  %35 = load ptr, ptr %astats.i, align 8
  %mapped = getelementptr inbounds i8, ptr %35, i64 40
  %36 = load i64, ptr %mapped, align 8
  %mapped42 = getelementptr inbounds i8, ptr %22, i64 56
  store i64 %36, ptr %mapped42, align 8
  %37 = load ptr, ptr %astats.i, align 8
  %retained = getelementptr inbounds i8, ptr %37, i64 160
  %38 = load i64, ptr %retained, align 8
  %retained45 = getelementptr inbounds i8, ptr %22, i64 64
  store i64 %38, ptr %retained45, align 8
  %background_thread.i = getelementptr inbounds i8, ptr %22, i64 72
  %call.i = tail call zeroext i1 @background_thread_stats_read(ptr noundef %tsdn, ptr noundef nonnull %background_thread.i) #14
  br i1 %call.i, label %if.then.i, label %ctl_background_thread_stats_read.exit

if.then.i:                                        ; preds = %for.end19
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(88) %background_thread.i, i8 0, i64 88, i1 false)
  %run_interval.i = getelementptr inbounds i8, ptr %22, i64 88
  tail call void @nstime_copy(ptr noundef nonnull %run_interval.i, ptr noundef nonnull @nstime_zero) #14
  br label %ctl_background_thread_stats_read.exit

ctl_background_thread_stats_read.exit:            ; preds = %for.end19, %if.then.i
  %39 = load ptr, ptr @ctl_stats, align 8
  %arrayidx.i37 = getelementptr inbounds i8, ptr %39, i64 224
  %max_counter_per_bg_thd.i = getelementptr inbounds i8, ptr %22, i64 96
  tail call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(64) %arrayidx.i37, ptr noundef nonnull align 8 dereferenceable(64) %max_counter_per_bg_thd.i, i64 64, i1 false)
  %n_waiting_thds.i.i = getelementptr inbounds i8, ptr %39, i64 260
  store atomic i32 0, ptr %n_waiting_thds.i.i monotonic, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i38

if.then.i38:                                      ; preds = %ctl_background_thread_stats_read.exit
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @background_thread_lock) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i38, %ctl_background_thread_stats_read.exit
  %40 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %40, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %41 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %41, %tsdn
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsdn, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %42 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %42, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %43 = load ptr, ptr @ctl_stats, align 8
  %mutex_prof_data = getelementptr inbounds i8, ptr %43, i64 160
  tail call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(64) %mutex_prof_data, ptr noundef nonnull align 8 dereferenceable(64) @background_thread_lock, i64 64, i1 false)
  %n_waiting_thds.i.i39 = getelementptr inbounds i8, ptr %43, i64 196
  store atomic i32 0, ptr %n_waiting_thds.i.i39 monotonic, align 4
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %44 = load ptr, ptr @ctl_stats, align 8
  %arrayidx48 = getelementptr inbounds i8, ptr %44, i64 288
  tail call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(64) %arrayidx48, ptr noundef nonnull align 8 dereferenceable(64) @ctl_mtx, i64 64, i1 false)
  %n_waiting_thds.i.i40 = getelementptr inbounds i8, ptr %44, i64 324
  store atomic i32 0, ptr %n_waiting_thds.i.i40 monotonic, align 4
  %45 = load ptr, ptr @ctl_arenas, align 8
  %46 = load i64, ptr %45, align 8
  %inc49 = add i64 %46, 1
  store i64 %inc49, ptr %45, align 8
  ret void
}

declare void @malloc_mutex_lock_slow(ptr noundef) local_unnamed_addr #1

; Function Attrs: nounwind
declare i32 @pthread_mutex_trylock(ptr noundef) local_unnamed_addr #3

; Function Attrs: mustprogress nocallback nofree nounwind willreturn memory(argmem: write)
declare void @llvm.memset.p0.i64(ptr nocapture writeonly, i8, i64, i1 immarg) #4

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn
declare ptr @llvm.stacksave.p0() #5

; Function Attrs: nounwind uwtable
define internal fastcc void @ctl_arena_refresh(ptr noundef %tsdn, ptr noundef %arena, ptr nocapture noundef %ctl_sdarena, i32 noundef %i, i1 noundef zeroext %destroyed) unnamed_addr #0 {
entry:
  %0 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %0, i64 824
  %1 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %1, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %entry
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %0, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %entry
  %2 = load ptr, ptr @ctl_arenas, align 8
  switch i32 %i, label %sw.default.i.i.i [
    i32 4096, label %arenas_i.exit
    i32 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %2, i64 8
  %3 = load i32, ptr %narenas.i.i.i, align 8
  %cmp.i.i.i = icmp eq i32 %3, %i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %4 = add i32 %i, 2
  %5 = zext i32 %4 to i64
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %5, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %2, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %6 = load ptr, ptr %arrayidx.i.i, align 8
  %nthreads.i = getelementptr inbounds i8, ptr %6, i64 24
  store i32 0, ptr %nthreads.i, align 8
  %7 = load ptr, ptr getelementptr inbounds ([0 x ptr], ptr @dss_prec_names, i64 0, i64 3), align 8
  %dss.i = getelementptr inbounds i8, ptr %6, i64 32
  store ptr %7, ptr %dss.i, align 8
  %dirty_decay_ms.i = getelementptr inbounds i8, ptr %6, i64 40
  %pactive.i = getelementptr inbounds i8, ptr %6, i64 56
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) %dirty_decay_ms.i, i8 -1, i64 16, i1 false)
  %astats.i = getelementptr inbounds i8, ptr %6, i64 80
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) %pactive.i, i8 0, i64 24, i1 false)
  %8 = load ptr, ptr %astats.i, align 8
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(37784) %8, i8 0, i64 37784, i1 false)
  %muzzy_decay_ms.i = getelementptr inbounds i8, ptr %6, i64 48
  %pdirty.i = getelementptr inbounds i8, ptr %6, i64 64
  %pmuzzy.i = getelementptr inbounds i8, ptr %6, i64 72
  %9 = load ptr, ptr %astats.i, align 8
  %bstats.i = getelementptr inbounds i8, ptr %9, i64 10432
  %lstats.i = getelementptr inbounds i8, ptr %9, i64 15616
  %estats.i = getelementptr inbounds i8, ptr %9, i64 25024
  %hpastats.i = getelementptr inbounds i8, ptr %9, i64 34576
  %secstats.i = getelementptr inbounds i8, ptr %9, i64 37776
  tail call void @arena_stats_merge(ptr noundef %tsdn, ptr noundef %arena, ptr noundef nonnull %nthreads.i, ptr noundef nonnull %dss.i, ptr noundef nonnull %dirty_decay_ms.i, ptr noundef nonnull %muzzy_decay_ms.i, ptr noundef nonnull %pactive.i, ptr noundef nonnull %pdirty.i, ptr noundef nonnull %pmuzzy.i, ptr noundef %9, ptr noundef nonnull %bstats.i, ptr noundef nonnull %lstats.i, ptr noundef nonnull %estats.i, ptr noundef nonnull %hpastats.i, ptr noundef nonnull %secstats.i) #14
  br label %for.body.i

for.body.i:                                       ; preds = %for.body.i, %arenas_i.exit
  %indvars.iv.i = phi i64 [ 0, %arenas_i.exit ], [ %indvars.iv.next.i, %for.body.i ]
  %10 = load ptr, ptr %astats.i, align 8
  %bstats12.i = getelementptr inbounds i8, ptr %10, i64 10432
  %arrayidx.i = getelementptr inbounds [36 x %struct.bin_stats_data_s], ptr %bstats12.i, i64 0, i64 %indvars.iv.i
  %curregs.i = getelementptr inbounds i8, ptr %arrayidx.i, i64 24
  %11 = load i64, ptr %curregs.i, align 8
  %arrayidx.i.i8 = getelementptr inbounds [232 x i64], ptr @sz_index2size_tab, i64 0, i64 %indvars.iv.i
  %12 = load i64, ptr %arrayidx.i.i8, align 8
  %mul.i = mul i64 %12, %11
  %allocated_small.i = getelementptr inbounds i8, ptr %10, i64 10384
  %13 = load i64, ptr %allocated_small.i, align 8
  %add.i = add i64 %13, %mul.i
  store i64 %add.i, ptr %allocated_small.i, align 8
  %14 = load i64, ptr %arrayidx.i, align 8
  %15 = load ptr, ptr %astats.i, align 8
  %nmalloc_small.i = getelementptr inbounds i8, ptr %15, i64 10392
  %16 = load i64, ptr %nmalloc_small.i, align 8
  %add15.i = add i64 %16, %14
  store i64 %add15.i, ptr %nmalloc_small.i, align 8
  %ndalloc.i = getelementptr inbounds i8, ptr %arrayidx.i, i64 8
  %17 = load i64, ptr %ndalloc.i, align 8
  %18 = load ptr, ptr %astats.i, align 8
  %ndalloc_small.i = getelementptr inbounds i8, ptr %18, i64 10400
  %19 = load i64, ptr %ndalloc_small.i, align 8
  %add17.i = add i64 %19, %17
  store i64 %add17.i, ptr %ndalloc_small.i, align 8
  %nrequests.i = getelementptr inbounds i8, ptr %arrayidx.i, i64 16
  %20 = load i64, ptr %nrequests.i, align 8
  %21 = load ptr, ptr %astats.i, align 8
  %nrequests_small.i = getelementptr inbounds i8, ptr %21, i64 10408
  %22 = load i64, ptr %nrequests_small.i, align 8
  %add19.i = add i64 %22, %20
  store i64 %add19.i, ptr %nrequests_small.i, align 8
  %nfills.i = getelementptr inbounds i8, ptr %arrayidx.i, i64 32
  %23 = load i64, ptr %nfills.i, align 8
  %24 = load ptr, ptr %astats.i, align 8
  %nfills_small.i = getelementptr inbounds i8, ptr %24, i64 10416
  %25 = load i64, ptr %nfills_small.i, align 8
  %add21.i = add i64 %25, %23
  store i64 %add21.i, ptr %nfills_small.i, align 8
  %nflushes.i = getelementptr inbounds i8, ptr %arrayidx.i, i64 40
  %26 = load i64, ptr %nflushes.i, align 8
  %27 = load ptr, ptr %astats.i, align 8
  %nflushes_small.i = getelementptr inbounds i8, ptr %27, i64 10424
  %28 = load i64, ptr %nflushes_small.i, align 8
  %add23.i = add i64 %28, %26
  store i64 %add23.i, ptr %nflushes_small.i, align 8
  %indvars.iv.next.i = add nuw nsw i64 %indvars.iv.i, 1
  %exitcond.not.i = icmp eq i64 %indvars.iv.next.i, 36
  br i1 %exitcond.not.i, label %ctl_arena_stats_amerge.exit, label %for.body.i, !llvm.loop !13

ctl_arena_stats_amerge.exit:                      ; preds = %for.body.i
  br i1 %destroyed, label %if.end.thread.i, label %if.then17.i

if.end.thread.i:                                  ; preds = %ctl_arena_stats_amerge.exit
  %astats497.i = getelementptr inbounds i8, ptr %ctl_sdarena, i64 80
  %29 = load ptr, ptr %astats497.i, align 8
  %30 = load ptr, ptr %astats.i, align 8
  br label %if.end34.i

if.then17.i:                                      ; preds = %ctl_arena_stats_amerge.exit
  %31 = load i32, ptr %nthreads.i, align 8
  %nthreads1.i = getelementptr inbounds i8, ptr %ctl_sdarena, i64 24
  %32 = load i32, ptr %nthreads1.i, align 8
  %add.i10 = add i32 %32, %31
  store i32 %add.i10, ptr %nthreads1.i, align 8
  %33 = load i64, ptr %pactive.i, align 8
  %pactive2.i = getelementptr inbounds i8, ptr %ctl_sdarena, i64 56
  %34 = load i64, ptr %pactive2.i, align 8
  %add3.i = add i64 %34, %33
  store i64 %add3.i, ptr %pactive2.i, align 8
  %35 = load i64, ptr %pdirty.i, align 8
  %pdirty4.i = getelementptr inbounds i8, ptr %ctl_sdarena, i64 64
  %36 = load i64, ptr %pdirty4.i, align 8
  %add5.i = add i64 %36, %35
  store i64 %add5.i, ptr %pdirty4.i, align 8
  %37 = load i64, ptr %pmuzzy.i, align 8
  %pmuzzy6.i = getelementptr inbounds i8, ptr %ctl_sdarena, i64 72
  %38 = load i64, ptr %pmuzzy6.i, align 8
  %add7.i = add i64 %38, %37
  store i64 %add7.i, ptr %pmuzzy6.i, align 8
  %astats.i14 = getelementptr inbounds i8, ptr %ctl_sdarena, i64 80
  %39 = load ptr, ptr %astats.i14, align 8
  %40 = load ptr, ptr %astats.i, align 8
  %mapped.i = getelementptr inbounds i8, ptr %40, i64 40
  %41 = load i64, ptr %mapped.i, align 8
  %mapped20.i = getelementptr inbounds i8, ptr %39, i64 40
  %42 = load i64, ptr %mapped20.i, align 8
  %add21.i15 = add i64 %42, %41
  store i64 %add21.i15, ptr %mapped20.i, align 8
  %pa_shard_stats.i = getelementptr inbounds i8, ptr %40, i64 104
  %retained.i = getelementptr inbounds i8, ptr %40, i64 160
  %43 = load i64, ptr %retained.i, align 8
  %pa_shard_stats24.i = getelementptr inbounds i8, ptr %39, i64 104
  %retained26.i = getelementptr inbounds i8, ptr %39, i64 160
  %44 = load i64, ptr %retained26.i, align 8
  %add27.i = add i64 %44, %43
  store i64 %add27.i, ptr %retained26.i, align 8
  %45 = load i64, ptr %pa_shard_stats.i, align 8
  %46 = load i64, ptr %pa_shard_stats24.i, align 8
  %add33.i = add i64 %46, %45
  store i64 %add33.i, ptr %pa_shard_stats24.i, align 8
  br label %if.end34.i

if.end34.i:                                       ; preds = %if.then17.i, %if.end.thread.i
  %47 = phi ptr [ %30, %if.end.thread.i ], [ %40, %if.then17.i ]
  %48 = phi ptr [ %29, %if.end.thread.i ], [ %39, %if.then17.i ]
  %pac_stats37.i = getelementptr inbounds i8, ptr %48, i64 112
  %pac_stats40.i = getelementptr inbounds i8, ptr %47, i64 112
  %49 = load atomic i64, ptr %pac_stats40.i monotonic, align 8
  %50 = load atomic i64, ptr %pac_stats37.i monotonic, align 8
  %add.i.i.i16 = add i64 %50, %49
  store atomic i64 %add.i.i.i16, ptr %pac_stats37.i monotonic, align 8
  %nmadvise.i = getelementptr inbounds i8, ptr %48, i64 120
  %nmadvise51.i = getelementptr inbounds i8, ptr %47, i64 120
  %51 = load atomic i64, ptr %nmadvise51.i monotonic, align 8
  %52 = load atomic i64, ptr %nmadvise.i monotonic, align 8
  %add.i.i174.i = add i64 %52, %51
  store atomic i64 %add.i.i174.i, ptr %nmadvise.i monotonic, align 8
  %purged.i = getelementptr inbounds i8, ptr %48, i64 128
  %purged60.i = getelementptr inbounds i8, ptr %47, i64 128
  %53 = load atomic i64, ptr %purged60.i monotonic, align 8
  %54 = load atomic i64, ptr %purged.i monotonic, align 8
  %add.i.i175.i = add i64 %54, %53
  store atomic i64 %add.i.i175.i, ptr %purged.i monotonic, align 8
  %decay_muzzy.i = getelementptr inbounds i8, ptr %48, i64 136
  %decay_muzzy68.i = getelementptr inbounds i8, ptr %47, i64 136
  %55 = load atomic i64, ptr %decay_muzzy68.i monotonic, align 8
  %56 = load atomic i64, ptr %decay_muzzy.i monotonic, align 8
  %add.i.i176.i = add i64 %56, %55
  store atomic i64 %add.i.i176.i, ptr %decay_muzzy.i monotonic, align 8
  %nmadvise74.i = getelementptr inbounds i8, ptr %48, i64 144
  %nmadvise79.i = getelementptr inbounds i8, ptr %47, i64 144
  %57 = load atomic i64, ptr %nmadvise79.i monotonic, align 8
  %58 = load atomic i64, ptr %nmadvise74.i monotonic, align 8
  %add.i.i177.i = add i64 %58, %57
  store atomic i64 %add.i.i177.i, ptr %nmadvise74.i monotonic, align 8
  %purged84.i = getelementptr inbounds i8, ptr %48, i64 152
  %purged89.i = getelementptr inbounds i8, ptr %47, i64 152
  %59 = load atomic i64, ptr %purged89.i monotonic, align 8
  %60 = load atomic i64, ptr %purged84.i monotonic, align 8
  %add.i.i178.i = add i64 %60, %59
  store atomic i64 %add.i.i178.i, ptr %purged84.i monotonic, align 8
  %mutex_prof_data.i = getelementptr inbounds i8, ptr %48, i64 200
  %mutex_prof_data92.i = getelementptr inbounds i8, ptr %47, i64 200
  tail call void @nstime_add(ptr noundef nonnull %mutex_prof_data.i, ptr noundef nonnull %mutex_prof_data92.i) #14
  %max_wait_time.i.i = getelementptr inbounds i8, ptr %48, i64 208
  %max_wait_time2.i.i = getelementptr inbounds i8, ptr %47, i64 208
  %call.i.i = tail call i32 @nstime_compare(ptr noundef nonnull %max_wait_time.i.i, ptr noundef nonnull %max_wait_time2.i.i) #14
  %cmp.i.i = icmp slt i32 %call.i.i, 0
  br i1 %cmp.i.i, label %if.then.i.i, label %if.end.i.i

if.then.i.i:                                      ; preds = %if.end34.i
  tail call void @nstime_copy(ptr noundef nonnull %max_wait_time.i.i, ptr noundef nonnull %max_wait_time2.i.i) #14
  br label %if.end.i.i

if.end.i.i:                                       ; preds = %if.then.i.i, %if.end34.i
  %n_wait_times.i.i = getelementptr inbounds i8, ptr %47, i64 216
  %61 = load i64, ptr %n_wait_times.i.i, align 8
  %n_wait_times5.i.i = getelementptr inbounds i8, ptr %48, i64 216
  %62 = load i64, ptr %n_wait_times5.i.i, align 8
  %add.i.i = add i64 %62, %61
  store i64 %add.i.i, ptr %n_wait_times5.i.i, align 8
  %n_spin_acquired.i.i = getelementptr inbounds i8, ptr %47, i64 224
  %63 = load i64, ptr %n_spin_acquired.i.i, align 8
  %n_spin_acquired6.i.i = getelementptr inbounds i8, ptr %48, i64 224
  %64 = load i64, ptr %n_spin_acquired6.i.i, align 8
  %add7.i.i = add i64 %64, %63
  store i64 %add7.i.i, ptr %n_spin_acquired6.i.i, align 8
  %max_n_thds.i.i = getelementptr inbounds i8, ptr %48, i64 232
  %65 = load i32, ptr %max_n_thds.i.i, align 8
  %max_n_thds8.i.i = getelementptr inbounds i8, ptr %47, i64 232
  %66 = load i32, ptr %max_n_thds8.i.i, align 8
  %cmp9.i.i = icmp ult i32 %65, %66
  br i1 %cmp9.i.i, label %if.then10.i.i, label %malloc_mutex_prof_merge.exit.i

if.then10.i.i:                                    ; preds = %if.end.i.i
  store i32 %66, ptr %max_n_thds.i.i, align 8
  br label %malloc_mutex_prof_merge.exit.i

malloc_mutex_prof_merge.exit.i:                   ; preds = %if.then10.i.i, %if.end.i.i
  %n_waiting_thds.i.i = getelementptr inbounds i8, ptr %48, i64 236
  %67 = load atomic i32, ptr %n_waiting_thds.i.i monotonic, align 4
  %n_waiting_thds15.i.i = getelementptr inbounds i8, ptr %47, i64 236
  %68 = load atomic i32, ptr %n_waiting_thds15.i.i monotonic, align 4
  %add17.i.i = add i32 %68, %67
  store atomic i32 %add17.i.i, ptr %n_waiting_thds.i.i monotonic, align 4
  %n_owner_switches.i.i = getelementptr inbounds i8, ptr %47, i64 240
  %69 = load i64, ptr %n_owner_switches.i.i, align 8
  %n_owner_switches19.i.i = getelementptr inbounds i8, ptr %48, i64 240
  %70 = load i64, ptr %n_owner_switches19.i.i, align 8
  %add20.i.i = add i64 %70, %69
  store i64 %add20.i.i, ptr %n_owner_switches19.i.i, align 8
  %n_lock_ops.i.i = getelementptr inbounds i8, ptr %47, i64 256
  %71 = load i64, ptr %n_lock_ops.i.i, align 8
  %n_lock_ops21.i.i = getelementptr inbounds i8, ptr %48, i64 256
  %72 = load i64, ptr %n_lock_ops21.i.i, align 8
  %add22.i.i = add i64 %72, %71
  store i64 %add22.i.i, ptr %n_lock_ops21.i.i, align 8
  %arrayidx96.i = getelementptr inbounds i8, ptr %48, i64 264
  %arrayidx99.i = getelementptr inbounds i8, ptr %47, i64 264
  tail call void @nstime_add(ptr noundef nonnull %arrayidx96.i, ptr noundef nonnull %arrayidx99.i) #14
  %max_wait_time.i179.i = getelementptr inbounds i8, ptr %48, i64 272
  %max_wait_time2.i180.i = getelementptr inbounds i8, ptr %47, i64 272
  %call.i181.i = tail call i32 @nstime_compare(ptr noundef nonnull %max_wait_time.i179.i, ptr noundef nonnull %max_wait_time2.i180.i) #14
  %cmp.i182.i = icmp slt i32 %call.i181.i, 0
  br i1 %cmp.i182.i, label %if.then.i203.i, label %if.end.i183.i

if.then.i203.i:                                   ; preds = %malloc_mutex_prof_merge.exit.i
  tail call void @nstime_copy(ptr noundef nonnull %max_wait_time.i179.i, ptr noundef nonnull %max_wait_time2.i180.i) #14
  br label %if.end.i183.i

if.end.i183.i:                                    ; preds = %if.then.i203.i, %malloc_mutex_prof_merge.exit.i
  %n_wait_times.i184.i = getelementptr inbounds i8, ptr %47, i64 280
  %73 = load i64, ptr %n_wait_times.i184.i, align 8
  %n_wait_times5.i185.i = getelementptr inbounds i8, ptr %48, i64 280
  %74 = load i64, ptr %n_wait_times5.i185.i, align 8
  %add.i186.i = add i64 %74, %73
  store i64 %add.i186.i, ptr %n_wait_times5.i185.i, align 8
  %n_spin_acquired.i187.i = getelementptr inbounds i8, ptr %47, i64 288
  %75 = load i64, ptr %n_spin_acquired.i187.i, align 8
  %n_spin_acquired6.i188.i = getelementptr inbounds i8, ptr %48, i64 288
  %76 = load i64, ptr %n_spin_acquired6.i188.i, align 8
  %add7.i189.i = add i64 %76, %75
  store i64 %add7.i189.i, ptr %n_spin_acquired6.i188.i, align 8
  %max_n_thds.i190.i = getelementptr inbounds i8, ptr %48, i64 296
  %77 = load i32, ptr %max_n_thds.i190.i, align 8
  %max_n_thds8.i191.i = getelementptr inbounds i8, ptr %47, i64 296
  %78 = load i32, ptr %max_n_thds8.i191.i, align 8
  %cmp9.i192.i = icmp ult i32 %77, %78
  br i1 %cmp9.i192.i, label %if.then10.i202.i, label %malloc_mutex_prof_merge.exit204.i

if.then10.i202.i:                                 ; preds = %if.end.i183.i
  store i32 %78, ptr %max_n_thds.i190.i, align 8
  br label %malloc_mutex_prof_merge.exit204.i

malloc_mutex_prof_merge.exit204.i:                ; preds = %if.then10.i202.i, %if.end.i183.i
  %n_waiting_thds.i193.i = getelementptr inbounds i8, ptr %48, i64 300
  %79 = load atomic i32, ptr %n_waiting_thds.i193.i monotonic, align 4
  %n_waiting_thds15.i194.i = getelementptr inbounds i8, ptr %47, i64 300
  %80 = load atomic i32, ptr %n_waiting_thds15.i194.i monotonic, align 4
  %add17.i195.i = add i32 %80, %79
  store atomic i32 %add17.i195.i, ptr %n_waiting_thds.i193.i monotonic, align 4
  %n_owner_switches.i196.i = getelementptr inbounds i8, ptr %47, i64 304
  %81 = load i64, ptr %n_owner_switches.i196.i, align 8
  %n_owner_switches19.i197.i = getelementptr inbounds i8, ptr %48, i64 304
  %82 = load i64, ptr %n_owner_switches19.i197.i, align 8
  %add20.i198.i = add i64 %82, %81
  store i64 %add20.i198.i, ptr %n_owner_switches19.i197.i, align 8
  %n_lock_ops.i199.i = getelementptr inbounds i8, ptr %47, i64 320
  %83 = load i64, ptr %n_lock_ops.i199.i, align 8
  %n_lock_ops21.i200.i = getelementptr inbounds i8, ptr %48, i64 320
  %84 = load i64, ptr %n_lock_ops21.i200.i, align 8
  %add22.i201.i = add i64 %84, %83
  store i64 %add22.i201.i, ptr %n_lock_ops21.i200.i, align 8
  %arrayidx102.i = getelementptr inbounds i8, ptr %48, i64 328
  %arrayidx105.i = getelementptr inbounds i8, ptr %47, i64 328
  tail call void @nstime_add(ptr noundef nonnull %arrayidx102.i, ptr noundef nonnull %arrayidx105.i) #14
  %max_wait_time.i205.i = getelementptr inbounds i8, ptr %48, i64 336
  %max_wait_time2.i206.i = getelementptr inbounds i8, ptr %47, i64 336
  %call.i207.i = tail call i32 @nstime_compare(ptr noundef nonnull %max_wait_time.i205.i, ptr noundef nonnull %max_wait_time2.i206.i) #14
  %cmp.i208.i = icmp slt i32 %call.i207.i, 0
  br i1 %cmp.i208.i, label %if.then.i229.i, label %if.end.i209.i

if.then.i229.i:                                   ; preds = %malloc_mutex_prof_merge.exit204.i
  tail call void @nstime_copy(ptr noundef nonnull %max_wait_time.i205.i, ptr noundef nonnull %max_wait_time2.i206.i) #14
  br label %if.end.i209.i

if.end.i209.i:                                    ; preds = %if.then.i229.i, %malloc_mutex_prof_merge.exit204.i
  %n_wait_times.i210.i = getelementptr inbounds i8, ptr %47, i64 344
  %85 = load i64, ptr %n_wait_times.i210.i, align 8
  %n_wait_times5.i211.i = getelementptr inbounds i8, ptr %48, i64 344
  %86 = load i64, ptr %n_wait_times5.i211.i, align 8
  %add.i212.i = add i64 %86, %85
  store i64 %add.i212.i, ptr %n_wait_times5.i211.i, align 8
  %n_spin_acquired.i213.i = getelementptr inbounds i8, ptr %47, i64 352
  %87 = load i64, ptr %n_spin_acquired.i213.i, align 8
  %n_spin_acquired6.i214.i = getelementptr inbounds i8, ptr %48, i64 352
  %88 = load i64, ptr %n_spin_acquired6.i214.i, align 8
  %add7.i215.i = add i64 %88, %87
  store i64 %add7.i215.i, ptr %n_spin_acquired6.i214.i, align 8
  %max_n_thds.i216.i = getelementptr inbounds i8, ptr %48, i64 360
  %89 = load i32, ptr %max_n_thds.i216.i, align 8
  %max_n_thds8.i217.i = getelementptr inbounds i8, ptr %47, i64 360
  %90 = load i32, ptr %max_n_thds8.i217.i, align 8
  %cmp9.i218.i = icmp ult i32 %89, %90
  br i1 %cmp9.i218.i, label %if.then10.i228.i, label %malloc_mutex_prof_merge.exit230.i

if.then10.i228.i:                                 ; preds = %if.end.i209.i
  store i32 %90, ptr %max_n_thds.i216.i, align 8
  br label %malloc_mutex_prof_merge.exit230.i

malloc_mutex_prof_merge.exit230.i:                ; preds = %if.then10.i228.i, %if.end.i209.i
  %n_waiting_thds.i219.i = getelementptr inbounds i8, ptr %48, i64 364
  %91 = load atomic i32, ptr %n_waiting_thds.i219.i monotonic, align 4
  %n_waiting_thds15.i220.i = getelementptr inbounds i8, ptr %47, i64 364
  %92 = load atomic i32, ptr %n_waiting_thds15.i220.i monotonic, align 4
  %add17.i221.i = add i32 %92, %91
  store atomic i32 %add17.i221.i, ptr %n_waiting_thds.i219.i monotonic, align 4
  %n_owner_switches.i222.i = getelementptr inbounds i8, ptr %47, i64 368
  %93 = load i64, ptr %n_owner_switches.i222.i, align 8
  %n_owner_switches19.i223.i = getelementptr inbounds i8, ptr %48, i64 368
  %94 = load i64, ptr %n_owner_switches19.i223.i, align 8
  %add20.i224.i = add i64 %94, %93
  store i64 %add20.i224.i, ptr %n_owner_switches19.i223.i, align 8
  %n_lock_ops.i225.i = getelementptr inbounds i8, ptr %47, i64 384
  %95 = load i64, ptr %n_lock_ops.i225.i, align 8
  %n_lock_ops21.i226.i = getelementptr inbounds i8, ptr %48, i64 384
  %96 = load i64, ptr %n_lock_ops21.i226.i, align 8
  %add22.i227.i = add i64 %96, %95
  store i64 %add22.i227.i, ptr %n_lock_ops21.i226.i, align 8
  %arrayidx108.i = getelementptr inbounds i8, ptr %48, i64 392
  %arrayidx111.i = getelementptr inbounds i8, ptr %47, i64 392
  tail call void @nstime_add(ptr noundef nonnull %arrayidx108.i, ptr noundef nonnull %arrayidx111.i) #14
  %max_wait_time.i231.i = getelementptr inbounds i8, ptr %48, i64 400
  %max_wait_time2.i232.i = getelementptr inbounds i8, ptr %47, i64 400
  %call.i233.i = tail call i32 @nstime_compare(ptr noundef nonnull %max_wait_time.i231.i, ptr noundef nonnull %max_wait_time2.i232.i) #14
  %cmp.i234.i = icmp slt i32 %call.i233.i, 0
  br i1 %cmp.i234.i, label %if.then.i255.i, label %if.end.i235.i

if.then.i255.i:                                   ; preds = %malloc_mutex_prof_merge.exit230.i
  tail call void @nstime_copy(ptr noundef nonnull %max_wait_time.i231.i, ptr noundef nonnull %max_wait_time2.i232.i) #14
  br label %if.end.i235.i

if.end.i235.i:                                    ; preds = %if.then.i255.i, %malloc_mutex_prof_merge.exit230.i
  %n_wait_times.i236.i = getelementptr inbounds i8, ptr %47, i64 408
  %97 = load i64, ptr %n_wait_times.i236.i, align 8
  %n_wait_times5.i237.i = getelementptr inbounds i8, ptr %48, i64 408
  %98 = load i64, ptr %n_wait_times5.i237.i, align 8
  %add.i238.i = add i64 %98, %97
  store i64 %add.i238.i, ptr %n_wait_times5.i237.i, align 8
  %n_spin_acquired.i239.i = getelementptr inbounds i8, ptr %47, i64 416
  %99 = load i64, ptr %n_spin_acquired.i239.i, align 8
  %n_spin_acquired6.i240.i = getelementptr inbounds i8, ptr %48, i64 416
  %100 = load i64, ptr %n_spin_acquired6.i240.i, align 8
  %add7.i241.i = add i64 %100, %99
  store i64 %add7.i241.i, ptr %n_spin_acquired6.i240.i, align 8
  %max_n_thds.i242.i = getelementptr inbounds i8, ptr %48, i64 424
  %101 = load i32, ptr %max_n_thds.i242.i, align 8
  %max_n_thds8.i243.i = getelementptr inbounds i8, ptr %47, i64 424
  %102 = load i32, ptr %max_n_thds8.i243.i, align 8
  %cmp9.i244.i = icmp ult i32 %101, %102
  br i1 %cmp9.i244.i, label %if.then10.i254.i, label %malloc_mutex_prof_merge.exit256.i

if.then10.i254.i:                                 ; preds = %if.end.i235.i
  store i32 %102, ptr %max_n_thds.i242.i, align 8
  br label %malloc_mutex_prof_merge.exit256.i

malloc_mutex_prof_merge.exit256.i:                ; preds = %if.then10.i254.i, %if.end.i235.i
  %n_waiting_thds.i245.i = getelementptr inbounds i8, ptr %48, i64 428
  %103 = load atomic i32, ptr %n_waiting_thds.i245.i monotonic, align 4
  %n_waiting_thds15.i246.i = getelementptr inbounds i8, ptr %47, i64 428
  %104 = load atomic i32, ptr %n_waiting_thds15.i246.i monotonic, align 4
  %add17.i247.i = add i32 %104, %103
  store atomic i32 %add17.i247.i, ptr %n_waiting_thds.i245.i monotonic, align 4
  %n_owner_switches.i248.i = getelementptr inbounds i8, ptr %47, i64 432
  %105 = load i64, ptr %n_owner_switches.i248.i, align 8
  %n_owner_switches19.i249.i = getelementptr inbounds i8, ptr %48, i64 432
  %106 = load i64, ptr %n_owner_switches19.i249.i, align 8
  %add20.i250.i = add i64 %106, %105
  store i64 %add20.i250.i, ptr %n_owner_switches19.i249.i, align 8
  %n_lock_ops.i251.i = getelementptr inbounds i8, ptr %47, i64 448
  %107 = load i64, ptr %n_lock_ops.i251.i, align 8
  %n_lock_ops21.i252.i = getelementptr inbounds i8, ptr %48, i64 448
  %108 = load i64, ptr %n_lock_ops21.i252.i, align 8
  %add22.i253.i = add i64 %108, %107
  store i64 %add22.i253.i, ptr %n_lock_ops21.i252.i, align 8
  %arrayidx114.i = getelementptr inbounds i8, ptr %48, i64 456
  %arrayidx117.i = getelementptr inbounds i8, ptr %47, i64 456
  tail call void @nstime_add(ptr noundef nonnull %arrayidx114.i, ptr noundef nonnull %arrayidx117.i) #14
  %max_wait_time.i257.i = getelementptr inbounds i8, ptr %48, i64 464
  %max_wait_time2.i258.i = getelementptr inbounds i8, ptr %47, i64 464
  %call.i259.i = tail call i32 @nstime_compare(ptr noundef nonnull %max_wait_time.i257.i, ptr noundef nonnull %max_wait_time2.i258.i) #14
  %cmp.i260.i = icmp slt i32 %call.i259.i, 0
  br i1 %cmp.i260.i, label %if.then.i281.i, label %if.end.i261.i

if.then.i281.i:                                   ; preds = %malloc_mutex_prof_merge.exit256.i
  tail call void @nstime_copy(ptr noundef nonnull %max_wait_time.i257.i, ptr noundef nonnull %max_wait_time2.i258.i) #14
  br label %if.end.i261.i

if.end.i261.i:                                    ; preds = %if.then.i281.i, %malloc_mutex_prof_merge.exit256.i
  %n_wait_times.i262.i = getelementptr inbounds i8, ptr %47, i64 472
  %109 = load i64, ptr %n_wait_times.i262.i, align 8
  %n_wait_times5.i263.i = getelementptr inbounds i8, ptr %48, i64 472
  %110 = load i64, ptr %n_wait_times5.i263.i, align 8
  %add.i264.i = add i64 %110, %109
  store i64 %add.i264.i, ptr %n_wait_times5.i263.i, align 8
  %n_spin_acquired.i265.i = getelementptr inbounds i8, ptr %47, i64 480
  %111 = load i64, ptr %n_spin_acquired.i265.i, align 8
  %n_spin_acquired6.i266.i = getelementptr inbounds i8, ptr %48, i64 480
  %112 = load i64, ptr %n_spin_acquired6.i266.i, align 8
  %add7.i267.i = add i64 %112, %111
  store i64 %add7.i267.i, ptr %n_spin_acquired6.i266.i, align 8
  %max_n_thds.i268.i = getelementptr inbounds i8, ptr %48, i64 488
  %113 = load i32, ptr %max_n_thds.i268.i, align 8
  %max_n_thds8.i269.i = getelementptr inbounds i8, ptr %47, i64 488
  %114 = load i32, ptr %max_n_thds8.i269.i, align 8
  %cmp9.i270.i = icmp ult i32 %113, %114
  br i1 %cmp9.i270.i, label %if.then10.i280.i, label %malloc_mutex_prof_merge.exit282.i

if.then10.i280.i:                                 ; preds = %if.end.i261.i
  store i32 %114, ptr %max_n_thds.i268.i, align 8
  br label %malloc_mutex_prof_merge.exit282.i

malloc_mutex_prof_merge.exit282.i:                ; preds = %if.then10.i280.i, %if.end.i261.i
  %n_waiting_thds.i271.i = getelementptr inbounds i8, ptr %48, i64 492
  %115 = load atomic i32, ptr %n_waiting_thds.i271.i monotonic, align 4
  %n_waiting_thds15.i272.i = getelementptr inbounds i8, ptr %47, i64 492
  %116 = load atomic i32, ptr %n_waiting_thds15.i272.i monotonic, align 4
  %add17.i273.i = add i32 %116, %115
  store atomic i32 %add17.i273.i, ptr %n_waiting_thds.i271.i monotonic, align 4
  %n_owner_switches.i274.i = getelementptr inbounds i8, ptr %47, i64 496
  %117 = load i64, ptr %n_owner_switches.i274.i, align 8
  %n_owner_switches19.i275.i = getelementptr inbounds i8, ptr %48, i64 496
  %118 = load i64, ptr %n_owner_switches19.i275.i, align 8
  %add20.i276.i = add i64 %118, %117
  store i64 %add20.i276.i, ptr %n_owner_switches19.i275.i, align 8
  %n_lock_ops.i277.i = getelementptr inbounds i8, ptr %47, i64 512
  %119 = load i64, ptr %n_lock_ops.i277.i, align 8
  %n_lock_ops21.i278.i = getelementptr inbounds i8, ptr %48, i64 512
  %120 = load i64, ptr %n_lock_ops21.i278.i, align 8
  %add22.i279.i = add i64 %120, %119
  store i64 %add22.i279.i, ptr %n_lock_ops21.i278.i, align 8
  %arrayidx120.i = getelementptr inbounds i8, ptr %48, i64 520
  %arrayidx123.i = getelementptr inbounds i8, ptr %47, i64 520
  tail call void @nstime_add(ptr noundef nonnull %arrayidx120.i, ptr noundef nonnull %arrayidx123.i) #14
  %max_wait_time.i283.i = getelementptr inbounds i8, ptr %48, i64 528
  %max_wait_time2.i284.i = getelementptr inbounds i8, ptr %47, i64 528
  %call.i285.i = tail call i32 @nstime_compare(ptr noundef nonnull %max_wait_time.i283.i, ptr noundef nonnull %max_wait_time2.i284.i) #14
  %cmp.i286.i = icmp slt i32 %call.i285.i, 0
  br i1 %cmp.i286.i, label %if.then.i307.i, label %if.end.i287.i

if.then.i307.i:                                   ; preds = %malloc_mutex_prof_merge.exit282.i
  tail call void @nstime_copy(ptr noundef nonnull %max_wait_time.i283.i, ptr noundef nonnull %max_wait_time2.i284.i) #14
  br label %if.end.i287.i

if.end.i287.i:                                    ; preds = %if.then.i307.i, %malloc_mutex_prof_merge.exit282.i
  %n_wait_times.i288.i = getelementptr inbounds i8, ptr %47, i64 536
  %121 = load i64, ptr %n_wait_times.i288.i, align 8
  %n_wait_times5.i289.i = getelementptr inbounds i8, ptr %48, i64 536
  %122 = load i64, ptr %n_wait_times5.i289.i, align 8
  %add.i290.i = add i64 %122, %121
  store i64 %add.i290.i, ptr %n_wait_times5.i289.i, align 8
  %n_spin_acquired.i291.i = getelementptr inbounds i8, ptr %47, i64 544
  %123 = load i64, ptr %n_spin_acquired.i291.i, align 8
  %n_spin_acquired6.i292.i = getelementptr inbounds i8, ptr %48, i64 544
  %124 = load i64, ptr %n_spin_acquired6.i292.i, align 8
  %add7.i293.i = add i64 %124, %123
  store i64 %add7.i293.i, ptr %n_spin_acquired6.i292.i, align 8
  %max_n_thds.i294.i = getelementptr inbounds i8, ptr %48, i64 552
  %125 = load i32, ptr %max_n_thds.i294.i, align 8
  %max_n_thds8.i295.i = getelementptr inbounds i8, ptr %47, i64 552
  %126 = load i32, ptr %max_n_thds8.i295.i, align 8
  %cmp9.i296.i = icmp ult i32 %125, %126
  br i1 %cmp9.i296.i, label %if.then10.i306.i, label %malloc_mutex_prof_merge.exit308.i

if.then10.i306.i:                                 ; preds = %if.end.i287.i
  store i32 %126, ptr %max_n_thds.i294.i, align 8
  br label %malloc_mutex_prof_merge.exit308.i

malloc_mutex_prof_merge.exit308.i:                ; preds = %if.then10.i306.i, %if.end.i287.i
  %n_waiting_thds.i297.i = getelementptr inbounds i8, ptr %48, i64 556
  %127 = load atomic i32, ptr %n_waiting_thds.i297.i monotonic, align 4
  %n_waiting_thds15.i298.i = getelementptr inbounds i8, ptr %47, i64 556
  %128 = load atomic i32, ptr %n_waiting_thds15.i298.i monotonic, align 4
  %add17.i299.i = add i32 %128, %127
  store atomic i32 %add17.i299.i, ptr %n_waiting_thds.i297.i monotonic, align 4
  %n_owner_switches.i300.i = getelementptr inbounds i8, ptr %47, i64 560
  %129 = load i64, ptr %n_owner_switches.i300.i, align 8
  %n_owner_switches19.i301.i = getelementptr inbounds i8, ptr %48, i64 560
  %130 = load i64, ptr %n_owner_switches19.i301.i, align 8
  %add20.i302.i = add i64 %130, %129
  store i64 %add20.i302.i, ptr %n_owner_switches19.i301.i, align 8
  %n_lock_ops.i303.i = getelementptr inbounds i8, ptr %47, i64 576
  %131 = load i64, ptr %n_lock_ops.i303.i, align 8
  %n_lock_ops21.i304.i = getelementptr inbounds i8, ptr %48, i64 576
  %132 = load i64, ptr %n_lock_ops21.i304.i, align 8
  %add22.i305.i = add i64 %132, %131
  store i64 %add22.i305.i, ptr %n_lock_ops21.i304.i, align 8
  %arrayidx126.i = getelementptr inbounds i8, ptr %48, i64 584
  %arrayidx129.i = getelementptr inbounds i8, ptr %47, i64 584
  tail call void @nstime_add(ptr noundef nonnull %arrayidx126.i, ptr noundef nonnull %arrayidx129.i) #14
  %max_wait_time.i309.i = getelementptr inbounds i8, ptr %48, i64 592
  %max_wait_time2.i310.i = getelementptr inbounds i8, ptr %47, i64 592
  %call.i311.i = tail call i32 @nstime_compare(ptr noundef nonnull %max_wait_time.i309.i, ptr noundef nonnull %max_wait_time2.i310.i) #14
  %cmp.i312.i = icmp slt i32 %call.i311.i, 0
  br i1 %cmp.i312.i, label %if.then.i333.i, label %if.end.i313.i

if.then.i333.i:                                   ; preds = %malloc_mutex_prof_merge.exit308.i
  tail call void @nstime_copy(ptr noundef nonnull %max_wait_time.i309.i, ptr noundef nonnull %max_wait_time2.i310.i) #14
  br label %if.end.i313.i

if.end.i313.i:                                    ; preds = %if.then.i333.i, %malloc_mutex_prof_merge.exit308.i
  %n_wait_times.i314.i = getelementptr inbounds i8, ptr %47, i64 600
  %133 = load i64, ptr %n_wait_times.i314.i, align 8
  %n_wait_times5.i315.i = getelementptr inbounds i8, ptr %48, i64 600
  %134 = load i64, ptr %n_wait_times5.i315.i, align 8
  %add.i316.i = add i64 %134, %133
  store i64 %add.i316.i, ptr %n_wait_times5.i315.i, align 8
  %n_spin_acquired.i317.i = getelementptr inbounds i8, ptr %47, i64 608
  %135 = load i64, ptr %n_spin_acquired.i317.i, align 8
  %n_spin_acquired6.i318.i = getelementptr inbounds i8, ptr %48, i64 608
  %136 = load i64, ptr %n_spin_acquired6.i318.i, align 8
  %add7.i319.i = add i64 %136, %135
  store i64 %add7.i319.i, ptr %n_spin_acquired6.i318.i, align 8
  %max_n_thds.i320.i = getelementptr inbounds i8, ptr %48, i64 616
  %137 = load i32, ptr %max_n_thds.i320.i, align 8
  %max_n_thds8.i321.i = getelementptr inbounds i8, ptr %47, i64 616
  %138 = load i32, ptr %max_n_thds8.i321.i, align 8
  %cmp9.i322.i = icmp ult i32 %137, %138
  br i1 %cmp9.i322.i, label %if.then10.i332.i, label %malloc_mutex_prof_merge.exit334.i

if.then10.i332.i:                                 ; preds = %if.end.i313.i
  store i32 %138, ptr %max_n_thds.i320.i, align 8
  br label %malloc_mutex_prof_merge.exit334.i

malloc_mutex_prof_merge.exit334.i:                ; preds = %if.then10.i332.i, %if.end.i313.i
  %n_waiting_thds.i323.i = getelementptr inbounds i8, ptr %48, i64 620
  %139 = load atomic i32, ptr %n_waiting_thds.i323.i monotonic, align 4
  %n_waiting_thds15.i324.i = getelementptr inbounds i8, ptr %47, i64 620
  %140 = load atomic i32, ptr %n_waiting_thds15.i324.i monotonic, align 4
  %add17.i325.i = add i32 %140, %139
  store atomic i32 %add17.i325.i, ptr %n_waiting_thds.i323.i monotonic, align 4
  %n_owner_switches.i326.i = getelementptr inbounds i8, ptr %47, i64 624
  %141 = load i64, ptr %n_owner_switches.i326.i, align 8
  %n_owner_switches19.i327.i = getelementptr inbounds i8, ptr %48, i64 624
  %142 = load i64, ptr %n_owner_switches19.i327.i, align 8
  %add20.i328.i = add i64 %142, %141
  store i64 %add20.i328.i, ptr %n_owner_switches19.i327.i, align 8
  %n_lock_ops.i329.i = getelementptr inbounds i8, ptr %47, i64 640
  %143 = load i64, ptr %n_lock_ops.i329.i, align 8
  %n_lock_ops21.i330.i = getelementptr inbounds i8, ptr %48, i64 640
  %144 = load i64, ptr %n_lock_ops21.i330.i, align 8
  %add22.i331.i = add i64 %144, %143
  store i64 %add22.i331.i, ptr %n_lock_ops21.i330.i, align 8
  %arrayidx132.i = getelementptr inbounds i8, ptr %48, i64 648
  %arrayidx135.i = getelementptr inbounds i8, ptr %47, i64 648
  tail call void @nstime_add(ptr noundef nonnull %arrayidx132.i, ptr noundef nonnull %arrayidx135.i) #14
  %max_wait_time.i335.i = getelementptr inbounds i8, ptr %48, i64 656
  %max_wait_time2.i336.i = getelementptr inbounds i8, ptr %47, i64 656
  %call.i337.i = tail call i32 @nstime_compare(ptr noundef nonnull %max_wait_time.i335.i, ptr noundef nonnull %max_wait_time2.i336.i) #14
  %cmp.i338.i = icmp slt i32 %call.i337.i, 0
  br i1 %cmp.i338.i, label %if.then.i359.i, label %if.end.i339.i

if.then.i359.i:                                   ; preds = %malloc_mutex_prof_merge.exit334.i
  tail call void @nstime_copy(ptr noundef nonnull %max_wait_time.i335.i, ptr noundef nonnull %max_wait_time2.i336.i) #14
  br label %if.end.i339.i

if.end.i339.i:                                    ; preds = %if.then.i359.i, %malloc_mutex_prof_merge.exit334.i
  %n_wait_times.i340.i = getelementptr inbounds i8, ptr %47, i64 664
  %145 = load i64, ptr %n_wait_times.i340.i, align 8
  %n_wait_times5.i341.i = getelementptr inbounds i8, ptr %48, i64 664
  %146 = load i64, ptr %n_wait_times5.i341.i, align 8
  %add.i342.i = add i64 %146, %145
  store i64 %add.i342.i, ptr %n_wait_times5.i341.i, align 8
  %n_spin_acquired.i343.i = getelementptr inbounds i8, ptr %47, i64 672
  %147 = load i64, ptr %n_spin_acquired.i343.i, align 8
  %n_spin_acquired6.i344.i = getelementptr inbounds i8, ptr %48, i64 672
  %148 = load i64, ptr %n_spin_acquired6.i344.i, align 8
  %add7.i345.i = add i64 %148, %147
  store i64 %add7.i345.i, ptr %n_spin_acquired6.i344.i, align 8
  %max_n_thds.i346.i = getelementptr inbounds i8, ptr %48, i64 680
  %149 = load i32, ptr %max_n_thds.i346.i, align 8
  %max_n_thds8.i347.i = getelementptr inbounds i8, ptr %47, i64 680
  %150 = load i32, ptr %max_n_thds8.i347.i, align 8
  %cmp9.i348.i = icmp ult i32 %149, %150
  br i1 %cmp9.i348.i, label %if.then10.i358.i, label %malloc_mutex_prof_merge.exit360.i

if.then10.i358.i:                                 ; preds = %if.end.i339.i
  store i32 %150, ptr %max_n_thds.i346.i, align 8
  br label %malloc_mutex_prof_merge.exit360.i

malloc_mutex_prof_merge.exit360.i:                ; preds = %if.then10.i358.i, %if.end.i339.i
  %n_waiting_thds.i349.i = getelementptr inbounds i8, ptr %48, i64 684
  %151 = load atomic i32, ptr %n_waiting_thds.i349.i monotonic, align 4
  %n_waiting_thds15.i350.i = getelementptr inbounds i8, ptr %47, i64 684
  %152 = load atomic i32, ptr %n_waiting_thds15.i350.i monotonic, align 4
  %add17.i351.i = add i32 %152, %151
  store atomic i32 %add17.i351.i, ptr %n_waiting_thds.i349.i monotonic, align 4
  %n_owner_switches.i352.i = getelementptr inbounds i8, ptr %47, i64 688
  %153 = load i64, ptr %n_owner_switches.i352.i, align 8
  %n_owner_switches19.i353.i = getelementptr inbounds i8, ptr %48, i64 688
  %154 = load i64, ptr %n_owner_switches19.i353.i, align 8
  %add20.i354.i = add i64 %154, %153
  store i64 %add20.i354.i, ptr %n_owner_switches19.i353.i, align 8
  %n_lock_ops.i355.i = getelementptr inbounds i8, ptr %47, i64 704
  %155 = load i64, ptr %n_lock_ops.i355.i, align 8
  %n_lock_ops21.i356.i = getelementptr inbounds i8, ptr %48, i64 704
  %156 = load i64, ptr %n_lock_ops21.i356.i, align 8
  %add22.i357.i = add i64 %156, %155
  store i64 %add22.i357.i, ptr %n_lock_ops21.i356.i, align 8
  %arrayidx138.i = getelementptr inbounds i8, ptr %48, i64 712
  %arrayidx141.i = getelementptr inbounds i8, ptr %47, i64 712
  tail call void @nstime_add(ptr noundef nonnull %arrayidx138.i, ptr noundef nonnull %arrayidx141.i) #14
  %max_wait_time.i361.i = getelementptr inbounds i8, ptr %48, i64 720
  %max_wait_time2.i362.i = getelementptr inbounds i8, ptr %47, i64 720
  %call.i363.i = tail call i32 @nstime_compare(ptr noundef nonnull %max_wait_time.i361.i, ptr noundef nonnull %max_wait_time2.i362.i) #14
  %cmp.i364.i = icmp slt i32 %call.i363.i, 0
  br i1 %cmp.i364.i, label %if.then.i385.i, label %if.end.i365.i

if.then.i385.i:                                   ; preds = %malloc_mutex_prof_merge.exit360.i
  tail call void @nstime_copy(ptr noundef nonnull %max_wait_time.i361.i, ptr noundef nonnull %max_wait_time2.i362.i) #14
  br label %if.end.i365.i

if.end.i365.i:                                    ; preds = %if.then.i385.i, %malloc_mutex_prof_merge.exit360.i
  %n_wait_times.i366.i = getelementptr inbounds i8, ptr %47, i64 728
  %157 = load i64, ptr %n_wait_times.i366.i, align 8
  %n_wait_times5.i367.i = getelementptr inbounds i8, ptr %48, i64 728
  %158 = load i64, ptr %n_wait_times5.i367.i, align 8
  %add.i368.i = add i64 %158, %157
  store i64 %add.i368.i, ptr %n_wait_times5.i367.i, align 8
  %n_spin_acquired.i369.i = getelementptr inbounds i8, ptr %47, i64 736
  %159 = load i64, ptr %n_spin_acquired.i369.i, align 8
  %n_spin_acquired6.i370.i = getelementptr inbounds i8, ptr %48, i64 736
  %160 = load i64, ptr %n_spin_acquired6.i370.i, align 8
  %add7.i371.i = add i64 %160, %159
  store i64 %add7.i371.i, ptr %n_spin_acquired6.i370.i, align 8
  %max_n_thds.i372.i = getelementptr inbounds i8, ptr %48, i64 744
  %161 = load i32, ptr %max_n_thds.i372.i, align 8
  %max_n_thds8.i373.i = getelementptr inbounds i8, ptr %47, i64 744
  %162 = load i32, ptr %max_n_thds8.i373.i, align 8
  %cmp9.i374.i = icmp ult i32 %161, %162
  br i1 %cmp9.i374.i, label %if.then10.i384.i, label %malloc_mutex_prof_merge.exit386.i

if.then10.i384.i:                                 ; preds = %if.end.i365.i
  store i32 %162, ptr %max_n_thds.i372.i, align 8
  br label %malloc_mutex_prof_merge.exit386.i

malloc_mutex_prof_merge.exit386.i:                ; preds = %if.then10.i384.i, %if.end.i365.i
  %n_waiting_thds.i375.i = getelementptr inbounds i8, ptr %48, i64 748
  %163 = load atomic i32, ptr %n_waiting_thds.i375.i monotonic, align 4
  %n_waiting_thds15.i376.i = getelementptr inbounds i8, ptr %47, i64 748
  %164 = load atomic i32, ptr %n_waiting_thds15.i376.i monotonic, align 4
  %add17.i377.i = add i32 %164, %163
  store atomic i32 %add17.i377.i, ptr %n_waiting_thds.i375.i monotonic, align 4
  %n_owner_switches.i378.i = getelementptr inbounds i8, ptr %47, i64 752
  %165 = load i64, ptr %n_owner_switches.i378.i, align 8
  %n_owner_switches19.i379.i = getelementptr inbounds i8, ptr %48, i64 752
  %166 = load i64, ptr %n_owner_switches19.i379.i, align 8
  %add20.i380.i = add i64 %166, %165
  store i64 %add20.i380.i, ptr %n_owner_switches19.i379.i, align 8
  %n_lock_ops.i381.i = getelementptr inbounds i8, ptr %47, i64 768
  %167 = load i64, ptr %n_lock_ops.i381.i, align 8
  %n_lock_ops21.i382.i = getelementptr inbounds i8, ptr %48, i64 768
  %168 = load i64, ptr %n_lock_ops21.i382.i, align 8
  %add22.i383.i = add i64 %168, %167
  store i64 %add22.i383.i, ptr %n_lock_ops21.i382.i, align 8
  %arrayidx144.i = getelementptr inbounds i8, ptr %48, i64 776
  %arrayidx147.i = getelementptr inbounds i8, ptr %47, i64 776
  tail call void @nstime_add(ptr noundef nonnull %arrayidx144.i, ptr noundef nonnull %arrayidx147.i) #14
  %max_wait_time.i387.i = getelementptr inbounds i8, ptr %48, i64 784
  %max_wait_time2.i388.i = getelementptr inbounds i8, ptr %47, i64 784
  %call.i389.i = tail call i32 @nstime_compare(ptr noundef nonnull %max_wait_time.i387.i, ptr noundef nonnull %max_wait_time2.i388.i) #14
  %cmp.i390.i = icmp slt i32 %call.i389.i, 0
  br i1 %cmp.i390.i, label %if.then.i411.i, label %if.end.i391.i

if.then.i411.i:                                   ; preds = %malloc_mutex_prof_merge.exit386.i
  tail call void @nstime_copy(ptr noundef nonnull %max_wait_time.i387.i, ptr noundef nonnull %max_wait_time2.i388.i) #14
  br label %if.end.i391.i

if.end.i391.i:                                    ; preds = %if.then.i411.i, %malloc_mutex_prof_merge.exit386.i
  %n_wait_times.i392.i = getelementptr inbounds i8, ptr %47, i64 792
  %169 = load i64, ptr %n_wait_times.i392.i, align 8
  %n_wait_times5.i393.i = getelementptr inbounds i8, ptr %48, i64 792
  %170 = load i64, ptr %n_wait_times5.i393.i, align 8
  %add.i394.i = add i64 %170, %169
  store i64 %add.i394.i, ptr %n_wait_times5.i393.i, align 8
  %n_spin_acquired.i395.i = getelementptr inbounds i8, ptr %47, i64 800
  %171 = load i64, ptr %n_spin_acquired.i395.i, align 8
  %n_spin_acquired6.i396.i = getelementptr inbounds i8, ptr %48, i64 800
  %172 = load i64, ptr %n_spin_acquired6.i396.i, align 8
  %add7.i397.i = add i64 %172, %171
  store i64 %add7.i397.i, ptr %n_spin_acquired6.i396.i, align 8
  %max_n_thds.i398.i = getelementptr inbounds i8, ptr %48, i64 808
  %173 = load i32, ptr %max_n_thds.i398.i, align 8
  %max_n_thds8.i399.i = getelementptr inbounds i8, ptr %47, i64 808
  %174 = load i32, ptr %max_n_thds8.i399.i, align 8
  %cmp9.i400.i = icmp ult i32 %173, %174
  br i1 %cmp9.i400.i, label %if.then10.i410.i, label %malloc_mutex_prof_merge.exit412.i

if.then10.i410.i:                                 ; preds = %if.end.i391.i
  store i32 %174, ptr %max_n_thds.i398.i, align 8
  br label %malloc_mutex_prof_merge.exit412.i

malloc_mutex_prof_merge.exit412.i:                ; preds = %if.then10.i410.i, %if.end.i391.i
  %n_waiting_thds.i401.i = getelementptr inbounds i8, ptr %48, i64 812
  %175 = load atomic i32, ptr %n_waiting_thds.i401.i monotonic, align 4
  %n_waiting_thds15.i402.i = getelementptr inbounds i8, ptr %47, i64 812
  %176 = load atomic i32, ptr %n_waiting_thds15.i402.i monotonic, align 4
  %add17.i403.i = add i32 %176, %175
  store atomic i32 %add17.i403.i, ptr %n_waiting_thds.i401.i monotonic, align 4
  %n_owner_switches.i404.i = getelementptr inbounds i8, ptr %47, i64 816
  %177 = load i64, ptr %n_owner_switches.i404.i, align 8
  %n_owner_switches19.i405.i = getelementptr inbounds i8, ptr %48, i64 816
  %178 = load i64, ptr %n_owner_switches19.i405.i, align 8
  %add20.i406.i = add i64 %178, %177
  store i64 %add20.i406.i, ptr %n_owner_switches19.i405.i, align 8
  %n_lock_ops.i407.i = getelementptr inbounds i8, ptr %47, i64 832
  %179 = load i64, ptr %n_lock_ops.i407.i, align 8
  %n_lock_ops21.i408.i = getelementptr inbounds i8, ptr %48, i64 832
  %180 = load i64, ptr %n_lock_ops21.i408.i, align 8
  %add22.i409.i = add i64 %180, %179
  store i64 %add22.i409.i, ptr %n_lock_ops21.i408.i, align 8
  %arrayidx150.i = getelementptr inbounds i8, ptr %48, i64 840
  %arrayidx153.i = getelementptr inbounds i8, ptr %47, i64 840
  tail call void @nstime_add(ptr noundef nonnull %arrayidx150.i, ptr noundef nonnull %arrayidx153.i) #14
  %max_wait_time.i413.i = getelementptr inbounds i8, ptr %48, i64 848
  %max_wait_time2.i414.i = getelementptr inbounds i8, ptr %47, i64 848
  %call.i415.i = tail call i32 @nstime_compare(ptr noundef nonnull %max_wait_time.i413.i, ptr noundef nonnull %max_wait_time2.i414.i) #14
  %cmp.i416.i = icmp slt i32 %call.i415.i, 0
  br i1 %cmp.i416.i, label %if.then.i437.i, label %if.end.i417.i

if.then.i437.i:                                   ; preds = %malloc_mutex_prof_merge.exit412.i
  tail call void @nstime_copy(ptr noundef nonnull %max_wait_time.i413.i, ptr noundef nonnull %max_wait_time2.i414.i) #14
  br label %if.end.i417.i

if.end.i417.i:                                    ; preds = %if.then.i437.i, %malloc_mutex_prof_merge.exit412.i
  %n_wait_times.i418.i = getelementptr inbounds i8, ptr %47, i64 856
  %181 = load i64, ptr %n_wait_times.i418.i, align 8
  %n_wait_times5.i419.i = getelementptr inbounds i8, ptr %48, i64 856
  %182 = load i64, ptr %n_wait_times5.i419.i, align 8
  %add.i420.i = add i64 %182, %181
  store i64 %add.i420.i, ptr %n_wait_times5.i419.i, align 8
  %n_spin_acquired.i421.i = getelementptr inbounds i8, ptr %47, i64 864
  %183 = load i64, ptr %n_spin_acquired.i421.i, align 8
  %n_spin_acquired6.i422.i = getelementptr inbounds i8, ptr %48, i64 864
  %184 = load i64, ptr %n_spin_acquired6.i422.i, align 8
  %add7.i423.i = add i64 %184, %183
  store i64 %add7.i423.i, ptr %n_spin_acquired6.i422.i, align 8
  %max_n_thds.i424.i = getelementptr inbounds i8, ptr %48, i64 872
  %185 = load i32, ptr %max_n_thds.i424.i, align 8
  %max_n_thds8.i425.i = getelementptr inbounds i8, ptr %47, i64 872
  %186 = load i32, ptr %max_n_thds8.i425.i, align 8
  %cmp9.i426.i = icmp ult i32 %185, %186
  br i1 %cmp9.i426.i, label %if.then10.i436.i, label %malloc_mutex_prof_merge.exit438.i

if.then10.i436.i:                                 ; preds = %if.end.i417.i
  store i32 %186, ptr %max_n_thds.i424.i, align 8
  br label %malloc_mutex_prof_merge.exit438.i

malloc_mutex_prof_merge.exit438.i:                ; preds = %if.then10.i436.i, %if.end.i417.i
  %n_waiting_thds.i427.i = getelementptr inbounds i8, ptr %48, i64 876
  %187 = load atomic i32, ptr %n_waiting_thds.i427.i monotonic, align 4
  %n_waiting_thds15.i428.i = getelementptr inbounds i8, ptr %47, i64 876
  %188 = load atomic i32, ptr %n_waiting_thds15.i428.i monotonic, align 4
  %add17.i429.i = add i32 %188, %187
  store atomic i32 %add17.i429.i, ptr %n_waiting_thds.i427.i monotonic, align 4
  %n_owner_switches.i430.i = getelementptr inbounds i8, ptr %47, i64 880
  %189 = load i64, ptr %n_owner_switches.i430.i, align 8
  %n_owner_switches19.i431.i = getelementptr inbounds i8, ptr %48, i64 880
  %190 = load i64, ptr %n_owner_switches19.i431.i, align 8
  %add20.i432.i = add i64 %190, %189
  store i64 %add20.i432.i, ptr %n_owner_switches19.i431.i, align 8
  %n_lock_ops.i433.i = getelementptr inbounds i8, ptr %47, i64 896
  %191 = load i64, ptr %n_lock_ops.i433.i, align 8
  %n_lock_ops21.i434.i = getelementptr inbounds i8, ptr %48, i64 896
  %192 = load i64, ptr %n_lock_ops21.i434.i, align 8
  %add22.i435.i = add i64 %192, %191
  store i64 %add22.i435.i, ptr %n_lock_ops21.i434.i, align 8
  %arrayidx156.i = getelementptr inbounds i8, ptr %48, i64 904
  %arrayidx159.i = getelementptr inbounds i8, ptr %47, i64 904
  tail call void @nstime_add(ptr noundef nonnull %arrayidx156.i, ptr noundef nonnull %arrayidx159.i) #14
  %max_wait_time.i439.i = getelementptr inbounds i8, ptr %48, i64 912
  %max_wait_time2.i440.i = getelementptr inbounds i8, ptr %47, i64 912
  %call.i441.i = tail call i32 @nstime_compare(ptr noundef nonnull %max_wait_time.i439.i, ptr noundef nonnull %max_wait_time2.i440.i) #14
  %cmp.i442.i = icmp slt i32 %call.i441.i, 0
  br i1 %cmp.i442.i, label %if.then.i463.i, label %if.end.i443.i

if.then.i463.i:                                   ; preds = %malloc_mutex_prof_merge.exit438.i
  tail call void @nstime_copy(ptr noundef nonnull %max_wait_time.i439.i, ptr noundef nonnull %max_wait_time2.i440.i) #14
  br label %if.end.i443.i

if.end.i443.i:                                    ; preds = %if.then.i463.i, %malloc_mutex_prof_merge.exit438.i
  %n_wait_times.i444.i = getelementptr inbounds i8, ptr %47, i64 920
  %193 = load i64, ptr %n_wait_times.i444.i, align 8
  %n_wait_times5.i445.i = getelementptr inbounds i8, ptr %48, i64 920
  %194 = load i64, ptr %n_wait_times5.i445.i, align 8
  %add.i446.i = add i64 %194, %193
  store i64 %add.i446.i, ptr %n_wait_times5.i445.i, align 8
  %n_spin_acquired.i447.i = getelementptr inbounds i8, ptr %47, i64 928
  %195 = load i64, ptr %n_spin_acquired.i447.i, align 8
  %n_spin_acquired6.i448.i = getelementptr inbounds i8, ptr %48, i64 928
  %196 = load i64, ptr %n_spin_acquired6.i448.i, align 8
  %add7.i449.i = add i64 %196, %195
  store i64 %add7.i449.i, ptr %n_spin_acquired6.i448.i, align 8
  %max_n_thds.i450.i = getelementptr inbounds i8, ptr %48, i64 936
  %197 = load i32, ptr %max_n_thds.i450.i, align 8
  %max_n_thds8.i451.i = getelementptr inbounds i8, ptr %47, i64 936
  %198 = load i32, ptr %max_n_thds8.i451.i, align 8
  %cmp9.i452.i = icmp ult i32 %197, %198
  br i1 %cmp9.i452.i, label %if.then10.i462.i, label %malloc_mutex_prof_merge.exit464.i

if.then10.i462.i:                                 ; preds = %if.end.i443.i
  store i32 %198, ptr %max_n_thds.i450.i, align 8
  br label %malloc_mutex_prof_merge.exit464.i

malloc_mutex_prof_merge.exit464.i:                ; preds = %if.then10.i462.i, %if.end.i443.i
  %n_waiting_thds.i453.i = getelementptr inbounds i8, ptr %48, i64 940
  %199 = load atomic i32, ptr %n_waiting_thds.i453.i monotonic, align 4
  %n_waiting_thds15.i454.i = getelementptr inbounds i8, ptr %47, i64 940
  %200 = load atomic i32, ptr %n_waiting_thds15.i454.i monotonic, align 4
  %add17.i455.i = add i32 %200, %199
  store atomic i32 %add17.i455.i, ptr %n_waiting_thds.i453.i monotonic, align 4
  %n_owner_switches.i456.i = getelementptr inbounds i8, ptr %47, i64 944
  %201 = load i64, ptr %n_owner_switches.i456.i, align 8
  %n_owner_switches19.i457.i = getelementptr inbounds i8, ptr %48, i64 944
  %202 = load i64, ptr %n_owner_switches19.i457.i, align 8
  %add20.i458.i = add i64 %202, %201
  store i64 %add20.i458.i, ptr %n_owner_switches19.i457.i, align 8
  %n_lock_ops.i459.i = getelementptr inbounds i8, ptr %47, i64 960
  %203 = load i64, ptr %n_lock_ops.i459.i, align 8
  %n_lock_ops21.i460.i = getelementptr inbounds i8, ptr %48, i64 960
  %204 = load i64, ptr %n_lock_ops21.i460.i, align 8
  %add22.i461.i = add i64 %204, %203
  store i64 %add22.i461.i, ptr %n_lock_ops21.i460.i, align 8
  br i1 %destroyed, label %if.end196.i, label %if.then161.i

if.then161.i:                                     ; preds = %malloc_mutex_prof_merge.exit464.i
  %205 = load i64, ptr %47, align 8
  %206 = load i64, ptr %48, align 8
  %add165.i = add i64 %206, %205
  store i64 %add165.i, ptr %48, align 8
  %metadata_edata.i = getelementptr inbounds i8, ptr %47, i64 8
  %207 = load i64, ptr %metadata_edata.i, align 8
  %metadata_edata168.i = getelementptr inbounds i8, ptr %48, i64 8
  %208 = load i64, ptr %metadata_edata168.i, align 8
  %add169.i = add i64 %208, %207
  store i64 %add169.i, ptr %metadata_edata168.i, align 8
  %metadata_rtree.i = getelementptr inbounds i8, ptr %47, i64 16
  %209 = load i64, ptr %metadata_rtree.i, align 8
  %metadata_rtree172.i = getelementptr inbounds i8, ptr %48, i64 16
  %210 = load i64, ptr %metadata_rtree172.i, align 8
  %add173.i = add i64 %210, %209
  store i64 %add173.i, ptr %metadata_rtree172.i, align 8
  %resident.i = getelementptr inbounds i8, ptr %47, i64 24
  %211 = load i64, ptr %resident.i, align 8
  %resident176.i = getelementptr inbounds i8, ptr %48, i64 24
  %212 = load i64, ptr %resident176.i, align 8
  %add177.i = add i64 %212, %211
  store i64 %add177.i, ptr %resident176.i, align 8
  %metadata_thp.i = getelementptr inbounds i8, ptr %47, i64 32
  %213 = load i64, ptr %metadata_thp.i, align 8
  %metadata_thp180.i = getelementptr inbounds i8, ptr %48, i64 32
  %214 = load i64, ptr %metadata_thp180.i, align 8
  %add181.i = add i64 %214, %213
  store i64 %add181.i, ptr %metadata_thp180.i, align 8
  %internal.i = getelementptr inbounds i8, ptr %48, i64 48
  %internal184.i = getelementptr inbounds i8, ptr %47, i64 48
  %215 = load atomic i64, ptr %internal.i monotonic, align 8
  %216 = load atomic i64, ptr %internal184.i monotonic, align 8
  %add.i465.i = add i64 %216, %215
  store atomic i64 %add.i465.i, ptr %internal.i monotonic, align 8
  %allocated_small.i17 = getelementptr inbounds i8, ptr %47, i64 10384
  %217 = load i64, ptr %allocated_small.i17, align 8
  %allocated_small191.i = getelementptr inbounds i8, ptr %48, i64 10384
  %218 = load i64, ptr %allocated_small191.i, align 8
  %add192.i = add i64 %218, %217
  store i64 %add192.i, ptr %allocated_small191.i, align 8
  br label %if.end196.i

if.end196.i:                                      ; preds = %if.then161.i, %malloc_mutex_prof_merge.exit464.i
  %nmalloc_small.i18 = getelementptr inbounds i8, ptr %47, i64 10392
  %219 = load i64, ptr %nmalloc_small.i18, align 8
  %nmalloc_small197.i = getelementptr inbounds i8, ptr %48, i64 10392
  %220 = load i64, ptr %nmalloc_small197.i, align 8
  %add198.i = add i64 %220, %219
  store i64 %add198.i, ptr %nmalloc_small197.i, align 8
  %ndalloc_small.i19 = getelementptr inbounds i8, ptr %47, i64 10400
  %221 = load i64, ptr %ndalloc_small.i19, align 8
  %ndalloc_small199.i = getelementptr inbounds i8, ptr %48, i64 10400
  %222 = load i64, ptr %ndalloc_small199.i, align 8
  %add200.i = add i64 %222, %221
  store i64 %add200.i, ptr %ndalloc_small199.i, align 8
  %nrequests_small.i20 = getelementptr inbounds i8, ptr %47, i64 10408
  %223 = load i64, ptr %nrequests_small.i20, align 8
  %nrequests_small201.i = getelementptr inbounds i8, ptr %48, i64 10408
  %224 = load i64, ptr %nrequests_small201.i, align 8
  %add202.i = add i64 %224, %223
  store i64 %add202.i, ptr %nrequests_small201.i, align 8
  %nfills_small.i21 = getelementptr inbounds i8, ptr %47, i64 10416
  %225 = load i64, ptr %nfills_small.i21, align 8
  %nfills_small203.i = getelementptr inbounds i8, ptr %48, i64 10416
  %226 = load i64, ptr %nfills_small203.i, align 8
  %add204.i = add i64 %226, %225
  store i64 %add204.i, ptr %nfills_small203.i, align 8
  %nflushes_small.i22 = getelementptr inbounds i8, ptr %47, i64 10424
  %227 = load i64, ptr %nflushes_small.i22, align 8
  %nflushes_small205.i = getelementptr inbounds i8, ptr %48, i64 10424
  %228 = load i64, ptr %nflushes_small205.i, align 8
  %add206.i = add i64 %228, %227
  store i64 %add206.i, ptr %nflushes_small205.i, align 8
  br i1 %destroyed, label %if.end216.i, label %if.then208.i

if.then208.i:                                     ; preds = %if.end196.i
  %allocated_large.i = getelementptr inbounds i8, ptr %47, i64 56
  %229 = load i64, ptr %allocated_large.i, align 8
  %allocated_large211.i = getelementptr inbounds i8, ptr %48, i64 56
  %230 = load i64, ptr %allocated_large211.i, align 8
  %add212.i = add i64 %230, %229
  store i64 %add212.i, ptr %allocated_large211.i, align 8
  br label %if.end216.i

if.end216.i:                                      ; preds = %if.then208.i, %if.end196.i
  %nmalloc_large.i = getelementptr inbounds i8, ptr %47, i64 64
  %231 = load i64, ptr %nmalloc_large.i, align 8
  %nmalloc_large219.i = getelementptr inbounds i8, ptr %48, i64 64
  %232 = load i64, ptr %nmalloc_large219.i, align 8
  %add220.i = add i64 %232, %231
  store i64 %add220.i, ptr %nmalloc_large219.i, align 8
  %ndalloc_large.i = getelementptr inbounds i8, ptr %47, i64 72
  %233 = load i64, ptr %ndalloc_large.i, align 8
  %ndalloc_large223.i = getelementptr inbounds i8, ptr %48, i64 72
  %234 = load i64, ptr %ndalloc_large223.i, align 8
  %add224.i = add i64 %234, %233
  store i64 %add224.i, ptr %ndalloc_large223.i, align 8
  %nrequests_large.i = getelementptr inbounds i8, ptr %47, i64 96
  %235 = load i64, ptr %nrequests_large.i, align 8
  %nrequests_large227.i = getelementptr inbounds i8, ptr %48, i64 96
  %236 = load i64, ptr %nrequests_large227.i, align 8
  %add228.i = add i64 %236, %235
  store i64 %add228.i, ptr %nrequests_large227.i, align 8
  %nflushes_large.i = getelementptr inbounds i8, ptr %47, i64 88
  %237 = load i64, ptr %nflushes_large.i, align 8
  %nflushes_large231.i = getelementptr inbounds i8, ptr %48, i64 88
  %238 = load i64, ptr %nflushes_large231.i, align 8
  %add232.i = add i64 %238, %237
  store i64 %add232.i, ptr %nflushes_large231.i, align 8
  %abandoned_vm.i = getelementptr inbounds i8, ptr %48, i64 176
  %abandoned_vm239.i = getelementptr inbounds i8, ptr %47, i64 176
  %239 = load atomic i64, ptr %abandoned_vm.i monotonic, align 8
  %240 = load atomic i64, ptr %abandoned_vm239.i monotonic, align 8
  %add.i466.i = add i64 %240, %239
  store atomic i64 %add.i466.i, ptr %abandoned_vm.i monotonic, align 8
  %tcache_bytes.i = getelementptr inbounds i8, ptr %47, i64 184
  %241 = load i64, ptr %tcache_bytes.i, align 8
  %tcache_bytes242.i = getelementptr inbounds i8, ptr %48, i64 184
  %242 = load i64, ptr %tcache_bytes242.i, align 8
  %add243.i = add i64 %242, %241
  store i64 %add243.i, ptr %tcache_bytes242.i, align 8
  %tcache_stashed_bytes.i = getelementptr inbounds i8, ptr %47, i64 192
  %243 = load i64, ptr %tcache_stashed_bytes.i, align 8
  %tcache_stashed_bytes246.i = getelementptr inbounds i8, ptr %48, i64 192
  %244 = load i64, ptr %tcache_stashed_bytes246.i, align 8
  %add247.i = add i64 %244, %243
  store i64 %add247.i, ptr %tcache_stashed_bytes246.i, align 8
  %245 = load i32, ptr %6, align 8
  %cmp.i = icmp eq i32 %245, 0
  br i1 %cmp.i, label %if.then248.i, label %if.end252.i

if.then248.i:                                     ; preds = %if.end216.i
  %uptime.i = getelementptr inbounds i8, ptr %48, i64 10376
  %uptime251.i = getelementptr inbounds i8, ptr %47, i64 10376
  %246 = load i64, ptr %uptime251.i, align 8
  store i64 %246, ptr %uptime.i, align 8
  br label %if.end252.i

if.end252.i:                                      ; preds = %if.then248.i, %if.end216.i
  %bstats255.i = getelementptr inbounds i8, ptr %47, i64 10432
  %bstats257.i = getelementptr inbounds i8, ptr %48, i64 10432
  br label %for.body.i23

for.cond302.preheader.i:                          ; preds = %malloc_mutex_prof_merge.exit492.i
  %lstats.i32 = getelementptr inbounds i8, ptr %48, i64 15616
  %lstats310.i = getelementptr inbounds i8, ptr %47, i64 15616
  br i1 %destroyed, label %for.body306.us.i, label %for.body306.i

for.body306.us.i:                                 ; preds = %for.cond302.preheader.i, %for.body306.us.i
  %indvars.iv508.i = phi i64 [ %indvars.iv.next509.i, %for.body306.us.i ], [ 0, %for.cond302.preheader.i ]
  %arrayidx308.us.i = getelementptr inbounds [196 x %struct.arena_stats_large_s], ptr %lstats.i32, i64 0, i64 %indvars.iv508.i
  %arrayidx312.us.i = getelementptr inbounds [196 x %struct.arena_stats_large_s], ptr %lstats310.i, i64 0, i64 %indvars.iv508.i
  %247 = load atomic i64, ptr %arrayidx312.us.i monotonic, align 8
  %248 = load atomic i64, ptr %arrayidx308.us.i monotonic, align 8
  %add.i.i493.us.i = add i64 %248, %247
  store atomic i64 %add.i.i493.us.i, ptr %arrayidx308.us.i monotonic, align 8
  %ndalloc317.us.i = getelementptr inbounds i8, ptr %arrayidx308.us.i, i64 8
  %ndalloc321.us.i = getelementptr inbounds i8, ptr %arrayidx312.us.i, i64 8
  %249 = load atomic i64, ptr %ndalloc321.us.i monotonic, align 8
  %250 = load atomic i64, ptr %ndalloc317.us.i monotonic, align 8
  %add.i.i494.us.i = add i64 %250, %249
  store atomic i64 %add.i.i494.us.i, ptr %ndalloc317.us.i monotonic, align 8
  %nrequests325.us.i = getelementptr inbounds i8, ptr %arrayidx308.us.i, i64 16
  %nrequests329.us.i = getelementptr inbounds i8, ptr %arrayidx312.us.i, i64 16
  %251 = load atomic i64, ptr %nrequests329.us.i monotonic, align 8
  %252 = load atomic i64, ptr %nrequests325.us.i monotonic, align 8
  %add.i.i495.us.i = add i64 %252, %251
  store atomic i64 %add.i.i495.us.i, ptr %nrequests325.us.i monotonic, align 8
  %indvars.iv.next509.i = add nuw nsw i64 %indvars.iv508.i, 1
  %exitcond511.not.i = icmp eq i64 %indvars.iv.next509.i, 196
  br i1 %exitcond511.not.i, label %for.cond347.preheader.i, label %for.body306.us.i, !llvm.loop !14

for.body.i23:                                     ; preds = %malloc_mutex_prof_merge.exit492.i, %if.end252.i
  %indvars.iv.i24 = phi i64 [ 0, %if.end252.i ], [ %indvars.iv.next.i30, %malloc_mutex_prof_merge.exit492.i ]
  %arrayidx256.i = getelementptr inbounds [36 x %struct.bin_stats_data_s], ptr %bstats255.i, i64 0, i64 %indvars.iv.i24
  %arrayidx259.i = getelementptr inbounds [36 x %struct.bin_stats_data_s], ptr %bstats257.i, i64 0, i64 %indvars.iv.i24
  %253 = load i64, ptr %arrayidx256.i, align 8
  %254 = load i64, ptr %arrayidx259.i, align 8
  %add262.i = add i64 %254, %253
  store i64 %add262.i, ptr %arrayidx259.i, align 8
  %ndalloc.i25 = getelementptr inbounds i8, ptr %arrayidx256.i, i64 8
  %255 = load i64, ptr %ndalloc.i25, align 8
  %ndalloc263.i = getelementptr inbounds i8, ptr %arrayidx259.i, i64 8
  %256 = load i64, ptr %ndalloc263.i, align 8
  %add264.i = add i64 %256, %255
  store i64 %add264.i, ptr %ndalloc263.i, align 8
  %nrequests.i26 = getelementptr inbounds i8, ptr %arrayidx256.i, i64 16
  %257 = load i64, ptr %nrequests.i26, align 8
  %nrequests265.i = getelementptr inbounds i8, ptr %arrayidx259.i, i64 16
  %258 = load i64, ptr %nrequests265.i, align 8
  %add266.i = add i64 %258, %257
  store i64 %add266.i, ptr %nrequests265.i, align 8
  br i1 %destroyed, label %if.end274.i, label %if.then268.i

if.then268.i:                                     ; preds = %for.body.i23
  %curregs.i27 = getelementptr inbounds i8, ptr %arrayidx256.i, i64 24
  %259 = load i64, ptr %curregs.i27, align 8
  %curregs269.i = getelementptr inbounds i8, ptr %arrayidx259.i, i64 24
  %260 = load i64, ptr %curregs269.i, align 8
  %add270.i = add i64 %260, %259
  store i64 %add270.i, ptr %curregs269.i, align 8
  br label %if.end274.i

if.end274.i:                                      ; preds = %if.then268.i, %for.body.i23
  %nfills.i28 = getelementptr inbounds i8, ptr %arrayidx256.i, i64 32
  %261 = load i64, ptr %nfills.i28, align 8
  %nfills275.i = getelementptr inbounds i8, ptr %arrayidx259.i, i64 32
  %262 = load i64, ptr %nfills275.i, align 8
  %add276.i = add i64 %262, %261
  store i64 %add276.i, ptr %nfills275.i, align 8
  %nflushes.i29 = getelementptr inbounds i8, ptr %arrayidx256.i, i64 40
  %263 = load i64, ptr %nflushes.i29, align 8
  %nflushes277.i = getelementptr inbounds i8, ptr %arrayidx259.i, i64 40
  %264 = load i64, ptr %nflushes277.i, align 8
  %add278.i = add i64 %264, %263
  store i64 %add278.i, ptr %nflushes277.i, align 8
  %nslabs.i = getelementptr inbounds i8, ptr %arrayidx256.i, i64 48
  %265 = load i64, ptr %nslabs.i, align 8
  %nslabs279.i = getelementptr inbounds i8, ptr %arrayidx259.i, i64 48
  %266 = load i64, ptr %nslabs279.i, align 8
  %add280.i = add i64 %266, %265
  store i64 %add280.i, ptr %nslabs279.i, align 8
  %reslabs.i = getelementptr inbounds i8, ptr %arrayidx256.i, i64 56
  %267 = load i64, ptr %reslabs.i, align 8
  %reslabs281.i = getelementptr inbounds i8, ptr %arrayidx259.i, i64 56
  %268 = load i64, ptr %reslabs281.i, align 8
  %add282.i = add i64 %268, %267
  store i64 %add282.i, ptr %reslabs281.i, align 8
  br i1 %destroyed, label %if.end294.i, label %if.then284.i

if.then284.i:                                     ; preds = %if.end274.i
  %curslabs.i = getelementptr inbounds i8, ptr %arrayidx256.i, i64 64
  %269 = load i64, ptr %curslabs.i, align 8
  %curslabs285.i = getelementptr inbounds i8, ptr %arrayidx259.i, i64 64
  %270 = load i64, ptr %curslabs285.i, align 8
  %add286.i = add i64 %270, %269
  store i64 %add286.i, ptr %curslabs285.i, align 8
  %nonfull_slabs.i = getelementptr inbounds i8, ptr %arrayidx256.i, i64 72
  %271 = load i64, ptr %nonfull_slabs.i, align 8
  %nonfull_slabs287.i = getelementptr inbounds i8, ptr %arrayidx259.i, i64 72
  %272 = load i64, ptr %nonfull_slabs287.i, align 8
  %add288.i = add i64 %272, %271
  store i64 %add288.i, ptr %nonfull_slabs287.i, align 8
  br label %if.end294.i

if.end294.i:                                      ; preds = %if.then284.i, %if.end274.i
  %mutex_data.i = getelementptr inbounds i8, ptr %arrayidx259.i, i64 80
  %mutex_data301.i = getelementptr inbounds i8, ptr %arrayidx256.i, i64 80
  tail call void @nstime_add(ptr noundef nonnull %mutex_data.i, ptr noundef nonnull %mutex_data301.i) #14
  %max_wait_time.i467.i = getelementptr inbounds i8, ptr %arrayidx259.i, i64 88
  %max_wait_time2.i468.i = getelementptr inbounds i8, ptr %arrayidx256.i, i64 88
  %call.i469.i = tail call i32 @nstime_compare(ptr noundef nonnull %max_wait_time.i467.i, ptr noundef nonnull %max_wait_time2.i468.i) #14
  %cmp.i470.i = icmp slt i32 %call.i469.i, 0
  br i1 %cmp.i470.i, label %if.then.i491.i, label %if.end.i471.i

if.then.i491.i:                                   ; preds = %if.end294.i
  tail call void @nstime_copy(ptr noundef nonnull %max_wait_time.i467.i, ptr noundef nonnull %max_wait_time2.i468.i) #14
  br label %if.end.i471.i

if.end.i471.i:                                    ; preds = %if.then.i491.i, %if.end294.i
  %n_wait_times.i472.i = getelementptr inbounds i8, ptr %arrayidx256.i, i64 96
  %273 = load i64, ptr %n_wait_times.i472.i, align 8
  %n_wait_times5.i473.i = getelementptr inbounds i8, ptr %arrayidx259.i, i64 96
  %274 = load i64, ptr %n_wait_times5.i473.i, align 8
  %add.i474.i = add i64 %274, %273
  store i64 %add.i474.i, ptr %n_wait_times5.i473.i, align 8
  %n_spin_acquired.i475.i = getelementptr inbounds i8, ptr %arrayidx256.i, i64 104
  %275 = load i64, ptr %n_spin_acquired.i475.i, align 8
  %n_spin_acquired6.i476.i = getelementptr inbounds i8, ptr %arrayidx259.i, i64 104
  %276 = load i64, ptr %n_spin_acquired6.i476.i, align 8
  %add7.i477.i = add i64 %276, %275
  store i64 %add7.i477.i, ptr %n_spin_acquired6.i476.i, align 8
  %max_n_thds.i478.i = getelementptr inbounds i8, ptr %arrayidx259.i, i64 112
  %277 = load i32, ptr %max_n_thds.i478.i, align 8
  %max_n_thds8.i479.i = getelementptr inbounds i8, ptr %arrayidx256.i, i64 112
  %278 = load i32, ptr %max_n_thds8.i479.i, align 8
  %cmp9.i480.i = icmp ult i32 %277, %278
  br i1 %cmp9.i480.i, label %if.then10.i490.i, label %malloc_mutex_prof_merge.exit492.i

if.then10.i490.i:                                 ; preds = %if.end.i471.i
  store i32 %278, ptr %max_n_thds.i478.i, align 8
  br label %malloc_mutex_prof_merge.exit492.i

malloc_mutex_prof_merge.exit492.i:                ; preds = %if.then10.i490.i, %if.end.i471.i
  %n_waiting_thds.i481.i = getelementptr inbounds i8, ptr %arrayidx259.i, i64 116
  %279 = load atomic i32, ptr %n_waiting_thds.i481.i monotonic, align 4
  %n_waiting_thds15.i482.i = getelementptr inbounds i8, ptr %arrayidx256.i, i64 116
  %280 = load atomic i32, ptr %n_waiting_thds15.i482.i monotonic, align 4
  %add17.i483.i = add i32 %280, %279
  store atomic i32 %add17.i483.i, ptr %n_waiting_thds.i481.i monotonic, align 4
  %n_owner_switches.i484.i = getelementptr inbounds i8, ptr %arrayidx256.i, i64 120
  %281 = load i64, ptr %n_owner_switches.i484.i, align 8
  %n_owner_switches19.i485.i = getelementptr inbounds i8, ptr %arrayidx259.i, i64 120
  %282 = load i64, ptr %n_owner_switches19.i485.i, align 8
  %add20.i486.i = add i64 %282, %281
  store i64 %add20.i486.i, ptr %n_owner_switches19.i485.i, align 8
  %n_lock_ops.i487.i = getelementptr inbounds i8, ptr %arrayidx256.i, i64 136
  %283 = load i64, ptr %n_lock_ops.i487.i, align 8
  %n_lock_ops21.i488.i = getelementptr inbounds i8, ptr %arrayidx259.i, i64 136
  %284 = load i64, ptr %n_lock_ops21.i488.i, align 8
  %add22.i489.i = add i64 %284, %283
  store i64 %add22.i489.i, ptr %n_lock_ops21.i488.i, align 8
  %indvars.iv.next.i30 = add nuw nsw i64 %indvars.iv.i24, 1
  %exitcond.not.i31 = icmp eq i64 %indvars.iv.next.i30, 36
  br i1 %exitcond.not.i31, label %for.cond302.preheader.i, label %for.body.i23, !llvm.loop !15

for.cond347.preheader.i:                          ; preds = %for.body306.i, %for.body306.us.i
  %estats.i33 = getelementptr inbounds i8, ptr %47, i64 25024
  %estats354.i = getelementptr inbounds i8, ptr %48, i64 25024
  br label %for.body351.i

for.body306.i:                                    ; preds = %for.cond302.preheader.i, %for.body306.i
  %indvars.iv504.i = phi i64 [ %indvars.iv.next505.i, %for.body306.i ], [ 0, %for.cond302.preheader.i ]
  %arrayidx308.i = getelementptr inbounds [196 x %struct.arena_stats_large_s], ptr %lstats.i32, i64 0, i64 %indvars.iv504.i
  %arrayidx312.i = getelementptr inbounds [196 x %struct.arena_stats_large_s], ptr %lstats310.i, i64 0, i64 %indvars.iv504.i
  %285 = load atomic i64, ptr %arrayidx312.i monotonic, align 8
  %286 = load atomic i64, ptr %arrayidx308.i monotonic, align 8
  %add.i.i493.i = add i64 %286, %285
  store atomic i64 %add.i.i493.i, ptr %arrayidx308.i monotonic, align 8
  %ndalloc317.i = getelementptr inbounds i8, ptr %arrayidx308.i, i64 8
  %ndalloc321.i = getelementptr inbounds i8, ptr %arrayidx312.i, i64 8
  %287 = load atomic i64, ptr %ndalloc321.i monotonic, align 8
  %288 = load atomic i64, ptr %ndalloc317.i monotonic, align 8
  %add.i.i494.i = add i64 %288, %287
  store atomic i64 %add.i.i494.i, ptr %ndalloc317.i monotonic, align 8
  %nrequests325.i = getelementptr inbounds i8, ptr %arrayidx308.i, i64 16
  %nrequests329.i = getelementptr inbounds i8, ptr %arrayidx312.i, i64 16
  %289 = load atomic i64, ptr %nrequests329.i monotonic, align 8
  %290 = load atomic i64, ptr %nrequests325.i monotonic, align 8
  %add.i.i495.i = add i64 %290, %289
  store atomic i64 %add.i.i495.i, ptr %nrequests325.i monotonic, align 8
  %curlextents.i = getelementptr inbounds i8, ptr %arrayidx312.i, i64 40
  %291 = load i64, ptr %curlextents.i, align 8
  %curlextents338.i = getelementptr inbounds i8, ptr %arrayidx308.i, i64 40
  %292 = load i64, ptr %curlextents338.i, align 8
  %add339.i = add i64 %292, %291
  store i64 %add339.i, ptr %curlextents338.i, align 8
  %indvars.iv.next505.i = add nuw nsw i64 %indvars.iv504.i, 1
  %exitcond507.not.i = icmp eq i64 %indvars.iv.next505.i, 196
  br i1 %exitcond507.not.i, label %for.cond347.preheader.i, label %for.body306.i, !llvm.loop !14

for.body351.i:                                    ; preds = %for.body351.i, %for.cond347.preheader.i
  %indvars.iv512.i = phi i64 [ 0, %for.cond347.preheader.i ], [ %indvars.iv.next513.i, %for.body351.i ]
  %arrayidx353.i = getelementptr inbounds [199 x %struct.pac_estats_s], ptr %estats.i33, i64 0, i64 %indvars.iv512.i
  %293 = load i64, ptr %arrayidx353.i, align 8
  %arrayidx356.i = getelementptr inbounds [199 x %struct.pac_estats_s], ptr %estats354.i, i64 0, i64 %indvars.iv512.i
  %294 = load i64, ptr %arrayidx356.i, align 8
  %add358.i = add i64 %294, %293
  store i64 %add358.i, ptr %arrayidx356.i, align 8
  %nmuzzy.i = getelementptr inbounds i8, ptr %arrayidx353.i, i64 16
  %295 = load i64, ptr %nmuzzy.i, align 8
  %nmuzzy365.i = getelementptr inbounds i8, ptr %arrayidx356.i, i64 16
  %296 = load i64, ptr %nmuzzy365.i, align 8
  %add366.i = add i64 %296, %295
  store i64 %add366.i, ptr %nmuzzy365.i, align 8
  %nretained.i = getelementptr inbounds i8, ptr %arrayidx353.i, i64 32
  %297 = load i64, ptr %nretained.i, align 8
  %nretained373.i = getelementptr inbounds i8, ptr %arrayidx356.i, i64 32
  %298 = load i64, ptr %nretained373.i, align 8
  %add374.i = add i64 %298, %297
  store i64 %add374.i, ptr %nretained373.i, align 8
  %dirty_bytes.i = getelementptr inbounds i8, ptr %arrayidx353.i, i64 8
  %299 = load i64, ptr %dirty_bytes.i, align 8
  %dirty_bytes381.i = getelementptr inbounds i8, ptr %arrayidx356.i, i64 8
  %300 = load i64, ptr %dirty_bytes381.i, align 8
  %add382.i = add i64 %300, %299
  store i64 %add382.i, ptr %dirty_bytes381.i, align 8
  %muzzy_bytes.i = getelementptr inbounds i8, ptr %arrayidx353.i, i64 24
  %301 = load i64, ptr %muzzy_bytes.i, align 8
  %muzzy_bytes389.i = getelementptr inbounds i8, ptr %arrayidx356.i, i64 24
  %302 = load i64, ptr %muzzy_bytes389.i, align 8
  %add390.i = add i64 %302, %301
  store i64 %add390.i, ptr %muzzy_bytes389.i, align 8
  %retained_bytes.i = getelementptr inbounds i8, ptr %arrayidx353.i, i64 40
  %303 = load i64, ptr %retained_bytes.i, align 8
  %retained_bytes397.i = getelementptr inbounds i8, ptr %arrayidx356.i, i64 40
  %304 = load i64, ptr %retained_bytes397.i, align 8
  %add398.i = add i64 %304, %303
  store i64 %add398.i, ptr %retained_bytes397.i, align 8
  %indvars.iv.next513.i = add nuw nsw i64 %indvars.iv512.i, 1
  %exitcond515.not.i = icmp eq i64 %indvars.iv.next513.i, 199
  br i1 %exitcond515.not.i, label %ctl_arena_stats_sdmerge.exit, label %for.body351.i, !llvm.loop !16

ctl_arena_stats_sdmerge.exit:                     ; preds = %for.body351.i
  %hpastats.i34 = getelementptr inbounds i8, ptr %48, i64 34576
  %hpastats402.i = getelementptr inbounds i8, ptr %47, i64 34576
  tail call void @hpa_shard_stats_accum(ptr noundef nonnull %hpastats.i34, ptr noundef nonnull %hpastats402.i) #14
  %secstats.i35 = getelementptr inbounds i8, ptr %48, i64 37776
  %secstats403.i = getelementptr inbounds i8, ptr %47, i64 37776
  %secstats403.val.i = load i64, ptr %secstats403.i, align 8
  %305 = load i64, ptr %secstats.i35, align 8
  %add.i496.i = add i64 %305, %secstats403.val.i
  store i64 %add.i496.i, ptr %secstats.i35, align 8
  ret void
}

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn
declare void @llvm.stackrestore.p0(ptr) #5

declare ptr @tsd_fetch_slow(ptr noundef, i1 noundef zeroext) local_unnamed_addr #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare nonnull ptr @llvm.threadlocal.address.p0(ptr nonnull) #6

declare ptr @arena_init(ptr noundef, i32 noundef, ptr noundef) local_unnamed_addr #1

declare void @arena_stats_merge(ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef) local_unnamed_addr #1

; Function Attrs: mustprogress nocallback nofree nounwind willreturn memory(argmem: readwrite)
declare void @llvm.memcpy.p0.p0.i64(ptr noalias nocapture writeonly, ptr noalias nocapture readonly, i64, i1 immarg) #7

declare void @hpa_shard_stats_accum(ptr noundef, ptr noundef) local_unnamed_addr #1

declare void @nstime_add(ptr noundef, ptr noundef) local_unnamed_addr #1

declare i32 @nstime_compare(ptr noundef, ptr noundef) local_unnamed_addr #1

declare void @nstime_copy(ptr noundef, ptr noundef) local_unnamed_addr #1

declare zeroext i1 @background_thread_stats_read(ptr noundef, ptr noundef) local_unnamed_addr #1

; Function Attrs: nounwind
declare i32 @pthread_mutex_unlock(ptr noundef) local_unnamed_addr #3

; Function Attrs: mustprogress nofree nounwind willreturn memory(argmem: read)
declare ptr @strchr(ptr noundef, i32 noundef) local_unnamed_addr #8

; Function Attrs: mustprogress nofree nounwind willreturn memory(argmem: read)
declare i64 @strlen(ptr nocapture noundef) local_unnamed_addr #8

; Function Attrs: mustprogress nofree nounwind willreturn memory(argmem: read)
declare i32 @strncmp(ptr nocapture noundef, ptr nocapture noundef, i64 noundef) local_unnamed_addr #8

declare i64 @malloc_strtoumax(ptr noundef, ptr noundef, i32 noundef) local_unnamed_addr #1

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @version_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca ptr, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store ptr @.str.15, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %0, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store ptr @.str.15, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @epoch_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp.not = icmp eq ptr %newp, null
  br i1 %cmp.not, label %do.body8, label %if.then

if.then:                                          ; preds = %malloc_mutex_lock.exit
  %cmp1.not = icmp eq i64 %newlen, 8
  br i1 %cmp1.not, label %if.then5, label %label_return

if.then5:                                         ; preds = %if.then
  tail call fastcc void @ctl_refresh(ptr noundef %tsd)
  br label %do.body8

do.body8:                                         ; preds = %malloc_mutex_lock.exit, %if.then5
  %cmp9 = icmp ne ptr %oldp, null
  %cmp10 = icmp ne ptr %oldlenp, null
  %or.cond = and i1 %cmp9, %cmp10
  br i1 %or.cond, label %if.then11, label %label_return

if.then11:                                        ; preds = %do.body8
  %3 = load i64, ptr %oldlenp, align 8
  %cmp12.not = icmp eq i64 %3, 8
  br i1 %cmp12.not, label %if.end15, label %if.then13

if.then13:                                        ; preds = %if.then11
  %spec.select = tail call i64 @llvm.umin.i64(i64 %3, i64 8)
  %4 = load ptr, ptr @ctl_arenas, align 8
  tail call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr align 1 %4, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end15:                                         ; preds = %if.then11
  %5 = load ptr, ptr @ctl_arenas, align 8
  %6 = load i64, ptr %5, align 8
  store i64 %6, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end15, %do.body8, %if.then, %if.then13
  %ret.0 = phi i32 [ 22, %if.then13 ], [ 22, %if.then ], [ 0, %do.body8 ], [ 0, %if.end15 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @background_thread_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i8, align 1
  tail call void @background_thread_ctl_init(ptr noundef %tsd) #14
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %call.i.i33 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i34 = icmp eq i32 %call.i.i33, 0
  br i1 %cmp.i.not.i34, label %if.end.i36, label %if.then.i35

if.then.i35:                                      ; preds = %malloc_mutex_lock.exit
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @background_thread_lock) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i36

if.end.i36:                                       ; preds = %if.then.i35, %malloc_mutex_lock.exit
  %3 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i37 = add i64 %3, 1
  store i64 %inc.i.i37, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %4 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i38 = icmp eq ptr %4, %tsd
  br i1 %cmp.not.i.i38, label %malloc_mutex_lock.exit41, label %if.then.i.i39

if.then.i.i39:                                    ; preds = %if.end.i36
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %5 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i40 = add i64 %5, 1
  store i64 %inc2.i.i40, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit41

malloc_mutex_lock.exit41:                         ; preds = %if.end.i36, %if.then.i.i39
  %cmp = icmp eq ptr %newp, null
  br i1 %cmp, label %monotonic.i, label %if.else

monotonic.i:                                      ; preds = %malloc_mutex_lock.exit41
  %6 = load atomic i8, ptr @background_thread_enabled_state monotonic, align 1
  %7 = and i8 %6, 1
  store i8 %7, ptr %oldval, align 1
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond = and i1 %cmp4, %cmp5
  br i1 %or.cond, label %if.then6, label %if.end56

if.then6:                                         ; preds = %monotonic.i
  %8 = load i64, ptr %oldlenp, align 8
  switch i64 %8, label %cond.end [
    i64 1, label %if.end
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then6
  br label %cond.end

cond.end:                                         ; preds = %if.then6, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then6 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end:                                           ; preds = %if.then6
  store i8 %7, ptr %oldp, align 1
  br label %if.end56

if.else:                                          ; preds = %malloc_mutex_lock.exit41
  %cmp12.not = icmp eq i64 %newlen, 1
  br i1 %cmp12.not, label %monotonic.i85, label %label_return

monotonic.i85:                                    ; preds = %if.else
  %9 = load atomic i8, ptr @background_thread_enabled_state monotonic, align 1
  %10 = and i8 %9, 1
  store i8 %10, ptr %oldval, align 1
  %cmp18 = icmp ne ptr %oldp, null
  %cmp20 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp18, %cmp20
  br i1 %or.cond1, label %if.then21, label %do.end34

if.then21:                                        ; preds = %monotonic.i85
  %11 = load i64, ptr %oldlenp, align 8
  switch i64 %11, label %cond.end28 [
    i64 1, label %if.end30
    i64 0, label %cond.false27
  ]

cond.false27:                                     ; preds = %if.then21
  br label %cond.end28

cond.end28:                                       ; preds = %if.then21, %cond.false27
  %cond29 = phi i64 [ 0, %cond.false27 ], [ 1, %if.then21 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond29, i1 false)
  store i64 %cond29, ptr %oldlenp, align 8
  br label %label_return

if.end30:                                         ; preds = %if.then21
  store i8 %10, ptr %oldp, align 1
  br label %do.end34

do.end34:                                         ; preds = %monotonic.i85, %if.end30
  %12 = load i8, ptr %newp, align 1
  %13 = and i8 %12, 1
  %cmp40 = icmp eq i8 %13, %10
  br i1 %cmp40, label %label_return, label %if.end43

if.end43:                                         ; preds = %do.end34
  %tobool35.not = icmp eq i8 %13, 0
  store atomic i8 %13, ptr @background_thread_enabled_state monotonic, align 1
  br i1 %tobool35.not, label %if.else51, label %if.then47

if.then47:                                        ; preds = %if.end43
  %call48 = tail call zeroext i1 @background_threads_enable(ptr noundef %tsd) #14
  br i1 %call48, label %label_return, label %if.end56

if.else51:                                        ; preds = %if.end43
  %call52 = tail call zeroext i1 @background_threads_disable(ptr noundef %tsd) #14
  br i1 %call52, label %label_return, label %if.end56

if.end56:                                         ; preds = %if.then47, %if.else51, %if.end, %monotonic.i
  br label %label_return

label_return:                                     ; preds = %if.else51, %if.then47, %do.end34, %if.else, %if.end56, %cond.end28, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 0, %if.end56 ], [ 22, %cond.end28 ], [ 22, %if.else ], [ 0, %do.end34 ], [ 14, %if.then47 ], [ 14, %if.else51 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i42 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @max_background_threads_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  tail call void @background_thread_ctl_init(ptr noundef %tsd) #14
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %call.i.i39 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i40 = icmp eq i32 %call.i.i39, 0
  br i1 %cmp.i.not.i40, label %if.end.i42, label %if.then.i41

if.then.i41:                                      ; preds = %malloc_mutex_lock.exit
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @background_thread_lock) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i42

if.end.i42:                                       ; preds = %if.then.i41, %malloc_mutex_lock.exit
  %3 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i43 = add i64 %3, 1
  store i64 %inc.i.i43, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %4 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i44 = icmp eq ptr %4, %tsd
  br i1 %cmp.not.i.i44, label %malloc_mutex_lock.exit47, label %if.then.i.i45

if.then.i.i45:                                    ; preds = %if.end.i42
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %5 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i46 = add i64 %5, 1
  store i64 %inc2.i.i46, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit47

malloc_mutex_lock.exit47:                         ; preds = %if.end.i42, %if.then.i.i45
  %cmp = icmp eq ptr %newp, null
  br i1 %cmp, label %if.then, label %if.else

if.then:                                          ; preds = %malloc_mutex_lock.exit47
  %6 = load i64, ptr @max_background_threads, align 8
  store i64 %6, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond = and i1 %cmp3, %cmp4
  br i1 %or.cond, label %if.then5, label %if.end47

if.then5:                                         ; preds = %if.then
  %7 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %7, 8
  br i1 %cmp6.not, label %if.end, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %7, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end:                                           ; preds = %if.then5
  store i64 %6, ptr %oldp, align 8
  br label %if.end47

if.else:                                          ; preds = %malloc_mutex_lock.exit47
  %cmp10.not = icmp eq i64 %newlen, 8
  br i1 %cmp10.not, label %if.end12, label %label_return

if.end12:                                         ; preds = %if.else
  %8 = load i64, ptr @max_background_threads, align 8
  store i64 %8, ptr %oldval, align 8
  %cmp14 = icmp ne ptr %oldp, null
  %cmp16 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp14, %cmp16
  br i1 %or.cond1, label %if.then17, label %do.end28

if.then17:                                        ; preds = %if.end12
  %9 = load i64, ptr %oldlenp, align 8
  %cmp18.not = icmp eq i64 %9, 8
  br i1 %cmp18.not, label %if.end26, label %if.then19

if.then19:                                        ; preds = %if.then17
  %spec.select38 = tail call i64 @llvm.umin.i64(i64 %9, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select38, i1 false)
  store i64 %spec.select38, ptr %oldlenp, align 8
  br label %label_return

if.end26:                                         ; preds = %if.then17
  store i64 %8, ptr %oldp, align 8
  br label %do.end28

do.end28:                                         ; preds = %if.end12, %if.end26
  %10 = load i64, ptr %newp, align 8
  %cmp29 = icmp eq i64 %10, %8
  br i1 %cmp29, label %label_return, label %if.end31

if.end31:                                         ; preds = %do.end28
  %11 = load i64, ptr @opt_max_background_threads, align 8
  %cmp32 = icmp ugt i64 %10, %11
  br i1 %cmp32, label %label_return, label %monotonic.i

monotonic.i:                                      ; preds = %if.end31
  %12 = load atomic i8, ptr @background_thread_enabled_state monotonic, align 1
  %13 = and i8 %12, 1
  %tobool.i80.not = icmp eq i8 %13, 0
  br i1 %tobool.i80.not, label %if.else45, label %if.then36

if.then36:                                        ; preds = %monotonic.i
  store atomic i8 0, ptr @background_thread_enabled_state monotonic, align 1
  %call38 = tail call zeroext i1 @background_threads_disable(ptr noundef %tsd) #14
  br i1 %call38, label %label_return, label %if.end40

if.end40:                                         ; preds = %if.then36
  store i64 %10, ptr @max_background_threads, align 8
  store atomic i8 1, ptr @background_thread_enabled_state monotonic, align 1
  %call42 = tail call zeroext i1 @background_threads_enable(ptr noundef %tsd) #14
  br i1 %call42, label %label_return, label %if.end47

if.else45:                                        ; preds = %monotonic.i
  store i64 %10, ptr @max_background_threads, align 8
  br label %if.end47

if.end47:                                         ; preds = %if.else45, %if.end40, %if.end, %if.then
  br label %label_return

label_return:                                     ; preds = %if.end40, %if.then36, %if.end31, %do.end28, %if.else, %if.end47, %if.then19, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 0, %if.end47 ], [ 22, %if.then19 ], [ 22, %if.else ], [ 0, %do.end28 ], [ 22, %if.end31 ], [ 14, %if.then36 ], [ 14, %if.end40 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i48 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

declare void @background_thread_ctl_init(ptr noundef) local_unnamed_addr #1

declare zeroext i1 @background_threads_enable(ptr noundef) local_unnamed_addr #1

declare zeroext i1 @background_threads_disable(ptr noundef) local_unnamed_addr #1

; Function Attrs: nounwind uwtable
define internal i32 @thread_arena_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %oldind = alloca i32, align 4
  %call = tail call fastcc ptr @arena_choose(ptr noundef %tsd)
  %cmp = icmp eq ptr %call, null
  br i1 %cmp, label %return, label %if.end

if.end:                                           ; preds = %entry
  %0 = getelementptr i8, ptr %call, i64 78944
  %call.val = load i32, ptr %0, align 32
  store i32 %call.val, ptr %oldind, align 4
  %cmp2.not = icmp eq ptr %newp, null
  br i1 %cmp2.not, label %do.body8, label %if.then3

if.then3:                                         ; preds = %if.end
  %cmp4.not = icmp eq i64 %newlen, 4
  br i1 %cmp4.not, label %if.end6, label %return

if.end6:                                          ; preds = %if.then3
  %1 = load i32, ptr %newp, align 4
  br label %do.body8

do.body8:                                         ; preds = %if.end6, %if.end
  %newind.0 = phi i32 [ %1, %if.end6 ], [ %call.val, %if.end ]
  %cmp9 = icmp ne ptr %oldp, null
  %cmp10 = icmp ne ptr %oldlenp, null
  %or.cond = and i1 %cmp9, %cmp10
  br i1 %or.cond, label %if.then11, label %do.end17

if.then11:                                        ; preds = %do.body8
  %2 = load i64, ptr %oldlenp, align 8
  %cmp12.not = icmp eq i64 %2, 4
  br i1 %cmp12.not, label %if.end15, label %if.then13

if.then13:                                        ; preds = %if.then11
  %spec.select = tail call i64 @llvm.umin.i64(i64 %2, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldind, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %return

if.end15:                                         ; preds = %if.then11
  store i32 %call.val, ptr %oldp, align 4
  br label %do.end17

do.end17:                                         ; preds = %do.body8, %if.end15
  %cmp18.not = icmp eq i32 %newind.0, %call.val
  br i1 %cmp18.not, label %return, label %if.then19

if.then19:                                        ; preds = %do.end17
  %call20 = tail call i32 @narenas_total_get() #14
  %cmp21.not = icmp ult i32 %newind.0, %call20
  br i1 %cmp21.not, label %if.end23, label %return

if.end23:                                         ; preds = %if.then19
  %3 = load i32, ptr @opt_percpu_arena, align 4
  %cmp24 = icmp ugt i32 %3, 2
  br i1 %cmp24, label %if.then25, label %if.end30

if.then25:                                        ; preds = %if.end23
  %cmp.i = icmp eq i32 %3, 4
  %4 = load i32, ptr @ncpus, align 4
  %cmp1.i = icmp ugt i32 %4, 1
  %or.cond1 = and i1 %cmp.i, %cmp1.i
  %rem.i = and i32 %4, 1
  %div3.i29 = lshr i32 %4, 1
  %spec.select32 = add nuw i32 %div3.i29, %rem.i
  %retval.i.0 = select i1 %or.cond1, i32 %spec.select32, i32 %4
  %cmp27 = icmp ult i32 %newind.0, %retval.i.0
  br i1 %cmp27, label %return, label %if.end30

if.end30:                                         ; preds = %if.then25, %if.end23
  %idxprom.i = zext i32 %newind.0 to i64
  %arrayidx.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %idxprom.i
  %5 = load atomic i64, ptr %arrayidx.i acquire, align 8
  %6 = inttoptr i64 %5 to ptr
  %cmp.i31 = icmp eq i64 %5, 0
  br i1 %cmp.i31, label %if.then3.i, label %arena_get.exit

if.then3.i:                                       ; preds = %if.end30
  %call4.i = tail call ptr @arena_init(ptr noundef %tsd, i32 noundef %newind.0, ptr noundef nonnull @arena_config_default) #14
  br label %arena_get.exit

arena_get.exit:                                   ; preds = %if.end30, %if.then3.i
  %ret.0.i = phi ptr [ %call4.i, %if.then3.i ], [ %6, %if.end30 ]
  %cmp33 = icmp eq ptr %ret.0.i, null
  br i1 %cmp33, label %return, label %if.end35

if.end35:                                         ; preds = %arena_get.exit
  tail call void @arena_migrate(ptr noundef %tsd, ptr noundef nonnull %call, ptr noundef nonnull %ret.0.i) #14
  %7 = load i8, ptr %tsd, align 1
  %8 = and i8 %7, 1
  %tobool.i56.not.not = icmp eq i8 %8, 0
  br i1 %tobool.i56.not.not, label %return, label %if.then37

if.then37:                                        ; preds = %if.end35
  %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache_slow.i = getelementptr inbounds i8, ptr %tsd, i64 256
  %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache.i = getelementptr inbounds i8, ptr %tsd, i64 864
  tail call void @tcache_arena_reassociate(ptr noundef nonnull %tsd, ptr noundef nonnull %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache_slow.i, ptr noundef nonnull %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache.i, ptr noundef nonnull %ret.0.i) #14
  br label %return

return:                                           ; preds = %if.then13, %if.then3, %if.then19, %if.then25, %arena_get.exit, %if.end35, %if.then37, %do.end17, %entry
  %retval.0 = phi i32 [ 11, %entry ], [ 22, %if.then13 ], [ 22, %if.then3 ], [ 14, %if.then19 ], [ 1, %if.then25 ], [ 11, %arena_get.exit ], [ 0, %if.end35 ], [ 0, %if.then37 ], [ 0, %do.end17 ]
  ret i32 %retval.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @thread_allocated_ctl(ptr nocapture noundef readonly %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %cant_access_tsd_items_directly_use_a_getter_or_setter_thread_allocated.i = getelementptr inbounds i8, ptr %tsd, i64 832
  %0 = load i64, ptr %cant_access_tsd_items_directly_use_a_getter_or_setter_thread_allocated.i, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @thread_allocatedp_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca ptr, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %cant_access_tsd_items_directly_use_a_getter_or_setter_thread_allocated.i = getelementptr inbounds i8, ptr %tsd, i64 832
  store ptr %cant_access_tsd_items_directly_use_a_getter_or_setter_thread_allocated.i, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %0, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store ptr %cant_access_tsd_items_directly_use_a_getter_or_setter_thread_allocated.i, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @thread_deallocated_ctl(ptr nocapture noundef readonly %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %cant_access_tsd_items_directly_use_a_getter_or_setter_thread_deallocated.i = getelementptr inbounds i8, ptr %tsd, i64 848
  %0 = load i64, ptr %cant_access_tsd_items_directly_use_a_getter_or_setter_thread_deallocated.i, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @thread_deallocatedp_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca ptr, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %cant_access_tsd_items_directly_use_a_getter_or_setter_thread_deallocated.i = getelementptr inbounds i8, ptr %tsd, i64 848
  store ptr %cant_access_tsd_items_directly_use_a_getter_or_setter_thread_deallocated.i, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %0, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store ptr %cant_access_tsd_items_directly_use_a_getter_or_setter_thread_deallocated.i, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @thread_idle_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef readnone %oldp, ptr noundef readnone %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %cmp = icmp ne ptr %oldp, null
  %cmp1 = icmp ne ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  %cmp3 = icmp ne ptr %newp, null
  %or.cond1 = or i1 %or.cond, %cmp3
  %cmp5 = icmp ne i64 %newlen, 0
  %or.cond2 = or i1 %or.cond1, %cmp5
  br i1 %or.cond2, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i8, ptr %tsd, align 1
  %1 = and i8 %0, 1
  %tobool.i.not.not = icmp eq i8 %1, 0
  br i1 %tobool.i.not.not, label %if.end7, label %if.then6

if.then6:                                         ; preds = %do.end
  tail call void @tcache_flush(ptr noundef nonnull %tsd) #14
  br label %if.end7

if.end7:                                          ; preds = %if.then6, %do.end
  %2 = load i32, ptr @opt_narenas, align 4
  %3 = load i32, ptr @ncpus, align 4
  %mul = shl i32 %3, 1
  %cmp8 = icmp ugt i32 %2, %mul
  br i1 %cmp8, label %if.then9, label %label_return

if.then9:                                         ; preds = %if.end7
  %call10 = tail call fastcc ptr @arena_choose(ptr noundef nonnull %tsd)
  %cmp11.not = icmp eq ptr %call10, null
  br i1 %cmp11.not, label %label_return, label %if.then12

if.then12:                                        ; preds = %if.then9
  tail call void @arena_decay(ptr noundef nonnull %tsd, ptr noundef nonnull %call10, i1 noundef zeroext false, i1 noundef zeroext true) #14
  br label %label_return

label_return:                                     ; preds = %if.end7, %if.then12, %if.then9, %entry
  %ret.0 = phi i32 [ 1, %entry ], [ 0, %if.then9 ], [ 0, %if.then12 ], [ 0, %if.end7 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal fastcc ptr @arena_choose(ptr noundef %tsd) unnamed_addr #0 {
entry:
  %cant_access_tsd_items_directly_use_a_getter_or_setter_reentrancy_level.i.i = getelementptr inbounds i8, ptr %tsd, i64 1
  %0 = load i8, ptr %cant_access_tsd_items_directly_use_a_getter_or_setter_reentrancy_level.i.i, align 1
  %cmp1.i = icmp sgt i8 %0, 0
  br i1 %cmp1.i, label %if.then5.i, label %cond.end.i

if.then5.i:                                       ; preds = %entry
  %1 = load atomic i64, ptr @arenas acquire, align 8
  %2 = inttoptr i64 %1 to ptr
  %cmp.i44.i = icmp eq i64 %1, 0
  br i1 %cmp.i44.i, label %if.then3.i.i, label %arena_choose_impl.exit

if.then3.i.i:                                     ; preds = %if.then5.i
  %call4.i.i = tail call ptr @arena_init(ptr noundef nonnull %tsd, i32 noundef 0, ptr noundef nonnull @arena_config_default) #14
  br label %arena_choose_impl.exit

cond.end.i:                                       ; preds = %entry
  %cant_access_tsd_items_directly_use_a_getter_or_setter_arena.i119.i = getelementptr inbounds i8, ptr %tsd, i64 144
  %3 = load ptr, ptr %cant_access_tsd_items_directly_use_a_getter_or_setter_arena.i119.i, align 8
  %cmp13.i = icmp eq ptr %3, null
  br i1 %cmp13.i, label %if.then21.i, label %if.end43.i

if.then21.i:                                      ; preds = %cond.end.i
  %call23.i = tail call ptr @arena_choose_hard(ptr noundef nonnull %tsd, i1 noundef zeroext false) #14
  %4 = load i8, ptr %tsd, align 1
  %5 = and i8 %4, 1
  %tobool.i123.not.not.i = icmp eq i8 %5, 0
  br i1 %tobool.i123.not.not.i, label %if.end43.i, label %if.then25.i

if.then25.i:                                      ; preds = %if.then21.i
  %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache_slow.i.i = getelementptr inbounds i8, ptr %tsd, i64 256
  %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache.i.i = getelementptr inbounds i8, ptr %tsd, i64 864
  %arena28.i = getelementptr inbounds i8, ptr %tsd, i64 296
  %6 = load ptr, ptr %arena28.i, align 8
  %cmp29.not.i = icmp eq ptr %6, null
  br i1 %cmp29.not.i, label %if.else.i, label %do.end33.i

do.end33.i:                                       ; preds = %if.then25.i
  %cmp35.not.i = icmp eq ptr %6, %call23.i
  br i1 %cmp35.not.i, label %if.end43.i, label %if.then37.i

if.then37.i:                                      ; preds = %do.end33.i
  tail call void @tcache_arena_reassociate(ptr noundef nonnull %tsd, ptr noundef nonnull %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache_slow.i.i, ptr noundef nonnull %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache.i.i, ptr noundef %call23.i) #14
  br label %if.end43.i

if.else.i:                                        ; preds = %if.then25.i
  tail call void @tcache_arena_associate(ptr noundef nonnull %tsd, ptr noundef nonnull %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache_slow.i.i, ptr noundef nonnull %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache.i.i, ptr noundef %call23.i) #14
  br label %if.end43.i

if.end43.i:                                       ; preds = %if.else.i, %if.then37.i, %do.end33.i, %if.then21.i, %cond.end.i
  %ret.0.i = phi ptr [ %call23.i, %if.then37.i ], [ %call23.i, %do.end33.i ], [ %call23.i, %if.else.i ], [ %call23.i, %if.then21.i ], [ %3, %cond.end.i ]
  %7 = load i32, ptr @opt_percpu_arena, align 4
  %cmp44.i = icmp ugt i32 %7, 2
  br i1 %cmp44.i, label %land.lhs.true47.i, label %arena_choose_impl.exit

land.lhs.true47.i:                                ; preds = %if.end43.i
  %8 = getelementptr i8, ptr %ret.0.i, i64 78944
  %ret.0.val43.i = load i32, ptr %8, align 32
  %cmp.i.i = icmp eq i32 %7, 4
  %9 = load i32, ptr @ncpus, align 4
  %cmp1.i.i = icmp ugt i32 %9, 1
  %or.cond.i = and i1 %cmp.i.i, %cmp1.i.i
  %rem.i.i = and i32 %9, 1
  %div3.i40.i = lshr i32 %9, 1
  %spec.select1.i = add nuw i32 %div3.i40.i, %rem.i.i
  %retval.i.0.i = select i1 %or.cond.i, i32 %spec.select1.i, i32 %9
  %cmp50.i = icmp ult i32 %ret.0.val43.i, %retval.i.0.i
  br i1 %cmp50.i, label %land.lhs.true52.i, label %arena_choose_impl.exit

land.lhs.true52.i:                                ; preds = %land.lhs.true47.i
  %last_thd.i = getelementptr inbounds i8, ptr %ret.0.i, i64 16
  %10 = load ptr, ptr %last_thd.i, align 16
  %cmp54.not.i = icmp eq ptr %10, %tsd
  br i1 %cmp54.not.i, label %arena_choose_impl.exit, label %if.then56.i

if.then56.i:                                      ; preds = %land.lhs.true52.i
  %call.i120.i = tail call i32 @sched_getcpu() #14
  %11 = load i32, ptr @opt_percpu_arena, align 4
  %cmp.i90.i = icmp eq i32 %11, 3
  br i1 %cmp.i90.i, label %percpu_arena_choose.exit.i, label %lor.lhs.false.i.i

lor.lhs.false.i.i:                                ; preds = %if.then56.i
  %12 = load i32, ptr @ncpus, align 4
  %div.i9142.i = lshr i32 %12, 1
  %cmp3.i.i = icmp ult i32 %call.i120.i, %div.i9142.i
  %sub.i.i = select i1 %cmp3.i.i, i32 0, i32 %div.i9142.i
  %spec.select.i = sub i32 %call.i120.i, %sub.i.i
  br label %percpu_arena_choose.exit.i

percpu_arena_choose.exit.i:                       ; preds = %lor.lhs.false.i.i, %if.then56.i
  %arena_ind.i.0.i = phi i32 [ %call.i120.i, %if.then56.i ], [ %spec.select.i, %lor.lhs.false.i.i ]
  %ret.0.val.i = load i32, ptr %8, align 32
  %cmp59.not.i = icmp eq i32 %ret.0.val.i, %arena_ind.i.0.i
  br i1 %cmp59.not.i, label %if.end63.i, label %if.then61.i

if.then61.i:                                      ; preds = %percpu_arena_choose.exit.i
  %13 = load ptr, ptr %cant_access_tsd_items_directly_use_a_getter_or_setter_arena.i119.i, align 8
  %14 = getelementptr i8, ptr %13, i64 78944
  %.val.i.i = load i32, ptr %14, align 32
  %cmp.not.i.i = icmp eq i32 %.val.i.i, %arena_ind.i.0.i
  br i1 %cmp.not.i.i, label %percpu_arena_update.exit.i, label %if.then.i45.i

if.then.i45.i:                                    ; preds = %if.then61.i
  %idxprom.i.i.i = zext i32 %arena_ind.i.0.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %idxprom.i.i.i
  %15 = load atomic i64, ptr %arrayidx.i.i.i acquire, align 8
  %16 = inttoptr i64 %15 to ptr
  %cmp.i.i.i = icmp eq i64 %15, 0
  br i1 %cmp.i.i.i, label %if.then3.i.i.i, label %arena_get.exit.i.i

if.then3.i.i.i:                                   ; preds = %if.then.i45.i
  %call4.i.i.i = tail call ptr @arena_init(ptr noundef nonnull %tsd, i32 noundef %arena_ind.i.0.i, ptr noundef nonnull @arena_config_default) #14
  br label %arena_get.exit.i.i

arena_get.exit.i.i:                               ; preds = %if.then3.i.i.i, %if.then.i45.i
  %ret.0.i.i.i = phi ptr [ %call4.i.i.i, %if.then3.i.i.i ], [ %16, %if.then.i45.i ]
  tail call void @arena_migrate(ptr noundef nonnull %tsd, ptr noundef nonnull %13, ptr noundef %ret.0.i.i.i) #14
  %17 = load i8, ptr %tsd, align 1
  %18 = and i8 %17, 1
  %tobool.i.not.not.i.i = icmp eq i8 %18, 0
  br i1 %tobool.i.not.not.i.i, label %percpu_arena_update.exit.i, label %if.then10.i.i

if.then10.i.i:                                    ; preds = %arena_get.exit.i.i
  %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache.i.i.i = getelementptr inbounds i8, ptr %tsd, i64 864
  %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache_slow.i.i.i = getelementptr inbounds i8, ptr %tsd, i64 256
  tail call void @tcache_arena_reassociate(ptr noundef nonnull %tsd, ptr noundef nonnull %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache_slow.i.i.i, ptr noundef nonnull %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache.i.i.i, ptr noundef %ret.0.i.i.i) #14
  br label %percpu_arena_update.exit.i

percpu_arena_update.exit.i:                       ; preds = %if.then10.i.i, %arena_get.exit.i.i, %if.then61.i
  %19 = load ptr, ptr %cant_access_tsd_items_directly_use_a_getter_or_setter_arena.i119.i, align 8
  br label %if.end63.i

if.end63.i:                                       ; preds = %percpu_arena_update.exit.i, %percpu_arena_choose.exit.i
  %ret.1.i = phi ptr [ %19, %percpu_arena_update.exit.i ], [ %ret.0.i, %percpu_arena_choose.exit.i ]
  %last_thd65.i = getelementptr inbounds i8, ptr %ret.1.i, i64 16
  store ptr %tsd, ptr %last_thd65.i, align 16
  br label %arena_choose_impl.exit

arena_choose_impl.exit:                           ; preds = %if.then5.i, %if.then3.i.i, %if.end43.i, %land.lhs.true47.i, %land.lhs.true52.i, %if.end63.i
  %retval.0.i = phi ptr [ %ret.1.i, %if.end63.i ], [ %ret.0.i, %land.lhs.true52.i ], [ %ret.0.i, %land.lhs.true47.i ], [ %ret.0.i, %if.end43.i ], [ %call4.i.i, %if.then3.i.i ], [ %2, %if.then5.i ]
  ret ptr %retval.0.i
}

declare void @arena_migrate(ptr noundef, ptr noundef, ptr noundef) local_unnamed_addr #1

declare void @tcache_arena_reassociate(ptr noundef, ptr noundef, ptr noundef, ptr noundef) local_unnamed_addr #1

declare ptr @arena_choose_hard(ptr noundef, i1 noundef zeroext) local_unnamed_addr #1

declare void @tcache_arena_associate(ptr noundef, ptr noundef, ptr noundef, ptr noundef) local_unnamed_addr #1

; Function Attrs: nounwind
declare i32 @sched_getcpu() local_unnamed_addr #3

; Function Attrs: nounwind uwtable
define internal i32 @thread_tcache_enabled_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i8, align 1
  %tsd.val = load i8, ptr %tsd, align 1
  %0 = and i8 %tsd.val, 1
  store i8 %0, ptr %oldval, align 1
  %cmp.not = icmp eq ptr %newp, null
  br i1 %cmp.not, label %do.body, label %if.then

if.then:                                          ; preds = %entry
  %cmp1.not = icmp eq i64 %newlen, 1
  br i1 %cmp1.not, label %if.end, label %label_return

if.end:                                           ; preds = %if.then
  %1 = load i8, ptr %newp, align 1
  %2 = and i8 %1, 1
  %tobool = icmp ne i8 %2, 0
  tail call void @tcache_enabled_set(ptr noundef nonnull %tsd, i1 noundef zeroext %tobool) #14
  br label %do.body

do.body:                                          ; preds = %entry, %if.end
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond = and i1 %cmp4, %cmp5
  br i1 %or.cond, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.body
  %3 = load i64, ptr %oldlenp, align 8
  switch i64 %3, label %cond.end [
    i64 1, label %if.end10
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then6
  br label %cond.end

cond.end:                                         ; preds = %if.then6, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then6 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i8 %0, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.body, %if.then, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 22, %if.then ], [ 0, %do.body ], [ 0, %if.end10 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @thread_tcache_max_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache.i = getelementptr inbounds i8, ptr %tsd, i64 864
  %0 = load ptr, ptr %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache.i, align 8
  %1 = getelementptr i8, ptr %0, i64 48
  %.val = load i32, ptr %1, align 8
  %sub.i30 = add i32 %.val, -1
  %idxprom.i.i31 = zext i32 %sub.i30 to i64
  %arrayidx.i.i32 = getelementptr inbounds [232 x i64], ptr @sz_index2size_tab, i64 0, i64 %idxprom.i.i31
  %2 = load i64, ptr %arrayidx.i.i32, align 8
  store i64 %2, ptr %oldval, align 8
  %cmp = icmp ne ptr %oldp, null
  %cmp3 = icmp ne ptr %oldlenp, null
  %or.cond = and i1 %cmp, %cmp3
  br i1 %or.cond, label %if.then, label %do.end8

if.then:                                          ; preds = %entry
  %3 = load i64, ptr %oldlenp, align 8
  %cmp4.not = icmp eq i64 %3, 8
  br i1 %cmp4.not, label %if.end, label %if.then5

if.then5:                                         ; preds = %if.then
  %spec.select = tail call i64 @llvm.umin.i64(i64 %3, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end:                                           ; preds = %if.then
  store i64 %2, ptr %oldp, align 8
  br label %do.end8

do.end8:                                          ; preds = %entry, %if.end
  %cmp9.not = icmp eq ptr %newp, null
  br i1 %cmp9.not, label %label_return, label %if.then10

if.then10:                                        ; preds = %do.end8
  %cmp11.not = icmp eq i64 %newlen, 8
  br i1 %cmp11.not, label %do.end21, label %label_return

do.end21:                                         ; preds = %if.then10
  %4 = load i64, ptr %newp, align 8
  %spec.store.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8388608)
  %cmp.i = icmp ult i64 %4, 4097
  br i1 %cmp.i, label %if.then.i, label %if.end14.i

if.then.i:                                        ; preds = %do.end21
  %sub.i46 = add nuw nsw i64 %spec.store.select, 7
  %shr.i = lshr i64 %sub.i46, 3
  %arrayidx.i = getelementptr inbounds [0 x i8], ptr @sz_size2index_tab, i64 0, i64 %shr.i
  %5 = load i8, ptr %arrayidx.i, align 1
  %idxprom.i.i = zext i8 %5 to i64
  %arrayidx.i.i = getelementptr inbounds [232 x i64], ptr @sz_index2size_tab, i64 0, i64 %idxprom.i.i
  %6 = load i64, ptr %arrayidx.i.i, align 8
  br label %sz_s2u.exit

if.end14.i:                                       ; preds = %do.end21
  %shl15.i = shl nuw nsw i64 %spec.store.select, 1
  %sub.i = add nsw i64 %shl15.i, -1
  %7 = tail call i64 @llvm.ctlz.i64(i64 %sub.i, i1 true), !range !17
  %8 = trunc i64 %7 to i32
  %conv1.i.i.i = xor i32 %8, 63
  %cmp18.i = icmp ult i32 %conv1.i.i.i, 7
  %conv17.i = zext nneg i32 %conv1.i.i.i to i64
  %sub23.i = add nsw i64 %conv17.i, -3
  %notmask = shl nsw i64 -1, %sub23.i
  %9 = xor i64 %notmask, -1
  %sub27.i = select i1 %cmp18.i, i64 15, i64 %9
  %add.i = add nuw i64 %sub27.i, %spec.store.select
  %not.i = xor i64 %sub27.i, -1
  %and.i = and i64 %add.i, %not.i
  br label %sz_s2u.exit

sz_s2u.exit:                                      ; preds = %if.end14.i, %if.then.i
  %retval.i.0 = phi i64 [ %6, %if.then.i ], [ %and.i, %if.end14.i ]
  %cmp26.not = icmp eq i64 %retval.i.0, %2
  br i1 %cmp26.not, label %label_return, label %if.then27

if.then27:                                        ; preds = %sz_s2u.exit
  tail call void @thread_tcache_max_set(ptr noundef nonnull %tsd, i64 noundef %retval.i.0) #14
  br label %label_return

label_return:                                     ; preds = %do.end8, %if.then27, %sz_s2u.exit, %if.then10, %if.then5
  %ret.0 = phi i32 [ 22, %if.then5 ], [ 22, %if.then10 ], [ 0, %sz_s2u.exit ], [ 0, %if.then27 ], [ 0, %do.end8 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @thread_tcache_flush_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef readnone %oldp, ptr noundef readnone %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %0 = load i8, ptr %tsd, align 1
  %1 = and i8 %0, 1
  %tobool.i.not.not = icmp eq i8 %1, 0
  br i1 %tobool.i.not.not, label %label_return, label %do.body

do.body:                                          ; preds = %entry
  %cmp = icmp ne ptr %oldp, null
  %cmp1 = icmp ne ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  %cmp3 = icmp ne ptr %newp, null
  %or.cond1 = or i1 %or.cond, %cmp3
  %cmp5 = icmp ne i64 %newlen, 0
  %or.cond2 = or i1 %or.cond1, %cmp5
  br i1 %or.cond2, label %label_return, label %do.end

do.end:                                           ; preds = %do.body
  tail call void @tcache_flush(ptr noundef nonnull %tsd) #14
  br label %label_return

label_return:                                     ; preds = %do.body, %entry, %do.end
  %ret.0 = phi i32 [ 0, %do.end ], [ 14, %entry ], [ 1, %do.body ]
  ret i32 %ret.0
}

declare void @tcache_enabled_set(ptr noundef, i1 noundef zeroext) local_unnamed_addr #1

declare void @thread_tcache_max_set(ptr noundef, i64 noundef) local_unnamed_addr #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare i64 @llvm.ctlz.i64(i64, i1 immarg) #6

declare void @tcache_flush(ptr noundef) local_unnamed_addr #1

; Function Attrs: nounwind uwtable
define internal i32 @thread_tcache_ncached_max_read_sizeclass_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %ncached_max = alloca i16, align 2
  %result = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp3.not = icmp eq i64 %newlen, 8
  %or.cond10 = and i1 %cmp, %cmp3.not
  br i1 %or.cond10, label %do.end, label %label_return

do.end:                                           ; preds = %entry
  %0 = load i64, ptr %newp, align 8
  store i16 0, ptr %ncached_max, align 2
  %call = call zeroext i1 @tcache_bin_ncached_max_read(ptr noundef %tsd, i64 noundef %0, ptr noundef nonnull %ncached_max) #14
  br i1 %call, label %label_return, label %if.end8

if.end8:                                          ; preds = %do.end
  %1 = load i16, ptr %ncached_max, align 2
  %conv = zext i16 %1 to i64
  store i64 %conv, ptr %result, align 8
  %cmp10 = icmp ne ptr %oldp, null
  %cmp12 = icmp ne ptr %oldlenp, null
  %or.cond = and i1 %cmp10, %cmp12
  br i1 %or.cond, label %if.then14, label %label_return

if.then14:                                        ; preds = %if.end8
  %2 = load i64, ptr %oldlenp, align 8
  %cmp15.not = icmp eq i64 %2, 8
  br i1 %cmp15.not, label %if.end20, label %if.then17

if.then17:                                        ; preds = %if.then14
  %spec.select = call i64 @llvm.umin.i64(i64 %2, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %result, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end20:                                         ; preds = %if.then14
  store i64 %conv, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end20, %if.end8, %do.end, %entry, %if.then17
  %ret.0 = phi i32 [ 22, %if.then17 ], [ 22, %entry ], [ 22, %do.end ], [ 0, %if.end8 ], [ 0, %if.end20 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @thread_tcache_ncached_max_write_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef readnone %oldp, ptr noundef readnone %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %cmp = icmp ne ptr %oldp, null
  %cmp1 = icmp ne ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %cmp2.not = icmp eq ptr %newp, null
  br i1 %cmp2.not, label %if.end27, label %if.then3

if.then3:                                         ; preds = %do.end
  %0 = load i8, ptr %tsd, align 1
  %1 = and i8 %0, 1
  %tobool.i.not.not = icmp eq i8 %1, 0
  br i1 %tobool.i.not.not, label %label_return, label %if.then8

if.then8:                                         ; preds = %if.then3
  %cmp9.not = icmp eq i64 %newlen, 8
  br i1 %cmp9.not, label %if.end11, label %label_return

if.end11:                                         ; preds = %if.then8
  %2 = load ptr, ptr %newp, align 8
  %cmp14 = icmp eq ptr %2, null
  br i1 %cmp14, label %label_return, label %if.end16

if.end16:                                         ; preds = %if.end11
  %call17 = tail call ptr @memchr(ptr noundef nonnull dereferenceable(1) %2, i32 noundef 0, i64 noundef 1000) #15
  %cmp18 = icmp eq ptr %call17, null
  br i1 %cmp18, label %label_return, label %if.end20

if.end20:                                         ; preds = %if.end16
  %cmp21 = icmp eq ptr %call17, %2
  br i1 %cmp21, label %label_return, label %if.end23

if.end23:                                         ; preds = %if.end20
  %3 = ptrtoint ptr %2 to i64
  %4 = ptrtoint ptr %call17 to i64
  %sub = sub i64 %4, %3
  %call24 = tail call zeroext i1 @tcache_bins_ncached_max_write(ptr noundef nonnull %tsd, ptr noundef nonnull %2, i64 noundef %sub) #14
  br i1 %call24, label %label_return, label %if.end27

if.end27:                                         ; preds = %if.end23, %do.end
  br label %label_return

label_return:                                     ; preds = %if.end23, %if.end20, %if.end16, %if.end11, %if.then8, %if.then3, %entry, %if.end27
  %ret.0 = phi i32 [ 0, %if.end27 ], [ 1, %entry ], [ 2, %if.then3 ], [ 22, %if.then8 ], [ 22, %if.end11 ], [ 22, %if.end16 ], [ 0, %if.end20 ], [ 22, %if.end23 ]
  ret i32 %ret.0
}

declare zeroext i1 @tcache_bin_ncached_max_read(ptr noundef, i64 noundef, ptr noundef) local_unnamed_addr #1

; Function Attrs: mustprogress nofree nounwind willreturn memory(argmem: read)
declare ptr @memchr(ptr noundef, i32 noundef, i64 noundef) local_unnamed_addr #8

declare zeroext i1 @tcache_bins_ncached_max_write(ptr noundef, ptr noundef, i64 noundef) local_unnamed_addr #1

; Function Attrs: nounwind uwtable
define internal i32 @thread_peak_read_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %result = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  tail call void @peak_event_update(ptr noundef %tsd) #14
  %call = tail call i64 @peak_event_max(ptr noundef %tsd) #14
  store i64 %call, ptr %result, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %0, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %result, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %call, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @thread_peak_reset_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef readnone %oldp, ptr noundef readnone %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %cmp = icmp ne ptr %oldp, null
  %cmp1 = icmp ne ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  %cmp3 = icmp ne ptr %newp, null
  %or.cond1 = or i1 %or.cond, %cmp3
  %cmp5 = icmp ne i64 %newlen, 0
  %or.cond2 = or i1 %or.cond1, %cmp5
  br i1 %or.cond2, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  tail call void @peak_event_zero(ptr noundef %tsd) #14
  br label %label_return

label_return:                                     ; preds = %entry, %do.end
  %ret.0 = phi i32 [ 0, %do.end ], [ 1, %entry ]
  ret i32 %ret.0
}

declare void @peak_event_update(ptr noundef) local_unnamed_addr #1

declare i64 @peak_event_max(ptr noundef) local_unnamed_addr #1

declare void @peak_event_zero(ptr noundef) local_unnamed_addr #1

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @thread_prof_name_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @thread_prof_active_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

declare void @arena_decay(ptr noundef, ptr noundef, i1 noundef zeroext, i1 noundef zeroext) local_unnamed_addr #1

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @config_cache_oblivious_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i8 1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  switch i64 %0, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @config_debug_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i8 0, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  switch i64 %0, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 0, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @config_fill_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i8 1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  switch i64 %0, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @config_lazy_lock_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i8 0, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  switch i64 %0, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 0, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @config_malloc_conf_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca ptr, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store ptr @.str.1, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %0, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store ptr @.str.1, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @config_opt_safety_checks_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i8 0, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  switch i64 %0, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 0, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @config_prof_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i8 0, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  switch i64 %0, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 0, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @config_prof_libgcc_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i8 0, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  switch i64 %0, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 0, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @config_prof_libunwind_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i8 0, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  switch i64 %0, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 0, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @config_stats_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i8 1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  switch i64 %0, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @config_utrace_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i8 0, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  switch i64 %0, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 0, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @config_xmalloc_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i8 0, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  switch i64 %0, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 0, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_abort_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i8, ptr @opt_abort, align 1
  %1 = and i8 %0, 1
  store i8 %1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  switch i64 %2, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 %1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_abort_conf_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i8, ptr @opt_abort_conf, align 1
  %1 = and i8 %0, 1
  store i8 %1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  switch i64 %2, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 %1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_cache_oblivious_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i8, ptr @opt_cache_oblivious, align 1
  %1 = and i8 %0, 1
  store i8 %1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  switch i64 %2, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 %1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_trust_madvise_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i8, ptr @opt_trust_madvise, align 1
  %1 = and i8 %0, 1
  store i8 %1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  switch i64 %2, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 %1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_confirm_conf_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i8, ptr @opt_confirm_conf, align 1
  %1 = and i8 %0, 1
  store i8 %1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  switch i64 %2, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 %1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_hpa_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i8, ptr @opt_hpa, align 1
  %1 = and i8 %0, 1
  store i8 %1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  switch i64 %2, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 %1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_hpa_slab_max_alloc_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @opt_hpa_opts, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_hpa_hugification_threshold_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.hpa_shard_opts_s, ptr @opt_hpa_opts, i64 0, i32 1), align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_hpa_hugify_delay_ms_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.hpa_shard_opts_s, ptr @opt_hpa_opts, i64 0, i32 4), align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_hpa_min_purge_interval_ms_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.hpa_shard_opts_s, ptr @opt_hpa_opts, i64 0, i32 5), align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_hpa_dirty_mult_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i32, ptr getelementptr inbounds (%struct.hpa_shard_opts_s, ptr @opt_hpa_opts, i64 0, i32 2), align 8
  store i32 %0, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %0, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_hpa_sec_nshards_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @opt_hpa_sec_opts, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_hpa_sec_max_alloc_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.sec_opts_s, ptr @opt_hpa_sec_opts, i64 0, i32 1), align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_hpa_sec_max_bytes_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.sec_opts_s, ptr @opt_hpa_sec_opts, i64 0, i32 2), align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_hpa_sec_bytes_after_flush_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.sec_opts_s, ptr @opt_hpa_sec_opts, i64 0, i32 3), align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_hpa_sec_batch_fill_extra_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.sec_opts_s, ptr @opt_hpa_sec_opts, i64 0, i32 4), align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_metadata_thp_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca ptr, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i32, ptr @opt_metadata_thp, align 4
  %idxprom = zext i32 %0 to i64
  %arrayidx = getelementptr inbounds [0 x ptr], ptr @metadata_thp_mode_names, i64 0, i64 %idxprom
  %1 = load ptr, ptr %arrayidx, align 8
  store ptr %1, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %2, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %2, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store ptr %1, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_retain_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i8, ptr @opt_retain, align 1
  %1 = and i8 %0, 1
  store i8 %1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  switch i64 %2, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 %1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_dss_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca ptr, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load ptr, ptr @opt_dss, align 8
  store ptr %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store ptr %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_narenas_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i32, ptr @opt_narenas, align 4
  store i32 %0, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %0, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_percpu_arena_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca ptr, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i32, ptr @opt_percpu_arena, align 4
  %idxprom = zext i32 %0 to i64
  %arrayidx = getelementptr inbounds [0 x ptr], ptr @percpu_arena_mode_names, i64 0, i64 %idxprom
  %1 = load ptr, ptr %arrayidx, align 8
  store ptr %1, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %2, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %2, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store ptr %1, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_oversize_threshold_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @opt_oversize_threshold, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_mutex_max_spin_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @opt_mutex_max_spin, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_background_thread_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i8, ptr @opt_background_thread, align 1
  %1 = and i8 %0, 1
  store i8 %1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  switch i64 %2, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 %1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_max_background_threads_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @opt_max_background_threads, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_dirty_decay_ms_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @opt_dirty_decay_ms, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_muzzy_decay_ms_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @opt_muzzy_decay_ms, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_stats_print_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i8, ptr @opt_stats_print, align 1
  %1 = and i8 %0, 1
  store i8 %1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  switch i64 %2, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 %1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @opt_stats_print_opts_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca ptr, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store ptr @opt_stats_print_opts, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %0, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store ptr @opt_stats_print_opts, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_stats_interval_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @opt_stats_interval, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @opt_stats_interval_opts_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca ptr, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store ptr @opt_stats_interval_opts, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %0, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store ptr @opt_stats_interval_opts, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_junk_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca ptr, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load ptr, ptr @opt_junk, align 8
  store ptr %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store ptr %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_zero_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i8, ptr @opt_zero, align 1
  %1 = and i8 %0, 1
  store i8 %1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  switch i64 %2, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 %1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_utrace_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_xmalloc_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_experimental_infallible_new_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i8, ptr @opt_experimental_infallible_new, align 1
  %1 = and i8 %0, 1
  store i8 %1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  switch i64 %2, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 %1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_tcache_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i8, ptr @opt_tcache, align 1
  %1 = and i8 %0, 1
  store i8 %1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  switch i64 %2, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 %1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_tcache_max_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @opt_tcache_max, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_tcache_nslots_small_min_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i32, ptr @opt_tcache_nslots_small_min, align 4
  store i32 %0, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %0, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_tcache_nslots_small_max_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i32, ptr @opt_tcache_nslots_small_max, align 4
  store i32 %0, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %0, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_tcache_nslots_large_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i32, ptr @opt_tcache_nslots_large, align 4
  store i32 %0, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %0, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_lg_tcache_nslots_mul_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @opt_lg_tcache_nslots_mul, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_tcache_gc_incr_bytes_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @opt_tcache_gc_incr_bytes, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_tcache_gc_delay_bytes_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @opt_tcache_gc_delay_bytes, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_lg_tcache_flush_small_div_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i32, ptr @opt_lg_tcache_flush_small_div, align 4
  store i32 %0, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %0, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_lg_tcache_flush_large_div_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i32, ptr @opt_lg_tcache_flush_large_div, align 4
  store i32 %0, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %0, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_thp_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca ptr, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i32, ptr @opt_thp, align 4
  %idxprom = zext i32 %0 to i64
  %arrayidx = getelementptr inbounds [0 x ptr], ptr @thp_mode_names, i64 0, i64 %idxprom
  %1 = load ptr, ptr %arrayidx, align 8
  store ptr %1, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %2, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %2, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store ptr %1, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_lg_extent_max_active_fit_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @opt_lg_extent_max_active_fit, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_prefix_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_active_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_thread_active_init_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_bt_max_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_lg_prof_sample_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_lg_prof_interval_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_gdump_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_final_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_leak_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_leak_error_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_accum_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_recent_alloc_max_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_stats_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_sys_thread_name_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_time_res_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_lg_san_uaf_align_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_zero_realloc_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca ptr, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i32, ptr @opt_zero_realloc_action, align 4
  %idxprom = zext i32 %0 to i64
  %arrayidx = getelementptr inbounds [0 x ptr], ptr @zero_realloc_mode_names, i64 0, i64 %idxprom
  %1 = load ptr, ptr %arrayidx, align 8
  store ptr %1, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %2, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %2, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store ptr %1, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_debug_double_free_max_scan_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i32, ptr @opt_debug_double_free_max_scan, align 4
  store i32 %0, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %0, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @tcache_create_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %tcache_ind = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.body2

do.body2:                                         ; preds = %entry
  %cmp3 = icmp eq ptr %oldp, null
  %cmp5 = icmp eq ptr %oldlenp, null
  %or.cond1 = or i1 %cmp3, %cmp5
  br i1 %or.cond1, label %if.then8, label %lor.lhs.false6

lor.lhs.false6:                                   ; preds = %do.body2
  %0 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %0, 4
  br i1 %cmp7.not, label %do.end13, label %if.then10

if.then8:                                         ; preds = %do.body2
  br i1 %cmp5, label %label_return, label %if.then10

if.then10:                                        ; preds = %lor.lhs.false6, %if.then8
  store i64 0, ptr %oldlenp, align 8
  br label %label_return

do.end13:                                         ; preds = %lor.lhs.false6
  %call = tail call ptr @b0get() #14
  %call14 = call zeroext i1 @tcaches_create(ptr noundef %tsd, ptr noundef %call, ptr noundef nonnull %tcache_ind) #14
  br i1 %call14, label %label_return, label %if.then20

if.then20:                                        ; preds = %do.end13
  %1 = load i64, ptr %oldlenp, align 8
  %cmp21.not = icmp eq i64 %1, 4
  br i1 %cmp21.not, label %if.end24, label %if.then22

if.then22:                                        ; preds = %if.then20
  %spec.select = call i64 @llvm.umin.i64(i64 %1, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %tcache_ind, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end24:                                         ; preds = %if.then20
  %2 = load i32, ptr %tcache_ind, align 4
  store i32 %2, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %do.end13, %if.then8, %if.then10, %entry, %if.end24, %if.then22
  %ret.0 = phi i32 [ 22, %if.then22 ], [ 0, %if.end24 ], [ 1, %entry ], [ 22, %if.then10 ], [ 22, %if.then8 ], [ 14, %do.end13 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @tcache_flush_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef readnone %oldp, ptr noundef readnone %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %cmp = icmp ne ptr %oldp, null
  %cmp1 = icmp ne ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.body2

do.body2:                                         ; preds = %entry
  %cmp3 = icmp eq ptr %newp, null
  %cmp5 = icmp ne i64 %newlen, 4
  %or.cond1 = or i1 %cmp3, %cmp5
  br i1 %or.cond1, label %label_return, label %if.end7

if.end7:                                          ; preds = %do.body2
  %0 = load i32, ptr %newp, align 4
  tail call void @tcaches_flush(ptr noundef %tsd, i32 noundef %0) #14
  br label %label_return

label_return:                                     ; preds = %do.body2, %entry, %if.end7
  %ret.0 = phi i32 [ 0, %if.end7 ], [ 1, %entry ], [ 22, %do.body2 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @tcache_destroy_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef readnone %oldp, ptr noundef readnone %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %cmp = icmp ne ptr %oldp, null
  %cmp1 = icmp ne ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.body2

do.body2:                                         ; preds = %entry
  %cmp3 = icmp eq ptr %newp, null
  %cmp5 = icmp ne i64 %newlen, 4
  %or.cond1 = or i1 %cmp3, %cmp5
  br i1 %or.cond1, label %label_return, label %if.end7

if.end7:                                          ; preds = %do.body2
  %0 = load i32, ptr %newp, align 4
  tail call void @tcaches_destroy(ptr noundef %tsd, i32 noundef %0) #14
  br label %label_return

label_return:                                     ; preds = %do.body2, %entry, %if.end7
  %ret.0 = phi i32 [ 0, %if.end7 ], [ 1, %entry ], [ 22, %do.body2 ]
  ret i32 %ret.0
}

declare zeroext i1 @tcaches_create(ptr noundef, ptr noundef, ptr noundef) local_unnamed_addr #1

declare void @tcaches_flush(ptr noundef, i32 noundef) local_unnamed_addr #1

declare void @tcaches_destroy(ptr noundef, i32 noundef) local_unnamed_addr #1

; Function Attrs: nounwind uwtable
define internal ptr @arena_i_index(ptr noundef %tsdn, ptr nocapture readnone %mib, i64 %miblen, i64 noundef %i) #0 {
entry:
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsdn
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsdn, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %3 = and i64 %i, -2
  %switch = icmp eq i64 %3, 4096
  br i1 %switch, label %sw.epilog, label %sw.default

sw.default:                                       ; preds = %malloc_mutex_lock.exit
  %4 = load ptr, ptr @ctl_arenas, align 8
  %narenas = getelementptr inbounds i8, ptr %4, i64 8
  %5 = load i32, ptr %narenas, align 8
  %conv = zext i32 %5 to i64
  %cmp = icmp ult i64 %conv, %i
  br i1 %cmp, label %label_return, label %sw.epilog

sw.epilog:                                        ; preds = %malloc_mutex_lock.exit, %sw.default
  br label %label_return

label_return:                                     ; preds = %sw.default, %sw.epilog
  %ret.0 = phi ptr [ @super_arena_i_node, %sw.epilog ], [ null, %sw.default ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret ptr %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @arena_i_initialized_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %initialized = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.body2

do.body2:                                         ; preds = %entry
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 8
  %0 = load i64, ptr %arrayidx, align 8
  %cmp3 = icmp ugt i64 %0, 4294967295
  br i1 %cmp3, label %label_return, label %if.end5

if.end5:                                          ; preds = %do.body2
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %if.end5
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %if.end5
  %1 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %1, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %2 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %2, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %3 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %3, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %malloc_mutex_lock.exit
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %malloc_mutex_lock.exit
  %6 = load ptr, ptr @ctl_arenas, align 8
  %trunc = trunc i64 %0 to i32
  switch i32 %trunc, label %sw.default.i.i.i [
    i32 4096, label %arenas_i.exit
    i32 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %0, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add nuw nsw i64 %0, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %initialized10 = getelementptr inbounds i8, ptr %9, i64 4
  %10 = load i8, ptr %initialized10, align 4
  %11 = and i8 %10, 1
  store i8 %11, ptr %initialized, align 1
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp12 = icmp ne ptr %oldp, null
  %cmp14 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp12, %cmp14
  br i1 %or.cond1, label %if.then16, label %label_return

if.then16:                                        ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  switch i64 %12, label %cond.end [
    i64 1, label %if.end22
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then16
  br label %cond.end

cond.end:                                         ; preds = %if.then16, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then16 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %initialized, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end22:                                         ; preds = %if.then16
  store i8 %11, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end22, %arenas_i.exit, %do.body2, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 14, %do.body2 ], [ 0, %arenas_i.exit ], [ 0, %if.end22 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @arena_i_decay_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef readnone %oldp, ptr noundef readnone %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %cmp = icmp ne ptr %oldp, null
  %cmp1 = icmp ne ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  %cmp3 = icmp ne ptr %newp, null
  %or.cond1 = or i1 %or.cond, %cmp3
  %cmp5 = icmp ne i64 %newlen, 0
  %or.cond2 = or i1 %or.cond1, %cmp5
  br i1 %or.cond2, label %label_return, label %do.body6

do.body6:                                         ; preds = %entry
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 8
  %0 = load i64, ptr %arrayidx, align 8
  %cmp7 = icmp ugt i64 %0, 4294967295
  br i1 %cmp7, label %label_return, label %if.end9

if.end9:                                          ; preds = %do.body6
  %conv = trunc i64 %0 to i32
  tail call fastcc void @arena_i_decay(ptr noundef %tsd, i32 noundef %conv, i1 noundef zeroext false)
  br label %label_return

label_return:                                     ; preds = %do.body6, %entry, %if.end9
  %ret.0 = phi i32 [ 0, %if.end9 ], [ 1, %entry ], [ 14, %do.body6 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @arena_i_purge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef readnone %oldp, ptr noundef readnone %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %cmp = icmp ne ptr %oldp, null
  %cmp1 = icmp ne ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  %cmp3 = icmp ne ptr %newp, null
  %or.cond1 = or i1 %or.cond, %cmp3
  %cmp5 = icmp ne i64 %newlen, 0
  %or.cond2 = or i1 %or.cond1, %cmp5
  br i1 %or.cond2, label %label_return, label %do.body6

do.body6:                                         ; preds = %entry
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 8
  %0 = load i64, ptr %arrayidx, align 8
  %cmp7 = icmp ugt i64 %0, 4294967295
  br i1 %cmp7, label %label_return, label %if.end9

if.end9:                                          ; preds = %do.body6
  %conv = trunc i64 %0 to i32
  tail call fastcc void @arena_i_decay(ptr noundef %tsd, i32 noundef %conv, i1 noundef zeroext true)
  br label %label_return

label_return:                                     ; preds = %do.body6, %entry, %if.end9
  %ret.0 = phi i32 [ 0, %if.end9 ], [ 1, %entry ], [ 14, %do.body6 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @arena_i_reset_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef readnone %oldp, ptr noundef readnone %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %cmp.i = icmp ne ptr %oldp, null
  %cmp1.i = icmp ne ptr %oldlenp, null
  %or.cond.i = or i1 %cmp.i, %cmp1.i
  %cmp3.i = icmp ne ptr %newp, null
  %or.cond1.i = or i1 %or.cond.i, %cmp3.i
  %cmp5.i = icmp ne i64 %newlen, 0
  %or.cond2.i = or i1 %or.cond1.i, %cmp5.i
  br i1 %or.cond2.i, label %return, label %do.body6.i

do.body6.i:                                       ; preds = %entry
  %arrayidx.i = getelementptr inbounds i8, ptr %mib, i64 8
  %0 = load i64, ptr %arrayidx.i, align 8
  %cmp7.i = icmp ugt i64 %0, 4294967295
  br i1 %cmp7.i, label %return, label %if.end9.i

if.end9.i:                                        ; preds = %do.body6.i
  %conv.i = trunc i64 %0 to i32
  %arrayidx.i.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %0
  %1 = load atomic i64, ptr %arrayidx.i.i acquire, align 8
  %2 = inttoptr i64 %1 to ptr
  %cmp13.i = icmp eq i64 %1, 0
  br i1 %cmp13.i, label %return, label %lor.lhs.false15.i

lor.lhs.false15.i:                                ; preds = %if.end9.i
  %3 = getelementptr i8, ptr %2, i64 78944
  %call12.val.i = load i32, ptr %3, align 32
  %4 = load i32, ptr @manual_arena_base, align 4
  %cmp.i7.i = icmp ugt i32 %4, %call12.val.i
  br i1 %cmp.i7.i, label %return, label %if.end

if.end:                                           ; preds = %lor.lhs.false15.i
  tail call fastcc void @arena_reset_prepare_background_thread(ptr noundef %tsd, i32 noundef %conv.i)
  tail call void @arena_reset(ptr noundef %tsd, ptr noundef nonnull %2) #14
  %5 = load atomic i8, ptr @background_thread_enabled_state monotonic, align 1
  %6 = and i8 %5, 1
  %tobool.i.not.i = icmp eq i8 %6, 0
  br i1 %tobool.i.not.i, label %arena_reset_finish_background_thread.exit, label %if.then.i

if.then.i:                                        ; preds = %if.end
  %7 = load ptr, ptr @background_thread_info, align 8
  %8 = load i64, ptr @max_background_threads, align 8
  %rem.i.i = urem i64 %0, %8
  %arrayidx.i.i7 = getelementptr inbounds %struct.background_thread_info_s, ptr %7, i64 %rem.i.i
  %lock.i.i.i = getelementptr inbounds i8, ptr %arrayidx.i.i7, i64 128
  %call.i.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull %lock.i.i.i) #14
  %cmp.i.not.i.i = icmp eq i32 %call.i.i.i, 0
  br i1 %cmp.i.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.then.i
  %mtx.i = getelementptr inbounds i8, ptr %arrayidx.i.i7, i64 56
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull %mtx.i) #14
  %locked.i.i = getelementptr inbounds i8, ptr %arrayidx.i.i7, i64 120
  store atomic i8 1, ptr %locked.i.i monotonic, align 1
  br label %if.end.i.i

if.end.i.i:                                       ; preds = %if.then.i.i, %if.then.i
  %n_lock_ops.i.i.i = getelementptr inbounds i8, ptr %arrayidx.i.i7, i64 112
  %9 = load i64, ptr %n_lock_ops.i.i.i, align 8
  %inc.i.i.i = add i64 %9, 1
  store i64 %inc.i.i.i, ptr %n_lock_ops.i.i.i, align 8
  %prev_owner.i.i.i = getelementptr inbounds i8, ptr %arrayidx.i.i7, i64 104
  %10 = load ptr, ptr %prev_owner.i.i.i, align 8
  %cmp.not.i.i.i = icmp eq ptr %10, %tsd
  br i1 %cmp.not.i.i.i, label %malloc_mutex_lock.exit.i, label %if.then.i.i.i

if.then.i.i.i:                                    ; preds = %if.end.i.i
  store ptr %tsd, ptr %prev_owner.i.i.i, align 8
  %n_owner_switches.i.i.i = getelementptr inbounds i8, ptr %arrayidx.i.i7, i64 96
  %11 = load i64, ptr %n_owner_switches.i.i.i, align 8
  %inc2.i.i.i = add i64 %11, 1
  store i64 %inc2.i.i.i, ptr %n_owner_switches.i.i.i, align 8
  br label %malloc_mutex_lock.exit.i

malloc_mutex_lock.exit.i:                         ; preds = %if.then.i.i.i, %if.end.i.i
  %state.i = getelementptr inbounds i8, ptr %arrayidx.i.i7, i64 168
  store i32 1, ptr %state.i, align 8
  %locked.i5.i = getelementptr inbounds i8, ptr %arrayidx.i.i7, i64 120
  store atomic i8 0, ptr %locked.i5.i monotonic, align 1
  %call1.i.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull %lock.i.i.i) #14
  br label %arena_reset_finish_background_thread.exit

arena_reset_finish_background_thread.exit:        ; preds = %if.end, %malloc_mutex_lock.exit.i
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i6.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  br label %return

return:                                           ; preds = %lor.lhs.false15.i, %if.end9.i, %do.body6.i, %entry, %arena_reset_finish_background_thread.exit
  %ret.0.i13 = phi i32 [ 0, %arena_reset_finish_background_thread.exit ], [ 14, %if.end9.i ], [ 14, %do.body6.i ], [ 1, %entry ], [ 14, %lor.lhs.false15.i ]
  ret i32 %ret.0.i13
}

; Function Attrs: nounwind uwtable
define internal i32 @arena_i_destroy_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef readnone %oldp, ptr noundef readnone %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp.i = icmp ne ptr %oldp, null
  %cmp1.i = icmp ne ptr %oldlenp, null
  %or.cond.i = or i1 %cmp.i, %cmp1.i
  %cmp3.i = icmp ne ptr %newp, null
  %or.cond1.i = or i1 %or.cond.i, %cmp3.i
  %cmp5.i = icmp ne i64 %newlen, 0
  %or.cond2.i = or i1 %or.cond1.i, %cmp5.i
  br i1 %or.cond2.i, label %label_return, label %do.body6.i

do.body6.i:                                       ; preds = %malloc_mutex_lock.exit
  %arrayidx.i = getelementptr inbounds i8, ptr %mib, i64 8
  %3 = load i64, ptr %arrayidx.i, align 8
  %cmp7.i = icmp ugt i64 %3, 4294967295
  br i1 %cmp7.i, label %label_return, label %if.end9.i

if.end9.i:                                        ; preds = %do.body6.i
  %conv.i = trunc i64 %3 to i32
  %arrayidx.i.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %3
  %4 = load atomic i64, ptr %arrayidx.i.i acquire, align 8
  %5 = inttoptr i64 %4 to ptr
  %cmp13.i = icmp eq i64 %4, 0
  br i1 %cmp13.i, label %label_return, label %lor.lhs.false15.i

lor.lhs.false15.i:                                ; preds = %if.end9.i
  %6 = getelementptr i8, ptr %5, i64 78944
  %call12.val.i = load i32, ptr %6, align 32
  %7 = load i32, ptr @manual_arena_base, align 4
  %cmp.i7.i = icmp ugt i32 %7, %call12.val.i
  br i1 %cmp.i7.i, label %label_return, label %if.end

if.end:                                           ; preds = %lor.lhs.false15.i
  %call2 = tail call i32 @arena_nthreads_get(ptr noundef nonnull %5, i1 noundef zeroext false) #14
  %cmp3.not = icmp eq i32 %call2, 0
  br i1 %cmp3.not, label %lor.lhs.false, label %label_return

lor.lhs.false:                                    ; preds = %if.end
  %call4 = tail call i32 @arena_nthreads_get(ptr noundef nonnull %5, i1 noundef zeroext true) #14
  %cmp5.not = icmp eq i32 %call4, 0
  br i1 %cmp5.not, label %if.end7, label %label_return

if.end7:                                          ; preds = %lor.lhs.false
  tail call fastcc void @arena_reset_prepare_background_thread(ptr noundef %tsd, i32 noundef %conv.i)
  tail call void @arena_reset(ptr noundef %tsd, ptr noundef nonnull %5) #14
  tail call void @arena_decay(ptr noundef %tsd, ptr noundef nonnull %5, i1 noundef zeroext false, i1 noundef zeroext true) #14
  %8 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %8, i64 824
  %9 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %9, 0
  br i1 %cmp6.i.not.i, label %arenas_i.exit, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %if.end7
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %8, i1 noundef zeroext false) #14
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %if.end7, %if.then11.i.i
  %10 = load ptr, ptr @ctl_arenas, align 8
  %arrayidx.i.i22 = getelementptr inbounds i8, ptr %10, i64 32
  %11 = load ptr, ptr %arrayidx.i.i22, align 8
  %initialized = getelementptr inbounds i8, ptr %11, i64 4
  store i8 1, ptr %initialized, align 4
  tail call fastcc void @ctl_arena_refresh(ptr noundef %tsd, ptr noundef nonnull %5, ptr noundef %11, i32 noundef %conv.i, i1 noundef zeroext true)
  tail call void @arena_destroy(ptr noundef %tsd, ptr noundef nonnull %5) #14
  %12 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i24 = icmp eq i8 %12, 0
  br i1 %cmp6.i.not.i24, label %tsd_fetch_impl.exit.i, label %if.then11.i.i25

if.then11.i.i25:                                  ; preds = %arenas_i.exit
  %call13.i.i26 = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %8, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i25, %arenas_i.exit
  %13 = load ptr, ptr @ctl_arenas, align 8
  switch i32 %conv.i, label %sw.default.i.i.i [
    i32 4096, label %arenas_i.exit29
    i32 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit29

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %13, i64 8
  %14 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %14 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit29, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add nuw nsw i64 %3, 2
  %15 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit29

arenas_i.exit29:                                  ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %15, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i27 = getelementptr inbounds i8, ptr %13, i64 24
  %arrayidx.i.i28 = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i27, i64 0, i64 %a.0.i.i.i
  %16 = load ptr, ptr %arrayidx.i.i28, align 8
  %initialized12 = getelementptr inbounds i8, ptr %16, i64 4
  store i8 0, ptr %initialized12, align 4
  %destroyed_link = getelementptr inbounds i8, ptr %16, i64 8
  store ptr %16, ptr %destroyed_link, align 8
  %qre_prev = getelementptr inbounds i8, ptr %16, i64 16
  store ptr %16, ptr %qre_prev, align 8
  %destroyed = getelementptr inbounds i8, ptr %13, i64 16
  %17 = load ptr, ptr %destroyed, align 8
  %cmp15 = icmp eq ptr %17, null
  br i1 %cmp15, label %if.end52, label %do.body18

do.body18:                                        ; preds = %arenas_i.exit29
  %qre_prev22 = getelementptr inbounds i8, ptr %17, i64 16
  %18 = load ptr, ptr %qre_prev22, align 8
  store ptr %18, ptr %destroyed_link, align 8
  %19 = load ptr, ptr %destroyed, align 8
  %qre_prev32 = getelementptr inbounds i8, ptr %19, i64 16
  store ptr %16, ptr %qre_prev32, align 8
  %20 = load ptr, ptr %qre_prev, align 8
  %destroyed_link35 = getelementptr inbounds i8, ptr %20, i64 8
  %21 = load ptr, ptr %destroyed_link35, align 8
  store ptr %21, ptr %qre_prev, align 8
  %22 = load ptr, ptr %destroyed, align 8
  %qre_prev44 = getelementptr inbounds i8, ptr %22, i64 16
  %23 = load ptr, ptr %qre_prev44, align 8
  %destroyed_link45 = getelementptr inbounds i8, ptr %23, i64 8
  store ptr %22, ptr %destroyed_link45, align 8
  %24 = load ptr, ptr %qre_prev, align 8
  %destroyed_link49 = getelementptr inbounds i8, ptr %24, i64 8
  store ptr %16, ptr %destroyed_link49, align 8
  %.pre = load ptr, ptr %destroyed_link, align 8
  br label %if.end52

if.end52:                                         ; preds = %do.body18, %arenas_i.exit29
  %25 = phi ptr [ %.pre, %do.body18 ], [ %16, %arenas_i.exit29 ]
  store ptr %25, ptr %destroyed, align 8
  %26 = load atomic i8, ptr @background_thread_enabled_state monotonic, align 1
  %27 = and i8 %26, 1
  %tobool.i.not.i = icmp eq i8 %27, 0
  br i1 %tobool.i.not.i, label %arena_reset_finish_background_thread.exit, label %if.then.i30

if.then.i30:                                      ; preds = %if.end52
  %28 = load ptr, ptr @background_thread_info, align 8
  %29 = load i64, ptr @max_background_threads, align 8
  %rem.i.i = urem i64 %3, %29
  %arrayidx.i.i32 = getelementptr inbounds %struct.background_thread_info_s, ptr %28, i64 %rem.i.i
  %lock.i.i.i = getelementptr inbounds i8, ptr %arrayidx.i.i32, i64 128
  %call.i.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull %lock.i.i.i) #14
  %cmp.i.not.i.i = icmp eq i32 %call.i.i.i, 0
  br i1 %cmp.i.not.i.i, label %if.end.i.i, label %if.then.i.i33

if.then.i.i33:                                    ; preds = %if.then.i30
  %mtx.i = getelementptr inbounds i8, ptr %arrayidx.i.i32, i64 56
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull %mtx.i) #14
  %locked.i.i = getelementptr inbounds i8, ptr %arrayidx.i.i32, i64 120
  store atomic i8 1, ptr %locked.i.i monotonic, align 1
  br label %if.end.i.i

if.end.i.i:                                       ; preds = %if.then.i.i33, %if.then.i30
  %n_lock_ops.i.i.i = getelementptr inbounds i8, ptr %arrayidx.i.i32, i64 112
  %30 = load i64, ptr %n_lock_ops.i.i.i, align 8
  %inc.i.i.i = add i64 %30, 1
  store i64 %inc.i.i.i, ptr %n_lock_ops.i.i.i, align 8
  %prev_owner.i.i.i = getelementptr inbounds i8, ptr %arrayidx.i.i32, i64 104
  %31 = load ptr, ptr %prev_owner.i.i.i, align 8
  %cmp.not.i.i.i = icmp eq ptr %31, %tsd
  br i1 %cmp.not.i.i.i, label %malloc_mutex_lock.exit.i, label %if.then.i.i.i

if.then.i.i.i:                                    ; preds = %if.end.i.i
  store ptr %tsd, ptr %prev_owner.i.i.i, align 8
  %n_owner_switches.i.i.i = getelementptr inbounds i8, ptr %arrayidx.i.i32, i64 96
  %32 = load i64, ptr %n_owner_switches.i.i.i, align 8
  %inc2.i.i.i = add i64 %32, 1
  store i64 %inc2.i.i.i, ptr %n_owner_switches.i.i.i, align 8
  br label %malloc_mutex_lock.exit.i

malloc_mutex_lock.exit.i:                         ; preds = %if.then.i.i.i, %if.end.i.i
  %state.i = getelementptr inbounds i8, ptr %arrayidx.i.i32, i64 168
  store i32 1, ptr %state.i, align 8
  %locked.i5.i = getelementptr inbounds i8, ptr %arrayidx.i.i32, i64 120
  store atomic i8 0, ptr %locked.i5.i monotonic, align 1
  %call1.i.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull %lock.i.i.i) #14
  br label %arena_reset_finish_background_thread.exit

arena_reset_finish_background_thread.exit:        ; preds = %if.end52, %malloc_mutex_lock.exit.i
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i6.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  br label %label_return

label_return:                                     ; preds = %lor.lhs.false15.i, %if.end9.i, %do.body6.i, %malloc_mutex_lock.exit, %if.end, %lor.lhs.false, %arena_reset_finish_background_thread.exit
  %ret.0 = phi i32 [ 0, %arena_reset_finish_background_thread.exit ], [ 14, %lor.lhs.false ], [ 14, %if.end ], [ 14, %if.end9.i ], [ 14, %do.body6.i ], [ 1, %malloc_mutex_lock.exit ], [ 14, %lor.lhs.false15.i ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @arena_i_dss_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %dss = alloca ptr, align 8
  store ptr null, ptr %dss, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp.not = icmp eq ptr %newp, null
  br i1 %cmp.not, label %do.body4.thread, label %if.then

if.then:                                          ; preds = %malloc_mutex_lock.exit
  %cmp1.not = icmp eq i64 %newlen, 8
  br i1 %cmp1.not, label %do.body4, label %label_return

do.body4:                                         ; preds = %if.then
  %3 = load ptr, ptr %newp, align 8
  store ptr %3, ptr %dss, align 8
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 8
  %4 = load i64, ptr %arrayidx, align 8
  %cmp5 = icmp ugt i64 %4, 4294967295
  br i1 %cmp5, label %label_return, label %if.end7

do.body4.thread:                                  ; preds = %malloc_mutex_lock.exit
  %arrayidx32 = getelementptr inbounds i8, ptr %mib, i64 8
  %5 = load i64, ptr %arrayidx32, align 8
  %cmp533 = icmp ugt i64 %5, 4294967295
  br i1 %cmp533, label %label_return, label %if.end23

if.end7:                                          ; preds = %do.body4
  %cmp10.not = icmp eq ptr %3, null
  br i1 %cmp10.not, label %if.end23, label %for.body

for.body:                                         ; preds = %if.end7, %for.inc
  %indvars.iv = phi i64 [ %indvars.iv.next, %for.inc ], [ 0, %if.end7 ]
  %arrayidx15 = getelementptr inbounds [0 x ptr], ptr @dss_prec_names, i64 0, i64 %indvars.iv
  %6 = load ptr, ptr %arrayidx15, align 8
  %call16 = tail call i32 @strcmp(ptr noundef nonnull dereferenceable(1) %6, ptr noundef nonnull dereferenceable(1) %3) #15
  %cmp17 = icmp eq i32 %call16, 0
  br i1 %cmp17, label %if.end23.loopexit, label %for.inc

for.inc:                                          ; preds = %for.body
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, 3
  br i1 %exitcond.not, label %label_return, label %for.body, !llvm.loop !18

if.end23.loopexit:                                ; preds = %for.body
  %7 = trunc i64 %indvars.iv to i32
  br label %if.end23

if.end23:                                         ; preds = %do.body4.thread, %if.end23.loopexit, %if.end7
  %8 = phi i64 [ %4, %if.end7 ], [ %4, %if.end23.loopexit ], [ %5, %do.body4.thread ]
  %dss_prec.1 = phi i32 [ 3, %if.end7 ], [ %7, %if.end23.loopexit ], [ 3, %do.body4.thread ]
  %conv38 = trunc i64 %8 to i32
  %cmp24 = icmp eq i32 %conv38, 4096
  br i1 %cmp24, label %if.then28, label %lor.lhs.false

lor.lhs.false:                                    ; preds = %if.end23
  %9 = load ptr, ptr @ctl_arenas, align 8
  %narenas = getelementptr inbounds i8, ptr %9, i64 8
  %10 = load i32, ptr %narenas, align 8
  %cmp26 = icmp eq i32 %10, %conv38
  br i1 %cmp26, label %if.then28, label %if.else

if.then28:                                        ; preds = %lor.lhs.false, %if.end23
  %cmp29.not = icmp eq i32 %dss_prec.1, 3
  br i1 %cmp29.not, label %if.end34, label %land.lhs.true

land.lhs.true:                                    ; preds = %if.then28
  %call31 = tail call zeroext i1 @extent_dss_prec_set(i32 noundef %dss_prec.1) #14
  br i1 %call31, label %label_return, label %if.end34

if.end34:                                         ; preds = %land.lhs.true, %if.then28
  %call35 = tail call i32 @extent_dss_prec_get() #14
  br label %if.end49

if.else:                                          ; preds = %lor.lhs.false
  %arrayidx.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %8
  %11 = load atomic i64, ptr %arrayidx.i acquire, align 8
  %12 = inttoptr i64 %11 to ptr
  %cmp38 = icmp eq i64 %11, 0
  br i1 %cmp38, label %label_return, label %lor.lhs.false40

lor.lhs.false40:                                  ; preds = %if.else
  %cmp41.not = icmp eq i32 %dss_prec.1, 3
  br i1 %cmp41.not, label %if.end47, label %land.lhs.true43

land.lhs.true43:                                  ; preds = %lor.lhs.false40
  %call44 = tail call zeroext i1 @arena_dss_prec_set(ptr noundef nonnull %12, i32 noundef %dss_prec.1) #14
  br i1 %call44, label %label_return, label %if.end47

if.end47:                                         ; preds = %land.lhs.true43, %lor.lhs.false40
  %call48 = tail call i32 @arena_dss_prec_get(ptr noundef nonnull %12) #14
  br label %if.end49

if.end49:                                         ; preds = %if.end47, %if.end34
  %dss_prec_old.0 = phi i32 [ %call35, %if.end34 ], [ %call48, %if.end47 ]
  %idxprom50 = zext i32 %dss_prec_old.0 to i64
  %arrayidx51 = getelementptr inbounds [0 x ptr], ptr @dss_prec_names, i64 0, i64 %idxprom50
  %13 = load ptr, ptr %arrayidx51, align 8
  store ptr %13, ptr %dss, align 8
  %cmp53 = icmp ne ptr %oldp, null
  %cmp56 = icmp ne ptr %oldlenp, null
  %or.cond = and i1 %cmp53, %cmp56
  br i1 %or.cond, label %if.then58, label %label_return

if.then58:                                        ; preds = %if.end49
  %14 = load i64, ptr %oldlenp, align 8
  %cmp59.not = icmp eq i64 %14, 8
  br i1 %cmp59.not, label %if.end64, label %if.then61

if.then61:                                        ; preds = %if.then58
  %spec.select = tail call i64 @llvm.umin.i64(i64 %14, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %dss, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end64:                                         ; preds = %if.then58
  store ptr %13, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %for.inc, %do.body4.thread, %if.end64, %if.end49, %if.else, %land.lhs.true43, %land.lhs.true, %do.body4, %if.then, %if.then61
  %ret.0 = phi i32 [ 22, %if.then61 ], [ 22, %if.then ], [ 14, %do.body4 ], [ 14, %land.lhs.true ], [ 14, %land.lhs.true43 ], [ 14, %if.else ], [ 0, %if.end49 ], [ 0, %if.end64 ], [ 14, %do.body4.thread ], [ 22, %for.inc ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nounwind willreturn memory(readwrite, inaccessiblemem: none) uwtable
define internal i32 @arena_i_oversize_threshold_ctl(ptr nocapture readnone %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #11 {
entry:
  %oldval = alloca i64, align 8
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 8
  %0 = load i64, ptr %arrayidx, align 8
  %cmp = icmp ugt i64 %0, 4294967295
  br i1 %cmp, label %label_return, label %if.end

if.end:                                           ; preds = %entry
  %arrayidx.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %0
  %1 = load atomic i64, ptr %arrayidx.i acquire, align 8
  %2 = inttoptr i64 %1 to ptr
  %cmp3 = icmp eq i64 %1, 0
  br i1 %cmp3, label %label_return, label %if.end6

if.end6:                                          ; preds = %if.end
  %cmp7 = icmp ne ptr %oldp, null
  %cmp9 = icmp ne ptr %oldlenp, null
  %or.cond = and i1 %cmp7, %cmp9
  br i1 %or.cond, label %if.then11, label %if.end28

if.then11:                                        ; preds = %if.end6
  %oversize_threshold = getelementptr inbounds i8, ptr %2, i64 69328
  %3 = load atomic i64, ptr %oversize_threshold monotonic, align 8
  store i64 %3, ptr %oldval, align 8
  %4 = load i64, ptr %oldlenp, align 8
  %cmp20.not = icmp eq i64 %4, 8
  br i1 %cmp20.not, label %if.end25, label %if.then22

if.then22:                                        ; preds = %if.then11
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end25:                                         ; preds = %if.then11
  store i64 %3, ptr %oldp, align 8
  br label %if.end28

if.end28:                                         ; preds = %if.end25, %if.end6
  %cmp29.not = icmp eq ptr %newp, null
  br i1 %cmp29.not, label %label_return, label %if.then31

if.then31:                                        ; preds = %if.end28
  %cmp32.not = icmp eq i64 %newlen, 8
  br i1 %cmp32.not, label %if.end35, label %label_return

if.end35:                                         ; preds = %if.then31
  %oversize_threshold38 = getelementptr inbounds i8, ptr %2, i64 69328
  %5 = load i64, ptr %newp, align 8
  store atomic i64 %5, ptr %oversize_threshold38 monotonic, align 8
  br label %label_return

label_return:                                     ; preds = %if.end28, %if.end35, %if.then31, %if.end, %entry, %if.then22
  %ret.0 = phi i32 [ 22, %if.then22 ], [ 14, %entry ], [ 14, %if.end ], [ 22, %if.then31 ], [ 0, %if.end35 ], [ 0, %if.end28 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @arena_i_dirty_decay_ms_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %oldval.i = alloca i64, align 8
  %0 = getelementptr i8, ptr %mib, i64 8
  %mib.val = load i64, ptr %0, align 8
  call void @llvm.lifetime.start.p0(i64 8, ptr nonnull %oldval.i)
  %cmp.i = icmp ugt i64 %mib.val, 4294967295
  br i1 %cmp.i, label %arena_i_decay_ms_ctl_impl.exit, label %if.end.i

if.end.i:                                         ; preds = %entry
  %conv.i = trunc i64 %mib.val to i32
  %arrayidx.i.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %mib.val
  %1 = load atomic i64, ptr %arrayidx.i.i acquire, align 8
  %2 = inttoptr i64 %1 to ptr
  %cmp3.i = icmp eq i64 %1, 0
  br i1 %cmp3.i, label %arena_i_decay_ms_ctl_impl.exit, label %if.end6.i

if.end6.i:                                        ; preds = %if.end.i
  %cmp8.i = icmp ne ptr %oldp, null
  %cmp10.i = icmp ne ptr %oldlenp, null
  %or.cond.i = and i1 %cmp8.i, %cmp10.i
  br i1 %or.cond.i, label %if.then12.i, label %if.end30.i

if.then12.i:                                      ; preds = %if.end6.i
  %call13.i = tail call i64 @arena_decay_ms_get(ptr noundef nonnull %2, i32 noundef 1) #14
  store i64 %call13.i, ptr %oldval.i, align 8
  %3 = load i64, ptr %oldlenp, align 8
  %cmp21.not.i = icmp eq i64 %3, 8
  br i1 %cmp21.not.i, label %if.end27.i, label %if.then23.i

if.then23.i:                                      ; preds = %if.then12.i
  %spec.select.i = tail call i64 @llvm.umin.i64(i64 %3, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval.i, i64 %spec.select.i, i1 false)
  store i64 %spec.select.i, ptr %oldlenp, align 8
  br label %arena_i_decay_ms_ctl_impl.exit

if.end27.i:                                       ; preds = %if.then12.i
  store i64 %call13.i, ptr %oldp, align 8
  br label %if.end30.i

if.end30.i:                                       ; preds = %if.end27.i, %if.end6.i
  %cmp31.not.i = icmp eq ptr %newp, null
  br i1 %cmp31.not.i, label %if.end52.i, label %if.then33.i

if.then33.i:                                      ; preds = %if.end30.i
  %cmp34.not.i = icmp eq i64 %newlen, 8
  br i1 %cmp34.not.i, label %if.end37.i, label %arena_i_decay_ms_ctl_impl.exit

if.end37.i:                                       ; preds = %if.then33.i
  %call38.i = tail call zeroext i1 @arena_is_huge(i32 noundef %conv.i) #14
  %.pre1.i = load i64, ptr %newp, align 8
  %cmp41.i = icmp sgt i64 %.pre1.i, 0
  %or.cond2.i = select i1 %call38.i, i1 %cmp41.i, i1 false
  br i1 %or.cond2.i, label %if.then43.i, label %if.end47.i

if.then43.i:                                      ; preds = %if.end37.i
  %call44.i = tail call zeroext i1 @background_thread_create(ptr noundef %tsd, i32 noundef %conv.i) #14
  br i1 %call44.i, label %arena_i_decay_ms_ctl_impl.exit, label %if.then43.if.end47_crit_edge.i

if.then43.if.end47_crit_edge.i:                   ; preds = %if.then43.i
  %.pre.i = load i64, ptr %newp, align 8
  br label %if.end47.i

if.end47.i:                                       ; preds = %if.then43.if.end47_crit_edge.i, %if.end37.i
  %4 = phi i64 [ %.pre.i, %if.then43.if.end47_crit_edge.i ], [ %.pre1.i, %if.end37.i ]
  %call49.i = tail call zeroext i1 @arena_decay_ms_set(ptr noundef %tsd, ptr noundef nonnull %2, i32 noundef 1, i64 noundef %4) #14
  br i1 %call49.i, label %arena_i_decay_ms_ctl_impl.exit, label %if.end52.i

if.end52.i:                                       ; preds = %if.end47.i, %if.end30.i
  br label %arena_i_decay_ms_ctl_impl.exit

arena_i_decay_ms_ctl_impl.exit:                   ; preds = %entry, %if.end.i, %if.then23.i, %if.then33.i, %if.then43.i, %if.end47.i, %if.end52.i
  %ret.0.i = phi i32 [ 22, %if.then23.i ], [ 0, %if.end52.i ], [ 14, %entry ], [ 14, %if.end.i ], [ 22, %if.then33.i ], [ 14, %if.then43.i ], [ 14, %if.end47.i ]
  call void @llvm.lifetime.end.p0(i64 8, ptr nonnull %oldval.i)
  ret i32 %ret.0.i
}

; Function Attrs: nounwind uwtable
define internal i32 @arena_i_muzzy_decay_ms_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %oldval.i = alloca i64, align 8
  %0 = getelementptr i8, ptr %mib, i64 8
  %mib.val = load i64, ptr %0, align 8
  call void @llvm.lifetime.start.p0(i64 8, ptr nonnull %oldval.i)
  %cmp.i = icmp ugt i64 %mib.val, 4294967295
  br i1 %cmp.i, label %arena_i_decay_ms_ctl_impl.exit, label %if.end.i

if.end.i:                                         ; preds = %entry
  %conv.i = trunc i64 %mib.val to i32
  %arrayidx.i.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %mib.val
  %1 = load atomic i64, ptr %arrayidx.i.i acquire, align 8
  %2 = inttoptr i64 %1 to ptr
  %cmp3.i = icmp eq i64 %1, 0
  br i1 %cmp3.i, label %arena_i_decay_ms_ctl_impl.exit, label %if.end6.i

if.end6.i:                                        ; preds = %if.end.i
  %cmp8.i = icmp ne ptr %oldp, null
  %cmp10.i = icmp ne ptr %oldlenp, null
  %or.cond.i = and i1 %cmp8.i, %cmp10.i
  br i1 %or.cond.i, label %if.then12.i, label %if.end30.i

if.then12.i:                                      ; preds = %if.end6.i
  %call13.i = tail call i64 @arena_decay_ms_get(ptr noundef nonnull %2, i32 noundef 2) #14
  store i64 %call13.i, ptr %oldval.i, align 8
  %3 = load i64, ptr %oldlenp, align 8
  %cmp21.not.i = icmp eq i64 %3, 8
  br i1 %cmp21.not.i, label %if.end27.i, label %if.then23.i

if.then23.i:                                      ; preds = %if.then12.i
  %spec.select.i = tail call i64 @llvm.umin.i64(i64 %3, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval.i, i64 %spec.select.i, i1 false)
  store i64 %spec.select.i, ptr %oldlenp, align 8
  br label %arena_i_decay_ms_ctl_impl.exit

if.end27.i:                                       ; preds = %if.then12.i
  store i64 %call13.i, ptr %oldp, align 8
  br label %if.end30.i

if.end30.i:                                       ; preds = %if.end27.i, %if.end6.i
  %cmp31.not.i = icmp eq ptr %newp, null
  br i1 %cmp31.not.i, label %if.end52.i, label %if.then33.i

if.then33.i:                                      ; preds = %if.end30.i
  %cmp34.not.i = icmp eq i64 %newlen, 8
  br i1 %cmp34.not.i, label %if.end37.i, label %arena_i_decay_ms_ctl_impl.exit

if.end37.i:                                       ; preds = %if.then33.i
  %call38.i = tail call zeroext i1 @arena_is_huge(i32 noundef %conv.i) #14
  %.pre1.i = load i64, ptr %newp, align 8
  %cmp41.i = icmp sgt i64 %.pre1.i, 0
  %or.cond2.i = select i1 %call38.i, i1 %cmp41.i, i1 false
  br i1 %or.cond2.i, label %if.then43.i, label %if.end47.i

if.then43.i:                                      ; preds = %if.end37.i
  %call44.i = tail call zeroext i1 @background_thread_create(ptr noundef %tsd, i32 noundef %conv.i) #14
  br i1 %call44.i, label %arena_i_decay_ms_ctl_impl.exit, label %if.then43.if.end47_crit_edge.i

if.then43.if.end47_crit_edge.i:                   ; preds = %if.then43.i
  %.pre.i = load i64, ptr %newp, align 8
  br label %if.end47.i

if.end47.i:                                       ; preds = %if.then43.if.end47_crit_edge.i, %if.end37.i
  %4 = phi i64 [ %.pre.i, %if.then43.if.end47_crit_edge.i ], [ %.pre1.i, %if.end37.i ]
  %call49.i = tail call zeroext i1 @arena_decay_ms_set(ptr noundef %tsd, ptr noundef nonnull %2, i32 noundef 2, i64 noundef %4) #14
  br i1 %call49.i, label %arena_i_decay_ms_ctl_impl.exit, label %if.end52.i

if.end52.i:                                       ; preds = %if.end47.i, %if.end30.i
  br label %arena_i_decay_ms_ctl_impl.exit

arena_i_decay_ms_ctl_impl.exit:                   ; preds = %entry, %if.end.i, %if.then23.i, %if.then33.i, %if.then43.i, %if.end47.i, %if.end52.i
  %ret.0.i = phi i32 [ 22, %if.then23.i ], [ 0, %if.end52.i ], [ 14, %entry ], [ 14, %if.end.i ], [ 22, %if.then33.i ], [ 14, %if.then43.i ], [ 14, %if.end47.i ]
  call void @llvm.lifetime.end.p0(i64 8, ptr nonnull %oldval.i)
  ret i32 %ret.0.i
}

; Function Attrs: nounwind uwtable
define internal i32 @arena_i_extent_hooks_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %old_extent_hooks = alloca ptr, align 8
  %config = alloca %struct.arena_config_s, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 8
  %3 = load i64, ptr %arrayidx, align 8
  %cmp = icmp ugt i64 %3, 4294967295
  br i1 %cmp, label %label_return, label %if.end

if.end:                                           ; preds = %malloc_mutex_lock.exit
  %conv = trunc i64 %3 to i32
  %call2 = tail call i32 @narenas_total_get() #14
  %cmp3 = icmp ugt i32 %call2, %conv
  br i1 %cmp3, label %if.then5, label %label_return

if.then5:                                         ; preds = %if.end
  %arrayidx.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %3
  %4 = load atomic i64, ptr %arrayidx.i acquire, align 8
  %5 = inttoptr i64 %4 to ptr
  %cmp8 = icmp eq i64 %4, 0
  br i1 %cmp8, label %if.then10, label %if.else

if.then10:                                        ; preds = %if.then5
  %6 = load i32, ptr @narenas_auto, align 4
  %cmp11.not = icmp ugt i32 %6, %conv
  br i1 %cmp11.not, label %if.end14, label %label_return

if.end14:                                         ; preds = %if.then10
  store ptr @ehooks_default_extent_hooks, ptr %old_extent_hooks, align 8
  %cmp16 = icmp ne ptr %oldp, null
  %cmp18 = icmp ne ptr %oldlenp, null
  %or.cond = and i1 %cmp16, %cmp18
  br i1 %or.cond, label %if.then20, label %do.end28

if.then20:                                        ; preds = %if.end14
  %7 = load i64, ptr %oldlenp, align 8
  %cmp21.not = icmp eq i64 %7, 8
  br i1 %cmp21.not, label %if.end26, label %if.then23

if.then23:                                        ; preds = %if.then20
  %spec.select = tail call i64 @llvm.umin.i64(i64 %7, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %old_extent_hooks, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end26:                                         ; preds = %if.then20
  store ptr @ehooks_default_extent_hooks, ptr %oldp, align 8
  br label %do.end28

do.end28:                                         ; preds = %if.end14, %if.end26
  %cmp29.not = icmp eq ptr %newp, null
  br i1 %cmp29.not, label %if.end110, label %if.then35

if.then35:                                        ; preds = %do.end28
  %cmp36.not = icmp eq i64 %newlen, 8
  br i1 %cmp36.not, label %do.end41, label %label_return

do.end41:                                         ; preds = %if.then35
  %8 = load ptr, ptr %newp, align 8
  call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) %config, ptr noundef nonnull align 8 dereferenceable(16) @arena_config_default, i64 16, i1 false)
  store ptr %8, ptr %config, align 8
  %call43 = call ptr @arena_init(ptr noundef %tsd, i32 noundef %conv, ptr noundef nonnull %config) #14
  %cmp44 = icmp eq ptr %call43, null
  br i1 %cmp44, label %label_return, label %if.end110

if.else:                                          ; preds = %if.then5
  %cmp49.not = icmp eq ptr %newp, null
  br i1 %cmp49.not, label %if.else84, label %if.then56

if.then56:                                        ; preds = %if.else
  %cmp57.not = icmp eq i64 %newlen, 8
  br i1 %cmp57.not, label %do.end62, label %label_return

do.end62:                                         ; preds = %if.then56
  %9 = load ptr, ptr %newp, align 8
  %call63 = tail call ptr @arena_set_extent_hooks(ptr noundef %tsd, ptr noundef nonnull %5, ptr noundef %9) #14
  store ptr %call63, ptr %old_extent_hooks, align 8
  %cmp65 = icmp ne ptr %oldp, null
  %cmp68 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp65, %cmp68
  br i1 %or.cond1, label %if.then70, label %if.end110

if.then70:                                        ; preds = %do.end62
  %10 = load i64, ptr %oldlenp, align 8
  %cmp71.not = icmp eq i64 %10, 8
  br i1 %cmp71.not, label %if.end110.sink.split, label %if.then73

if.then73:                                        ; preds = %if.then70
  %spec.select50 = tail call i64 @llvm.umin.i64(i64 %10, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %old_extent_hooks, i64 %spec.select50, i1 false)
  store i64 %spec.select50, ptr %oldlenp, align 8
  br label %label_return

if.else84:                                        ; preds = %if.else
  %call85 = tail call ptr @arena_get_ehooks(ptr noundef nonnull %5) #14
  %ptr.i = getelementptr inbounds i8, ptr %call85, i64 8
  %11 = load atomic i64, ptr %ptr.i acquire, align 8
  %12 = inttoptr i64 %11 to ptr
  store ptr %12, ptr %old_extent_hooks, align 8
  %cmp88 = icmp ne ptr %oldp, null
  %cmp91 = icmp ne ptr %oldlenp, null
  %or.cond2 = and i1 %cmp88, %cmp91
  br i1 %or.cond2, label %if.then93, label %if.end110

if.then93:                                        ; preds = %if.else84
  %13 = load i64, ptr %oldlenp, align 8
  %cmp94.not = icmp eq i64 %13, 8
  br i1 %cmp94.not, label %if.end110.sink.split, label %if.then96

if.then96:                                        ; preds = %if.then93
  %spec.select51 = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %old_extent_hooks, i64 %spec.select51, i1 false)
  store i64 %spec.select51, ptr %oldlenp, align 8
  br label %label_return

if.end110.sink.split:                             ; preds = %if.then93, %if.then70
  %.sink = phi ptr [ %call63, %if.then70 ], [ %12, %if.then93 ]
  store ptr %.sink, ptr %oldp, align 8
  br label %if.end110

if.end110:                                        ; preds = %if.end110.sink.split, %do.end41, %do.end28, %if.else84, %do.end62
  br label %label_return

label_return:                                     ; preds = %if.end, %if.then56, %do.end41, %if.then35, %if.then10, %malloc_mutex_lock.exit, %if.end110, %if.then96, %if.then73, %if.then23
  %ret.0 = phi i32 [ 22, %if.then23 ], [ 0, %if.end110 ], [ 22, %if.then73 ], [ 22, %if.then96 ], [ 14, %malloc_mutex_lock.exit ], [ 14, %if.then10 ], [ 22, %if.then35 ], [ 14, %do.end41 ], [ 22, %if.then56 ], [ 14, %if.end ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @arena_i_retain_grow_limit_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %old_limit = alloca i64, align 8
  %new_limit = alloca i64, align 8
  %0 = load i8, ptr @opt_retain, align 1
  %1 = and i8 %0, 1
  %tobool.not = icmp eq i8 %1, 0
  br i1 %tobool.not, label %return, label %if.end

if.end:                                           ; preds = %entry
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %if.end
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %if.end
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %2, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %3 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %3, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %4 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %4, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 8
  %5 = load i64, ptr %arrayidx, align 8
  %cmp = icmp ugt i64 %5, 4294967295
  br i1 %cmp, label %label_return, label %if.end2

if.end2:                                          ; preds = %malloc_mutex_lock.exit
  %conv = trunc i64 %5 to i32
  %call4 = tail call i32 @narenas_total_get() #14
  %cmp5 = icmp ugt i32 %call4, %conv
  br i1 %cmp5, label %land.lhs.true, label %label_return

land.lhs.true:                                    ; preds = %if.end2
  %arrayidx.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %5
  %6 = load atomic i64, ptr %arrayidx.i acquire, align 8
  %7 = inttoptr i64 %6 to ptr
  %cmp9.not = icmp eq i64 %6, 0
  br i1 %cmp9.not, label %label_return, label %if.then11

if.then11:                                        ; preds = %land.lhs.true
  %cmp12.not = icmp eq ptr %newp, null
  br i1 %cmp12.not, label %if.end25, label %if.then18

if.then18:                                        ; preds = %if.then11
  %cmp19.not = icmp eq i64 %newlen, 8
  br i1 %cmp19.not, label %if.end22, label %label_return

if.end22:                                         ; preds = %if.then18
  %8 = load i64, ptr %newp, align 8
  store i64 %8, ptr %new_limit, align 8
  br label %if.end25

if.end25:                                         ; preds = %if.end22, %if.then11
  %new_limit. = phi ptr [ %new_limit, %if.end22 ], [ null, %if.then11 ]
  %call28 = call zeroext i1 @arena_retain_grow_limit_get_set(ptr noundef %tsd, ptr noundef nonnull %7, ptr noundef nonnull %old_limit, ptr noundef %new_limit.) #14
  br i1 %call28, label %label_return, label %do.body31

do.body31:                                        ; preds = %if.end25
  %cmp32 = icmp ne ptr %oldp, null
  %cmp35 = icmp ne ptr %oldlenp, null
  %or.cond = and i1 %cmp32, %cmp35
  br i1 %or.cond, label %if.then37, label %label_return

if.then37:                                        ; preds = %do.body31
  %9 = load i64, ptr %oldlenp, align 8
  %cmp38.not = icmp eq i64 %9, 8
  br i1 %cmp38.not, label %if.end47, label %if.then40

if.then40:                                        ; preds = %if.then37
  %spec.select = call i64 @llvm.umin.i64(i64 %9, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %old_limit, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end47:                                         ; preds = %if.then37
  %10 = load i64, ptr %old_limit, align 8
  store i64 %10, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end2, %land.lhs.true, %if.end25, %if.end47, %do.body31, %if.then18, %malloc_mutex_lock.exit, %if.then40
  %ret.0 = phi i32 [ 22, %if.then40 ], [ 14, %malloc_mutex_lock.exit ], [ 22, %if.then18 ], [ 0, %do.body31 ], [ 0, %if.end47 ], [ 14, %if.end25 ], [ 14, %land.lhs.true ], [ 14, %if.end2 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  br label %return

return:                                           ; preds = %entry, %label_return
  %retval.0 = phi i32 [ %ret.0, %label_return ], [ 2, %entry ]
  ret i32 %retval.0
}

; Function Attrs: nounwind uwtable
define internal i32 @arena_i_name_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef readonly %oldp, ptr noundef readonly %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 8
  %3 = load i64, ptr %arrayidx, align 8
  %cmp = icmp ugt i64 %3, 4294967295
  br i1 %cmp, label %label_return, label %if.end

if.end:                                           ; preds = %malloc_mutex_lock.exit
  %conv = trunc i64 %3 to i32
  %cmp2 = icmp eq i32 %conv, 4096
  br i1 %cmp2, label %label_return, label %lor.lhs.false

lor.lhs.false:                                    ; preds = %if.end
  %4 = load ptr, ptr @ctl_arenas, align 8
  %narenas = getelementptr inbounds i8, ptr %4, i64 8
  %5 = load i32, ptr %narenas, align 8
  %cmp4.not = icmp ugt i32 %5, %conv
  br i1 %cmp4.not, label %if.end7, label %label_return

if.end7:                                          ; preds = %lor.lhs.false
  %arrayidx.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %3
  %6 = load atomic i64, ptr %arrayidx.i acquire, align 8
  %7 = inttoptr i64 %6 to ptr
  %cmp10 = icmp eq i64 %6, 0
  br i1 %cmp10, label %label_return, label %if.end13

if.end13:                                         ; preds = %if.end7
  %cmp14 = icmp ne ptr %oldp, null
  %cmp16 = icmp ne ptr %oldlenp, null
  %or.cond = and i1 %cmp14, %cmp16
  br i1 %or.cond, label %if.then18, label %if.end23

if.then18:                                        ; preds = %if.end13
  %8 = load i64, ptr %oldlenp, align 8
  %cmp19.not = icmp eq i64 %8, 8
  br i1 %cmp19.not, label %if.end22, label %label_return

if.end22:                                         ; preds = %if.then18
  %9 = load ptr, ptr %oldp, align 8
  tail call void @arena_name_get(ptr noundef nonnull %7, ptr noundef %9) #14
  br label %if.end23

if.end23:                                         ; preds = %if.end22, %if.end13
  %cmp24.not = icmp eq ptr %newp, null
  br i1 %cmp24.not, label %label_return, label %if.then30

if.then30:                                        ; preds = %if.end23
  %cmp31.not = icmp eq i64 %newlen, 8
  br i1 %cmp31.not, label %if.end34, label %label_return

if.end34:                                         ; preds = %if.then30
  %10 = load ptr, ptr %newp, align 8
  %cmp37 = icmp eq ptr %10, null
  br i1 %cmp37, label %label_return, label %if.end40

if.end40:                                         ; preds = %if.end34
  tail call void @arena_name_set(ptr noundef nonnull %7, ptr noundef nonnull %10) #14
  br label %label_return

label_return:                                     ; preds = %if.end23, %if.end40, %if.end34, %if.then30, %if.then18, %if.end7, %if.end, %lor.lhs.false, %malloc_mutex_lock.exit
  %ret.0 = phi i32 [ 14, %malloc_mutex_lock.exit ], [ 22, %lor.lhs.false ], [ 22, %if.end ], [ 14, %if.end7 ], [ 22, %if.then18 ], [ 22, %if.then30 ], [ 22, %if.end34 ], [ 0, %if.end40 ], [ 0, %if.end23 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal fastcc void @arena_i_decay(ptr noundef %tsdn, i32 noundef %arena_ind, i1 noundef zeroext %all) unnamed_addr #0 {
entry:
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsdn
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsdn, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %3 = load ptr, ptr @ctl_arenas, align 8
  %narenas1 = getelementptr inbounds i8, ptr %3, i64 8
  %4 = load i32, ptr %narenas1, align 8
  %cmp = icmp eq i32 %arena_ind, 4096
  %cmp2 = icmp eq i32 %4, %arena_ind
  %or.cond = select i1 %cmp, i1 true, i1 %cmp2
  br i1 %or.cond, label %if.then, label %do.end

if.then:                                          ; preds = %malloc_mutex_lock.exit
  %5 = zext i32 %4 to i64
  %6 = tail call ptr @llvm.stacksave.p0()
  %vla = alloca ptr, i64 %5, align 16
  %cmp326.not = icmp eq i32 %4, 0
  br i1 %cmp326.not, label %for.end.thread, label %for.body

for.end.thread:                                   ; preds = %if.then
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i36 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  br label %for.end15

for.body:                                         ; preds = %if.then, %for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %for.body ], [ 0, %if.then ]
  %arrayidx.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %indvars.iv
  %7 = load atomic i64, ptr %arrayidx.i acquire, align 8
  %8 = inttoptr i64 %7 to ptr
  %arrayidx = getelementptr inbounds ptr, ptr %vla, i64 %indvars.iv
  store ptr %8, ptr %arrayidx, align 8
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, %5
  br i1 %exitcond.not, label %for.end, label %for.body, !llvm.loop !19

for.end:                                          ; preds = %for.body
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  br i1 %cmp326.not, label %for.end15, label %for.body6

for.body6:                                        ; preds = %for.end, %for.inc13
  %indvars.iv31 = phi i64 [ %indvars.iv.next32, %for.inc13 ], [ 0, %for.end ]
  %arrayidx8 = getelementptr inbounds ptr, ptr %vla, i64 %indvars.iv31
  %9 = load ptr, ptr %arrayidx8, align 8
  %cmp9.not = icmp eq ptr %9, null
  br i1 %cmp9.not, label %for.inc13, label %if.then10

if.then10:                                        ; preds = %for.body6
  tail call void @arena_decay(ptr noundef %tsdn, ptr noundef nonnull %9, i1 noundef zeroext false, i1 noundef zeroext %all) #14
  br label %for.inc13

for.inc13:                                        ; preds = %for.body6, %if.then10
  %indvars.iv.next32 = add nuw nsw i64 %indvars.iv31, 1
  %exitcond35.not = icmp eq i64 %indvars.iv.next32, %5
  br i1 %exitcond35.not, label %for.end15, label %for.body6, !llvm.loop !20

for.end15:                                        ; preds = %for.inc13, %for.end.thread, %for.end
  tail call void @llvm.stackrestore.p0(ptr %6)
  br label %if.end21

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %idxprom.i22 = zext i32 %arena_ind to i64
  %arrayidx.i23 = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %idxprom.i22
  %10 = load atomic i64, ptr %arrayidx.i23 acquire, align 8
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i25 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp17.not = icmp eq i64 %10, 0
  br i1 %cmp17.not, label %if.end21, label %if.then18

if.then18:                                        ; preds = %do.end
  %11 = inttoptr i64 %10 to ptr
  tail call void @arena_decay(ptr noundef %tsdn, ptr noundef nonnull %11, i1 noundef zeroext false, i1 noundef zeroext %all) #14
  br label %if.end21

if.end21:                                         ; preds = %do.end, %if.then18, %for.end15
  ret void
}

; Function Attrs: nounwind uwtable
define internal fastcc void @arena_reset_prepare_background_thread(ptr noundef %tsd, i32 noundef %arena_ind) unnamed_addr #0 {
entry:
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @background_thread_lock) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %3 = load atomic i8, ptr @background_thread_enabled_state monotonic, align 1
  %4 = and i8 %3, 1
  %tobool.i.not = icmp eq i8 %4, 0
  br i1 %tobool.i.not, label %if.end, label %if.then

if.then:                                          ; preds = %malloc_mutex_lock.exit
  %conv = zext i32 %arena_ind to i64
  %5 = load ptr, ptr @background_thread_info, align 8
  %6 = load i64, ptr @max_background_threads, align 8
  %rem.i = urem i64 %conv, %6
  %arrayidx.i = getelementptr inbounds %struct.background_thread_info_s, ptr %5, i64 %rem.i
  %lock.i.i = getelementptr inbounds i8, ptr %arrayidx.i, i64 128
  %call.i.i5 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull %lock.i.i) #14
  %cmp.i.not.i6 = icmp eq i32 %call.i.i5, 0
  br i1 %cmp.i.not.i6, label %if.end.i8, label %if.then.i7

if.then.i7:                                       ; preds = %if.then
  %mtx = getelementptr inbounds i8, ptr %arrayidx.i, i64 56
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull %mtx) #14
  %locked.i = getelementptr inbounds i8, ptr %arrayidx.i, i64 120
  store atomic i8 1, ptr %locked.i monotonic, align 1
  br label %if.end.i8

if.end.i8:                                        ; preds = %if.then.i7, %if.then
  %n_lock_ops.i.i = getelementptr inbounds i8, ptr %arrayidx.i, i64 112
  %7 = load i64, ptr %n_lock_ops.i.i, align 8
  %inc.i.i9 = add i64 %7, 1
  store i64 %inc.i.i9, ptr %n_lock_ops.i.i, align 8
  %prev_owner.i.i = getelementptr inbounds i8, ptr %arrayidx.i, i64 104
  %8 = load ptr, ptr %prev_owner.i.i, align 8
  %cmp.not.i.i10 = icmp eq ptr %8, %tsd
  br i1 %cmp.not.i.i10, label %malloc_mutex_lock.exit13, label %if.then.i.i11

if.then.i.i11:                                    ; preds = %if.end.i8
  store ptr %tsd, ptr %prev_owner.i.i, align 8
  %n_owner_switches.i.i = getelementptr inbounds i8, ptr %arrayidx.i, i64 96
  %9 = load i64, ptr %n_owner_switches.i.i, align 8
  %inc2.i.i12 = add i64 %9, 1
  store i64 %inc2.i.i12, ptr %n_owner_switches.i.i, align 8
  br label %malloc_mutex_lock.exit13

malloc_mutex_lock.exit13:                         ; preds = %if.end.i8, %if.then.i.i11
  %state = getelementptr inbounds i8, ptr %arrayidx.i, i64 168
  store i32 2, ptr %state, align 8
  %locked.i14 = getelementptr inbounds i8, ptr %arrayidx.i, i64 120
  store atomic i8 0, ptr %locked.i14 monotonic, align 1
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull %lock.i.i) #14
  br label %if.end

if.end:                                           ; preds = %malloc_mutex_lock.exit13, %malloc_mutex_lock.exit
  ret void
}

declare void @arena_reset(ptr noundef, ptr noundef) local_unnamed_addr #1

declare i32 @arena_nthreads_get(ptr noundef, i1 noundef zeroext) local_unnamed_addr #1

declare void @arena_destroy(ptr noundef, ptr noundef) local_unnamed_addr #1

; Function Attrs: mustprogress nofree nounwind willreturn memory(argmem: read)
declare i32 @strcmp(ptr nocapture noundef, ptr nocapture noundef) local_unnamed_addr #8

declare zeroext i1 @extent_dss_prec_set(i32 noundef) local_unnamed_addr #1

declare i32 @extent_dss_prec_get() local_unnamed_addr #1

declare zeroext i1 @arena_dss_prec_set(ptr noundef, i32 noundef) local_unnamed_addr #1

declare i32 @arena_dss_prec_get(ptr noundef) local_unnamed_addr #1

declare i64 @arena_decay_ms_get(ptr noundef, i32 noundef) local_unnamed_addr #1

declare zeroext i1 @arena_is_huge(i32 noundef) local_unnamed_addr #1

declare zeroext i1 @background_thread_create(ptr noundef, i32 noundef) local_unnamed_addr #1

declare zeroext i1 @arena_decay_ms_set(ptr noundef, ptr noundef, i32 noundef, i64 noundef) local_unnamed_addr #1

declare ptr @arena_set_extent_hooks(ptr noundef, ptr noundef, ptr noundef) local_unnamed_addr #1

declare ptr @arena_get_ehooks(ptr noundef) local_unnamed_addr #1

declare zeroext i1 @arena_retain_grow_limit_get_set(ptr noundef, ptr noundef, ptr noundef, ptr noundef) local_unnamed_addr #1

declare void @arena_name_get(ptr noundef, ptr noundef) local_unnamed_addr #1

declare void @arena_name_set(ptr noundef, ptr noundef) local_unnamed_addr #1

; Function Attrs: nounwind uwtable
define internal i32 @arenas_narenas_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %narenas = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_arenas, align 8
  %narenas2 = getelementptr inbounds i8, ptr %3, i64 8
  %4 = load i32, ptr %narenas2, align 8
  store i32 %4, ptr %narenas, align 4
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %5, 4
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %narenas, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i32 %4, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @arenas_dirty_decay_ms_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %oldval.i = alloca i64, align 8
  call void @llvm.lifetime.start.p0(i64 8, ptr nonnull %oldval.i)
  %cmp.i = icmp ne ptr %oldp, null
  %cmp1.i = icmp ne ptr %oldlenp, null
  %or.cond.i = and i1 %cmp.i, %cmp1.i
  br i1 %or.cond.i, label %if.then.i, label %if.end15.i

if.then.i:                                        ; preds = %entry
  %call.i = tail call i64 @arena_dirty_decay_ms_default_get() #14
  store i64 %call.i, ptr %oldval.i, align 8
  %0 = load i64, ptr %oldlenp, align 8
  %cmp7.not.i = icmp eq i64 %0, 8
  br i1 %cmp7.not.i, label %if.end.i, label %if.then8.i

if.then8.i:                                       ; preds = %if.then.i
  %spec.select.i = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval.i, i64 %spec.select.i, i1 false)
  store i64 %spec.select.i, ptr %oldlenp, align 8
  br label %arenas_decay_ms_ctl_impl.exit

if.end.i:                                         ; preds = %if.then.i
  store i64 %call.i, ptr %oldp, align 8
  br label %if.end15.i

if.end15.i:                                       ; preds = %if.end.i, %entry
  %cmp16.not.i = icmp eq ptr %newp, null
  br i1 %cmp16.not.i, label %if.end28.i, label %if.then17.i

if.then17.i:                                      ; preds = %if.end15.i
  %cmp18.not.i = icmp eq i64 %newlen, 8
  br i1 %cmp18.not.i, label %if.end20.i, label %arenas_decay_ms_ctl_impl.exit

if.end20.i:                                       ; preds = %if.then17.i
  %1 = load i64, ptr %newp, align 8
  %call23.i = tail call zeroext i1 @arena_dirty_decay_ms_default_set(i64 noundef %1) #14
  br i1 %call23.i, label %arenas_decay_ms_ctl_impl.exit, label %if.end28.i

if.end28.i:                                       ; preds = %if.end20.i, %if.end15.i
  br label %arenas_decay_ms_ctl_impl.exit

arenas_decay_ms_ctl_impl.exit:                    ; preds = %if.then8.i, %if.then17.i, %if.end20.i, %if.end28.i
  %ret.0.i = phi i32 [ 22, %if.then8.i ], [ 0, %if.end28.i ], [ 22, %if.then17.i ], [ 14, %if.end20.i ]
  call void @llvm.lifetime.end.p0(i64 8, ptr nonnull %oldval.i)
  ret i32 %ret.0.i
}

; Function Attrs: nounwind uwtable
define internal i32 @arenas_muzzy_decay_ms_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %oldval.i = alloca i64, align 8
  call void @llvm.lifetime.start.p0(i64 8, ptr nonnull %oldval.i)
  %cmp.i = icmp ne ptr %oldp, null
  %cmp1.i = icmp ne ptr %oldlenp, null
  %or.cond.i = and i1 %cmp.i, %cmp1.i
  br i1 %or.cond.i, label %if.then.i, label %if.end15.i

if.then.i:                                        ; preds = %entry
  %call2.i = tail call i64 @arena_muzzy_decay_ms_default_get() #14
  store i64 %call2.i, ptr %oldval.i, align 8
  %0 = load i64, ptr %oldlenp, align 8
  %cmp7.not.i = icmp eq i64 %0, 8
  br i1 %cmp7.not.i, label %if.end.i, label %if.then8.i

if.then8.i:                                       ; preds = %if.then.i
  %spec.select.i = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval.i, i64 %spec.select.i, i1 false)
  store i64 %spec.select.i, ptr %oldlenp, align 8
  br label %arenas_decay_ms_ctl_impl.exit

if.end.i:                                         ; preds = %if.then.i
  store i64 %call2.i, ptr %oldp, align 8
  br label %if.end15.i

if.end15.i:                                       ; preds = %if.end.i, %entry
  %cmp16.not.i = icmp eq ptr %newp, null
  br i1 %cmp16.not.i, label %if.end28.i, label %if.then17.i

if.then17.i:                                      ; preds = %if.end15.i
  %cmp18.not.i = icmp eq i64 %newlen, 8
  br i1 %cmp18.not.i, label %if.end20.i, label %arenas_decay_ms_ctl_impl.exit

if.end20.i:                                       ; preds = %if.then17.i
  %1 = load i64, ptr %newp, align 8
  %call25.i = tail call zeroext i1 @arena_muzzy_decay_ms_default_set(i64 noundef %1) #14
  br i1 %call25.i, label %arenas_decay_ms_ctl_impl.exit, label %if.end28.i

if.end28.i:                                       ; preds = %if.end20.i, %if.end15.i
  br label %arenas_decay_ms_ctl_impl.exit

arenas_decay_ms_ctl_impl.exit:                    ; preds = %if.then8.i, %if.then17.i, %if.end20.i, %if.end28.i
  %ret.0.i = phi i32 [ 22, %if.then8.i ], [ 0, %if.end28.i ], [ 22, %if.then17.i ], [ 14, %if.end20.i ]
  call void @llvm.lifetime.end.p0(i64 8, ptr nonnull %oldval.i)
  ret i32 %ret.0.i
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @arenas_quantum_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i64 16, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %0, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 16, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @arenas_page_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i64 4096, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %0, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 4096, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @arenas_tcache_max_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @global_do_not_change_tcache_maxclass, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @arenas_nbins_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i32 36, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %0, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 36, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @arenas_nhbins_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i32, ptr @global_do_not_change_tcache_nbins, align 4
  store i32 %0, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %0, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @arenas_nlextents_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i32 196, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %0, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 196, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @arenas_create_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %arena_ind = alloca i32, align 4
  %config = alloca %struct.arena_config_s, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp eq ptr %oldp, null
  %cmp1 = icmp eq ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %if.then, label %lor.lhs.false2

lor.lhs.false2:                                   ; preds = %malloc_mutex_lock.exit
  %3 = load i64, ptr %oldlenp, align 8
  %cmp3.not = icmp eq i64 %3, 4
  br i1 %cmp3.not, label %do.end, label %if.then5

if.then:                                          ; preds = %malloc_mutex_lock.exit
  br i1 %cmp1, label %label_return, label %if.then5

if.then5:                                         ; preds = %lor.lhs.false2, %if.then
  store i64 0, ptr %oldlenp, align 8
  br label %label_return

do.end:                                           ; preds = %lor.lhs.false2
  call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) %config, ptr noundef nonnull align 8 dereferenceable(16) @arena_config_default, i64 16, i1 false)
  %cmp8.not = icmp eq ptr %newp, null
  br i1 %cmp8.not, label %do.end14, label %if.then9

if.then9:                                         ; preds = %do.end
  %cmp10.not = icmp eq i64 %newlen, 8
  br i1 %cmp10.not, label %if.end12, label %label_return

if.end12:                                         ; preds = %if.then9
  %4 = load ptr, ptr %newp, align 8
  store ptr %4, ptr %config, align 8
  br label %do.end14

do.end14:                                         ; preds = %do.end, %if.end12
  %call15 = call fastcc i32 @ctl_arena_init(ptr noundef %tsd, ptr noundef nonnull %config)
  store i32 %call15, ptr %arena_ind, align 4
  %cmp16 = icmp eq i32 %call15, -1
  br i1 %cmp16, label %label_return, label %if.then22

if.then22:                                        ; preds = %do.end14
  %5 = load i64, ptr %oldlenp, align 8
  %cmp23.not = icmp eq i64 %5, 4
  br i1 %cmp23.not, label %if.end26, label %if.then24

if.then24:                                        ; preds = %if.then22
  %spec.select = call i64 @llvm.umin.i64(i64 %5, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %arena_ind, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end26:                                         ; preds = %if.then22
  store i32 %call15, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %do.end14, %if.then9, %if.then, %if.then5, %if.end26, %if.then24
  %ret.0 = phi i32 [ 22, %if.then24 ], [ 0, %if.end26 ], [ 22, %if.then5 ], [ 22, %if.then ], [ 22, %if.then9 ], [ 11, %do.end14 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @arenas_lookup_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %rtree_ctx_fallback.i = alloca %struct.rtree_ctx_s, align 8
  %contents.i = alloca %struct.rtree_contents_s, align 8
  %arena_ind = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp.not = icmp eq ptr %newp, null
  br i1 %cmp.not, label %do.end, label %if.then

if.then:                                          ; preds = %malloc_mutex_lock.exit
  %cmp1.not = icmp eq i64 %newlen, 8
  br i1 %cmp1.not, label %if.end, label %label_return

if.end:                                           ; preds = %if.then
  %3 = load ptr, ptr %newp, align 8
  %4 = ptrtoint ptr %3 to i64
  br label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit, %if.end
  %ptr.0 = phi i64 [ %4, %if.end ], [ 0, %malloc_mutex_lock.exit ]
  %cmp.i.i = icmp eq ptr %tsd, null
  br i1 %cmp.i.i, label %if.then.i31, label %if.end.i30.split

if.then.i31:                                      ; preds = %do.end
  call void @rtree_ctx_data_init(ptr noundef nonnull %rtree_ctx_fallback.i) #14
  %call1.i20 = call fastcc zeroext i1 @rtree_read_independent(ptr noundef null, ptr noundef nonnull %rtree_ctx_fallback.i, i64 noundef %ptr.0, ptr noundef nonnull %contents.i)
  br label %tsdn_rtree_ctx.exit

if.end.i30.split:                                 ; preds = %do.end
  %cant_access_tsd_items_directly_use_a_getter_or_setter_rtree_ctx.i = getelementptr inbounds i8, ptr %tsd, i64 440
  %call1.i21 = call fastcc zeroext i1 @rtree_read_independent(ptr noundef nonnull %tsd, ptr noundef nonnull %cant_access_tsd_items_directly_use_a_getter_or_setter_rtree_ctx.i, i64 noundef %ptr.0, ptr noundef nonnull %contents.i)
  br label %tsdn_rtree_ctx.exit

tsdn_rtree_ctx.exit:                              ; preds = %if.end.i30.split, %if.then.i31
  %phi.call = phi i1 [ %call1.i20, %if.then.i31 ], [ %call1.i21, %if.end.i30.split ]
  %5 = load ptr, ptr %contents.i, align 8
  %alloc_ctx.sroa.2.0 = select i1 %phi.call, ptr undef, ptr %5
  %cmp6 = icmp eq ptr %alloc_ctx.sroa.2.0, null
  %or.cond = select i1 %phi.call, i1 true, i1 %cmp6
  br i1 %or.cond, label %label_return, label %if.end8

if.end8:                                          ; preds = %tsdn_rtree_ctx.exit
  %edata.val.i = load i64, ptr %alloc_ctx.sroa.2.0, align 8
  %conv.i.i = and i64 %edata.val.i, 4095
  %arrayidx.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %conv.i.i
  %6 = load atomic i64, ptr %arrayidx.i monotonic, align 8
  %cmp11 = icmp eq i64 %6, 0
  br i1 %cmp11, label %label_return, label %if.end13

if.end13:                                         ; preds = %if.end8
  %7 = inttoptr i64 %6 to ptr
  %8 = getelementptr i8, ptr %7, i64 78944
  %call10.val = load i32, ptr %8, align 32
  store i32 %call10.val, ptr %arena_ind, align 4
  %cmp16 = icmp ne ptr %oldp, null
  %cmp17 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp16, %cmp17
  br i1 %or.cond1, label %if.then18, label %label_return

if.then18:                                        ; preds = %if.end13
  %9 = load i64, ptr %oldlenp, align 8
  %cmp19.not = icmp eq i64 %9, 4
  br i1 %cmp19.not, label %if.end22, label %if.then20

if.then20:                                        ; preds = %if.then18
  %spec.select = call i64 @llvm.umin.i64(i64 %9, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %arena_ind, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end22:                                         ; preds = %if.then18
  store i32 %call10.val, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end22, %if.end13, %if.then, %if.end8, %tsdn_rtree_ctx.exit, %if.then20
  %ret.0 = phi i32 [ 22, %tsdn_rtree_ctx.exit ], [ 22, %if.end8 ], [ 22, %if.then20 ], [ 22, %if.then ], [ 0, %if.end13 ], [ 0, %if.end22 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

declare i64 @arena_dirty_decay_ms_default_get() local_unnamed_addr #1

declare i64 @arena_muzzy_decay_ms_default_get() local_unnamed_addr #1

declare zeroext i1 @arena_dirty_decay_ms_default_set(i64 noundef) local_unnamed_addr #1

declare zeroext i1 @arena_muzzy_decay_ms_default_set(i64 noundef) local_unnamed_addr #1

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal ptr @arenas_bin_i_index(ptr nocapture readnone %tsdn, ptr nocapture readnone %mib, i64 %miblen, i64 noundef %i) #2 {
entry:
  %cmp = icmp ugt i64 %i, 36
  %.super_arenas_bin_i_node = select i1 %cmp, ptr null, ptr @super_arenas_bin_i_node
  ret ptr %.super_arenas_bin_i_node
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @arenas_bin_i_size_ctl(ptr nocapture readnone %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %0 = load i64, ptr %arrayidx, align 8
  %arrayidx2 = getelementptr inbounds [36 x %struct.bin_info_s], ptr @bin_infos, i64 0, i64 %0
  %1 = load i64, ptr %arrayidx2, align 8
  store i64 %1, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %2, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %2, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %1, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %entry, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end10 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @arenas_bin_i_nregs_ctl(ptr nocapture readnone %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %0 = load i64, ptr %arrayidx, align 8
  %nregs = getelementptr inbounds [36 x %struct.bin_info_s], ptr @bin_infos, i64 0, i64 %0, i32 2
  %1 = load i32, ptr %nregs, align 8
  store i32 %1, ptr %oldval, align 4
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %2, 4
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %2, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i32 %1, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %entry, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end10 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @arenas_bin_i_slab_size_ctl(ptr nocapture readnone %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %0 = load i64, ptr %arrayidx, align 8
  %slab_size = getelementptr inbounds [36 x %struct.bin_info_s], ptr @bin_infos, i64 0, i64 %0, i32 1
  %1 = load i64, ptr %slab_size, align 8
  store i64 %1, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %2, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %2, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %1, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %entry, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end10 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @arenas_bin_i_nshards_ctl(ptr nocapture readnone %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %0 = load i64, ptr %arrayidx, align 8
  %n_shards = getelementptr inbounds [36 x %struct.bin_info_s], ptr @bin_infos, i64 0, i64 %0, i32 3
  %1 = load i32, ptr %n_shards, align 4
  store i32 %1, ptr %oldval, align 4
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %2, 4
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %2, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i32 %1, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %entry, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end10 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal ptr @arenas_lextent_i_index(ptr nocapture readnone %tsdn, ptr nocapture readnone %mib, i64 %miblen, i64 noundef %i) #2 {
entry:
  %cmp = icmp ugt i64 %i, 196
  %.super_arenas_lextent_i_node = select i1 %cmp, ptr null, ptr @super_arenas_lextent_i_node
  ret ptr %.super_arenas_lextent_i_node
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @arenas_lextent_i_size_ctl(ptr nocapture readnone %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %0 = load i64, ptr %arrayidx, align 8
  %conv3 = add i64 %0, 36
  %idxprom.i = and i64 %conv3, 4294967295
  %arrayidx.i = getelementptr inbounds [232 x i64], ptr @sz_index2size_tab, i64 0, i64 %idxprom.i
  %1 = load i64, ptr %arrayidx.i, align 8
  store i64 %1, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp7
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %2, 8
  br i1 %cmp10.not, label %if.end15, label %if.then12

if.then12:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %2, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end15:                                         ; preds = %if.then9
  store i64 %1, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end15, %do.end, %entry, %if.then12
  %ret.0 = phi i32 [ 22, %if.then12 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end15 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal fastcc i32 @ctl_arena_init(ptr noundef %tsd, ptr noundef %config) unnamed_addr #0 {
entry:
  %0 = load ptr, ptr @ctl_arenas, align 8
  %destroyed = getelementptr inbounds i8, ptr %0, i64 16
  %1 = load ptr, ptr %destroyed, align 8
  %cmp = icmp eq ptr %1, null
  br i1 %cmp, label %if.else57, label %cond.end

cond.end:                                         ; preds = %entry
  %qre_prev = getelementptr inbounds i8, ptr %1, i64 16
  %2 = load ptr, ptr %qre_prev, align 8
  %cmp3.not = icmp eq ptr %2, null
  br i1 %cmp3.not, label %if.else57, label %do.body

do.body:                                          ; preds = %cond.end
  %cmp6 = icmp eq ptr %1, %2
  br i1 %cmp6, label %if.then7, label %if.end

if.then7:                                         ; preds = %do.body
  %destroyed_link10 = getelementptr inbounds i8, ptr %1, i64 8
  %3 = load ptr, ptr %destroyed_link10, align 8
  store ptr %3, ptr %destroyed, align 8
  br label %if.end

if.end:                                           ; preds = %if.then7, %do.body
  %4 = phi ptr [ %3, %if.then7 ], [ %1, %do.body ]
  %cmp15.not = icmp eq ptr %4, %2
  br i1 %cmp15.not, label %do.body50, label %do.body17

do.body17:                                        ; preds = %if.end
  %destroyed_link18 = getelementptr inbounds i8, ptr %2, i64 8
  %5 = load ptr, ptr %destroyed_link18, align 8
  %qre_prev21 = getelementptr inbounds i8, ptr %5, i64 16
  %6 = load ptr, ptr %qre_prev21, align 8
  %qre_prev23 = getelementptr inbounds i8, ptr %2, i64 16
  %7 = load ptr, ptr %qre_prev23, align 8
  %destroyed_link24 = getelementptr inbounds i8, ptr %7, i64 8
  store ptr %6, ptr %destroyed_link24, align 8
  %8 = load ptr, ptr %qre_prev23, align 8
  %9 = load ptr, ptr %destroyed_link18, align 8
  %qre_prev31 = getelementptr inbounds i8, ptr %9, i64 16
  store ptr %8, ptr %qre_prev31, align 8
  %destroyed_link34 = getelementptr inbounds i8, ptr %8, i64 8
  %10 = load ptr, ptr %destroyed_link34, align 8
  store ptr %10, ptr %qre_prev23, align 8
  %11 = load ptr, ptr %destroyed_link18, align 8
  %qre_prev43 = getelementptr inbounds i8, ptr %11, i64 16
  %12 = load ptr, ptr %qre_prev43, align 8
  %destroyed_link44 = getelementptr inbounds i8, ptr %12, i64 8
  store ptr %11, ptr %destroyed_link44, align 8
  %13 = load ptr, ptr %qre_prev23, align 8
  %destroyed_link48 = getelementptr inbounds i8, ptr %13, i64 8
  store ptr %2, ptr %destroyed_link48, align 8
  br label %if.end58

do.body50:                                        ; preds = %if.end
  store ptr null, ptr %destroyed, align 8
  br label %if.end58

if.else57:                                        ; preds = %entry, %cond.end
  %narenas = getelementptr inbounds i8, ptr %0, i64 8
  br label %if.end58

if.end58:                                         ; preds = %do.body50, %do.body17, %if.else57
  %arena_ind.0.in = phi ptr [ %narenas, %if.else57 ], [ %2, %do.body17 ], [ %2, %do.body50 ]
  %arena_ind.0 = load i32, ptr %arena_ind.0.in, align 8
  %conv = zext i32 %arena_ind.0 to i64
  %arenas.i = getelementptr inbounds i8, ptr %0, i64 24
  switch i32 %arena_ind.0, label %sw.default.i.i [
    i32 4096, label %arenas_i2a_impl.exit.i
    i32 4097, label %sw.bb2.i.i
  ]

sw.bb2.i.i:                                       ; preds = %if.end58
  br label %arenas_i2a_impl.exit.i

sw.default.i.i:                                   ; preds = %if.end58
  %add.i.i = add nuw nsw i64 %conv, 2
  %14 = and i64 %add.i.i, 4294967295
  br label %arenas_i2a_impl.exit.i

arenas_i2a_impl.exit.i:                           ; preds = %sw.default.i.i, %sw.bb2.i.i, %if.end58
  %a.0.i.i = phi i64 [ %14, %sw.default.i.i ], [ 1, %sw.bb2.i.i ], [ 0, %if.end58 ]
  %arrayidx.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i, i64 0, i64 %a.0.i.i
  %15 = load ptr, ptr %arrayidx.i, align 8
  %cmp.i = icmp eq ptr %15, null
  br i1 %cmp.i, label %if.then.i, label %if.end62

if.then.i:                                        ; preds = %arenas_i2a_impl.exit.i
  %call4.i = tail call ptr @b0get() #14
  %call5.i = tail call ptr @base_alloc(ptr noundef %tsd, ptr noundef %call4.i, i64 noundef 37872, i64 noundef 16) #14
  %cmp6.i = icmp eq ptr %call5.i, null
  br i1 %cmp6.i, label %return, label %if.end.i

if.end.i:                                         ; preds = %if.then.i
  %astats.i = getelementptr inbounds i8, ptr %call5.i, i64 88
  %astats8.i = getelementptr inbounds i8, ptr %call5.i, i64 80
  store ptr %astats.i, ptr %astats8.i, align 8
  store i32 %arena_ind.0, ptr %call5.i, align 8
  %16 = load ptr, ptr @ctl_arenas, align 8
  %arenas9.i = getelementptr inbounds i8, ptr %16, i64 24
  switch i32 %arena_ind.0, label %sw.default.i12.i [
    i32 4096, label %arenas_i2a_impl.exit20.i
    i32 4097, label %sw.bb2.i10.i
  ]

sw.bb2.i10.i:                                     ; preds = %if.end.i
  br label %arenas_i2a_impl.exit20.i

sw.default.i12.i:                                 ; preds = %if.end.i
  %add.i15.i = add nuw nsw i64 %conv, 2
  %17 = and i64 %add.i15.i, 4294967295
  br label %arenas_i2a_impl.exit20.i

arenas_i2a_impl.exit20.i:                         ; preds = %sw.default.i12.i, %sw.bb2.i10.i, %if.end.i
  %a.0.i11.i = phi i64 [ %17, %sw.default.i12.i ], [ 1, %sw.bb2.i10.i ], [ 0, %if.end.i ]
  %arrayidx13.i = getelementptr inbounds [4097 x ptr], ptr %arenas9.i, i64 0, i64 %a.0.i11.i
  store ptr %call5.i, ptr %arrayidx13.i, align 8
  br label %if.end62

if.end62:                                         ; preds = %arenas_i2a_impl.exit20.i, %arenas_i2a_impl.exit.i
  %call64 = tail call ptr @arena_init(ptr noundef %tsd, i32 noundef %arena_ind.0, ptr noundef %config) #14
  %cmp65 = icmp eq ptr %call64, null
  br i1 %cmp65, label %return, label %if.end68

if.end68:                                         ; preds = %if.end62
  %18 = load ptr, ptr @ctl_arenas, align 8
  %narenas69 = getelementptr inbounds i8, ptr %18, i64 8
  %19 = load i32, ptr %narenas69, align 8
  %cmp70 = icmp eq i32 %arena_ind.0, %19
  br i1 %cmp70, label %if.then72, label %return

if.then72:                                        ; preds = %if.end68
  %inc = add i32 %arena_ind.0, 1
  store i32 %inc, ptr %narenas69, align 8
  br label %return

return:                                           ; preds = %if.then.i, %if.end68, %if.then72, %if.end62
  %retval.0 = phi i32 [ -1, %if.end62 ], [ %arena_ind.0, %if.then72 ], [ %arena_ind.0, %if.end68 ], [ -1, %if.then.i ]
  ret i32 %retval.0
}

; Function Attrs: nounwind uwtable
define internal fastcc zeroext i1 @rtree_read_independent(ptr noundef %tsdn, ptr noundef %rtree_ctx, i64 noundef %key, ptr nocapture noundef writeonly %r_contents) unnamed_addr #0 {
entry:
  %shr.i = lshr i64 %key, 30
  %and.i = and i64 %shr.i, 15
  %and.i10 = and i64 %key, -1073741824
  %arrayidx.i = getelementptr inbounds [16 x %struct.rtree_ctx_cache_elm_s], ptr %rtree_ctx, i64 0, i64 %and.i
  %0 = load i64, ptr %arrayidx.i, align 8
  %cmp.i = icmp eq i64 %0, %and.i10
  br i1 %cmp.i, label %if.then.i, label %if.end.i

if.then.i:                                        ; preds = %entry
  %leaf11.i = getelementptr inbounds i8, ptr %arrayidx.i, i64 8
  %1 = load ptr, ptr %leaf11.i, align 8
  %shr.i18 = lshr i64 %key, 12
  %and.i19 = and i64 %shr.i18, 262143
  %arrayidx15.i = getelementptr inbounds %struct.rtree_leaf_elm_s, ptr %1, i64 %and.i19
  br label %rtree_leaf_elm_lookup.exit

if.end.i:                                         ; preds = %entry
  %l2_cache.i = getelementptr inbounds i8, ptr %rtree_ctx, i64 256
  %2 = load i64, ptr %l2_cache.i, align 8
  %cmp19.i = icmp eq i64 %2, %and.i10
  br i1 %cmp19.i, label %if.then27.i, label %for.body.i

if.then27.i:                                      ; preds = %if.end.i
  %leaf31.i = getelementptr inbounds i8, ptr %rtree_ctx, i64 264
  %3 = load ptr, ptr %leaf31.i, align 8
  store i64 %0, ptr %l2_cache.i, align 8
  %leaf42.i = getelementptr inbounds i8, ptr %arrayidx.i, i64 8
  %4 = load ptr, ptr %leaf42.i, align 8
  store ptr %4, ptr %leaf31.i, align 8
  store i64 %and.i10, ptr %arrayidx.i, align 8
  store ptr %3, ptr %leaf42.i, align 8
  %shr.i37 = lshr i64 %key, 12
  %and.i38 = and i64 %shr.i37, 262143
  %arrayidx54.i = getelementptr inbounds %struct.rtree_leaf_elm_s, ptr %3, i64 %and.i38
  br label %rtree_leaf_elm_lookup.exit

for.body.i:                                       ; preds = %if.end.i, %if.end137.i
  %indvars.iv = phi i64 [ %indvars.iv.next, %if.end137.i ], [ 1, %if.end.i ]
  %arrayidx61.i = getelementptr inbounds [8 x %struct.rtree_ctx_cache_elm_s], ptr %l2_cache.i, i64 0, i64 %indvars.iv
  %5 = load i64, ptr %arrayidx61.i, align 8
  %cmp63.i = icmp eq i64 %5, %and.i10
  br i1 %cmp63.i, label %if.then71.i, label %if.end137.i

if.then71.i:                                      ; preds = %for.body.i
  %leaf76.i = getelementptr inbounds i8, ptr %arrayidx61.i, i64 8
  %6 = load ptr, ptr %leaf76.i, align 8
  %sub.i = add nuw i64 %indvars.iv, 4294967295
  %idxprom83.i = and i64 %sub.i, 4294967295
  %arrayidx84.i = getelementptr inbounds [8 x %struct.rtree_ctx_cache_elm_s], ptr %l2_cache.i, i64 0, i64 %idxprom83.i
  %7 = load i64, ptr %arrayidx84.i, align 8
  store i64 %7, ptr %arrayidx61.i, align 8
  %leaf94.i = getelementptr inbounds i8, ptr %arrayidx84.i, i64 8
  %8 = load ptr, ptr %leaf94.i, align 8
  store ptr %8, ptr %leaf76.i, align 8
  store i64 %0, ptr %arrayidx84.i, align 8
  %leaf109.i = getelementptr inbounds i8, ptr %arrayidx.i, i64 8
  %9 = load ptr, ptr %leaf109.i, align 8
  store ptr %9, ptr %leaf94.i, align 8
  store i64 %and.i10, ptr %arrayidx.i, align 8
  store ptr %6, ptr %leaf109.i, align 8
  %shr.i56 = lshr i64 %key, 12
  %and.i57 = and i64 %shr.i56, 262143
  %arrayidx136.i = getelementptr inbounds %struct.rtree_leaf_elm_s, ptr %6, i64 %and.i57
  br label %rtree_leaf_elm_lookup.exit

if.end137.i:                                      ; preds = %for.body.i
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, 8
  br i1 %exitcond.not, label %for.end.i, label %for.body.i, !llvm.loop !21

for.end.i:                                        ; preds = %if.end137.i
  %call141.i = tail call ptr @rtree_leaf_elm_lookup_hard(ptr noundef %tsdn, ptr noundef nonnull @arena_emap_global, ptr noundef nonnull %rtree_ctx, i64 noundef %key, i1 noundef zeroext false, i1 noundef zeroext false) #14
  br label %rtree_leaf_elm_lookup.exit

rtree_leaf_elm_lookup.exit:                       ; preds = %for.end.i, %if.then71.i, %if.then27.i, %if.then.i
  %retval.i.0 = phi ptr [ %arrayidx15.i, %if.then.i ], [ %arrayidx54.i, %if.then27.i ], [ %arrayidx136.i, %if.then71.i ], [ %call141.i, %for.end.i ]
  %cmp = icmp eq ptr %retval.i.0, null
  br i1 %cmp, label %return, label %acquire.i.i

acquire.i.i:                                      ; preds = %rtree_leaf_elm_lookup.exit
  %10 = load atomic i64, ptr %retval.i.0 acquire, align 8, !noalias !22
  %shr.i69 = lshr i64 %10, 48
  %conv.i70 = trunc i64 %shr.i69 to i32
  %11 = trunc i64 %10 to i8
  %frombool.i73 = and i8 %11, 1
  %12 = lshr i8 %11, 1
  %frombool5.i = and i8 %12, 1
  %13 = trunc i64 %10 to i32
  %14 = lshr i32 %13, 2
  %conv8.i = and i32 %14, 7
  %shl.i74 = shl i64 %10, 16
  %shr10.i = ashr exact i64 %shl.i74, 16
  %and11.i = and i64 %shr10.i, -128
  %15 = inttoptr i64 %and11.i to ptr
  store ptr %15, ptr %r_contents, align 8
  %tmp.sroa.2.0..sroa_idx = getelementptr inbounds i8, ptr %r_contents, i64 8
  store i32 %conv.i70, ptr %tmp.sroa.2.0..sroa_idx, align 8
  %tmp.sroa.3.0..sroa_idx = getelementptr inbounds i8, ptr %r_contents, i64 12
  store i32 %conv8.i, ptr %tmp.sroa.3.0..sroa_idx, align 4
  %tmp.sroa.4.0..sroa_idx = getelementptr inbounds i8, ptr %r_contents, i64 16
  store i8 %frombool5.i, ptr %tmp.sroa.4.0..sroa_idx, align 8
  %tmp.sroa.5.0..sroa_idx = getelementptr inbounds i8, ptr %r_contents, i64 17
  store i8 %frombool.i73, ptr %tmp.sroa.5.0..sroa_idx, align 1
  br label %return

return:                                           ; preds = %rtree_leaf_elm_lookup.exit, %acquire.i.i
  ret i1 %cmp
}

declare void @rtree_ctx_data_init(ptr noundef) local_unnamed_addr #1

declare ptr @rtree_leaf_elm_lookup_hard(ptr noundef, ptr noundef, ptr noundef, i64 noundef, i1 noundef zeroext, i1 noundef zeroext) local_unnamed_addr #1

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @prof_thread_active_init_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @prof_active_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @prof_dump_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @prof_gdump_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @prof_prefix_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @prof_reset_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @prof_interval_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @lg_prof_sample_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @prof_log_start_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @prof_log_stop_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal noalias ptr @prof_stats_bins_i_index(ptr nocapture readnone %tsdn, ptr nocapture readnone %mib, i64 %miblen, i64 %i) #2 {
entry:
  ret ptr null
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal noalias ptr @prof_stats_lextents_i_index(ptr nocapture readnone %tsdn, ptr nocapture readnone %mib, i64 %miblen, i64 %i) #2 {
entry:
  ret ptr null
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_allocated_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %4 = load i64, ptr %3, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_active_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %active = getelementptr inbounds i8, ptr %3, i64 8
  %4 = load i64, ptr %active, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_metadata_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %metadata = getelementptr inbounds i8, ptr %3, i64 16
  %4 = load i64, ptr %metadata, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_metadata_edata_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %metadata_edata = getelementptr inbounds i8, ptr %3, i64 24
  %4 = load i64, ptr %metadata_edata, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_metadata_rtree_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %metadata_rtree = getelementptr inbounds i8, ptr %3, i64 32
  %4 = load i64, ptr %metadata_rtree, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_metadata_thp_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %metadata_thp = getelementptr inbounds i8, ptr %3, i64 40
  %4 = load i64, ptr %metadata_thp, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_resident_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %resident = getelementptr inbounds i8, ptr %3, i64 48
  %4 = load i64, ptr %resident, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mapped_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %mapped = getelementptr inbounds i8, ptr %3, i64 56
  %4 = load i64, ptr %mapped, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_retained_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %retained = getelementptr inbounds i8, ptr %3, i64 64
  %4 = load i64, ptr %retained, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_zero_reallocs_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %monotonic.i

monotonic.i:                                      ; preds = %malloc_mutex_lock.exit
  %3 = load atomic i64, ptr @zero_realloc_count monotonic, align 8
  store i64 %3, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %monotonic.i
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %3, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %monotonic.i, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %monotonic.i ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_background_thread_num_threads_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %background_thread = getelementptr inbounds i8, ptr %3, i64 72
  %4 = load i64, ptr %background_thread, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_background_thread_num_runs_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %num_runs = getelementptr inbounds i8, ptr %3, i64 80
  %4 = load i64, ptr %num_runs, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_background_thread_run_interval_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %run_interval = getelementptr inbounds i8, ptr %3, i64 88
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %run_interval) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

declare i64 @nstime_ns(ptr noundef) local_unnamed_addr #1

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_reset_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #0 {
entry:
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  tail call void @malloc_mutex_prof_data_reset(ptr noundef %tsd, ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %call.i.i74 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i75 = icmp eq i32 %call.i.i74, 0
  br i1 %cmp.i.not.i75, label %if.end.i77, label %if.then.i76

if.then.i76:                                      ; preds = %malloc_mutex_lock.exit
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @background_thread_lock) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i77

if.end.i77:                                       ; preds = %if.then.i76, %malloc_mutex_lock.exit
  %3 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i78 = add i64 %3, 1
  store i64 %inc.i.i78, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %4 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i79 = icmp eq ptr %4, %tsd
  br i1 %cmp.not.i.i79, label %malloc_mutex_lock.exit82, label %if.then.i.i80

if.then.i.i80:                                    ; preds = %if.end.i77
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %5 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i81 = add i64 %5, 1
  store i64 %inc2.i.i81, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit82

malloc_mutex_lock.exit82:                         ; preds = %if.end.i77, %if.then.i.i80
  tail call void @malloc_mutex_prof_data_reset(ptr noundef %tsd, ptr noundef nonnull @background_thread_lock) #14
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i83 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %call1 = tail call i32 @narenas_total_get() #14
  %cmp254.not = icmp eq i32 %call1, 0
  br i1 %cmp254.not, label %for.end88, label %for.body.preheader

for.body.preheader:                               ; preds = %malloc_mutex_lock.exit82
  %wide.trip.count = zext i32 %call1 to i64
  br label %for.body

for.body:                                         ; preds = %for.body.preheader, %for.inc86
  %indvars.iv260 = phi i64 [ 0, %for.body.preheader ], [ %indvars.iv.next261, %for.inc86 ]
  %arrayidx.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %indvars.iv260
  %6 = load atomic i64, ptr %arrayidx.i acquire, align 8
  %7 = inttoptr i64 %6 to ptr
  %tobool.not = icmp eq i64 %6, 0
  br i1 %tobool.not, label %for.inc86, label %if.end

if.end:                                           ; preds = %for.body
  %large_mtx = getelementptr inbounds i8, ptr %7, i64 10552
  %lock.i.i = getelementptr inbounds i8, ptr %7, i64 10624
  %call.i.i84 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull %lock.i.i) #14
  %cmp.i.not.i85 = icmp eq i32 %call.i.i84, 0
  br i1 %cmp.i.not.i85, label %if.end.i87, label %if.then.i86

if.then.i86:                                      ; preds = %if.end
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull %large_mtx) #14
  %locked.i = getelementptr inbounds i8, ptr %7, i64 10616
  store atomic i8 1, ptr %locked.i monotonic, align 1
  br label %if.end.i87

if.end.i87:                                       ; preds = %if.then.i86, %if.end
  %n_lock_ops.i.i = getelementptr inbounds i8, ptr %7, i64 10608
  %8 = load i64, ptr %n_lock_ops.i.i, align 8
  %inc.i.i88 = add i64 %8, 1
  store i64 %inc.i.i88, ptr %n_lock_ops.i.i, align 8
  %prev_owner.i.i = getelementptr inbounds i8, ptr %7, i64 10600
  %9 = load ptr, ptr %prev_owner.i.i, align 8
  %cmp.not.i.i89 = icmp eq ptr %9, %tsd
  br i1 %cmp.not.i.i89, label %malloc_mutex_lock.exit92, label %if.then.i.i90

if.then.i.i90:                                    ; preds = %if.end.i87
  store ptr %tsd, ptr %prev_owner.i.i, align 8
  %n_owner_switches.i.i = getelementptr inbounds i8, ptr %7, i64 10592
  %10 = load i64, ptr %n_owner_switches.i.i, align 8
  %inc2.i.i91 = add i64 %10, 1
  store i64 %inc2.i.i91, ptr %n_owner_switches.i.i, align 8
  br label %malloc_mutex_lock.exit92

malloc_mutex_lock.exit92:                         ; preds = %if.end.i87, %if.then.i.i90
  tail call void @malloc_mutex_prof_data_reset(ptr noundef %tsd, ptr noundef nonnull %large_mtx) #14
  %locked.i93 = getelementptr inbounds i8, ptr %7, i64 10616
  store atomic i8 0, ptr %locked.i93 monotonic, align 1
  %call1.i94 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull %lock.i.i) #14
  %mtx = getelementptr inbounds i8, ptr %7, i64 78784
  %lock.i.i95 = getelementptr inbounds i8, ptr %7, i64 78856
  %call.i.i96 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull %lock.i.i95) #14
  %cmp.i.not.i97 = icmp eq i32 %call.i.i96, 0
  br i1 %cmp.i.not.i97, label %if.end.i100, label %if.then.i98

if.then.i98:                                      ; preds = %malloc_mutex_lock.exit92
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull %mtx) #14
  %locked.i99 = getelementptr inbounds i8, ptr %7, i64 78848
  store atomic i8 1, ptr %locked.i99 monotonic, align 1
  br label %if.end.i100

if.end.i100:                                      ; preds = %if.then.i98, %malloc_mutex_lock.exit92
  %n_lock_ops.i.i101 = getelementptr inbounds i8, ptr %7, i64 78840
  %11 = load i64, ptr %n_lock_ops.i.i101, align 8
  %inc.i.i102 = add i64 %11, 1
  store i64 %inc.i.i102, ptr %n_lock_ops.i.i101, align 8
  %prev_owner.i.i103 = getelementptr inbounds i8, ptr %7, i64 78832
  %12 = load ptr, ptr %prev_owner.i.i103, align 8
  %cmp.not.i.i104 = icmp eq ptr %12, %tsd
  br i1 %cmp.not.i.i104, label %malloc_mutex_lock.exit108, label %if.then.i.i105

if.then.i.i105:                                   ; preds = %if.end.i100
  store ptr %tsd, ptr %prev_owner.i.i103, align 8
  %n_owner_switches.i.i106 = getelementptr inbounds i8, ptr %7, i64 78824
  %13 = load i64, ptr %n_owner_switches.i.i106, align 8
  %inc2.i.i107 = add i64 %13, 1
  store i64 %inc2.i.i107, ptr %n_owner_switches.i.i106, align 8
  br label %malloc_mutex_lock.exit108

malloc_mutex_lock.exit108:                        ; preds = %if.end.i100, %if.then.i.i105
  tail call void @malloc_mutex_prof_data_reset(ptr noundef %tsd, ptr noundef nonnull %mtx) #14
  %locked.i109 = getelementptr inbounds i8, ptr %7, i64 78848
  store atomic i8 0, ptr %locked.i109 monotonic, align 1
  %call1.i111 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull %lock.i.i95) #14
  %ecache_dirty = getelementptr inbounds i8, ptr %7, i64 10744
  %lock.i.i112 = getelementptr inbounds i8, ptr %7, i64 10816
  %call.i.i113 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull %lock.i.i112) #14
  %cmp.i.not.i114 = icmp eq i32 %call.i.i113, 0
  br i1 %cmp.i.not.i114, label %if.end.i117, label %if.then.i115

if.then.i115:                                     ; preds = %malloc_mutex_lock.exit108
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull %ecache_dirty) #14
  %locked.i116 = getelementptr inbounds i8, ptr %7, i64 10808
  store atomic i8 1, ptr %locked.i116 monotonic, align 1
  br label %if.end.i117

if.end.i117:                                      ; preds = %if.then.i115, %malloc_mutex_lock.exit108
  %n_lock_ops.i.i118 = getelementptr inbounds i8, ptr %7, i64 10800
  %14 = load i64, ptr %n_lock_ops.i.i118, align 8
  %inc.i.i119 = add i64 %14, 1
  store i64 %inc.i.i119, ptr %n_lock_ops.i.i118, align 8
  %prev_owner.i.i120 = getelementptr inbounds i8, ptr %7, i64 10792
  %15 = load ptr, ptr %prev_owner.i.i120, align 8
  %cmp.not.i.i121 = icmp eq ptr %15, %tsd
  br i1 %cmp.not.i.i121, label %malloc_mutex_lock.exit125, label %if.then.i.i122

if.then.i.i122:                                   ; preds = %if.end.i117
  store ptr %tsd, ptr %prev_owner.i.i120, align 8
  %n_owner_switches.i.i123 = getelementptr inbounds i8, ptr %7, i64 10784
  %16 = load i64, ptr %n_owner_switches.i.i123, align 8
  %inc2.i.i124 = add i64 %16, 1
  store i64 %inc2.i.i124, ptr %n_owner_switches.i.i123, align 8
  br label %malloc_mutex_lock.exit125

malloc_mutex_lock.exit125:                        ; preds = %if.end.i117, %if.then.i.i122
  tail call void @malloc_mutex_prof_data_reset(ptr noundef %tsd, ptr noundef nonnull %ecache_dirty) #14
  %locked.i126 = getelementptr inbounds i8, ptr %7, i64 10808
  store atomic i8 0, ptr %locked.i126 monotonic, align 1
  %call1.i128 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull %lock.i.i112) #14
  %ecache_muzzy = getelementptr inbounds i8, ptr %7, i64 30184
  %lock.i.i129 = getelementptr inbounds i8, ptr %7, i64 30256
  %call.i.i130 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull %lock.i.i129) #14
  %cmp.i.not.i131 = icmp eq i32 %call.i.i130, 0
  br i1 %cmp.i.not.i131, label %if.end.i134, label %if.then.i132

if.then.i132:                                     ; preds = %malloc_mutex_lock.exit125
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull %ecache_muzzy) #14
  %locked.i133 = getelementptr inbounds i8, ptr %7, i64 30248
  store atomic i8 1, ptr %locked.i133 monotonic, align 1
  br label %if.end.i134

if.end.i134:                                      ; preds = %if.then.i132, %malloc_mutex_lock.exit125
  %n_lock_ops.i.i135 = getelementptr inbounds i8, ptr %7, i64 30240
  %17 = load i64, ptr %n_lock_ops.i.i135, align 8
  %inc.i.i136 = add i64 %17, 1
  store i64 %inc.i.i136, ptr %n_lock_ops.i.i135, align 8
  %prev_owner.i.i137 = getelementptr inbounds i8, ptr %7, i64 30232
  %18 = load ptr, ptr %prev_owner.i.i137, align 8
  %cmp.not.i.i138 = icmp eq ptr %18, %tsd
  br i1 %cmp.not.i.i138, label %malloc_mutex_lock.exit142, label %if.then.i.i139

if.then.i.i139:                                   ; preds = %if.end.i134
  store ptr %tsd, ptr %prev_owner.i.i137, align 8
  %n_owner_switches.i.i140 = getelementptr inbounds i8, ptr %7, i64 30224
  %19 = load i64, ptr %n_owner_switches.i.i140, align 8
  %inc2.i.i141 = add i64 %19, 1
  store i64 %inc2.i.i141, ptr %n_owner_switches.i.i140, align 8
  br label %malloc_mutex_lock.exit142

malloc_mutex_lock.exit142:                        ; preds = %if.end.i134, %if.then.i.i139
  tail call void @malloc_mutex_prof_data_reset(ptr noundef %tsd, ptr noundef nonnull %ecache_muzzy) #14
  %locked.i143 = getelementptr inbounds i8, ptr %7, i64 30248
  store atomic i8 0, ptr %locked.i143 monotonic, align 1
  %call1.i145 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull %lock.i.i129) #14
  %ecache_retained = getelementptr inbounds i8, ptr %7, i64 49624
  %lock.i.i146 = getelementptr inbounds i8, ptr %7, i64 49696
  %call.i.i147 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull %lock.i.i146) #14
  %cmp.i.not.i148 = icmp eq i32 %call.i.i147, 0
  br i1 %cmp.i.not.i148, label %if.end.i151, label %if.then.i149

if.then.i149:                                     ; preds = %malloc_mutex_lock.exit142
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull %ecache_retained) #14
  %locked.i150 = getelementptr inbounds i8, ptr %7, i64 49688
  store atomic i8 1, ptr %locked.i150 monotonic, align 1
  br label %if.end.i151

if.end.i151:                                      ; preds = %if.then.i149, %malloc_mutex_lock.exit142
  %n_lock_ops.i.i152 = getelementptr inbounds i8, ptr %7, i64 49680
  %20 = load i64, ptr %n_lock_ops.i.i152, align 8
  %inc.i.i153 = add i64 %20, 1
  store i64 %inc.i.i153, ptr %n_lock_ops.i.i152, align 8
  %prev_owner.i.i154 = getelementptr inbounds i8, ptr %7, i64 49672
  %21 = load ptr, ptr %prev_owner.i.i154, align 8
  %cmp.not.i.i155 = icmp eq ptr %21, %tsd
  br i1 %cmp.not.i.i155, label %malloc_mutex_lock.exit159, label %if.then.i.i156

if.then.i.i156:                                   ; preds = %if.end.i151
  store ptr %tsd, ptr %prev_owner.i.i154, align 8
  %n_owner_switches.i.i157 = getelementptr inbounds i8, ptr %7, i64 49664
  %22 = load i64, ptr %n_owner_switches.i.i157, align 8
  %inc2.i.i158 = add i64 %22, 1
  store i64 %inc2.i.i158, ptr %n_owner_switches.i.i157, align 8
  br label %malloc_mutex_lock.exit159

malloc_mutex_lock.exit159:                        ; preds = %if.end.i151, %if.then.i.i156
  tail call void @malloc_mutex_prof_data_reset(ptr noundef %tsd, ptr noundef nonnull %ecache_retained) #14
  %locked.i160 = getelementptr inbounds i8, ptr %7, i64 49688
  store atomic i8 0, ptr %locked.i160 monotonic, align 1
  %call1.i162 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull %lock.i.i146) #14
  %decay_dirty = getelementptr inbounds i8, ptr %7, i64 69336
  %lock.i.i163 = getelementptr inbounds i8, ptr %7, i64 69408
  %call.i.i164 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull %lock.i.i163) #14
  %cmp.i.not.i165 = icmp eq i32 %call.i.i164, 0
  br i1 %cmp.i.not.i165, label %if.end.i168, label %if.then.i166

if.then.i166:                                     ; preds = %malloc_mutex_lock.exit159
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull %decay_dirty) #14
  %locked.i167 = getelementptr inbounds i8, ptr %7, i64 69400
  store atomic i8 1, ptr %locked.i167 monotonic, align 1
  br label %if.end.i168

if.end.i168:                                      ; preds = %if.then.i166, %malloc_mutex_lock.exit159
  %n_lock_ops.i.i169 = getelementptr inbounds i8, ptr %7, i64 69392
  %23 = load i64, ptr %n_lock_ops.i.i169, align 8
  %inc.i.i170 = add i64 %23, 1
  store i64 %inc.i.i170, ptr %n_lock_ops.i.i169, align 8
  %prev_owner.i.i171 = getelementptr inbounds i8, ptr %7, i64 69384
  %24 = load ptr, ptr %prev_owner.i.i171, align 8
  %cmp.not.i.i172 = icmp eq ptr %24, %tsd
  br i1 %cmp.not.i.i172, label %malloc_mutex_lock.exit176, label %if.then.i.i173

if.then.i.i173:                                   ; preds = %if.end.i168
  store ptr %tsd, ptr %prev_owner.i.i171, align 8
  %n_owner_switches.i.i174 = getelementptr inbounds i8, ptr %7, i64 69376
  %25 = load i64, ptr %n_owner_switches.i.i174, align 8
  %inc2.i.i175 = add i64 %25, 1
  store i64 %inc2.i.i175, ptr %n_owner_switches.i.i174, align 8
  br label %malloc_mutex_lock.exit176

malloc_mutex_lock.exit176:                        ; preds = %if.end.i168, %if.then.i.i173
  tail call void @malloc_mutex_prof_data_reset(ptr noundef %tsd, ptr noundef nonnull %decay_dirty) #14
  %locked.i177 = getelementptr inbounds i8, ptr %7, i64 69400
  store atomic i8 0, ptr %locked.i177 monotonic, align 1
  %call1.i179 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull %lock.i.i163) #14
  %decay_muzzy = getelementptr inbounds i8, ptr %7, i64 71120
  %lock.i.i180 = getelementptr inbounds i8, ptr %7, i64 71192
  %call.i.i181 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull %lock.i.i180) #14
  %cmp.i.not.i182 = icmp eq i32 %call.i.i181, 0
  br i1 %cmp.i.not.i182, label %if.end.i185, label %if.then.i183

if.then.i183:                                     ; preds = %malloc_mutex_lock.exit176
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull %decay_muzzy) #14
  %locked.i184 = getelementptr inbounds i8, ptr %7, i64 71184
  store atomic i8 1, ptr %locked.i184 monotonic, align 1
  br label %if.end.i185

if.end.i185:                                      ; preds = %if.then.i183, %malloc_mutex_lock.exit176
  %n_lock_ops.i.i186 = getelementptr inbounds i8, ptr %7, i64 71176
  %26 = load i64, ptr %n_lock_ops.i.i186, align 8
  %inc.i.i187 = add i64 %26, 1
  store i64 %inc.i.i187, ptr %n_lock_ops.i.i186, align 8
  %prev_owner.i.i188 = getelementptr inbounds i8, ptr %7, i64 71168
  %27 = load ptr, ptr %prev_owner.i.i188, align 8
  %cmp.not.i.i189 = icmp eq ptr %27, %tsd
  br i1 %cmp.not.i.i189, label %malloc_mutex_lock.exit193, label %if.then.i.i190

if.then.i.i190:                                   ; preds = %if.end.i185
  store ptr %tsd, ptr %prev_owner.i.i188, align 8
  %n_owner_switches.i.i191 = getelementptr inbounds i8, ptr %7, i64 71160
  %28 = load i64, ptr %n_owner_switches.i.i191, align 8
  %inc2.i.i192 = add i64 %28, 1
  store i64 %inc2.i.i192, ptr %n_owner_switches.i.i191, align 8
  br label %malloc_mutex_lock.exit193

malloc_mutex_lock.exit193:                        ; preds = %if.end.i185, %if.then.i.i190
  tail call void @malloc_mutex_prof_data_reset(ptr noundef %tsd, ptr noundef nonnull %decay_muzzy) #14
  %locked.i194 = getelementptr inbounds i8, ptr %7, i64 71184
  store atomic i8 0, ptr %locked.i194 monotonic, align 1
  %call1.i196 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull %lock.i.i180) #14
  %tcache_ql_mtx = getelementptr inbounds i8, ptr %7, i64 10424
  %lock.i.i197 = getelementptr inbounds i8, ptr %7, i64 10496
  %call.i.i198 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull %lock.i.i197) #14
  %cmp.i.not.i199 = icmp eq i32 %call.i.i198, 0
  br i1 %cmp.i.not.i199, label %if.end.i202, label %if.then.i200

if.then.i200:                                     ; preds = %malloc_mutex_lock.exit193
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull %tcache_ql_mtx) #14
  %locked.i201 = getelementptr inbounds i8, ptr %7, i64 10488
  store atomic i8 1, ptr %locked.i201 monotonic, align 1
  br label %if.end.i202

if.end.i202:                                      ; preds = %if.then.i200, %malloc_mutex_lock.exit193
  %n_lock_ops.i.i203 = getelementptr inbounds i8, ptr %7, i64 10480
  %29 = load i64, ptr %n_lock_ops.i.i203, align 8
  %inc.i.i204 = add i64 %29, 1
  store i64 %inc.i.i204, ptr %n_lock_ops.i.i203, align 8
  %prev_owner.i.i205 = getelementptr inbounds i8, ptr %7, i64 10472
  %30 = load ptr, ptr %prev_owner.i.i205, align 8
  %cmp.not.i.i206 = icmp eq ptr %30, %tsd
  br i1 %cmp.not.i.i206, label %malloc_mutex_lock.exit210, label %if.then.i.i207

if.then.i.i207:                                   ; preds = %if.end.i202
  store ptr %tsd, ptr %prev_owner.i.i205, align 8
  %n_owner_switches.i.i208 = getelementptr inbounds i8, ptr %7, i64 10464
  %31 = load i64, ptr %n_owner_switches.i.i208, align 8
  %inc2.i.i209 = add i64 %31, 1
  store i64 %inc2.i.i209, ptr %n_owner_switches.i.i208, align 8
  br label %malloc_mutex_lock.exit210

malloc_mutex_lock.exit210:                        ; preds = %if.end.i202, %if.then.i.i207
  tail call void @malloc_mutex_prof_data_reset(ptr noundef %tsd, ptr noundef nonnull %tcache_ql_mtx) #14
  %locked.i211 = getelementptr inbounds i8, ptr %7, i64 10488
  store atomic i8 0, ptr %locked.i211 monotonic, align 1
  %call1.i213 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull %lock.i.i197) #14
  %base = getelementptr inbounds i8, ptr %7, i64 78952
  %32 = load ptr, ptr %base, align 8
  %lock.i.i214 = getelementptr inbounds i8, ptr %32, i64 104
  %call.i.i215 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull %lock.i.i214) #14
  %cmp.i.not.i216 = icmp eq i32 %call.i.i215, 0
  br i1 %cmp.i.not.i216, label %if.end.i219, label %if.then.i217

if.then.i217:                                     ; preds = %malloc_mutex_lock.exit210
  %mtx67 = getelementptr inbounds i8, ptr %32, i64 32
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull %mtx67) #14
  %locked.i218 = getelementptr inbounds i8, ptr %32, i64 96
  store atomic i8 1, ptr %locked.i218 monotonic, align 1
  br label %if.end.i219

if.end.i219:                                      ; preds = %if.then.i217, %malloc_mutex_lock.exit210
  %n_lock_ops.i.i220 = getelementptr inbounds i8, ptr %32, i64 88
  %33 = load i64, ptr %n_lock_ops.i.i220, align 8
  %inc.i.i221 = add i64 %33, 1
  store i64 %inc.i.i221, ptr %n_lock_ops.i.i220, align 8
  %prev_owner.i.i222 = getelementptr inbounds i8, ptr %32, i64 80
  %34 = load ptr, ptr %prev_owner.i.i222, align 8
  %cmp.not.i.i223 = icmp eq ptr %34, %tsd
  br i1 %cmp.not.i.i223, label %malloc_mutex_lock.exit227, label %if.then.i.i224

if.then.i.i224:                                   ; preds = %if.end.i219
  store ptr %tsd, ptr %prev_owner.i.i222, align 8
  %n_owner_switches.i.i225 = getelementptr inbounds i8, ptr %32, i64 72
  %35 = load i64, ptr %n_owner_switches.i.i225, align 8
  %inc2.i.i226 = add i64 %35, 1
  store i64 %inc2.i.i226, ptr %n_owner_switches.i.i225, align 8
  br label %malloc_mutex_lock.exit227

malloc_mutex_lock.exit227:                        ; preds = %if.end.i219, %if.then.i.i224
  %36 = load ptr, ptr %base, align 8
  %mtx69 = getelementptr inbounds i8, ptr %36, i64 32
  tail call void @malloc_mutex_prof_data_reset(ptr noundef %tsd, ptr noundef nonnull %mtx69) #14
  %37 = load ptr, ptr %base, align 8
  %locked.i228 = getelementptr inbounds i8, ptr %37, i64 96
  store atomic i8 0, ptr %locked.i228 monotonic, align 1
  %lock.i229 = getelementptr inbounds i8, ptr %37, i64 104
  %call1.i230 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull %lock.i229) #14
  br label %for.cond76.preheader

for.cond76.preheader:                             ; preds = %malloc_mutex_lock.exit227, %for.inc83
  %indvars.iv257 = phi i64 [ 0, %malloc_mutex_lock.exit227 ], [ %indvars.iv.next258, %for.inc83 ]
  %n_shards = getelementptr inbounds [36 x %struct.bin_info_s], ptr @bin_infos, i64 0, i64 %indvars.iv257, i32 3
  %38 = load i32, ptr %n_shards, align 4
  %cmp77250.not = icmp eq i32 %38, 0
  br i1 %cmp77250.not, label %for.inc83, label %for.body79.lr.ph

for.body79.lr.ph:                                 ; preds = %for.cond76.preheader
  %arrayidx.i232 = getelementptr inbounds [36 x i32], ptr @arena_bin_offsets, i64 0, i64 %indvars.iv257
  br label %for.body79

for.body79:                                       ; preds = %for.body79.lr.ph, %malloc_mutex_lock.exit246
  %indvars.iv = phi i64 [ 0, %for.body79.lr.ph ], [ %indvars.iv.next, %malloc_mutex_lock.exit246 ]
  %39 = load i32, ptr %arrayidx.i232, align 4
  %idx.ext.i = zext i32 %39 to i64
  %add.ptr.i = getelementptr inbounds i8, ptr %7, i64 %idx.ext.i
  %add.ptr2.i = getelementptr inbounds %struct.bin_s, ptr %add.ptr.i, i64 %indvars.iv
  %lock.i.i233 = getelementptr inbounds i8, ptr %add.ptr2.i, i64 72
  %call.i.i234 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull %lock.i.i233) #14
  %cmp.i.not.i235 = icmp eq i32 %call.i.i234, 0
  br i1 %cmp.i.not.i235, label %if.end.i238, label %if.then.i236

if.then.i236:                                     ; preds = %for.body79
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull %add.ptr2.i) #14
  %locked.i237 = getelementptr inbounds i8, ptr %add.ptr2.i, i64 64
  store atomic i8 1, ptr %locked.i237 monotonic, align 1
  br label %if.end.i238

if.end.i238:                                      ; preds = %if.then.i236, %for.body79
  %n_lock_ops.i.i239 = getelementptr inbounds i8, ptr %add.ptr2.i, i64 56
  %40 = load i64, ptr %n_lock_ops.i.i239, align 8
  %inc.i.i240 = add i64 %40, 1
  store i64 %inc.i.i240, ptr %n_lock_ops.i.i239, align 8
  %prev_owner.i.i241 = getelementptr inbounds i8, ptr %add.ptr2.i, i64 48
  %41 = load ptr, ptr %prev_owner.i.i241, align 8
  %cmp.not.i.i242 = icmp eq ptr %41, %tsd
  br i1 %cmp.not.i.i242, label %malloc_mutex_lock.exit246, label %if.then.i.i243

if.then.i.i243:                                   ; preds = %if.end.i238
  store ptr %tsd, ptr %prev_owner.i.i241, align 8
  %n_owner_switches.i.i244 = getelementptr inbounds i8, ptr %add.ptr2.i, i64 40
  %42 = load i64, ptr %n_owner_switches.i.i244, align 8
  %inc2.i.i245 = add i64 %42, 1
  store i64 %inc2.i.i245, ptr %n_owner_switches.i.i244, align 8
  br label %malloc_mutex_lock.exit246

malloc_mutex_lock.exit246:                        ; preds = %if.end.i238, %if.then.i.i243
  tail call void @malloc_mutex_prof_data_reset(ptr noundef %tsd, ptr noundef nonnull %add.ptr2.i) #14
  %locked.i247 = getelementptr inbounds i8, ptr %add.ptr2.i, i64 64
  store atomic i8 0, ptr %locked.i247 monotonic, align 1
  %call1.i249 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull %lock.i.i233) #14
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %43 = load i32, ptr %n_shards, align 4
  %44 = zext i32 %43 to i64
  %cmp77 = icmp ult i64 %indvars.iv.next, %44
  br i1 %cmp77, label %for.body79, label %for.inc83, !llvm.loop !25

for.inc83:                                        ; preds = %malloc_mutex_lock.exit246, %for.cond76.preheader
  %indvars.iv.next258 = add nuw nsw i64 %indvars.iv257, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next258, 36
  br i1 %exitcond.not, label %for.inc86, label %for.cond76.preheader, !llvm.loop !26

for.inc86:                                        ; preds = %for.inc83, %for.body
  %indvars.iv.next261 = add nuw nsw i64 %indvars.iv260, 1
  %exitcond263.not = icmp eq i64 %indvars.iv.next261, %wide.trip.count
  br i1 %exitcond263.not, label %for.end88, label %for.body, !llvm.loop !27

for.end88:                                        ; preds = %for.inc86, %malloc_mutex_lock.exit82
  ret i32 0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_background_thread_num_ops_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_lock_ops = getelementptr inbounds i8, ptr %3, i64 216
  %4 = load i64, ptr %n_lock_ops, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_background_thread_num_wait_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_wait_times = getelementptr inbounds i8, ptr %3, i64 176
  %4 = load i64, ptr %n_wait_times, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_background_thread_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_spin_acquired = getelementptr inbounds i8, ptr %3, i64 184
  %4 = load i64, ptr %n_spin_acquired, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_background_thread_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_owner_switches = getelementptr inbounds i8, ptr %3, i64 200
  %4 = load i64, ptr %n_owner_switches, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_background_thread_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %mutex_prof_data = getelementptr inbounds i8, ptr %3, i64 160
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %mutex_prof_data) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_background_thread_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_wait_time = getelementptr inbounds i8, ptr %3, i64 168
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_background_thread_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_n_thds = getelementptr inbounds i8, ptr %3, i64 192
  %4 = load i32, ptr %max_n_thds, align 8
  store i32 %4, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %4, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_max_per_bg_thd_num_ops_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_lock_ops = getelementptr inbounds i8, ptr %3, i64 280
  %4 = load i64, ptr %n_lock_ops, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_max_per_bg_thd_num_wait_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_wait_times = getelementptr inbounds i8, ptr %3, i64 240
  %4 = load i64, ptr %n_wait_times, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_max_per_bg_thd_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_spin_acquired = getelementptr inbounds i8, ptr %3, i64 248
  %4 = load i64, ptr %n_spin_acquired, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_max_per_bg_thd_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_owner_switches = getelementptr inbounds i8, ptr %3, i64 264
  %4 = load i64, ptr %n_owner_switches, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_max_per_bg_thd_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %arrayidx = getelementptr inbounds i8, ptr %3, i64 224
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_max_per_bg_thd_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_wait_time = getelementptr inbounds i8, ptr %3, i64 232
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_max_per_bg_thd_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_n_thds = getelementptr inbounds i8, ptr %3, i64 256
  %4 = load i32, ptr %max_n_thds, align 8
  store i32 %4, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %4, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_ctl_num_ops_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_lock_ops = getelementptr inbounds i8, ptr %3, i64 344
  %4 = load i64, ptr %n_lock_ops, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_ctl_num_wait_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_wait_times = getelementptr inbounds i8, ptr %3, i64 304
  %4 = load i64, ptr %n_wait_times, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_ctl_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_spin_acquired = getelementptr inbounds i8, ptr %3, i64 312
  %4 = load i64, ptr %n_spin_acquired, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_ctl_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_owner_switches = getelementptr inbounds i8, ptr %3, i64 328
  %4 = load i64, ptr %n_owner_switches, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_ctl_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %arrayidx = getelementptr inbounds i8, ptr %3, i64 288
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_ctl_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_wait_time = getelementptr inbounds i8, ptr %3, i64 296
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_ctl_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_n_thds = getelementptr inbounds i8, ptr %3, i64 320
  %4 = load i32, ptr %max_n_thds, align 8
  store i32 %4, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %4, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_num_ops_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_lock_ops = getelementptr inbounds i8, ptr %3, i64 408
  %4 = load i64, ptr %n_lock_ops, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_num_wait_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_wait_times = getelementptr inbounds i8, ptr %3, i64 368
  %4 = load i64, ptr %n_wait_times, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_spin_acquired = getelementptr inbounds i8, ptr %3, i64 376
  %4 = load i64, ptr %n_spin_acquired, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_owner_switches = getelementptr inbounds i8, ptr %3, i64 392
  %4 = load i64, ptr %n_owner_switches, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %arrayidx = getelementptr inbounds i8, ptr %3, i64 352
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_wait_time = getelementptr inbounds i8, ptr %3, i64 360
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_n_thds = getelementptr inbounds i8, ptr %3, i64 384
  %4 = load i32, ptr %max_n_thds, align 8
  store i32 %4, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %4, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_thds_data_num_ops_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_lock_ops = getelementptr inbounds i8, ptr %3, i64 472
  %4 = load i64, ptr %n_lock_ops, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_thds_data_num_wait_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_wait_times = getelementptr inbounds i8, ptr %3, i64 432
  %4 = load i64, ptr %n_wait_times, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_thds_data_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_spin_acquired = getelementptr inbounds i8, ptr %3, i64 440
  %4 = load i64, ptr %n_spin_acquired, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_thds_data_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_owner_switches = getelementptr inbounds i8, ptr %3, i64 456
  %4 = load i64, ptr %n_owner_switches, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_thds_data_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %arrayidx = getelementptr inbounds i8, ptr %3, i64 416
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_thds_data_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_wait_time = getelementptr inbounds i8, ptr %3, i64 424
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_thds_data_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_n_thds = getelementptr inbounds i8, ptr %3, i64 448
  %4 = load i32, ptr %max_n_thds, align 8
  store i32 %4, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %4, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_dump_num_ops_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_lock_ops = getelementptr inbounds i8, ptr %3, i64 536
  %4 = load i64, ptr %n_lock_ops, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_dump_num_wait_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_wait_times = getelementptr inbounds i8, ptr %3, i64 496
  %4 = load i64, ptr %n_wait_times, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_dump_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_spin_acquired = getelementptr inbounds i8, ptr %3, i64 504
  %4 = load i64, ptr %n_spin_acquired, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_dump_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_owner_switches = getelementptr inbounds i8, ptr %3, i64 520
  %4 = load i64, ptr %n_owner_switches, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_dump_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %arrayidx = getelementptr inbounds i8, ptr %3, i64 480
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_dump_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_wait_time = getelementptr inbounds i8, ptr %3, i64 488
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_dump_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_n_thds = getelementptr inbounds i8, ptr %3, i64 512
  %4 = load i32, ptr %max_n_thds, align 8
  store i32 %4, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %4, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_alloc_num_ops_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_lock_ops = getelementptr inbounds i8, ptr %3, i64 600
  %4 = load i64, ptr %n_lock_ops, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_alloc_num_wait_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_wait_times = getelementptr inbounds i8, ptr %3, i64 560
  %4 = load i64, ptr %n_wait_times, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_alloc_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_spin_acquired = getelementptr inbounds i8, ptr %3, i64 568
  %4 = load i64, ptr %n_spin_acquired, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_alloc_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_owner_switches = getelementptr inbounds i8, ptr %3, i64 584
  %4 = load i64, ptr %n_owner_switches, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_alloc_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %arrayidx = getelementptr inbounds i8, ptr %3, i64 544
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_alloc_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_wait_time = getelementptr inbounds i8, ptr %3, i64 552
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_alloc_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_n_thds = getelementptr inbounds i8, ptr %3, i64 576
  %4 = load i32, ptr %max_n_thds, align 8
  store i32 %4, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %4, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_dump_num_ops_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_lock_ops = getelementptr inbounds i8, ptr %3, i64 664
  %4 = load i64, ptr %n_lock_ops, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_dump_num_wait_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_wait_times = getelementptr inbounds i8, ptr %3, i64 624
  %4 = load i64, ptr %n_wait_times, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_dump_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_spin_acquired = getelementptr inbounds i8, ptr %3, i64 632
  %4 = load i64, ptr %n_spin_acquired, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_dump_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_owner_switches = getelementptr inbounds i8, ptr %3, i64 648
  %4 = load i64, ptr %n_owner_switches, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_dump_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %arrayidx = getelementptr inbounds i8, ptr %3, i64 608
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_dump_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_wait_time = getelementptr inbounds i8, ptr %3, i64 616
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_dump_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_n_thds = getelementptr inbounds i8, ptr %3, i64 640
  %4 = load i32, ptr %max_n_thds, align 8
  store i32 %4, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %4, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_stats_num_ops_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_lock_ops = getelementptr inbounds i8, ptr %3, i64 728
  %4 = load i64, ptr %n_lock_ops, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_stats_num_wait_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_wait_times = getelementptr inbounds i8, ptr %3, i64 688
  %4 = load i64, ptr %n_wait_times, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_stats_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_spin_acquired = getelementptr inbounds i8, ptr %3, i64 696
  %4 = load i64, ptr %n_spin_acquired, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_stats_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_owner_switches = getelementptr inbounds i8, ptr %3, i64 712
  %4 = load i64, ptr %n_owner_switches, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_stats_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %arrayidx = getelementptr inbounds i8, ptr %3, i64 672
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_stats_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_wait_time = getelementptr inbounds i8, ptr %3, i64 680
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_stats_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_n_thds = getelementptr inbounds i8, ptr %3, i64 704
  %4 = load i32, ptr %max_n_thds, align 8
  store i32 %4, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %4, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

declare void @malloc_mutex_prof_data_reset(ptr noundef, ptr noundef) local_unnamed_addr #1

; Function Attrs: nounwind uwtable
define internal ptr @stats_arenas_i_index(ptr noundef %tsdn, ptr nocapture readnone %mib, i64 %miblen, i64 noundef %i) #0 {
entry:
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsdn
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsdn, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  switch i64 %i, label %sw.default.i.i [
    i64 4096, label %ctl_arenas_i_verify.exit
    i64 4097, label %sw.bb2.i.i
  ]

sw.bb2.i.i:                                       ; preds = %malloc_mutex_lock.exit
  br label %ctl_arenas_i_verify.exit

sw.default.i.i:                                   ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_arenas, align 8
  %narenas.i.i = getelementptr inbounds i8, ptr %3, i64 8
  %4 = load i32, ptr %narenas.i.i, align 8
  %conv.i.i = zext i32 %4 to i64
  %cmp.i.i = icmp eq i64 %conv.i.i, %i
  br i1 %cmp.i.i, label %ctl_arenas_i_verify.exit, label %if.else.i.i

if.else.i.i:                                      ; preds = %sw.default.i.i
  %cmp9.not.i.i = icmp ule i64 %conv.i.i, %i
  %conv13.i.i = trunc i64 %i to i32
  %add.i.i = add i32 %conv13.i.i, 2
  %cmp.i = icmp eq i32 %add.i.i, -1
  %or.cond.i = or i1 %cmp.i, %cmp9.not.i.i
  br i1 %or.cond.i, label %ctl_arenas_i_verify.exit.thread, label %ctl_arenas_i_verify.exit

ctl_arenas_i_verify.exit:                         ; preds = %malloc_mutex_lock.exit, %sw.bb2.i.i, %sw.default.i.i, %if.else.i.i
  %a.0.i4.i = phi i32 [ 0, %sw.default.i.i ], [ 0, %malloc_mutex_lock.exit ], [ 1, %sw.bb2.i.i ], [ %add.i.i, %if.else.i.i ]
  %conv.i = zext i32 %a.0.i4.i to i64
  %5 = load ptr, ptr @ctl_arenas, align 8
  %arenas.i = getelementptr inbounds i8, ptr %5, i64 24
  %arrayidx.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i, i64 0, i64 %conv.i
  %6 = load ptr, ptr %arrayidx.i, align 8
  %initialized.i = getelementptr inbounds i8, ptr %6, i64 4
  %7 = load i8, ptr %initialized.i, align 4
  %.fr4 = freeze i8 %7
  %8 = and i8 %.fr4, 1
  %tobool.not.i = icmp eq i8 %8, 0
  br i1 %tobool.not.i, label %ctl_arenas_i_verify.exit.thread, label %9

ctl_arenas_i_verify.exit.thread:                  ; preds = %if.else.i.i, %ctl_arenas_i_verify.exit
  br label %9

9:                                                ; preds = %ctl_arenas_i_verify.exit, %ctl_arenas_i_verify.exit.thread
  %10 = phi ptr [ null, %ctl_arenas_i_verify.exit.thread ], [ @super_stats_arenas_i_node, %ctl_arenas_i_verify.exit ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret ptr %10
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_nthreads_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %nthreads = getelementptr inbounds i8, ptr %9, i64 24
  %10 = load i32, ptr %nthreads, align 8
  store i32 %10, ptr %oldval, align 4
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %11, 4
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i32 %10, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_uptime_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %uptime = getelementptr inbounds i8, ptr %10, i64 10376
  %call4 = tail call i64 @nstime_ns(ptr noundef nonnull %uptime) #14
  store i64 %call4, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %11, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %call4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_dss_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca ptr, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %dss = getelementptr inbounds i8, ptr %9, i64 32
  %10 = load ptr, ptr %dss, align 8
  store ptr %10, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %11, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store ptr %10, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_dirty_decay_ms_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %dirty_decay_ms = getelementptr inbounds i8, ptr %9, i64 40
  %10 = load i64, ptr %dirty_decay_ms, align 8
  store i64 %10, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %11, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %10, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_muzzy_decay_ms_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %muzzy_decay_ms = getelementptr inbounds i8, ptr %9, i64 48
  %10 = load i64, ptr %muzzy_decay_ms, align 8
  store i64 %10, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %11, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %10, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_pactive_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %pactive = getelementptr inbounds i8, ptr %9, i64 56
  %10 = load i64, ptr %pactive, align 8
  store i64 %10, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %11, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %10, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_pdirty_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %pdirty = getelementptr inbounds i8, ptr %9, i64 64
  %10 = load i64, ptr %pdirty, align 8
  store i64 %10, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %11, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %10, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_pmuzzy_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %pmuzzy = getelementptr inbounds i8, ptr %9, i64 72
  %10 = load i64, ptr %pmuzzy, align 8
  store i64 %10, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %11, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %10, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mapped_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %mapped = getelementptr inbounds i8, ptr %10, i64 40
  %11 = load i64, ptr %mapped, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_retained_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %retained = getelementptr inbounds i8, ptr %10, i64 160
  %11 = load i64, ptr %retained, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_extent_avail_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %pa_shard_stats = getelementptr inbounds i8, ptr %10, i64 104
  %11 = load i64, ptr %pa_shard_stats, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_dirty_npurge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %pac_stats = getelementptr inbounds i8, ptr %10, i64 112
  %11 = load atomic i64, ptr %pac_stats monotonic, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_dirty_nmadvise_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %nmadvise = getelementptr inbounds i8, ptr %10, i64 120
  %11 = load atomic i64, ptr %nmadvise monotonic, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_dirty_purged_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %purged = getelementptr inbounds i8, ptr %10, i64 128
  %11 = load atomic i64, ptr %purged monotonic, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_muzzy_npurge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %decay_muzzy = getelementptr inbounds i8, ptr %10, i64 136
  %11 = load atomic i64, ptr %decay_muzzy monotonic, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_muzzy_nmadvise_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %nmadvise = getelementptr inbounds i8, ptr %10, i64 144
  %11 = load atomic i64, ptr %nmadvise monotonic, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_muzzy_purged_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %purged = getelementptr inbounds i8, ptr %10, i64 152
  %11 = load atomic i64, ptr %purged monotonic, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_base_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %11 = load i64, ptr %10, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_internal_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %internal = getelementptr inbounds i8, ptr %10, i64 48
  %11 = load atomic i64, ptr %internal monotonic, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_metadata_edata_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %metadata_edata = getelementptr inbounds i8, ptr %10, i64 8
  %11 = load i64, ptr %metadata_edata, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_metadata_rtree_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %metadata_rtree = getelementptr inbounds i8, ptr %10, i64 16
  %11 = load i64, ptr %metadata_rtree, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_metadata_thp_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %metadata_thp = getelementptr inbounds i8, ptr %10, i64 32
  %11 = load i64, ptr %metadata_thp, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_tcache_bytes_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %tcache_bytes = getelementptr inbounds i8, ptr %10, i64 184
  %11 = load i64, ptr %tcache_bytes, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_tcache_stashed_bytes_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %tcache_stashed_bytes = getelementptr inbounds i8, ptr %10, i64 192
  %11 = load i64, ptr %tcache_stashed_bytes, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_resident_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %resident = getelementptr inbounds i8, ptr %10, i64 24
  %11 = load i64, ptr %resident, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_abandoned_vm_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %abandoned_vm = getelementptr inbounds i8, ptr %10, i64 176
  %11 = load atomic i64, ptr %abandoned_vm monotonic, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_sec_bytes_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %secstats = getelementptr inbounds i8, ptr %10, i64 37776
  %11 = load i64, ptr %secstats, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %12, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_small_allocated_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %allocated_small = getelementptr inbounds i8, ptr %10, i64 10384
  %11 = load i64, ptr %allocated_small, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %12, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_small_nmalloc_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %nmalloc_small = getelementptr inbounds i8, ptr %10, i64 10392
  %11 = load i64, ptr %nmalloc_small, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %12, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_small_ndalloc_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %ndalloc_small = getelementptr inbounds i8, ptr %10, i64 10400
  %11 = load i64, ptr %ndalloc_small, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %12, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_small_nrequests_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %nrequests_small = getelementptr inbounds i8, ptr %10, i64 10408
  %11 = load i64, ptr %nrequests_small, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %12, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_small_nfills_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %nfills_small = getelementptr inbounds i8, ptr %10, i64 10416
  %11 = load i64, ptr %nfills_small, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %12, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_small_nflushes_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %nflushes_small = getelementptr inbounds i8, ptr %10, i64 10424
  %11 = load i64, ptr %nflushes_small, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %12, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_large_allocated_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %allocated_large = getelementptr inbounds i8, ptr %10, i64 56
  %11 = load i64, ptr %allocated_large, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_large_nmalloc_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %nmalloc_large = getelementptr inbounds i8, ptr %10, i64 64
  %11 = load i64, ptr %nmalloc_large, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_large_ndalloc_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %ndalloc_large = getelementptr inbounds i8, ptr %10, i64 72
  %11 = load i64, ptr %ndalloc_large, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_large_nrequests_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %nrequests_large = getelementptr inbounds i8, ptr %10, i64 96
  %11 = load i64, ptr %nrequests_large, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_large_nfills_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %nmalloc_large = getelementptr inbounds i8, ptr %10, i64 64
  %11 = load i64, ptr %nmalloc_large, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_large_nflushes_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %nflushes_large = getelementptr inbounds i8, ptr %10, i64 88
  %11 = load i64, ptr %nflushes_large, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal ptr @stats_arenas_i_bins_j_index(ptr nocapture readnone %tsdn, ptr nocapture readnone %mib, i64 %miblen, i64 noundef %j) #2 {
entry:
  %cmp = icmp ugt i64 %j, 36
  %.super_stats_arenas_i_bins_j_node = select i1 %cmp, ptr null, ptr @super_stats_arenas_i_bins_j_node
  ret ptr %.super_stats_arenas_i_bins_j_node
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_nmalloc_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %bstats = getelementptr inbounds i8, ptr %10, i64 10432
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %arrayidx4 = getelementptr inbounds [36 x %struct.bin_stats_data_s], ptr %bstats, i64 0, i64 %11
  %12 = load i64, ptr %arrayidx4, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_ndalloc_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %bstats = getelementptr inbounds i8, ptr %10, i64 10432
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %ndalloc = getelementptr inbounds [36 x %struct.bin_stats_data_s], ptr %bstats, i64 0, i64 %11, i32 0, i32 1
  %12 = load i64, ptr %ndalloc, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_nrequests_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %bstats = getelementptr inbounds i8, ptr %10, i64 10432
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %nrequests = getelementptr inbounds [36 x %struct.bin_stats_data_s], ptr %bstats, i64 0, i64 %11, i32 0, i32 2
  %12 = load i64, ptr %nrequests, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_curregs_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %bstats = getelementptr inbounds i8, ptr %10, i64 10432
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %curregs = getelementptr inbounds [36 x %struct.bin_stats_data_s], ptr %bstats, i64 0, i64 %11, i32 0, i32 3
  %12 = load i64, ptr %curregs, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_nfills_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %bstats = getelementptr inbounds i8, ptr %10, i64 10432
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %nfills = getelementptr inbounds [36 x %struct.bin_stats_data_s], ptr %bstats, i64 0, i64 %11, i32 0, i32 4
  %12 = load i64, ptr %nfills, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_nflushes_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %bstats = getelementptr inbounds i8, ptr %10, i64 10432
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %nflushes = getelementptr inbounds [36 x %struct.bin_stats_data_s], ptr %bstats, i64 0, i64 %11, i32 0, i32 5
  %12 = load i64, ptr %nflushes, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_nslabs_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %bstats = getelementptr inbounds i8, ptr %10, i64 10432
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %nslabs = getelementptr inbounds [36 x %struct.bin_stats_data_s], ptr %bstats, i64 0, i64 %11, i32 0, i32 6
  %12 = load i64, ptr %nslabs, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_nreslabs_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %bstats = getelementptr inbounds i8, ptr %10, i64 10432
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %reslabs = getelementptr inbounds [36 x %struct.bin_stats_data_s], ptr %bstats, i64 0, i64 %11, i32 0, i32 7
  %12 = load i64, ptr %reslabs, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_curslabs_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %bstats = getelementptr inbounds i8, ptr %10, i64 10432
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %curslabs = getelementptr inbounds [36 x %struct.bin_stats_data_s], ptr %bstats, i64 0, i64 %11, i32 0, i32 8
  %12 = load i64, ptr %curslabs, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_nonfull_slabs_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %bstats = getelementptr inbounds i8, ptr %10, i64 10432
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %nonfull_slabs = getelementptr inbounds [36 x %struct.bin_stats_data_s], ptr %bstats, i64 0, i64 %11, i32 0, i32 9
  %12 = load i64, ptr %nonfull_slabs, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_mutex_num_ops_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %bstats = getelementptr inbounds i8, ptr %10, i64 10432
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %n_lock_ops = getelementptr inbounds [36 x %struct.bin_stats_data_s], ptr %bstats, i64 0, i64 %11, i32 1, i32 8
  %12 = load i64, ptr %n_lock_ops, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_mutex_num_wait_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %bstats = getelementptr inbounds i8, ptr %10, i64 10432
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %n_wait_times = getelementptr inbounds [36 x %struct.bin_stats_data_s], ptr %bstats, i64 0, i64 %11, i32 1, i32 2
  %12 = load i64, ptr %n_wait_times, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_mutex_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %bstats = getelementptr inbounds i8, ptr %10, i64 10432
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %n_spin_acquired = getelementptr inbounds [36 x %struct.bin_stats_data_s], ptr %bstats, i64 0, i64 %11, i32 1, i32 3
  %12 = load i64, ptr %n_spin_acquired, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_mutex_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %bstats = getelementptr inbounds i8, ptr %10, i64 10432
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %n_owner_switches = getelementptr inbounds [36 x %struct.bin_stats_data_s], ptr %bstats, i64 0, i64 %11, i32 1, i32 6
  %12 = load i64, ptr %n_owner_switches, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_mutex_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %bstats = getelementptr inbounds i8, ptr %10, i64 10432
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %mutex_data = getelementptr inbounds [36 x %struct.bin_stats_data_s], ptr %bstats, i64 0, i64 %11, i32 1
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %mutex_data) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %12, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_mutex_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %bstats = getelementptr inbounds i8, ptr %10, i64 10432
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %max_wait_time = getelementptr inbounds [36 x %struct.bin_stats_data_s], ptr %bstats, i64 0, i64 %11, i32 1, i32 1
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %12, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_mutex_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %bstats = getelementptr inbounds i8, ptr %10, i64 10432
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %max_n_thds = getelementptr inbounds [36 x %struct.bin_stats_data_s], ptr %bstats, i64 0, i64 %11, i32 1, i32 4
  %12 = load i32, ptr %max_n_thds, align 8
  store i32 %12, ptr %oldval, align 4
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 4
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i32 %12, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal ptr @stats_arenas_i_lextents_j_index(ptr nocapture readnone %tsdn, ptr nocapture readnone %mib, i64 %miblen, i64 noundef %j) #2 {
entry:
  %cmp = icmp ugt i64 %j, 196
  %.super_stats_arenas_i_lextents_j_node = select i1 %cmp, ptr null, ptr @super_stats_arenas_i_lextents_j_node
  ret ptr %.super_stats_arenas_i_lextents_j_node
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_lextents_j_nmalloc_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %lstats = getelementptr inbounds i8, ptr %10, i64 15616
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %arrayidx4 = getelementptr inbounds [196 x %struct.arena_stats_large_s], ptr %lstats, i64 0, i64 %11
  %12 = load atomic i64, ptr %arrayidx4 monotonic, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %13, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_lextents_j_ndalloc_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %lstats = getelementptr inbounds i8, ptr %10, i64 15616
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %ndalloc = getelementptr inbounds [196 x %struct.arena_stats_large_s], ptr %lstats, i64 0, i64 %11, i32 1
  %12 = load atomic i64, ptr %ndalloc monotonic, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %13, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_lextents_j_nrequests_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %lstats = getelementptr inbounds i8, ptr %10, i64 15616
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %nrequests = getelementptr inbounds [196 x %struct.arena_stats_large_s], ptr %lstats, i64 0, i64 %11, i32 2
  %12 = load atomic i64, ptr %nrequests monotonic, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %13, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_lextents_j_curlextents_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %lstats = getelementptr inbounds i8, ptr %10, i64 15616
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %curlextents = getelementptr inbounds [196 x %struct.arena_stats_large_s], ptr %lstats, i64 0, i64 %11, i32 5
  %12 = load i64, ptr %curlextents, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal ptr @stats_arenas_i_extents_j_index(ptr nocapture readnone %tsdn, ptr nocapture readnone %mib, i64 %miblen, i64 noundef %j) #2 {
entry:
  %cmp = icmp ugt i64 %j, 198
  %.super_stats_arenas_i_extents_j_node = select i1 %cmp, ptr null, ptr @super_stats_arenas_i_extents_j_node
  ret ptr %.super_stats_arenas_i_extents_j_node
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_extents_j_ndirty_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %estats = getelementptr inbounds i8, ptr %10, i64 25024
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %arrayidx4 = getelementptr inbounds [199 x %struct.pac_estats_s], ptr %estats, i64 0, i64 %11
  %12 = load i64, ptr %arrayidx4, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_extents_j_nmuzzy_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %estats = getelementptr inbounds i8, ptr %10, i64 25024
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %nmuzzy = getelementptr inbounds [199 x %struct.pac_estats_s], ptr %estats, i64 0, i64 %11, i32 2
  %12 = load i64, ptr %nmuzzy, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_extents_j_nretained_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %estats = getelementptr inbounds i8, ptr %10, i64 25024
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %nretained = getelementptr inbounds [199 x %struct.pac_estats_s], ptr %estats, i64 0, i64 %11, i32 4
  %12 = load i64, ptr %nretained, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_extents_j_dirty_bytes_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %estats = getelementptr inbounds i8, ptr %10, i64 25024
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %dirty_bytes = getelementptr inbounds [199 x %struct.pac_estats_s], ptr %estats, i64 0, i64 %11, i32 1
  %12 = load i64, ptr %dirty_bytes, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_extents_j_muzzy_bytes_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %estats = getelementptr inbounds i8, ptr %10, i64 25024
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %muzzy_bytes = getelementptr inbounds [199 x %struct.pac_estats_s], ptr %estats, i64 0, i64 %11, i32 3
  %12 = load i64, ptr %muzzy_bytes, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_extents_j_retained_bytes_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %estats = getelementptr inbounds i8, ptr %10, i64 25024
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 32
  %11 = load i64, ptr %arrayidx3, align 8
  %retained_bytes = getelementptr inbounds [199 x %struct.pac_estats_s], ptr %estats, i64 0, i64 %11, i32 5
  %12 = load i64, ptr %retained_bytes, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_large_num_ops_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_lock_ops = getelementptr inbounds i8, ptr %10, i64 256
  %11 = load i64, ptr %n_lock_ops, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_large_num_wait_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_wait_times = getelementptr inbounds i8, ptr %10, i64 216
  %11 = load i64, ptr %n_wait_times, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_large_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_spin_acquired = getelementptr inbounds i8, ptr %10, i64 224
  %11 = load i64, ptr %n_spin_acquired, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_large_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_owner_switches = getelementptr inbounds i8, ptr %10, i64 240
  %11 = load i64, ptr %n_owner_switches, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_large_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %mutex_prof_data = getelementptr inbounds i8, ptr %10, i64 200
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %mutex_prof_data) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_large_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %max_wait_time = getelementptr inbounds i8, ptr %10, i64 208
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_large_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %max_n_thds = getelementptr inbounds i8, ptr %10, i64 232
  %11 = load i32, ptr %max_n_thds, align 8
  store i32 %11, ptr %oldval, align 4
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 4
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i32 %11, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extent_avail_num_ops_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_lock_ops = getelementptr inbounds i8, ptr %10, i64 320
  %11 = load i64, ptr %n_lock_ops, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extent_avail_num_wait_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_wait_times = getelementptr inbounds i8, ptr %10, i64 280
  %11 = load i64, ptr %n_wait_times, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extent_avail_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_spin_acquired = getelementptr inbounds i8, ptr %10, i64 288
  %11 = load i64, ptr %n_spin_acquired, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extent_avail_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_owner_switches = getelementptr inbounds i8, ptr %10, i64 304
  %11 = load i64, ptr %n_owner_switches, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extent_avail_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %arrayidx4 = getelementptr inbounds i8, ptr %10, i64 264
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx4) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extent_avail_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %max_wait_time = getelementptr inbounds i8, ptr %10, i64 272
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extent_avail_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %max_n_thds = getelementptr inbounds i8, ptr %10, i64 296
  %11 = load i32, ptr %max_n_thds, align 8
  store i32 %11, ptr %oldval, align 4
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 4
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i32 %11, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_dirty_num_ops_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_lock_ops = getelementptr inbounds i8, ptr %10, i64 384
  %11 = load i64, ptr %n_lock_ops, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_dirty_num_wait_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_wait_times = getelementptr inbounds i8, ptr %10, i64 344
  %11 = load i64, ptr %n_wait_times, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_dirty_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_spin_acquired = getelementptr inbounds i8, ptr %10, i64 352
  %11 = load i64, ptr %n_spin_acquired, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_dirty_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_owner_switches = getelementptr inbounds i8, ptr %10, i64 368
  %11 = load i64, ptr %n_owner_switches, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_dirty_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %arrayidx4 = getelementptr inbounds i8, ptr %10, i64 328
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx4) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_dirty_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %max_wait_time = getelementptr inbounds i8, ptr %10, i64 336
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_dirty_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %max_n_thds = getelementptr inbounds i8, ptr %10, i64 360
  %11 = load i32, ptr %max_n_thds, align 8
  store i32 %11, ptr %oldval, align 4
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 4
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i32 %11, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_muzzy_num_ops_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_lock_ops = getelementptr inbounds i8, ptr %10, i64 448
  %11 = load i64, ptr %n_lock_ops, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_muzzy_num_wait_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_wait_times = getelementptr inbounds i8, ptr %10, i64 408
  %11 = load i64, ptr %n_wait_times, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_muzzy_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_spin_acquired = getelementptr inbounds i8, ptr %10, i64 416
  %11 = load i64, ptr %n_spin_acquired, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_muzzy_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_owner_switches = getelementptr inbounds i8, ptr %10, i64 432
  %11 = load i64, ptr %n_owner_switches, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_muzzy_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %arrayidx4 = getelementptr inbounds i8, ptr %10, i64 392
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx4) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_muzzy_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %max_wait_time = getelementptr inbounds i8, ptr %10, i64 400
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_muzzy_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %max_n_thds = getelementptr inbounds i8, ptr %10, i64 424
  %11 = load i32, ptr %max_n_thds, align 8
  store i32 %11, ptr %oldval, align 4
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 4
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i32 %11, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_retained_num_ops_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_lock_ops = getelementptr inbounds i8, ptr %10, i64 512
  %11 = load i64, ptr %n_lock_ops, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_retained_num_wait_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_wait_times = getelementptr inbounds i8, ptr %10, i64 472
  %11 = load i64, ptr %n_wait_times, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_retained_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_spin_acquired = getelementptr inbounds i8, ptr %10, i64 480
  %11 = load i64, ptr %n_spin_acquired, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_retained_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_owner_switches = getelementptr inbounds i8, ptr %10, i64 496
  %11 = load i64, ptr %n_owner_switches, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_retained_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %arrayidx4 = getelementptr inbounds i8, ptr %10, i64 456
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx4) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_retained_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %max_wait_time = getelementptr inbounds i8, ptr %10, i64 464
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_retained_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %max_n_thds = getelementptr inbounds i8, ptr %10, i64 488
  %11 = load i32, ptr %max_n_thds, align 8
  store i32 %11, ptr %oldval, align 4
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 4
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i32 %11, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_dirty_num_ops_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_lock_ops = getelementptr inbounds i8, ptr %10, i64 576
  %11 = load i64, ptr %n_lock_ops, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_dirty_num_wait_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_wait_times = getelementptr inbounds i8, ptr %10, i64 536
  %11 = load i64, ptr %n_wait_times, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_dirty_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_spin_acquired = getelementptr inbounds i8, ptr %10, i64 544
  %11 = load i64, ptr %n_spin_acquired, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_dirty_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_owner_switches = getelementptr inbounds i8, ptr %10, i64 560
  %11 = load i64, ptr %n_owner_switches, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_dirty_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %arrayidx4 = getelementptr inbounds i8, ptr %10, i64 520
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx4) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_dirty_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %max_wait_time = getelementptr inbounds i8, ptr %10, i64 528
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_dirty_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %max_n_thds = getelementptr inbounds i8, ptr %10, i64 552
  %11 = load i32, ptr %max_n_thds, align 8
  store i32 %11, ptr %oldval, align 4
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 4
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i32 %11, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_muzzy_num_ops_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_lock_ops = getelementptr inbounds i8, ptr %10, i64 640
  %11 = load i64, ptr %n_lock_ops, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_muzzy_num_wait_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_wait_times = getelementptr inbounds i8, ptr %10, i64 600
  %11 = load i64, ptr %n_wait_times, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_muzzy_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_spin_acquired = getelementptr inbounds i8, ptr %10, i64 608
  %11 = load i64, ptr %n_spin_acquired, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_muzzy_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_owner_switches = getelementptr inbounds i8, ptr %10, i64 624
  %11 = load i64, ptr %n_owner_switches, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_muzzy_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %arrayidx4 = getelementptr inbounds i8, ptr %10, i64 584
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx4) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_muzzy_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %max_wait_time = getelementptr inbounds i8, ptr %10, i64 592
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_muzzy_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %max_n_thds = getelementptr inbounds i8, ptr %10, i64 616
  %11 = load i32, ptr %max_n_thds, align 8
  store i32 %11, ptr %oldval, align 4
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 4
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i32 %11, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_base_num_ops_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_lock_ops = getelementptr inbounds i8, ptr %10, i64 704
  %11 = load i64, ptr %n_lock_ops, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_base_num_wait_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_wait_times = getelementptr inbounds i8, ptr %10, i64 664
  %11 = load i64, ptr %n_wait_times, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_base_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_spin_acquired = getelementptr inbounds i8, ptr %10, i64 672
  %11 = load i64, ptr %n_spin_acquired, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_base_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_owner_switches = getelementptr inbounds i8, ptr %10, i64 688
  %11 = load i64, ptr %n_owner_switches, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_base_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %arrayidx4 = getelementptr inbounds i8, ptr %10, i64 648
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx4) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_base_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %max_wait_time = getelementptr inbounds i8, ptr %10, i64 656
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_base_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %max_n_thds = getelementptr inbounds i8, ptr %10, i64 680
  %11 = load i32, ptr %max_n_thds, align 8
  store i32 %11, ptr %oldval, align 4
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 4
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i32 %11, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_tcache_list_num_ops_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_lock_ops = getelementptr inbounds i8, ptr %10, i64 768
  %11 = load i64, ptr %n_lock_ops, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_tcache_list_num_wait_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_wait_times = getelementptr inbounds i8, ptr %10, i64 728
  %11 = load i64, ptr %n_wait_times, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_tcache_list_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_spin_acquired = getelementptr inbounds i8, ptr %10, i64 736
  %11 = load i64, ptr %n_spin_acquired, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_tcache_list_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_owner_switches = getelementptr inbounds i8, ptr %10, i64 752
  %11 = load i64, ptr %n_owner_switches, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_tcache_list_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %arrayidx4 = getelementptr inbounds i8, ptr %10, i64 712
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx4) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_tcache_list_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %max_wait_time = getelementptr inbounds i8, ptr %10, i64 720
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_tcache_list_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %max_n_thds = getelementptr inbounds i8, ptr %10, i64 744
  %11 = load i32, ptr %max_n_thds, align 8
  store i32 %11, ptr %oldval, align 4
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 4
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i32 %11, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_num_ops_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_lock_ops = getelementptr inbounds i8, ptr %10, i64 832
  %11 = load i64, ptr %n_lock_ops, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_num_wait_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_wait_times = getelementptr inbounds i8, ptr %10, i64 792
  %11 = load i64, ptr %n_wait_times, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_spin_acquired = getelementptr inbounds i8, ptr %10, i64 800
  %11 = load i64, ptr %n_spin_acquired, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_owner_switches = getelementptr inbounds i8, ptr %10, i64 816
  %11 = load i64, ptr %n_owner_switches, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %arrayidx4 = getelementptr inbounds i8, ptr %10, i64 776
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx4) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %max_wait_time = getelementptr inbounds i8, ptr %10, i64 784
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %max_n_thds = getelementptr inbounds i8, ptr %10, i64 808
  %11 = load i32, ptr %max_n_thds, align 8
  store i32 %11, ptr %oldval, align 4
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 4
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i32 %11, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_grow_num_ops_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_lock_ops = getelementptr inbounds i8, ptr %10, i64 896
  %11 = load i64, ptr %n_lock_ops, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_grow_num_wait_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_wait_times = getelementptr inbounds i8, ptr %10, i64 856
  %11 = load i64, ptr %n_wait_times, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_grow_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_spin_acquired = getelementptr inbounds i8, ptr %10, i64 864
  %11 = load i64, ptr %n_spin_acquired, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_grow_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_owner_switches = getelementptr inbounds i8, ptr %10, i64 880
  %11 = load i64, ptr %n_owner_switches, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_grow_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %arrayidx4 = getelementptr inbounds i8, ptr %10, i64 840
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx4) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_grow_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %max_wait_time = getelementptr inbounds i8, ptr %10, i64 848
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_grow_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %max_n_thds = getelementptr inbounds i8, ptr %10, i64 872
  %11 = load i32, ptr %max_n_thds, align 8
  store i32 %11, ptr %oldval, align 4
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 4
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i32 %11, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_sec_num_ops_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_lock_ops = getelementptr inbounds i8, ptr %10, i64 960
  %11 = load i64, ptr %n_lock_ops, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_sec_num_wait_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_wait_times = getelementptr inbounds i8, ptr %10, i64 920
  %11 = load i64, ptr %n_wait_times, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_sec_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_spin_acquired = getelementptr inbounds i8, ptr %10, i64 928
  %11 = load i64, ptr %n_spin_acquired, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_sec_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %n_owner_switches = getelementptr inbounds i8, ptr %10, i64 944
  %11 = load i64, ptr %n_owner_switches, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_sec_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %arrayidx4 = getelementptr inbounds i8, ptr %10, i64 904
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx4) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_sec_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %max_wait_time = getelementptr inbounds i8, ptr %10, i64 912
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_sec_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %max_n_thds = getelementptr inbounds i8, ptr %10, i64 936
  %11 = load i32, ptr %max_n_thds, align 8
  store i32 %11, ptr %oldval, align 4
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 4
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i32 %11, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_npurge_passes_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %nonderived_stats = getelementptr inbounds i8, ptr %10, i64 37744
  %11 = load i64, ptr %nonderived_stats, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %12, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_npurges_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %npurges = getelementptr inbounds i8, ptr %10, i64 37752
  %11 = load i64, ptr %npurges, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %12, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_nhugifies_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %nhugifies = getelementptr inbounds i8, ptr %10, i64 37760
  %11 = load i64, ptr %nhugifies, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %12, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_ndehugifies_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %ndehugifies = getelementptr inbounds i8, ptr %10, i64 37768
  %11 = load i64, ptr %ndehugifies, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %12, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_full_slabs_npageslabs_nonhuge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %full_slabs = getelementptr inbounds i8, ptr %10, i64 37648
  %11 = load i64, ptr %full_slabs, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_full_slabs_npageslabs_huge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i8, ptr %10, i64 37672
  %11 = load i64, ptr %arrayidx3, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_full_slabs_nactive_nonhuge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %nactive = getelementptr inbounds i8, ptr %10, i64 37656
  %11 = load i64, ptr %nactive, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_full_slabs_nactive_huge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %nactive = getelementptr inbounds i8, ptr %10, i64 37680
  %11 = load i64, ptr %nactive, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_full_slabs_ndirty_nonhuge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %ndirty = getelementptr inbounds i8, ptr %10, i64 37664
  %11 = load i64, ptr %ndirty, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_full_slabs_ndirty_huge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %ndirty = getelementptr inbounds i8, ptr %10, i64 37688
  %11 = load i64, ptr %ndirty, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_empty_slabs_npageslabs_nonhuge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %empty_slabs = getelementptr inbounds i8, ptr %10, i64 37696
  %11 = load i64, ptr %empty_slabs, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_empty_slabs_npageslabs_huge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i8, ptr %10, i64 37720
  %11 = load i64, ptr %arrayidx3, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_empty_slabs_nactive_nonhuge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %nactive = getelementptr inbounds i8, ptr %10, i64 37704
  %11 = load i64, ptr %nactive, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_empty_slabs_nactive_huge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %nactive = getelementptr inbounds i8, ptr %10, i64 37728
  %11 = load i64, ptr %nactive, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_empty_slabs_ndirty_nonhuge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %ndirty = getelementptr inbounds i8, ptr %10, i64 37712
  %11 = load i64, ptr %ndirty, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_empty_slabs_ndirty_huge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %ndirty = getelementptr inbounds i8, ptr %10, i64 37736
  %11 = load i64, ptr %ndirty, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal ptr @stats_arenas_i_hpa_shard_nonfull_slabs_j_index(ptr nocapture readnone %tsdn, ptr nocapture readnone %mib, i64 %miblen, i64 noundef %j) #2 {
entry:
  %cmp = icmp ugt i64 %j, 63
  %.super_stats_arenas_i_hpa_shard_nonfull_slabs_j_node = select i1 %cmp, ptr null, ptr @super_stats_arenas_i_hpa_shard_nonfull_slabs_j_node
  ret ptr %.super_stats_arenas_i_hpa_shard_nonfull_slabs_j_node
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_nonfull_slabs_j_npageslabs_nonhuge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %hpastats = getelementptr inbounds i8, ptr %10, i64 34576
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 40
  %11 = load i64, ptr %arrayidx3, align 8
  %arrayidx4 = getelementptr inbounds [64 x [2 x %struct.psset_bin_stats_s]], ptr %hpastats, i64 0, i64 %11
  %12 = load i64, ptr %arrayidx4, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %13, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_nonfull_slabs_j_npageslabs_huge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %hpastats = getelementptr inbounds i8, ptr %10, i64 34576
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 40
  %11 = load i64, ptr %arrayidx3, align 8
  %arrayidx5 = getelementptr inbounds [64 x [2 x %struct.psset_bin_stats_s]], ptr %hpastats, i64 0, i64 %11, i64 1
  %12 = load i64, ptr %arrayidx5, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %13, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_nonfull_slabs_j_nactive_nonhuge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %hpastats = getelementptr inbounds i8, ptr %10, i64 34576
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 40
  %11 = load i64, ptr %arrayidx3, align 8
  %nactive = getelementptr inbounds [64 x [2 x %struct.psset_bin_stats_s]], ptr %hpastats, i64 0, i64 %11, i64 0, i32 1
  %12 = load i64, ptr %nactive, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %13, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_nonfull_slabs_j_nactive_huge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %hpastats = getelementptr inbounds i8, ptr %10, i64 34576
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 40
  %11 = load i64, ptr %arrayidx3, align 8
  %nactive = getelementptr inbounds [64 x [2 x %struct.psset_bin_stats_s]], ptr %hpastats, i64 0, i64 %11, i64 1, i32 1
  %12 = load i64, ptr %nactive, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %13, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_nonfull_slabs_j_ndirty_nonhuge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %hpastats = getelementptr inbounds i8, ptr %10, i64 34576
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 40
  %11 = load i64, ptr %arrayidx3, align 8
  %ndirty = getelementptr inbounds [64 x [2 x %struct.psset_bin_stats_s]], ptr %hpastats, i64 0, i64 %11, i64 0, i32 2
  %12 = load i64, ptr %ndirty, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %13, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_nonfull_slabs_j_ndirty_huge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds i8, ptr %4, i64 824
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds i8, ptr %6, i64 8
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arenas.i.i = getelementptr inbounds i8, ptr %6, i64 24
  %arrayidx.i.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i.i, i64 0, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds i8, ptr %9, i64 80
  %10 = load ptr, ptr %astats, align 8
  %hpastats = getelementptr inbounds i8, ptr %10, i64 34576
  %arrayidx3 = getelementptr inbounds i8, ptr %mib, i64 40
  %11 = load i64, ptr %arrayidx3, align 8
  %ndirty = getelementptr inbounds [64 x [2 x %struct.psset_bin_stats_s]], ptr %hpastats, i64 0, i64 %11, i64 1, i32 2
  %12 = load i64, ptr %ndirty, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %13, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @experimental_arenas_create_ext_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %arena_ind = alloca i32, align 4
  %config = alloca %struct.arena_config_s, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) %config, ptr noundef nonnull align 8 dereferenceable(16) @arena_config_default, i64 16, i1 false)
  %cmp = icmp eq ptr %oldp, null
  %cmp1 = icmp eq ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %if.then, label %lor.lhs.false2

lor.lhs.false2:                                   ; preds = %malloc_mutex_lock.exit
  %3 = load i64, ptr %oldlenp, align 8
  %cmp3.not = icmp eq i64 %3, 4
  br i1 %cmp3.not, label %do.body7, label %if.then5

if.then:                                          ; preds = %malloc_mutex_lock.exit
  br i1 %cmp1, label %label_return, label %if.then5

if.then5:                                         ; preds = %lor.lhs.false2, %if.then
  store i64 0, ptr %oldlenp, align 8
  br label %label_return

do.body7:                                         ; preds = %lor.lhs.false2
  %cmp8.not = icmp eq ptr %newp, null
  br i1 %cmp8.not, label %do.end14, label %if.then9

if.then9:                                         ; preds = %do.body7
  %cmp10.not = icmp eq i64 %newlen, 16
  br i1 %cmp10.not, label %if.end12, label %label_return

if.end12:                                         ; preds = %if.then9
  call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) %config, ptr noundef nonnull align 8 dereferenceable(16) %newp, i64 16, i1 false)
  br label %do.end14

do.end14:                                         ; preds = %do.body7, %if.end12
  %call15 = call fastcc i32 @ctl_arena_init(ptr noundef %tsd, ptr noundef nonnull %config)
  store i32 %call15, ptr %arena_ind, align 4
  %cmp16 = icmp eq i32 %call15, -1
  br i1 %cmp16, label %label_return, label %if.then22

if.then22:                                        ; preds = %do.end14
  %4 = load i64, ptr %oldlenp, align 8
  %cmp23.not = icmp eq i64 %4, 4
  br i1 %cmp23.not, label %if.end26, label %if.then24

if.then24:                                        ; preds = %if.then22
  %spec.select = call i64 @llvm.umin.i64(i64 %4, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %arena_ind, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end26:                                         ; preds = %if.then22
  store i32 %call15, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %do.end14, %if.then9, %if.then, %if.then5, %if.end26, %if.then24
  %ret.0 = phi i32 [ 22, %if.then24 ], [ 0, %if.end26 ], [ 22, %if.then5 ], [ 22, %if.then ], [ 22, %if.then9 ], [ 11, %do.end14 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @experimental_batch_alloc_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %filled = alloca i64, align 8
  %cmp = icmp eq ptr %oldp, null
  %cmp1 = icmp eq ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %if.then, label %lor.lhs.false2

lor.lhs.false2:                                   ; preds = %entry
  %0 = load i64, ptr %oldlenp, align 8
  %cmp3.not = icmp eq i64 %0, 8
  br i1 %cmp3.not, label %do.body7, label %if.then5

if.then:                                          ; preds = %entry
  br i1 %cmp1, label %label_return, label %if.then5

if.then5:                                         ; preds = %lor.lhs.false2, %if.then
  store i64 0, ptr %oldlenp, align 8
  br label %label_return

do.body7:                                         ; preds = %lor.lhs.false2
  %cmp8 = icmp eq ptr %newp, null
  %cmp10 = icmp ne i64 %newlen, 32
  %or.cond1 = or i1 %cmp8, %cmp10
  br i1 %or.cond1, label %label_return, label %if.end12

if.end12:                                         ; preds = %do.body7
  %batch_alloc_packet.sroa.0.0.copyload = load ptr, ptr %newp, align 8
  %batch_alloc_packet.sroa.2.0..sroa_idx = getelementptr inbounds i8, ptr %newp, i64 8
  %batch_alloc_packet.sroa.2.0.copyload = load i64, ptr %batch_alloc_packet.sroa.2.0..sroa_idx, align 8
  %batch_alloc_packet.sroa.3.0..sroa_idx = getelementptr inbounds i8, ptr %newp, i64 16
  %batch_alloc_packet.sroa.3.0.copyload = load i64, ptr %batch_alloc_packet.sroa.3.0..sroa_idx, align 8
  %batch_alloc_packet.sroa.4.0..sroa_idx = getelementptr inbounds i8, ptr %newp, i64 24
  %batch_alloc_packet.sroa.4.0.copyload = load i32, ptr %batch_alloc_packet.sroa.4.0..sroa_idx, align 8
  %call = tail call i64 @batch_alloc(ptr noundef %batch_alloc_packet.sroa.0.0.copyload, i64 noundef %batch_alloc_packet.sroa.2.0.copyload, i64 noundef %batch_alloc_packet.sroa.3.0.copyload, i32 noundef %batch_alloc_packet.sroa.4.0.copyload) #14
  store i64 %call, ptr %filled, align 8
  %1 = load i64, ptr %oldlenp, align 8
  %cmp18.not = icmp eq i64 %1, 8
  br i1 %cmp18.not, label %if.end21, label %if.then19

if.then19:                                        ; preds = %if.end12
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %filled, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end21:                                         ; preds = %if.end12
  store i64 %call, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %do.body7, %if.then, %if.then5, %if.end21, %if.then19
  %ret.0 = phi i32 [ 22, %if.then19 ], [ 0, %if.end21 ], [ 22, %if.then5 ], [ 22, %if.then ], [ 22, %do.body7 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @experimental_hooks_install_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %hooks = alloca %struct.hooks_s, align 8
  %handle = alloca ptr, align 8
  %cmp = icmp eq ptr %oldp, null
  %cmp1 = icmp eq ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  %cmp3 = icmp eq ptr %newp, null
  %or.cond1 = or i1 %or.cond, %cmp3
  %cmp6.not = icmp ne i64 %newlen, 32
  %or.cond15.not = or i1 %cmp6.not, %or.cond1
  br i1 %or.cond15.not, label %label_return, label %if.end8

if.end8:                                          ; preds = %entry
  call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) %hooks, ptr noundef nonnull align 8 dereferenceable(32) %newp, i64 32, i1 false)
  %call10 = call ptr @hook_install(ptr noundef %tsd, ptr noundef nonnull %hooks) #14
  store ptr %call10, ptr %handle, align 8
  %cmp11 = icmp eq ptr %call10, null
  br i1 %cmp11, label %label_return, label %if.then17

if.then17:                                        ; preds = %if.end8
  %0 = load i64, ptr %oldlenp, align 8
  %cmp18.not = icmp eq i64 %0, 8
  br i1 %cmp18.not, label %if.end21, label %if.then19

if.then19:                                        ; preds = %if.then17
  %spec.select = call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %handle, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end21:                                         ; preds = %if.then17
  store ptr %call10, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end8, %entry, %if.end21, %if.then19
  %ret.0 = phi i32 [ 22, %if.then19 ], [ 0, %if.end21 ], [ 22, %entry ], [ 11, %if.end8 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @experimental_hooks_remove_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef readnone %oldp, ptr noundef readnone %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %cmp = icmp ne ptr %oldp, null
  %cmp1 = icmp ne ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %cmp3.not = icmp ne ptr %newp, null
  %cmp5.not = icmp eq i64 %newlen, 8
  %or.cond5 = and i1 %cmp3.not, %cmp5.not
  br i1 %or.cond5, label %do.end9, label %label_return

do.end9:                                          ; preds = %do.end
  %0 = load ptr, ptr %newp, align 8
  %cmp10 = icmp eq ptr %0, null
  br i1 %cmp10, label %label_return, label %if.end12

if.end12:                                         ; preds = %do.end9
  tail call void @hook_remove(ptr noundef %tsd, ptr noundef nonnull %0) #14
  br label %label_return

label_return:                                     ; preds = %do.end, %do.end9, %entry, %if.end12
  %ret.0 = phi i32 [ 0, %if.end12 ], [ 1, %entry ], [ 22, %do.end9 ], [ 22, %do.end ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @experimental_hooks_prof_backtrace_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %old_hook = alloca ptr, align 8
  %cmp = icmp eq ptr %oldp, null
  %cmp1 = icmp eq ptr %newp, null
  %or.cond = and i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %if.end

if.end:                                           ; preds = %entry
  br i1 %cmp, label %if.end13, label %if.then3

if.then3:                                         ; preds = %if.end
  %call = tail call ptr @prof_backtrace_hook_get() #14
  store ptr %call, ptr %old_hook, align 8
  %cmp6.not = icmp eq ptr %oldlenp, null
  br i1 %cmp6.not, label %if.end13, label %if.then7

if.then7:                                         ; preds = %if.then3
  %0 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %0, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %old_hook, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store ptr %call, ptr %oldp, align 8
  br label %if.end13

if.end13:                                         ; preds = %if.end11, %if.then3, %if.end
  br i1 %cmp1, label %label_return, label %if.then15

if.then15:                                        ; preds = %if.end13
  %1 = load i8, ptr @opt_prof, align 1
  %2 = and i8 %1, 1
  %tobool.not = icmp eq i8 %2, 0
  br i1 %tobool.not, label %label_return, label %if.then20

if.then20:                                        ; preds = %if.then15
  %cmp21.not = icmp eq i64 %newlen, 8
  br i1 %cmp21.not, label %if.end23, label %label_return

if.end23:                                         ; preds = %if.then20
  %3 = load ptr, ptr %newp, align 8
  %cmp26 = icmp eq ptr %3, null
  br i1 %cmp26, label %label_return, label %if.end28

if.end28:                                         ; preds = %if.end23
  tail call void @prof_backtrace_hook_set(ptr noundef nonnull %3) #14
  br label %label_return

label_return:                                     ; preds = %if.end13, %if.end28, %if.end23, %if.then20, %if.then15, %entry, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 22, %entry ], [ 2, %if.then15 ], [ 22, %if.then20 ], [ 22, %if.end23 ], [ 0, %if.end28 ], [ 0, %if.end13 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @experimental_hooks_prof_dump_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %old_hook = alloca ptr, align 8
  %cmp = icmp eq ptr %oldp, null
  %cmp1 = icmp eq ptr %newp, null
  %or.cond = and i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %if.end

if.end:                                           ; preds = %entry
  br i1 %cmp, label %if.end13, label %if.then3

if.then3:                                         ; preds = %if.end
  %call = tail call ptr @prof_dump_hook_get() #14
  store ptr %call, ptr %old_hook, align 8
  %cmp6.not = icmp eq ptr %oldlenp, null
  br i1 %cmp6.not, label %if.end13, label %if.then7

if.then7:                                         ; preds = %if.then3
  %0 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %0, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %old_hook, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store ptr %call, ptr %oldp, align 8
  br label %if.end13

if.end13:                                         ; preds = %if.end11, %if.then3, %if.end
  br i1 %cmp1, label %label_return, label %if.then15

if.then15:                                        ; preds = %if.end13
  %1 = load i8, ptr @opt_prof, align 1
  %2 = and i8 %1, 1
  %tobool.not = icmp eq i8 %2, 0
  br i1 %tobool.not, label %label_return, label %if.then20

if.then20:                                        ; preds = %if.then15
  %cmp21.not = icmp eq i64 %newlen, 8
  br i1 %cmp21.not, label %do.end25, label %label_return

do.end25:                                         ; preds = %if.then20
  %3 = load ptr, ptr %newp, align 8
  tail call void @prof_dump_hook_set(ptr noundef %3) #14
  br label %label_return

label_return:                                     ; preds = %if.end13, %do.end25, %if.then20, %if.then15, %entry, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 22, %entry ], [ 2, %if.then15 ], [ 22, %if.then20 ], [ 0, %do.end25 ], [ 0, %if.end13 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @experimental_hooks_prof_sample_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %old_hook = alloca ptr, align 8
  %cmp = icmp eq ptr %oldp, null
  %cmp1 = icmp eq ptr %newp, null
  %or.cond = and i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %if.end

if.end:                                           ; preds = %entry
  br i1 %cmp, label %if.end13, label %if.then3

if.then3:                                         ; preds = %if.end
  %call = tail call ptr @prof_sample_hook_get() #14
  store ptr %call, ptr %old_hook, align 8
  %cmp6.not = icmp eq ptr %oldlenp, null
  br i1 %cmp6.not, label %if.end13, label %if.then7

if.then7:                                         ; preds = %if.then3
  %0 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %0, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %old_hook, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store ptr %call, ptr %oldp, align 8
  br label %if.end13

if.end13:                                         ; preds = %if.end11, %if.then3, %if.end
  br i1 %cmp1, label %label_return, label %if.then15

if.then15:                                        ; preds = %if.end13
  %1 = load i8, ptr @opt_prof, align 1
  %2 = and i8 %1, 1
  %tobool.not = icmp eq i8 %2, 0
  br i1 %tobool.not, label %label_return, label %if.then20

if.then20:                                        ; preds = %if.then15
  %cmp21.not = icmp eq i64 %newlen, 8
  br i1 %cmp21.not, label %do.end25, label %label_return

do.end25:                                         ; preds = %if.then20
  %3 = load ptr, ptr %newp, align 8
  tail call void @prof_sample_hook_set(ptr noundef %3) #14
  br label %label_return

label_return:                                     ; preds = %if.end13, %do.end25, %if.then20, %if.then15, %entry, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 22, %entry ], [ 2, %if.then15 ], [ 22, %if.then20 ], [ 0, %do.end25 ], [ 0, %if.end13 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @experimental_hooks_prof_sample_free_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %old_hook = alloca ptr, align 8
  %cmp = icmp eq ptr %oldp, null
  %cmp1 = icmp eq ptr %newp, null
  %or.cond = and i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %if.end

if.end:                                           ; preds = %entry
  br i1 %cmp, label %if.end13, label %if.then3

if.then3:                                         ; preds = %if.end
  %call = tail call ptr @prof_sample_free_hook_get() #14
  store ptr %call, ptr %old_hook, align 8
  %cmp6.not = icmp eq ptr %oldlenp, null
  br i1 %cmp6.not, label %if.end13, label %if.then7

if.then7:                                         ; preds = %if.then3
  %0 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %0, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %old_hook, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store ptr %call, ptr %oldp, align 8
  br label %if.end13

if.end13:                                         ; preds = %if.end11, %if.then3, %if.end
  br i1 %cmp1, label %label_return, label %if.then15

if.then15:                                        ; preds = %if.end13
  %1 = load i8, ptr @opt_prof, align 1
  %2 = and i8 %1, 1
  %tobool.not = icmp eq i8 %2, 0
  br i1 %tobool.not, label %label_return, label %if.then20

if.then20:                                        ; preds = %if.then15
  %cmp21.not = icmp eq i64 %newlen, 8
  br i1 %cmp21.not, label %do.end25, label %label_return

do.end25:                                         ; preds = %if.then20
  %3 = load ptr, ptr %newp, align 8
  tail call void @prof_sample_free_hook_set(ptr noundef %3) #14
  br label %label_return

label_return:                                     ; preds = %if.end13, %do.end25, %if.then20, %if.then15, %entry, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 22, %entry ], [ 2, %if.then15 ], [ 22, %if.then20 ], [ 0, %do.end25 ], [ 0, %if.end13 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @experimental_hooks_safety_check_abort_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef readnone %oldp, ptr noundef readnone %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %cmp = icmp ne ptr %oldp, null
  %cmp1 = icmp ne ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %cmp2.not = icmp eq ptr %newp, null
  br i1 %cmp2.not, label %label_return, label %if.then3

if.then3:                                         ; preds = %do.end
  %cmp4.not = icmp eq i64 %newlen, 8
  br i1 %cmp4.not, label %do.end14, label %label_return

do.end14:                                         ; preds = %if.then3
  %0 = load ptr, ptr %newp, align 8
  tail call void @safety_check_set_abort(ptr noundef %0) #14
  br label %label_return

label_return:                                     ; preds = %do.end, %do.end14, %if.then3, %entry
  %ret.0 = phi i32 [ 1, %entry ], [ 22, %if.then3 ], [ 0, %do.end14 ], [ 0, %do.end ]
  ret i32 %ret.0
}

declare ptr @hook_install(ptr noundef, ptr noundef) local_unnamed_addr #1

declare void @hook_remove(ptr noundef, ptr noundef) local_unnamed_addr #1

declare ptr @prof_backtrace_hook_get() local_unnamed_addr #1

declare void @prof_backtrace_hook_set(ptr noundef) local_unnamed_addr #1

declare ptr @prof_dump_hook_get() local_unnamed_addr #1

declare void @prof_dump_hook_set(ptr noundef) local_unnamed_addr #1

declare ptr @prof_sample_hook_get() local_unnamed_addr #1

declare void @prof_sample_hook_set(ptr noundef) local_unnamed_addr #1

declare ptr @prof_sample_free_hook_get() local_unnamed_addr #1

declare void @prof_sample_free_hook_set(ptr noundef) local_unnamed_addr #1

declare void @safety_check_set_abort(ptr noundef) local_unnamed_addr #1

; Function Attrs: nounwind uwtable
define internal i32 @experimental_utilization_query_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef %oldp, ptr noundef readonly %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %cmp = icmp eq ptr %oldp, null
  %cmp1 = icmp eq ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %lor.lhs.false2

lor.lhs.false2:                                   ; preds = %entry
  %0 = load i64, ptr %oldlenp, align 8
  %cmp3 = icmp eq i64 %0, 48
  %cmp5 = icmp ne ptr %newp, null
  %or.cond1.not12 = and i1 %cmp5, %cmp3
  %cmp9.not = icmp eq i64 %newlen, 8
  %or.cond11 = and i1 %cmp9.not, %or.cond1.not12
  br i1 %or.cond11, label %do.end13, label %label_return

do.end13:                                         ; preds = %lor.lhs.false2
  %1 = load ptr, ptr %newp, align 8
  %nfree = getelementptr inbounds i8, ptr %oldp, i64 8
  %nregs = getelementptr inbounds i8, ptr %oldp, i64 16
  %size = getelementptr inbounds i8, ptr %oldp, i64 24
  %bin_nfree = getelementptr inbounds i8, ptr %oldp, i64 32
  %bin_nregs = getelementptr inbounds i8, ptr %oldp, i64 40
  tail call void @inspect_extent_util_stats_verbose_get(ptr noundef %tsd, ptr noundef %1, ptr noundef nonnull %nfree, ptr noundef nonnull %nregs, ptr noundef nonnull %size, ptr noundef nonnull %bin_nfree, ptr noundef nonnull %bin_nregs, ptr noundef nonnull %oldp) #14
  br label %label_return

label_return:                                     ; preds = %entry, %lor.lhs.false2, %do.end13
  %ret.0 = phi i32 [ 0, %do.end13 ], [ 22, %lor.lhs.false2 ], [ 22, %entry ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @experimental_utilization_batch_query_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef %oldp, ptr noundef readonly %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %div17 = lshr i64 %newlen, 3
  %cmp = icmp eq ptr %oldp, null
  %cmp1 = icmp eq ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  %cmp3 = icmp eq ptr %newp, null
  %or.cond1 = or i1 %or.cond, %cmp3
  %cmp5 = icmp eq i64 %newlen, 0
  %or.cond2 = or i1 %or.cond1, %cmp5
  %0 = and i64 %newlen, 7
  %cmp7.not = icmp ne i64 %0, 0
  %or.cond18.not = or i1 %cmp7.not, %or.cond2
  br i1 %or.cond18.not, label %label_return, label %lor.lhs.false8

lor.lhs.false8:                                   ; preds = %entry
  %1 = load i64, ptr %oldlenp, align 8
  %mul9 = mul i64 %div17, 24
  %cmp10.not = icmp eq i64 %1, %mul9
  br i1 %cmp10.not, label %for.cond.preheader, label %label_return

for.cond.preheader:                               ; preds = %lor.lhs.false8
  %cmp1119.not = icmp ult i64 %newlen, 8
  br i1 %cmp1119.not, label %label_return, label %for.body

for.body:                                         ; preds = %for.cond.preheader, %for.body
  %i.020 = phi i64 [ %inc, %for.body ], [ 0, %for.cond.preheader ]
  %arrayidx = getelementptr inbounds ptr, ptr %newp, i64 %i.020
  %2 = load ptr, ptr %arrayidx, align 8
  %arrayidx12 = getelementptr inbounds %struct.inspect_extent_util_stats_s, ptr %oldp, i64 %i.020
  %nregs = getelementptr inbounds i8, ptr %arrayidx12, i64 8
  %size = getelementptr inbounds i8, ptr %arrayidx12, i64 16
  tail call void @inspect_extent_util_stats_get(ptr noundef %tsd, ptr noundef %2, ptr noundef %arrayidx12, ptr noundef nonnull %nregs, ptr noundef nonnull %size) #14
  %inc = add nuw nsw i64 %i.020, 1
  %exitcond.not = icmp eq i64 %inc, %div17
  br i1 %exitcond.not, label %label_return, label %for.body, !llvm.loop !28

label_return:                                     ; preds = %for.body, %for.cond.preheader, %entry, %lor.lhs.false8
  %ret.0 = phi i32 [ 22, %lor.lhs.false8 ], [ 22, %entry ], [ 0, %for.cond.preheader ], [ 0, %for.body ]
  ret i32 %ret.0
}

declare void @inspect_extent_util_stats_verbose_get(ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef) local_unnamed_addr #1

declare void @inspect_extent_util_stats_get(ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef) local_unnamed_addr #1

; Function Attrs: nounwind uwtable
define internal ptr @experimental_arenas_i_index(ptr noundef %tsdn, ptr nocapture readnone %mib, i64 %miblen, i64 noundef %i) #0 {
entry:
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsdn
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsdn, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  switch i64 %i, label %sw.default.i.i [
    i64 4096, label %ctl_arenas_i_verify.exit
    i64 4097, label %sw.bb2.i.i
  ]

sw.bb2.i.i:                                       ; preds = %malloc_mutex_lock.exit
  br label %ctl_arenas_i_verify.exit

sw.default.i.i:                                   ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_arenas, align 8
  %narenas.i.i = getelementptr inbounds i8, ptr %3, i64 8
  %4 = load i32, ptr %narenas.i.i, align 8
  %conv.i.i = zext i32 %4 to i64
  %cmp.i.i = icmp eq i64 %conv.i.i, %i
  br i1 %cmp.i.i, label %ctl_arenas_i_verify.exit, label %if.else.i.i

if.else.i.i:                                      ; preds = %sw.default.i.i
  %cmp9.not.i.i = icmp ule i64 %conv.i.i, %i
  %conv13.i.i = trunc i64 %i to i32
  %add.i.i = add i32 %conv13.i.i, 2
  %cmp.i = icmp eq i32 %add.i.i, -1
  %or.cond.i = or i1 %cmp.i, %cmp9.not.i.i
  br i1 %or.cond.i, label %ctl_arenas_i_verify.exit.thread, label %ctl_arenas_i_verify.exit

ctl_arenas_i_verify.exit:                         ; preds = %malloc_mutex_lock.exit, %sw.bb2.i.i, %sw.default.i.i, %if.else.i.i
  %a.0.i4.i = phi i32 [ 0, %sw.default.i.i ], [ 0, %malloc_mutex_lock.exit ], [ 1, %sw.bb2.i.i ], [ %add.i.i, %if.else.i.i ]
  %conv.i = zext i32 %a.0.i4.i to i64
  %5 = load ptr, ptr @ctl_arenas, align 8
  %arenas.i = getelementptr inbounds i8, ptr %5, i64 24
  %arrayidx.i = getelementptr inbounds [4097 x ptr], ptr %arenas.i, i64 0, i64 %conv.i
  %6 = load ptr, ptr %arrayidx.i, align 8
  %initialized.i = getelementptr inbounds i8, ptr %6, i64 4
  %7 = load i8, ptr %initialized.i, align 4
  %.fr4 = freeze i8 %7
  %8 = and i8 %.fr4, 1
  %tobool.not.i = icmp eq i8 %8, 0
  br i1 %tobool.not.i, label %ctl_arenas_i_verify.exit.thread, label %9

ctl_arenas_i_verify.exit.thread:                  ; preds = %if.else.i.i, %ctl_arenas_i_verify.exit
  br label %9

9:                                                ; preds = %ctl_arenas_i_verify.exit, %ctl_arenas_i_verify.exit.thread
  %10 = phi ptr [ null, %ctl_arenas_i_verify.exit.thread ], [ @super_experimental_arenas_i_node, %ctl_arenas_i_verify.exit ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  ret ptr %10
}

; Function Attrs: nounwind uwtable
define internal i32 @experimental_arenas_i_pactivep_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %pactivep = alloca ptr, align 8
  %cmp = icmp eq ptr %oldp, null
  %cmp1 = icmp eq ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %return, label %lor.lhs.false2

lor.lhs.false2:                                   ; preds = %entry
  %0 = load i64, ptr %oldlenp, align 8
  %cmp3.not = icmp eq i64 %0, 8
  br i1 %cmp3.not, label %if.end, label %return

if.end:                                           ; preds = %lor.lhs.false2
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %if.end
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %if.end
  %1 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %1, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %2 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %2, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %3 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %3, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp4 = icmp ne ptr %newp, null
  %cmp6 = icmp ne i64 %newlen, 0
  %or.cond1 = or i1 %cmp4, %cmp6
  br i1 %or.cond1, label %label_return, label %do.body9

do.body9:                                         ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i8, ptr %mib, i64 16
  %4 = load i64, ptr %arrayidx, align 8
  %cmp10 = icmp ugt i64 %4, 4294967295
  br i1 %cmp10, label %label_return, label %if.end12

if.end12:                                         ; preds = %do.body9
  %conv = trunc i64 %4 to i32
  %call15 = tail call i32 @narenas_total_get() #14
  %cmp16 = icmp ugt i32 %call15, %conv
  br i1 %cmp16, label %land.lhs.true, label %label_return

land.lhs.true:                                    ; preds = %if.end12
  %arrayidx.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %4
  %5 = load atomic i64, ptr %arrayidx.i acquire, align 8
  %cmp20.not = icmp eq i64 %5, 0
  br i1 %cmp20.not, label %label_return, label %if.then22

if.then22:                                        ; preds = %land.lhs.true
  %6 = inttoptr i64 %5 to ptr
  %nactive = getelementptr inbounds i8, ptr %6, i64 10672
  store ptr %nactive, ptr %pactivep, align 8
  %7 = load i64, ptr %oldlenp, align 8
  %cmp30.not = icmp eq i64 %7, 8
  br i1 %cmp30.not, label %if.end35, label %if.then32

if.then32:                                        ; preds = %if.then22
  %spec.select = tail call i64 @llvm.umin.i64(i64 %7, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %pactivep, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end35:                                         ; preds = %if.then22
  store ptr %nactive, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %land.lhs.true, %do.body9, %malloc_mutex_lock.exit, %if.end35, %if.then32
  %ret.0 = phi i32 [ 22, %if.then32 ], [ 0, %if.end35 ], [ 1, %malloc_mutex_lock.exit ], [ 14, %do.body9 ], [ 14, %land.lhs.true ], [ 14, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1, i32 0) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2, i32 0, i32 0)) #14
  br label %return

return:                                           ; preds = %entry, %lor.lhs.false2, %label_return
  %retval.0 = phi i32 [ %ret.0, %label_return ], [ 22, %lor.lhs.false2 ], [ 22, %entry ]
  ret i32 %retval.0
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @experimental_prof_recent_alloc_max_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @experimental_prof_recent_alloc_dump_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

declare i64 @batch_alloc(ptr noundef, i64 noundef, i64 noundef, i32 noundef) local_unnamed_addr #1

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @experimental_thread_activity_callback_ctl(ptr nocapture noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #9 {
entry:
  %t_old.sroa.0 = alloca <2 x ptr>, align 16
  %cant_access_tsd_items_directly_use_a_getter_or_setter_activity_callback_thunk.i = getelementptr inbounds i8, ptr %tsd, i64 240
  %0 = load <2 x ptr>, ptr %cant_access_tsd_items_directly_use_a_getter_or_setter_activity_callback_thunk.i, align 8
  store <2 x ptr> %0, ptr %t_old.sroa.0, align 16
  %cmp = icmp ne ptr %oldp, null
  %cmp1 = icmp ne ptr %oldlenp, null
  %or.cond = and i1 %cmp, %cmp1
  br i1 %or.cond, label %if.then, label %do.end

if.then:                                          ; preds = %entry
  %1 = load i64, ptr %oldlenp, align 8
  %cmp2.not = icmp eq i64 %1, 16
  br i1 %cmp2.not, label %if.end, label %if.then3

if.then3:                                         ; preds = %if.then
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 16)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 16 %t_old.sroa.0, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end:                                           ; preds = %if.then
  %t_old.sroa.0.0.t_old.sroa.0.0.copyload = load <2 x ptr>, ptr %t_old.sroa.0, align 16
  store <2 x ptr> %t_old.sroa.0.0.t_old.sroa.0.0.copyload, ptr %oldp, align 8
  br label %do.end

do.end:                                           ; preds = %entry, %if.end
  %cmp6.not = icmp eq ptr %newp, null
  br i1 %cmp6.not, label %label_return, label %if.then10

if.then10:                                        ; preds = %do.end
  %cmp11.not = icmp eq i64 %newlen, 16
  br i1 %cmp11.not, label %do.end15, label %label_return

do.end15:                                         ; preds = %if.then10
  %2 = load <2 x ptr>, ptr %newp, align 8
  store <2 x ptr> %2, ptr %cant_access_tsd_items_directly_use_a_getter_or_setter_activity_callback_thunk.i, align 8
  br label %label_return

label_return:                                     ; preds = %do.end, %do.end15, %if.then10, %if.then3
  %ret.0 = phi i32 [ 22, %if.then3 ], [ 22, %if.then10 ], [ 0, %do.end15 ], [ 0, %do.end ]
  ret i32 %ret.0
}

; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare i64 @llvm.umin.i64(i64, i64) #12

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(argmem: readwrite)
declare void @llvm.lifetime.start.p0(i64 immarg, ptr nocapture) #13

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(argmem: readwrite)
declare void @llvm.lifetime.end.p0(i64 immarg, ptr nocapture) #13

attributes #0 = { nounwind uwtable "frame-pointer"="all" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #1 = { "frame-pointer"="all" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #2 = { mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable "frame-pointer"="all" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #3 = { nounwind "frame-pointer"="all" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #4 = { mustprogress nocallback nofree nounwind willreturn memory(argmem: write) }
attributes #5 = { mustprogress nocallback nofree nosync nounwind willreturn }
attributes #6 = { mustprogress nocallback nofree nosync nounwind speculatable willreturn memory(none) }
attributes #7 = { mustprogress nocallback nofree nounwind willreturn memory(argmem: readwrite) }
attributes #8 = { mustprogress nofree nounwind willreturn memory(argmem: read) "frame-pointer"="all" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #9 = { mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable "frame-pointer"="all" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #10 = { mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable "frame-pointer"="all" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #11 = { mustprogress nofree nounwind willreturn memory(readwrite, inaccessiblemem: none) uwtable "frame-pointer"="all" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #12 = { nocallback nofree nosync nounwind speculatable willreturn memory(none) }
attributes #13 = { nocallback nofree nosync nounwind willreturn memory(argmem: readwrite) }
attributes #14 = { nounwind }
attributes #15 = { nounwind willreturn memory(read) }

!llvm.module.flags = !{!0, !1, !2, !3, !4}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 8, !"PIC Level", i32 2}
!2 = !{i32 7, !"PIE Level", i32 2}
!3 = !{i32 7, !"uwtable", i32 2}
!4 = !{i32 7, !"frame-pointer", i32 2}
!5 = !{i32 0, i32 3}
!6 = distinct !{!6, !7}
!7 = !{!"llvm.loop.mustprogress"}
!8 = distinct !{!8, !7}
!9 = distinct !{!9, !7}
!10 = distinct !{!10, !7}
!11 = distinct !{!11, !7}
!12 = distinct !{!12, !7}
!13 = distinct !{!13, !7}
!14 = distinct !{!14, !7}
!15 = distinct !{!15, !7}
!16 = distinct !{!16, !7}
!17 = !{i64 0, i64 65}
!18 = distinct !{!18, !7}
!19 = distinct !{!19, !7}
!20 = distinct !{!20, !7}
!21 = distinct !{!21, !7}
!22 = !{!23}
!23 = distinct !{!23, !24, !"rtree_leaf_elm_read: %agg.result"}
!24 = distinct !{!24, !"rtree_leaf_elm_read"}
!25 = distinct !{!25, !7}
!26 = distinct !{!26, !7}
!27 = distinct !{!27, !7}
!28 = distinct !{!28, !7}
