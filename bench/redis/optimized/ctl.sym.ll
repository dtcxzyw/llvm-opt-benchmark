; ModuleID = 'bench/redis/original/ctl.sym.ll'
source_filename = "bench/redis/original/ctl.sym.ll"
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

%struct.ctl_named_node_s = type { %struct.ctl_node_s, ptr, i64, ptr, ptr }
%struct.ctl_node_s = type { i8 }
%struct.malloc_mutex_s = type { %union.anon }
%union.anon = type { %struct.anon }
%struct.anon = type { %struct.mutex_prof_data_t, %union.pthread_mutex_t, %struct.atomic_b_t }
%struct.mutex_prof_data_t = type { %struct.nstime_t, %struct.nstime_t, i64, i64, i32, %struct.atomic_u32_t, i64, ptr, i64 }
%struct.nstime_t = type { i64 }
%struct.atomic_u32_t = type { i32 }
%union.pthread_mutex_t = type { %struct.__pthread_mutex_s }
%struct.__pthread_mutex_s = type { i32, i32, i32, i32, i32, i16, i16, %struct.__pthread_internal_list }
%struct.__pthread_internal_list = type { ptr, ptr }
%struct.atomic_b_t = type { i8 }
%struct.tsd_s = type { i8, i8, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, ptr, i64, i64, i64, ptr, ptr, %struct.ticker_geom_s, i8, %struct.tsd_binshards_s, %struct.tsd_link_t, i8, %struct.peak_s, %struct.activity_callback_thunk_s, %struct.tcache_slow_s, %struct.rtree_ctx_s, %struct.atomic_u8_t, i64, i64, i64, i64, %struct.tcache_s, %struct.witness_tsd_s }
%struct.ticker_geom_s = type { i32, i32 }
%struct.tsd_binshards_s = type { [39 x i8] }
%struct.tsd_link_t = type { ptr, ptr }
%struct.peak_s = type { i64, i64 }
%struct.activity_callback_thunk_s = type { ptr, ptr }
%struct.tcache_slow_s = type { %struct.anon.3, %struct.cache_bin_array_descriptor_s, ptr, i32, [39 x i8], [39 x i8], [39 x i8], ptr, ptr }
%struct.anon.3 = type { ptr, ptr }
%struct.cache_bin_array_descriptor_s = type { %struct.anon.4, ptr }
%struct.anon.4 = type { ptr, ptr }
%struct.rtree_ctx_s = type { [16 x %struct.rtree_ctx_cache_elm_s], [8 x %struct.rtree_ctx_cache_elm_s] }
%struct.rtree_ctx_cache_elm_s = type { i64, ptr }
%struct.atomic_u8_t = type { i8 }
%struct.tcache_s = type { ptr, [76 x %struct.cache_bin_s] }
%struct.cache_bin_s = type { ptr, %struct.cache_bin_stats_s, i16, i16, i16 }
%struct.cache_bin_stats_s = type { i64 }
%struct.witness_tsd_s = type { %struct.witness_list_t, i8 }
%struct.witness_list_t = type { ptr }
%struct.atomic_p_t = type { ptr }
%struct.arena_config_s = type { ptr, i8 }
%struct.ctl_indexed_node_s = type { %struct.ctl_node_s, ptr }
%struct.hpa_shard_opts_s = type { i64, i64, i32, i8, i64, i64 }
%struct.sec_opts_s = type { i64, i64, i64, i64, i64 }
%struct.extent_hooks_s = type { ptr, ptr, ptr, ptr, ptr, ptr, ptr, ptr, ptr }
%struct.bin_info_s = type { i64, i64, i32, i32, %struct.bitmap_info_s }
%struct.bitmap_info_s = type { i64, i64 }
%struct.emap_s = type { %struct.rtree_s }
%struct.rtree_s = type { ptr, %struct.malloc_mutex_s, [262144 x %struct.rtree_node_elm_s] }
%struct.rtree_node_elm_s = type { %struct.atomic_p_t }
%struct.atomic_zu_t = type { i64 }
%struct.ctl_arenas_s = type { i64, i32, %struct.anon.2, [4097 x ptr] }
%struct.anon.2 = type { ptr }
%struct.container_s = type { %struct.ctl_arena_s, %struct.ctl_arena_stats_s }
%struct.ctl_arena_s = type { i32, i8, %struct.anon.1, i32, ptr, i64, i64, i64, i64, i64, ptr }
%struct.anon.1 = type { ptr, ptr }
%struct.ctl_arena_stats_s = type { %struct.arena_stats_s, i64, i64, i64, i64, i64, i64, [39 x %struct.bin_stats_data_s], [196 x %struct.arena_stats_large_s], [199 x %struct.pac_estats_s], %struct.hpa_shard_stats_s, %struct.sec_stats_s }
%struct.arena_stats_s = type { i64, i64, i64, i64, %struct.atomic_zu_t, i64, i64, i64, i64, i64, i64, %struct.pa_shard_stats_s, i64, i64, [12 x %struct.mutex_prof_data_t], [196 x %struct.arena_stats_large_s], %struct.nstime_t }
%struct.pa_shard_stats_s = type { i64, %struct.pac_stats_s }
%struct.pac_stats_s = type { %struct.pac_decay_stats_s, %struct.pac_decay_stats_s, i64, %struct.atomic_zu_t, %struct.atomic_zu_t }
%struct.pac_decay_stats_s = type { %struct.locked_u64_s, %struct.locked_u64_s, %struct.locked_u64_s }
%struct.locked_u64_s = type { %struct.atomic_u64_t }
%struct.atomic_u64_t = type { i64 }
%struct.bin_stats_data_s = type { %struct.bin_stats_s, %struct.mutex_prof_data_t }
%struct.bin_stats_s = type { i64, i64, i64, i64, i64, i64, i64, i64, i64, i64 }
%struct.arena_stats_large_s = type { %struct.locked_u64_s, %struct.locked_u64_s, %struct.locked_u64_s, %struct.locked_u64_s, %struct.locked_u64_s, i64 }
%struct.pac_estats_s = type { i64, i64, i64, i64, i64, i64 }
%struct.hpa_shard_stats_s = type { %struct.psset_stats_s, %struct.hpa_shard_nonderived_stats_s }
%struct.psset_stats_s = type { [64 x [2 x %struct.psset_bin_stats_s]], [2 x %struct.psset_bin_stats_s], [2 x %struct.psset_bin_stats_s] }
%struct.psset_bin_stats_s = type { i64, i64, i64 }
%struct.hpa_shard_nonderived_stats_s = type { i64, i64, i64, i64 }
%struct.sec_stats_s = type { i64 }
%struct.ctl_stats_s = type { i64, i64, i64, i64, i64, i64, i64, %struct.background_thread_stats_s, [9 x %struct.mutex_prof_data_t] }
%struct.background_thread_stats_s = type { i64, i64, %struct.nstime_t, %struct.mutex_prof_data_t }
%struct.arena_s = type { [2 x %struct.atomic_u_t], %struct.atomic_u_t, ptr, %struct.arena_stats_s, %struct.anon.5, %struct.anon.6, %struct.malloc_mutex_s, %struct.atomic_u_t, %struct.edata_list_active_t, %struct.malloc_mutex_s, %struct.pa_shard_s, i32, ptr, %struct.nstime_t, [0 x %struct.bin_s] }
%struct.anon.5 = type { ptr }
%struct.anon.6 = type { ptr }
%struct.atomic_u_t = type { i32 }
%struct.edata_list_active_t = type { %struct.anon.7 }
%struct.anon.7 = type { ptr }
%struct.pa_shard_s = type { ptr, %struct.atomic_zu_t, %struct.atomic_b_t, i8, %struct.pac_s, %struct.sec_s, %struct.hpa_shard_s, %struct.edata_cache_s, i32, ptr, ptr, ptr, ptr }
%struct.pac_s = type { %struct.pai_s, %struct.ecache_s, %struct.ecache_s, %struct.ecache_s, ptr, ptr, ptr, %struct.exp_grow_s, %struct.malloc_mutex_s, %struct.san_bump_alloc_s, %struct.atomic_zu_t, %struct.decay_s, %struct.decay_s, ptr, ptr, %struct.atomic_zu_t }
%struct.pai_s = type { ptr, ptr, ptr, ptr, ptr, ptr, ptr }
%struct.ecache_s = type { %struct.malloc_mutex_s, %struct.eset_s, %struct.eset_s, i32, i32, i8 }
%struct.eset_s = type { [4 x i64], [200 x %struct.eset_bin_s], [200 x %struct.eset_bin_stats_s], %struct.edata_list_inactive_t, %struct.atomic_zu_t, i32 }
%struct.eset_bin_s = type { %struct.edata_heap_t, %struct.edata_cmp_summary_s }
%struct.edata_heap_t = type { %struct.ph_s }
%struct.ph_s = type { ptr, i64 }
%struct.edata_cmp_summary_s = type { i64, i64 }
%struct.eset_bin_stats_s = type { %struct.atomic_zu_t, %struct.atomic_zu_t }
%struct.edata_list_inactive_t = type { %struct.anon.8 }
%struct.anon.8 = type { ptr }
%struct.exp_grow_s = type { i32, i32 }
%struct.san_bump_alloc_s = type { %struct.malloc_mutex_s, ptr }
%struct.decay_s = type { %struct.malloc_mutex_s, i8, %struct.atomic_zd_t, %struct.nstime_t, %struct.nstime_t, i64, %struct.nstime_t, i64, i64, [200 x i64], i64 }
%struct.atomic_zd_t = type { i64 }
%struct.sec_s = type { %struct.pai_s, ptr, %struct.sec_opts_s, ptr, i32 }
%struct.hpa_shard_s = type { %struct.pai_s, ptr, %struct.malloc_mutex_s, %struct.malloc_mutex_s, ptr, %struct.edata_cache_fast_s, %struct.psset_s, i64, i32, ptr, %struct.hpa_shard_opts_s, i64, %struct.hpa_shard_nonderived_stats_s, %struct.nstime_t }
%struct.edata_cache_fast_s = type { %struct.edata_list_inactive_t, ptr, i8 }
%struct.psset_s = type { [64 x %struct.hpdata_age_heap_t], [1 x i64], %struct.psset_bin_stats_s, %struct.psset_stats_s, %struct.hpdata_empty_list_t, [128 x %struct.hpdata_purge_list_t], [2 x i64], %struct.hpdata_hugify_list_t }
%struct.hpdata_age_heap_t = type { %struct.ph_s }
%struct.hpdata_empty_list_t = type { %struct.anon.9 }
%struct.anon.9 = type { ptr }
%struct.hpdata_purge_list_t = type { %struct.anon.10 }
%struct.anon.10 = type { ptr }
%struct.hpdata_hugify_list_t = type { %struct.anon.11 }
%struct.anon.11 = type { ptr }
%struct.edata_cache_s = type { %struct.edata_avail_t, %struct.atomic_zu_t, %struct.malloc_mutex_s, ptr }
%struct.edata_avail_t = type { %struct.ph_s }
%struct.bin_s = type { %struct.malloc_mutex_s, %struct.bin_stats_s, ptr, %struct.edata_heap_t, %struct.edata_list_active_t }
%struct.background_thread_info_s = type { i64, %union.pthread_cond_t, %struct.malloc_mutex_s, i32, %struct.atomic_b_t, %struct.nstime_t, i64, i64, %struct.nstime_t }
%union.pthread_cond_t = type { %struct.__pthread_cond_s }
%struct.__pthread_cond_s = type { %union.__atomic_wide_counter, %union.__atomic_wide_counter, [2 x i32], [2 x i32], i32, i32, [2 x i32] }
%union.__atomic_wide_counter = type { i64 }
%struct.ehooks_s = type { i32, %struct.atomic_p_t }
%struct.rtree_contents_s = type { ptr, %struct.rtree_metadata_s }
%struct.rtree_metadata_s = type { i32, i32, i8, i8 }
%struct.rtree_leaf_elm_s = type { %struct.atomic_p_t }
%struct.base_s = type { %struct.ehooks_s, %struct.ehooks_s, %struct.malloc_mutex_s, i8, i32, i64, ptr, [235 x %struct.edata_heap_t], i64, i64, i64, i64 }
%struct.hooks_s = type { ptr, ptr, ptr, ptr }
%struct.inspect_extent_util_stats_verbose_s = type { ptr, i64, i64, i64, i64, i64 }
%struct.inspect_extent_util_stats_s = type { i64, i64, i64 }

@ctl_initialized = internal unnamed_addr global i1 false, align 1
@super_root_node = internal constant [1 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.1, i64 13, ptr @root_node, ptr null }], align 16
@ctl_mtx = internal global %struct.malloc_mutex_s zeroinitializer, align 8
@.str = private unnamed_addr constant [4 x i8] c"ctl\00", align 1
@ctl_arenas = internal unnamed_addr global ptr null, align 8
@ctl_stats = internal unnamed_addr global ptr null, align 8
@dss_prec_names = external local_unnamed_addr global [0 x ptr], align 8
@background_thread_lock = external global %struct.malloc_mutex_s, align 8
@tsd_tls = external thread_local(initialexec) global %struct.tsd_s, align 8
@arenas = external local_unnamed_addr global [0 x %struct.atomic_p_t], align 8
@arena_config_default = external constant %struct.arena_config_s, align 8
@sz_index2size_tab = external local_unnamed_addr global [235 x i64], align 16
@nstime_zero = internal constant %struct.nstime_t zeroinitializer, align 8
@.str.1 = private unnamed_addr constant [1 x i8] zeroinitializer, align 1
@root_node = internal constant [13 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.2, i64 0, ptr null, ptr @version_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.3, i64 0, ptr null, ptr @epoch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.4, i64 0, ptr null, ptr @background_thread_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.5, i64 0, ptr null, ptr @max_background_threads_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.6, i64 9, ptr @thread_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.7, i64 12, ptr @config_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.8, i64 65, ptr @opt_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.9, i64 3, ptr @tcache_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.10, i64 1, ptr @arena_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.11, i64 13, ptr @arenas_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.12, i64 11, ptr @prof_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.13, i64 11, ptr @stats_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.14, i64 7, ptr @experimental_node, ptr null }], align 16
@.str.2 = private unnamed_addr constant [8 x i8] c"version\00", align 1
@.str.3 = private unnamed_addr constant [6 x i8] c"epoch\00", align 1
@.str.4 = private unnamed_addr constant [18 x i8] c"background_thread\00", align 1
@.str.5 = private unnamed_addr constant [23 x i8] c"max_background_threads\00", align 1
@.str.6 = private unnamed_addr constant [7 x i8] c"thread\00", align 1
@thread_node = internal constant [9 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.10, i64 0, ptr null, ptr @thread_arena_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.16, i64 0, ptr null, ptr @thread_allocated_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.17, i64 0, ptr null, ptr @thread_allocatedp_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.18, i64 0, ptr null, ptr @thread_deallocated_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.19, i64 0, ptr null, ptr @thread_deallocatedp_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.9, i64 2, ptr @thread_tcache_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.20, i64 2, ptr @thread_peak_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.12, i64 2, ptr @thread_prof_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.21, i64 0, ptr null, ptr @thread_idle_ctl }], align 16
@.str.7 = private unnamed_addr constant [7 x i8] c"config\00", align 1
@config_node = internal constant [12 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.28, i64 0, ptr null, ptr @config_cache_oblivious_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.29, i64 0, ptr null, ptr @config_debug_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.30, i64 0, ptr null, ptr @config_fill_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.31, i64 0, ptr null, ptr @config_lazy_lock_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.32, i64 0, ptr null, ptr @config_malloc_conf_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.33, i64 0, ptr null, ptr @config_opt_safety_checks_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.12, i64 0, ptr null, ptr @config_prof_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.34, i64 0, ptr null, ptr @config_prof_libgcc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.35, i64 0, ptr null, ptr @config_prof_libunwind_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.13, i64 0, ptr null, ptr @config_stats_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.36, i64 0, ptr null, ptr @config_utrace_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.37, i64 0, ptr null, ptr @config_xmalloc_ctl }], align 16
@.str.8 = private unnamed_addr constant [4 x i8] c"opt\00", align 1
@opt_node = internal constant [65 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.38, i64 0, ptr null, ptr @opt_abort_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.39, i64 0, ptr null, ptr @opt_abort_conf_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.28, i64 0, ptr null, ptr @opt_cache_oblivious_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.40, i64 0, ptr null, ptr @opt_trust_madvise_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.41, i64 0, ptr null, ptr @opt_confirm_conf_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.42, i64 0, ptr null, ptr @opt_hpa_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.43, i64 0, ptr null, ptr @opt_hpa_slab_max_alloc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.44, i64 0, ptr null, ptr @opt_hpa_hugification_threshold_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.45, i64 0, ptr null, ptr @opt_hpa_hugify_delay_ms_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.46, i64 0, ptr null, ptr @opt_hpa_min_purge_interval_ms_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.47, i64 0, ptr null, ptr @opt_hpa_dirty_mult_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.48, i64 0, ptr null, ptr @opt_hpa_sec_nshards_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.49, i64 0, ptr null, ptr @opt_hpa_sec_max_alloc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.50, i64 0, ptr null, ptr @opt_hpa_sec_max_bytes_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.51, i64 0, ptr null, ptr @opt_hpa_sec_bytes_after_flush_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.52, i64 0, ptr null, ptr @opt_hpa_sec_batch_fill_extra_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.53, i64 0, ptr null, ptr @opt_metadata_thp_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.54, i64 0, ptr null, ptr @opt_retain_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.55, i64 0, ptr null, ptr @opt_dss_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.56, i64 0, ptr null, ptr @opt_narenas_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.57, i64 0, ptr null, ptr @opt_percpu_arena_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.58, i64 0, ptr null, ptr @opt_oversize_threshold_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.59, i64 0, ptr null, ptr @opt_mutex_max_spin_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.4, i64 0, ptr null, ptr @opt_background_thread_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.5, i64 0, ptr null, ptr @opt_max_background_threads_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.60, i64 0, ptr null, ptr @opt_dirty_decay_ms_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.61, i64 0, ptr null, ptr @opt_muzzy_decay_ms_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.62, i64 0, ptr null, ptr @opt_stats_print_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.63, i64 0, ptr null, ptr @opt_stats_print_opts_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.64, i64 0, ptr null, ptr @opt_stats_interval_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.65, i64 0, ptr null, ptr @opt_stats_interval_opts_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.66, i64 0, ptr null, ptr @opt_junk_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.67, i64 0, ptr null, ptr @opt_zero_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.36, i64 0, ptr null, ptr @opt_utrace_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.37, i64 0, ptr null, ptr @opt_xmalloc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.68, i64 0, ptr null, ptr @opt_experimental_infallible_new_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.9, i64 0, ptr null, ptr @opt_tcache_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.69, i64 0, ptr null, ptr @opt_tcache_max_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.70, i64 0, ptr null, ptr @opt_tcache_nslots_small_min_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.71, i64 0, ptr null, ptr @opt_tcache_nslots_small_max_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.72, i64 0, ptr null, ptr @opt_tcache_nslots_large_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.73, i64 0, ptr null, ptr @opt_lg_tcache_nslots_mul_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.74, i64 0, ptr null, ptr @opt_tcache_gc_incr_bytes_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.75, i64 0, ptr null, ptr @opt_tcache_gc_delay_bytes_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.76, i64 0, ptr null, ptr @opt_lg_tcache_flush_small_div_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.77, i64 0, ptr null, ptr @opt_lg_tcache_flush_large_div_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.78, i64 0, ptr null, ptr @opt_thp_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.79, i64 0, ptr null, ptr @opt_lg_extent_max_active_fit_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.12, i64 0, ptr null, ptr @opt_prof_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.80, i64 0, ptr null, ptr @opt_prof_prefix_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.81, i64 0, ptr null, ptr @opt_prof_active_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.82, i64 0, ptr null, ptr @opt_prof_thread_active_init_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.83, i64 0, ptr null, ptr @opt_lg_prof_sample_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.84, i64 0, ptr null, ptr @opt_lg_prof_interval_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.85, i64 0, ptr null, ptr @opt_prof_gdump_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.86, i64 0, ptr null, ptr @opt_prof_final_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.87, i64 0, ptr null, ptr @opt_prof_leak_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.88, i64 0, ptr null, ptr @opt_prof_leak_error_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.89, i64 0, ptr null, ptr @opt_prof_accum_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.90, i64 0, ptr null, ptr @opt_prof_recent_alloc_max_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.91, i64 0, ptr null, ptr @opt_prof_stats_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.92, i64 0, ptr null, ptr @opt_prof_sys_thread_name_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.93, i64 0, ptr null, ptr @opt_prof_time_res_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.94, i64 0, ptr null, ptr @opt_lg_san_uaf_align_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.95, i64 0, ptr null, ptr @opt_zero_realloc_ctl }], align 16
@.str.9 = private unnamed_addr constant [7 x i8] c"tcache\00", align 1
@tcache_node = internal constant [3 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.96, i64 0, ptr null, ptr @tcache_create_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.23, i64 0, ptr null, ptr @tcache_flush_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.97, i64 0, ptr null, ptr @tcache_destroy_ctl }], align 16
@.str.10 = private unnamed_addr constant [6 x i8] c"arena\00", align 1
@arena_node = internal constant [1 x %struct.ctl_indexed_node_s] [%struct.ctl_indexed_node_s { %struct.ctl_node_s zeroinitializer, ptr @arena_i_index }], align 16
@.str.11 = private unnamed_addr constant [7 x i8] c"arenas\00", align 1
@arenas_node = internal constant [13 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.56, i64 0, ptr null, ptr @arenas_narenas_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.60, i64 0, ptr null, ptr @arenas_dirty_decay_ms_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.61, i64 0, ptr null, ptr @arenas_muzzy_decay_ms_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.103, i64 0, ptr null, ptr @arenas_quantum_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.104, i64 0, ptr null, ptr @arenas_page_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.69, i64 0, ptr null, ptr @arenas_tcache_max_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.105, i64 0, ptr null, ptr @arenas_nbins_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.106, i64 0, ptr null, ptr @arenas_nhbins_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.107, i64 1, ptr @arenas_bin_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.108, i64 0, ptr null, ptr @arenas_nlextents_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.109, i64 1, ptr @arenas_lextent_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.96, i64 0, ptr null, ptr @arenas_create_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.110, i64 0, ptr null, ptr @arenas_lookup_ctl }], align 16
@.str.12 = private unnamed_addr constant [5 x i8] c"prof\00", align 1
@prof_node = internal constant [11 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.115, i64 0, ptr null, ptr @prof_thread_active_init_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.27, i64 0, ptr null, ptr @prof_active_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.116, i64 0, ptr null, ptr @prof_dump_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.117, i64 0, ptr null, ptr @prof_gdump_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.118, i64 0, ptr null, ptr @prof_prefix_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.25, i64 0, ptr null, ptr @prof_reset_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.119, i64 0, ptr null, ptr @prof_interval_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.120, i64 0, ptr null, ptr @lg_prof_sample_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.121, i64 0, ptr null, ptr @prof_log_start_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.122, i64 0, ptr null, ptr @prof_log_stop_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.13, i64 2, ptr @prof_stats_node, ptr null }], align 16
@.str.13 = private unnamed_addr constant [6 x i8] c"stats\00", align 1
@stats_node = internal constant [11 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.16, i64 0, ptr null, ptr @stats_allocated_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.27, i64 0, ptr null, ptr @stats_active_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.125, i64 0, ptr null, ptr @stats_metadata_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.53, i64 0, ptr null, ptr @stats_metadata_thp_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.126, i64 0, ptr null, ptr @stats_resident_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.127, i64 0, ptr null, ptr @stats_mapped_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.128, i64 0, ptr null, ptr @stats_retained_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.4, i64 3, ptr @stats_background_thread_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.129, i64 10, ptr @stats_mutexes_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.11, i64 1, ptr @stats_arenas_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.130, i64 0, ptr null, ptr @stats_zero_reallocs_ctl }], align 16
@.str.14 = private unnamed_addr constant [13 x i8] c"experimental\00", align 1
@experimental_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.206, i64 5, ptr @experimental_hooks_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.207, i64 2, ptr @experimental_utilization_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.11, i64 1, ptr @experimental_arenas_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.208, i64 0, ptr null, ptr @experimental_arenas_create_ext_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.209, i64 2, ptr @experimental_prof_recent_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.210, i64 0, ptr null, ptr @experimental_batch_alloc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.6, i64 1, ptr @experimental_thread_node, ptr null }], align 16
@.str.15 = private unnamed_addr constant [11 x i8] c"5.3.0-0-g0\00", align 1
@background_thread_enabled_state = external local_unnamed_addr global %struct.atomic_b_t, align 1
@max_background_threads = external local_unnamed_addr global i64, align 8
@opt_max_background_threads = external local_unnamed_addr global i64, align 8
@.str.16 = private unnamed_addr constant [10 x i8] c"allocated\00", align 1
@.str.17 = private unnamed_addr constant [11 x i8] c"allocatedp\00", align 1
@.str.18 = private unnamed_addr constant [12 x i8] c"deallocated\00", align 1
@.str.19 = private unnamed_addr constant [13 x i8] c"deallocatedp\00", align 1
@thread_tcache_node = internal constant [2 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.22, i64 0, ptr null, ptr @thread_tcache_enabled_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.23, i64 0, ptr null, ptr @thread_tcache_flush_ctl }], align 16
@.str.20 = private unnamed_addr constant [5 x i8] c"peak\00", align 1
@thread_peak_node = internal constant [2 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.24, i64 0, ptr null, ptr @thread_peak_read_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.25, i64 0, ptr null, ptr @thread_peak_reset_ctl }], align 16
@thread_prof_node = internal constant [2 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.26, i64 0, ptr null, ptr @thread_prof_name_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.27, i64 0, ptr null, ptr @thread_prof_active_ctl }], align 16
@.str.21 = private unnamed_addr constant [5 x i8] c"idle\00", align 1
@opt_percpu_arena = external local_unnamed_addr global i32, align 4
@ncpus = external local_unnamed_addr global i32, align 4
@.str.22 = private unnamed_addr constant [8 x i8] c"enabled\00", align 1
@.str.23 = private unnamed_addr constant [6 x i8] c"flush\00", align 1
@.str.24 = private unnamed_addr constant [5 x i8] c"read\00", align 1
@.str.25 = private unnamed_addr constant [6 x i8] c"reset\00", align 1
@.str.26 = private unnamed_addr constant [5 x i8] c"name\00", align 1
@.str.27 = private unnamed_addr constant [7 x i8] c"active\00", align 1
@opt_narenas = external local_unnamed_addr global i32, align 4
@.str.28 = private unnamed_addr constant [16 x i8] c"cache_oblivious\00", align 1
@.str.29 = private unnamed_addr constant [6 x i8] c"debug\00", align 1
@.str.30 = private unnamed_addr constant [5 x i8] c"fill\00", align 1
@.str.31 = private unnamed_addr constant [10 x i8] c"lazy_lock\00", align 1
@.str.32 = private unnamed_addr constant [12 x i8] c"malloc_conf\00", align 1
@.str.33 = private unnamed_addr constant [18 x i8] c"opt_safety_checks\00", align 1
@.str.34 = private unnamed_addr constant [12 x i8] c"prof_libgcc\00", align 1
@.str.35 = private unnamed_addr constant [15 x i8] c"prof_libunwind\00", align 1
@.str.36 = private unnamed_addr constant [7 x i8] c"utrace\00", align 1
@.str.37 = private unnamed_addr constant [8 x i8] c"xmalloc\00", align 1
@.str.38 = private unnamed_addr constant [6 x i8] c"abort\00", align 1
@.str.39 = private unnamed_addr constant [11 x i8] c"abort_conf\00", align 1
@.str.40 = private unnamed_addr constant [14 x i8] c"trust_madvise\00", align 1
@.str.41 = private unnamed_addr constant [13 x i8] c"confirm_conf\00", align 1
@.str.42 = private unnamed_addr constant [4 x i8] c"hpa\00", align 1
@.str.43 = private unnamed_addr constant [19 x i8] c"hpa_slab_max_alloc\00", align 1
@.str.44 = private unnamed_addr constant [27 x i8] c"hpa_hugification_threshold\00", align 1
@.str.45 = private unnamed_addr constant [20 x i8] c"hpa_hugify_delay_ms\00", align 1
@.str.46 = private unnamed_addr constant [26 x i8] c"hpa_min_purge_interval_ms\00", align 1
@.str.47 = private unnamed_addr constant [15 x i8] c"hpa_dirty_mult\00", align 1
@.str.48 = private unnamed_addr constant [16 x i8] c"hpa_sec_nshards\00", align 1
@.str.49 = private unnamed_addr constant [18 x i8] c"hpa_sec_max_alloc\00", align 1
@.str.50 = private unnamed_addr constant [18 x i8] c"hpa_sec_max_bytes\00", align 1
@.str.51 = private unnamed_addr constant [26 x i8] c"hpa_sec_bytes_after_flush\00", align 1
@.str.52 = private unnamed_addr constant [25 x i8] c"hpa_sec_batch_fill_extra\00", align 1
@.str.53 = private unnamed_addr constant [13 x i8] c"metadata_thp\00", align 1
@.str.54 = private unnamed_addr constant [7 x i8] c"retain\00", align 1
@.str.55 = private unnamed_addr constant [4 x i8] c"dss\00", align 1
@.str.56 = private unnamed_addr constant [8 x i8] c"narenas\00", align 1
@.str.57 = private unnamed_addr constant [13 x i8] c"percpu_arena\00", align 1
@.str.58 = private unnamed_addr constant [19 x i8] c"oversize_threshold\00", align 1
@.str.59 = private unnamed_addr constant [15 x i8] c"mutex_max_spin\00", align 1
@.str.60 = private unnamed_addr constant [15 x i8] c"dirty_decay_ms\00", align 1
@.str.61 = private unnamed_addr constant [15 x i8] c"muzzy_decay_ms\00", align 1
@.str.62 = private unnamed_addr constant [12 x i8] c"stats_print\00", align 1
@.str.63 = private unnamed_addr constant [17 x i8] c"stats_print_opts\00", align 1
@.str.64 = private unnamed_addr constant [15 x i8] c"stats_interval\00", align 1
@.str.65 = private unnamed_addr constant [20 x i8] c"stats_interval_opts\00", align 1
@.str.66 = private unnamed_addr constant [5 x i8] c"junk\00", align 1
@.str.67 = private unnamed_addr constant [5 x i8] c"zero\00", align 1
@.str.68 = private unnamed_addr constant [28 x i8] c"experimental_infallible_new\00", align 1
@.str.69 = private unnamed_addr constant [11 x i8] c"tcache_max\00", align 1
@.str.70 = private unnamed_addr constant [24 x i8] c"tcache_nslots_small_min\00", align 1
@.str.71 = private unnamed_addr constant [24 x i8] c"tcache_nslots_small_max\00", align 1
@.str.72 = private unnamed_addr constant [20 x i8] c"tcache_nslots_large\00", align 1
@.str.73 = private unnamed_addr constant [21 x i8] c"lg_tcache_nslots_mul\00", align 1
@.str.74 = private unnamed_addr constant [21 x i8] c"tcache_gc_incr_bytes\00", align 1
@.str.75 = private unnamed_addr constant [22 x i8] c"tcache_gc_delay_bytes\00", align 1
@.str.76 = private unnamed_addr constant [26 x i8] c"lg_tcache_flush_small_div\00", align 1
@.str.77 = private unnamed_addr constant [26 x i8] c"lg_tcache_flush_large_div\00", align 1
@.str.78 = private unnamed_addr constant [4 x i8] c"thp\00", align 1
@.str.79 = private unnamed_addr constant [25 x i8] c"lg_extent_max_active_fit\00", align 1
@.str.80 = private unnamed_addr constant [12 x i8] c"prof_prefix\00", align 1
@.str.81 = private unnamed_addr constant [12 x i8] c"prof_active\00", align 1
@.str.82 = private unnamed_addr constant [24 x i8] c"prof_thread_active_init\00", align 1
@.str.83 = private unnamed_addr constant [15 x i8] c"lg_prof_sample\00", align 1
@.str.84 = private unnamed_addr constant [17 x i8] c"lg_prof_interval\00", align 1
@.str.85 = private unnamed_addr constant [11 x i8] c"prof_gdump\00", align 1
@.str.86 = private unnamed_addr constant [11 x i8] c"prof_final\00", align 1
@.str.87 = private unnamed_addr constant [10 x i8] c"prof_leak\00", align 1
@.str.88 = private unnamed_addr constant [16 x i8] c"prof_leak_error\00", align 1
@.str.89 = private unnamed_addr constant [11 x i8] c"prof_accum\00", align 1
@.str.90 = private unnamed_addr constant [22 x i8] c"prof_recent_alloc_max\00", align 1
@.str.91 = private unnamed_addr constant [11 x i8] c"prof_stats\00", align 1
@.str.92 = private unnamed_addr constant [21 x i8] c"prof_sys_thread_name\00", align 1
@.str.93 = private unnamed_addr constant [21 x i8] c"prof_time_resolution\00", align 1
@.str.94 = private unnamed_addr constant [17 x i8] c"lg_san_uaf_align\00", align 1
@.str.95 = private unnamed_addr constant [13 x i8] c"zero_realloc\00", align 1
@opt_abort = external local_unnamed_addr global i8, align 1
@opt_abort_conf = external local_unnamed_addr global i8, align 1
@opt_cache_oblivious = external local_unnamed_addr global i8, align 1
@opt_trust_madvise = external local_unnamed_addr global i8, align 1
@opt_confirm_conf = external local_unnamed_addr global i8, align 1
@opt_hpa = external local_unnamed_addr global i8, align 1
@opt_hpa_opts = external local_unnamed_addr global %struct.hpa_shard_opts_s, align 8
@opt_hpa_sec_opts = external local_unnamed_addr global %struct.sec_opts_s, align 8
@metadata_thp_mode_names = external local_unnamed_addr global [0 x ptr], align 8
@opt_metadata_thp = external local_unnamed_addr global i32, align 4
@opt_retain = external local_unnamed_addr global i8, align 1
@opt_dss = external local_unnamed_addr global ptr, align 8
@percpu_arena_mode_names = external local_unnamed_addr global [0 x ptr], align 8
@opt_oversize_threshold = external local_unnamed_addr global i64, align 8
@opt_mutex_max_spin = external local_unnamed_addr global i64, align 8
@opt_background_thread = external local_unnamed_addr global i8, align 1
@opt_dirty_decay_ms = external local_unnamed_addr global i64, align 8
@opt_muzzy_decay_ms = external local_unnamed_addr global i64, align 8
@opt_stats_print = external local_unnamed_addr global i8, align 1
@opt_stats_print_opts = external global [11 x i8], align 1
@opt_stats_interval = external local_unnamed_addr global i64, align 8
@opt_stats_interval_opts = external global [11 x i8], align 1
@opt_junk = external local_unnamed_addr global ptr, align 8
@opt_zero = external local_unnamed_addr global i8, align 1
@opt_tcache = external local_unnamed_addr global i8, align 1
@opt_tcache_max = external local_unnamed_addr global i64, align 8
@opt_tcache_nslots_small_min = external local_unnamed_addr global i32, align 4
@opt_tcache_nslots_small_max = external local_unnamed_addr global i32, align 4
@opt_tcache_nslots_large = external local_unnamed_addr global i32, align 4
@opt_lg_tcache_nslots_mul = external local_unnamed_addr global i64, align 8
@opt_tcache_gc_incr_bytes = external local_unnamed_addr global i64, align 8
@opt_tcache_gc_delay_bytes = external local_unnamed_addr global i64, align 8
@opt_lg_tcache_flush_small_div = external local_unnamed_addr global i32, align 4
@opt_lg_tcache_flush_large_div = external local_unnamed_addr global i32, align 4
@thp_mode_names = external local_unnamed_addr global [0 x ptr], align 8
@opt_thp = external local_unnamed_addr global i32, align 4
@opt_lg_extent_max_active_fit = external local_unnamed_addr global i64, align 8
@zero_realloc_mode_names = external local_unnamed_addr global [0 x ptr], align 8
@opt_zero_realloc_action = external local_unnamed_addr global i32, align 4
@.str.96 = private unnamed_addr constant [7 x i8] c"create\00", align 1
@.str.97 = private unnamed_addr constant [8 x i8] c"destroy\00", align 1
@super_arena_i_node = internal constant [1 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.1, i64 11, ptr @arena_i_node, ptr null }], align 16
@arena_i_node = internal constant [11 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.98, i64 0, ptr null, ptr @arena_i_initialized_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.99, i64 0, ptr null, ptr @arena_i_decay_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.100, i64 0, ptr null, ptr @arena_i_purge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.25, i64 0, ptr null, ptr @arena_i_reset_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.97, i64 0, ptr null, ptr @arena_i_destroy_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.55, i64 0, ptr null, ptr @arena_i_dss_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.58, i64 0, ptr null, ptr @arena_i_oversize_threshold_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.60, i64 0, ptr null, ptr @arena_i_dirty_decay_ms_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.61, i64 0, ptr null, ptr @arena_i_muzzy_decay_ms_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.101, i64 0, ptr null, ptr @arena_i_extent_hooks_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.102, i64 0, ptr null, ptr @arena_i_retain_grow_limit_ctl }], align 16
@.str.98 = private unnamed_addr constant [12 x i8] c"initialized\00", align 1
@.str.99 = private unnamed_addr constant [6 x i8] c"decay\00", align 1
@.str.100 = private unnamed_addr constant [6 x i8] c"purge\00", align 1
@.str.101 = private unnamed_addr constant [13 x i8] c"extent_hooks\00", align 1
@.str.102 = private unnamed_addr constant [18 x i8] c"retain_grow_limit\00", align 1
@manual_arena_base = external local_unnamed_addr global i32, align 4
@background_thread_info = external local_unnamed_addr global ptr, align 8
@narenas_auto = external local_unnamed_addr global i32, align 4
@ehooks_default_extent_hooks = external constant %struct.extent_hooks_s, align 8
@.str.103 = private unnamed_addr constant [8 x i8] c"quantum\00", align 1
@.str.104 = private unnamed_addr constant [5 x i8] c"page\00", align 1
@.str.105 = private unnamed_addr constant [6 x i8] c"nbins\00", align 1
@.str.106 = private unnamed_addr constant [7 x i8] c"nhbins\00", align 1
@.str.107 = private unnamed_addr constant [4 x i8] c"bin\00", align 1
@arenas_bin_node = internal constant [1 x %struct.ctl_indexed_node_s] [%struct.ctl_indexed_node_s { %struct.ctl_node_s zeroinitializer, ptr @arenas_bin_i_index }], align 16
@.str.108 = private unnamed_addr constant [10 x i8] c"nlextents\00", align 1
@.str.109 = private unnamed_addr constant [8 x i8] c"lextent\00", align 1
@arenas_lextent_node = internal constant [1 x %struct.ctl_indexed_node_s] [%struct.ctl_indexed_node_s { %struct.ctl_node_s zeroinitializer, ptr @arenas_lextent_i_index }], align 16
@.str.110 = private unnamed_addr constant [7 x i8] c"lookup\00", align 1
@tcache_maxclass = external local_unnamed_addr global i64, align 8
@nhbins = external local_unnamed_addr global i32, align 4
@super_arenas_bin_i_node = internal constant [1 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.1, i64 4, ptr @arenas_bin_i_node, ptr null }], align 16
@arenas_bin_i_node = internal constant [4 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.111, i64 0, ptr null, ptr @arenas_bin_i_size_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.112, i64 0, ptr null, ptr @arenas_bin_i_nregs_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.113, i64 0, ptr null, ptr @arenas_bin_i_slab_size_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.114, i64 0, ptr null, ptr @arenas_bin_i_nshards_ctl }], align 16
@.str.111 = private unnamed_addr constant [5 x i8] c"size\00", align 1
@.str.112 = private unnamed_addr constant [6 x i8] c"nregs\00", align 1
@.str.113 = private unnamed_addr constant [10 x i8] c"slab_size\00", align 1
@.str.114 = private unnamed_addr constant [8 x i8] c"nshards\00", align 1
@bin_infos = external local_unnamed_addr global [39 x %struct.bin_info_s], align 16
@super_arenas_lextent_i_node = internal constant [1 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.1, i64 1, ptr @arenas_lextent_i_node, ptr null }], align 16
@arenas_lextent_i_node = internal constant [1 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.111, i64 0, ptr null, ptr @arenas_lextent_i_size_ctl }], align 16
@arena_emap_global = external global %struct.emap_s, align 8
@.str.115 = private unnamed_addr constant [19 x i8] c"thread_active_init\00", align 1
@.str.116 = private unnamed_addr constant [5 x i8] c"dump\00", align 1
@.str.117 = private unnamed_addr constant [6 x i8] c"gdump\00", align 1
@.str.118 = private unnamed_addr constant [7 x i8] c"prefix\00", align 1
@.str.119 = private unnamed_addr constant [9 x i8] c"interval\00", align 1
@.str.120 = private unnamed_addr constant [10 x i8] c"lg_sample\00", align 1
@.str.121 = private unnamed_addr constant [10 x i8] c"log_start\00", align 1
@.str.122 = private unnamed_addr constant [9 x i8] c"log_stop\00", align 1
@prof_stats_node = internal constant [2 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.123, i64 1, ptr @prof_stats_bins_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.124, i64 1, ptr @prof_stats_lextents_node, ptr null }], align 16
@.str.123 = private unnamed_addr constant [5 x i8] c"bins\00", align 1
@prof_stats_bins_node = internal constant [1 x %struct.ctl_indexed_node_s] [%struct.ctl_indexed_node_s { %struct.ctl_node_s zeroinitializer, ptr @prof_stats_bins_i_index }], align 16
@.str.124 = private unnamed_addr constant [9 x i8] c"lextents\00", align 1
@prof_stats_lextents_node = internal constant [1 x %struct.ctl_indexed_node_s] [%struct.ctl_indexed_node_s { %struct.ctl_node_s zeroinitializer, ptr @prof_stats_lextents_i_index }], align 16
@.str.125 = private unnamed_addr constant [9 x i8] c"metadata\00", align 1
@.str.126 = private unnamed_addr constant [9 x i8] c"resident\00", align 1
@.str.127 = private unnamed_addr constant [7 x i8] c"mapped\00", align 1
@.str.128 = private unnamed_addr constant [9 x i8] c"retained\00", align 1
@stats_background_thread_node = internal constant [3 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.131, i64 0, ptr null, ptr @stats_background_thread_num_threads_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.132, i64 0, ptr null, ptr @stats_background_thread_num_runs_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.133, i64 0, ptr null, ptr @stats_background_thread_run_interval_ctl }], align 16
@.str.129 = private unnamed_addr constant [8 x i8] c"mutexes\00", align 1
@stats_mutexes_node = internal constant [10 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.4, i64 7, ptr @stats_mutexes_background_thread_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.134, i64 7, ptr @stats_mutexes_max_per_bg_thd_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str, i64 7, ptr @stats_mutexes_ctl_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.12, i64 7, ptr @stats_mutexes_prof_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.135, i64 7, ptr @stats_mutexes_prof_thds_data_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.136, i64 7, ptr @stats_mutexes_prof_dump_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.137, i64 7, ptr @stats_mutexes_prof_recent_alloc_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.138, i64 7, ptr @stats_mutexes_prof_recent_dump_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.91, i64 7, ptr @stats_mutexes_prof_stats_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.25, i64 0, ptr null, ptr @stats_mutexes_reset_ctl }], align 16
@stats_arenas_node = internal constant [1 x %struct.ctl_indexed_node_s] [%struct.ctl_indexed_node_s { %struct.ctl_node_s zeroinitializer, ptr @stats_arenas_i_index }], align 16
@.str.130 = private unnamed_addr constant [14 x i8] c"zero_reallocs\00", align 1
@.str.131 = private unnamed_addr constant [12 x i8] c"num_threads\00", align 1
@.str.132 = private unnamed_addr constant [9 x i8] c"num_runs\00", align 1
@.str.133 = private unnamed_addr constant [13 x i8] c"run_interval\00", align 1
@stats_mutexes_background_thread_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.139, i64 0, ptr null, ptr @stats_mutexes_background_thread_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.140, i64 0, ptr null, ptr @stats_mutexes_background_thread_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.141, i64 0, ptr null, ptr @stats_mutexes_background_thread_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.142, i64 0, ptr null, ptr @stats_mutexes_background_thread_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.143, i64 0, ptr null, ptr @stats_mutexes_background_thread_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.144, i64 0, ptr null, ptr @stats_mutexes_background_thread_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.145, i64 0, ptr null, ptr @stats_mutexes_background_thread_max_num_thds_ctl }], align 16
@.str.134 = private unnamed_addr constant [15 x i8] c"max_per_bg_thd\00", align 1
@stats_mutexes_max_per_bg_thd_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.139, i64 0, ptr null, ptr @stats_mutexes_max_per_bg_thd_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.140, i64 0, ptr null, ptr @stats_mutexes_max_per_bg_thd_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.141, i64 0, ptr null, ptr @stats_mutexes_max_per_bg_thd_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.142, i64 0, ptr null, ptr @stats_mutexes_max_per_bg_thd_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.143, i64 0, ptr null, ptr @stats_mutexes_max_per_bg_thd_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.144, i64 0, ptr null, ptr @stats_mutexes_max_per_bg_thd_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.145, i64 0, ptr null, ptr @stats_mutexes_max_per_bg_thd_max_num_thds_ctl }], align 16
@stats_mutexes_ctl_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.139, i64 0, ptr null, ptr @stats_mutexes_ctl_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.140, i64 0, ptr null, ptr @stats_mutexes_ctl_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.141, i64 0, ptr null, ptr @stats_mutexes_ctl_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.142, i64 0, ptr null, ptr @stats_mutexes_ctl_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.143, i64 0, ptr null, ptr @stats_mutexes_ctl_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.144, i64 0, ptr null, ptr @stats_mutexes_ctl_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.145, i64 0, ptr null, ptr @stats_mutexes_ctl_max_num_thds_ctl }], align 16
@stats_mutexes_prof_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.139, i64 0, ptr null, ptr @stats_mutexes_prof_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.140, i64 0, ptr null, ptr @stats_mutexes_prof_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.141, i64 0, ptr null, ptr @stats_mutexes_prof_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.142, i64 0, ptr null, ptr @stats_mutexes_prof_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.143, i64 0, ptr null, ptr @stats_mutexes_prof_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.144, i64 0, ptr null, ptr @stats_mutexes_prof_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.145, i64 0, ptr null, ptr @stats_mutexes_prof_max_num_thds_ctl }], align 16
@.str.135 = private unnamed_addr constant [15 x i8] c"prof_thds_data\00", align 1
@stats_mutexes_prof_thds_data_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.139, i64 0, ptr null, ptr @stats_mutexes_prof_thds_data_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.140, i64 0, ptr null, ptr @stats_mutexes_prof_thds_data_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.141, i64 0, ptr null, ptr @stats_mutexes_prof_thds_data_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.142, i64 0, ptr null, ptr @stats_mutexes_prof_thds_data_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.143, i64 0, ptr null, ptr @stats_mutexes_prof_thds_data_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.144, i64 0, ptr null, ptr @stats_mutexes_prof_thds_data_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.145, i64 0, ptr null, ptr @stats_mutexes_prof_thds_data_max_num_thds_ctl }], align 16
@.str.136 = private unnamed_addr constant [10 x i8] c"prof_dump\00", align 1
@stats_mutexes_prof_dump_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.139, i64 0, ptr null, ptr @stats_mutexes_prof_dump_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.140, i64 0, ptr null, ptr @stats_mutexes_prof_dump_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.141, i64 0, ptr null, ptr @stats_mutexes_prof_dump_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.142, i64 0, ptr null, ptr @stats_mutexes_prof_dump_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.143, i64 0, ptr null, ptr @stats_mutexes_prof_dump_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.144, i64 0, ptr null, ptr @stats_mutexes_prof_dump_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.145, i64 0, ptr null, ptr @stats_mutexes_prof_dump_max_num_thds_ctl }], align 16
@.str.137 = private unnamed_addr constant [18 x i8] c"prof_recent_alloc\00", align 1
@stats_mutexes_prof_recent_alloc_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.139, i64 0, ptr null, ptr @stats_mutexes_prof_recent_alloc_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.140, i64 0, ptr null, ptr @stats_mutexes_prof_recent_alloc_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.141, i64 0, ptr null, ptr @stats_mutexes_prof_recent_alloc_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.142, i64 0, ptr null, ptr @stats_mutexes_prof_recent_alloc_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.143, i64 0, ptr null, ptr @stats_mutexes_prof_recent_alloc_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.144, i64 0, ptr null, ptr @stats_mutexes_prof_recent_alloc_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.145, i64 0, ptr null, ptr @stats_mutexes_prof_recent_alloc_max_num_thds_ctl }], align 16
@.str.138 = private unnamed_addr constant [17 x i8] c"prof_recent_dump\00", align 1
@stats_mutexes_prof_recent_dump_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.139, i64 0, ptr null, ptr @stats_mutexes_prof_recent_dump_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.140, i64 0, ptr null, ptr @stats_mutexes_prof_recent_dump_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.141, i64 0, ptr null, ptr @stats_mutexes_prof_recent_dump_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.142, i64 0, ptr null, ptr @stats_mutexes_prof_recent_dump_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.143, i64 0, ptr null, ptr @stats_mutexes_prof_recent_dump_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.144, i64 0, ptr null, ptr @stats_mutexes_prof_recent_dump_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.145, i64 0, ptr null, ptr @stats_mutexes_prof_recent_dump_max_num_thds_ctl }], align 16
@stats_mutexes_prof_stats_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.139, i64 0, ptr null, ptr @stats_mutexes_prof_stats_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.140, i64 0, ptr null, ptr @stats_mutexes_prof_stats_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.141, i64 0, ptr null, ptr @stats_mutexes_prof_stats_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.142, i64 0, ptr null, ptr @stats_mutexes_prof_stats_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.143, i64 0, ptr null, ptr @stats_mutexes_prof_stats_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.144, i64 0, ptr null, ptr @stats_mutexes_prof_stats_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.145, i64 0, ptr null, ptr @stats_mutexes_prof_stats_max_num_thds_ctl }], align 16
@.str.139 = private unnamed_addr constant [8 x i8] c"num_ops\00", align 1
@.str.140 = private unnamed_addr constant [9 x i8] c"num_wait\00", align 1
@.str.141 = private unnamed_addr constant [13 x i8] c"num_spin_acq\00", align 1
@.str.142 = private unnamed_addr constant [17 x i8] c"num_owner_switch\00", align 1
@.str.143 = private unnamed_addr constant [16 x i8] c"total_wait_time\00", align 1
@.str.144 = private unnamed_addr constant [14 x i8] c"max_wait_time\00", align 1
@.str.145 = private unnamed_addr constant [13 x i8] c"max_num_thds\00", align 1
@arena_bin_offsets = external local_unnamed_addr global [39 x i32], align 16
@super_stats_arenas_i_node = internal constant [1 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.1, i64 32, ptr @stats_arenas_i_node, ptr null }], align 16
@stats_arenas_i_node = internal constant [32 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.146, i64 0, ptr null, ptr @stats_arenas_i_nthreads_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.147, i64 0, ptr null, ptr @stats_arenas_i_uptime_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.55, i64 0, ptr null, ptr @stats_arenas_i_dss_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.60, i64 0, ptr null, ptr @stats_arenas_i_dirty_decay_ms_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.61, i64 0, ptr null, ptr @stats_arenas_i_muzzy_decay_ms_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.148, i64 0, ptr null, ptr @stats_arenas_i_pactive_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.149, i64 0, ptr null, ptr @stats_arenas_i_pdirty_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.150, i64 0, ptr null, ptr @stats_arenas_i_pmuzzy_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.127, i64 0, ptr null, ptr @stats_arenas_i_mapped_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.128, i64 0, ptr null, ptr @stats_arenas_i_retained_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.151, i64 0, ptr null, ptr @stats_arenas_i_extent_avail_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.152, i64 0, ptr null, ptr @stats_arenas_i_dirty_npurge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.153, i64 0, ptr null, ptr @stats_arenas_i_dirty_nmadvise_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.154, i64 0, ptr null, ptr @stats_arenas_i_dirty_purged_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.155, i64 0, ptr null, ptr @stats_arenas_i_muzzy_npurge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.156, i64 0, ptr null, ptr @stats_arenas_i_muzzy_nmadvise_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.157, i64 0, ptr null, ptr @stats_arenas_i_muzzy_purged_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.158, i64 0, ptr null, ptr @stats_arenas_i_base_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.159, i64 0, ptr null, ptr @stats_arenas_i_internal_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.53, i64 0, ptr null, ptr @stats_arenas_i_metadata_thp_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.160, i64 0, ptr null, ptr @stats_arenas_i_tcache_bytes_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.161, i64 0, ptr null, ptr @stats_arenas_i_tcache_stashed_bytes_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.126, i64 0, ptr null, ptr @stats_arenas_i_resident_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.162, i64 0, ptr null, ptr @stats_arenas_i_abandoned_vm_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.163, i64 0, ptr null, ptr @stats_arenas_i_hpa_sec_bytes_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.164, i64 6, ptr @stats_arenas_i_small_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.165, i64 6, ptr @stats_arenas_i_large_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.123, i64 1, ptr @stats_arenas_i_bins_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.124, i64 1, ptr @stats_arenas_i_lextents_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.166, i64 1, ptr @stats_arenas_i_extents_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.129, i64 12, ptr @stats_arenas_i_mutexes_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.167, i64 7, ptr @stats_arenas_i_hpa_shard_node, ptr null }], align 16
@.str.146 = private unnamed_addr constant [9 x i8] c"nthreads\00", align 1
@.str.147 = private unnamed_addr constant [7 x i8] c"uptime\00", align 1
@.str.148 = private unnamed_addr constant [8 x i8] c"pactive\00", align 1
@.str.149 = private unnamed_addr constant [7 x i8] c"pdirty\00", align 1
@.str.150 = private unnamed_addr constant [7 x i8] c"pmuzzy\00", align 1
@.str.151 = private unnamed_addr constant [13 x i8] c"extent_avail\00", align 1
@.str.152 = private unnamed_addr constant [13 x i8] c"dirty_npurge\00", align 1
@.str.153 = private unnamed_addr constant [15 x i8] c"dirty_nmadvise\00", align 1
@.str.154 = private unnamed_addr constant [13 x i8] c"dirty_purged\00", align 1
@.str.155 = private unnamed_addr constant [13 x i8] c"muzzy_npurge\00", align 1
@.str.156 = private unnamed_addr constant [15 x i8] c"muzzy_nmadvise\00", align 1
@.str.157 = private unnamed_addr constant [13 x i8] c"muzzy_purged\00", align 1
@.str.158 = private unnamed_addr constant [5 x i8] c"base\00", align 1
@.str.159 = private unnamed_addr constant [9 x i8] c"internal\00", align 1
@.str.160 = private unnamed_addr constant [13 x i8] c"tcache_bytes\00", align 1
@.str.161 = private unnamed_addr constant [21 x i8] c"tcache_stashed_bytes\00", align 1
@.str.162 = private unnamed_addr constant [13 x i8] c"abandoned_vm\00", align 1
@.str.163 = private unnamed_addr constant [14 x i8] c"hpa_sec_bytes\00", align 1
@.str.164 = private unnamed_addr constant [6 x i8] c"small\00", align 1
@stats_arenas_i_small_node = internal constant [6 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.16, i64 0, ptr null, ptr @stats_arenas_i_small_allocated_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.168, i64 0, ptr null, ptr @stats_arenas_i_small_nmalloc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.169, i64 0, ptr null, ptr @stats_arenas_i_small_ndalloc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.170, i64 0, ptr null, ptr @stats_arenas_i_small_nrequests_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.171, i64 0, ptr null, ptr @stats_arenas_i_small_nfills_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.172, i64 0, ptr null, ptr @stats_arenas_i_small_nflushes_ctl }], align 16
@.str.165 = private unnamed_addr constant [6 x i8] c"large\00", align 1
@stats_arenas_i_large_node = internal constant [6 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.16, i64 0, ptr null, ptr @stats_arenas_i_large_allocated_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.168, i64 0, ptr null, ptr @stats_arenas_i_large_nmalloc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.169, i64 0, ptr null, ptr @stats_arenas_i_large_ndalloc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.170, i64 0, ptr null, ptr @stats_arenas_i_large_nrequests_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.171, i64 0, ptr null, ptr @stats_arenas_i_large_nfills_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.172, i64 0, ptr null, ptr @stats_arenas_i_large_nflushes_ctl }], align 16
@stats_arenas_i_bins_node = internal constant [1 x %struct.ctl_indexed_node_s] [%struct.ctl_indexed_node_s { %struct.ctl_node_s zeroinitializer, ptr @stats_arenas_i_bins_j_index }], align 16
@stats_arenas_i_lextents_node = internal constant [1 x %struct.ctl_indexed_node_s] [%struct.ctl_indexed_node_s { %struct.ctl_node_s zeroinitializer, ptr @stats_arenas_i_lextents_j_index }], align 16
@.str.166 = private unnamed_addr constant [8 x i8] c"extents\00", align 1
@stats_arenas_i_extents_node = internal constant [1 x %struct.ctl_indexed_node_s] [%struct.ctl_indexed_node_s { %struct.ctl_node_s zeroinitializer, ptr @stats_arenas_i_extents_j_index }], align 16
@stats_arenas_i_mutexes_node = internal constant [12 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.165, i64 7, ptr @stats_arenas_i_mutexes_large_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.151, i64 7, ptr @stats_arenas_i_mutexes_extent_avail_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.186, i64 7, ptr @stats_arenas_i_mutexes_extents_dirty_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.187, i64 7, ptr @stats_arenas_i_mutexes_extents_muzzy_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.188, i64 7, ptr @stats_arenas_i_mutexes_extents_retained_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.189, i64 7, ptr @stats_arenas_i_mutexes_decay_dirty_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.190, i64 7, ptr @stats_arenas_i_mutexes_decay_muzzy_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.158, i64 7, ptr @stats_arenas_i_mutexes_base_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.191, i64 7, ptr @stats_arenas_i_mutexes_tcache_list_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.167, i64 7, ptr @stats_arenas_i_mutexes_hpa_shard_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.192, i64 7, ptr @stats_arenas_i_mutexes_hpa_shard_grow_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.193, i64 7, ptr @stats_arenas_i_mutexes_hpa_sec_node, ptr null }], align 16
@.str.167 = private unnamed_addr constant [10 x i8] c"hpa_shard\00", align 1
@stats_arenas_i_hpa_shard_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.194, i64 6, ptr @stats_arenas_i_hpa_shard_full_slabs_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.195, i64 6, ptr @stats_arenas_i_hpa_shard_empty_slabs_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.177, i64 1, ptr @stats_arenas_i_hpa_shard_nonfull_slabs_node, ptr null }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.196, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_npurge_passes_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.197, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_npurges_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.198, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_nhugifies_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.199, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_ndehugifies_ctl }], align 16
@.str.168 = private unnamed_addr constant [8 x i8] c"nmalloc\00", align 1
@.str.169 = private unnamed_addr constant [8 x i8] c"ndalloc\00", align 1
@.str.170 = private unnamed_addr constant [10 x i8] c"nrequests\00", align 1
@.str.171 = private unnamed_addr constant [7 x i8] c"nfills\00", align 1
@.str.172 = private unnamed_addr constant [9 x i8] c"nflushes\00", align 1
@super_stats_arenas_i_bins_j_node = internal constant [1 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.1, i64 11, ptr @stats_arenas_i_bins_j_node, ptr null }], align 16
@stats_arenas_i_bins_j_node = internal constant [11 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.168, i64 0, ptr null, ptr @stats_arenas_i_bins_j_nmalloc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.169, i64 0, ptr null, ptr @stats_arenas_i_bins_j_ndalloc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.170, i64 0, ptr null, ptr @stats_arenas_i_bins_j_nrequests_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.173, i64 0, ptr null, ptr @stats_arenas_i_bins_j_curregs_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.171, i64 0, ptr null, ptr @stats_arenas_i_bins_j_nfills_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.172, i64 0, ptr null, ptr @stats_arenas_i_bins_j_nflushes_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.174, i64 0, ptr null, ptr @stats_arenas_i_bins_j_nslabs_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.175, i64 0, ptr null, ptr @stats_arenas_i_bins_j_nreslabs_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.176, i64 0, ptr null, ptr @stats_arenas_i_bins_j_curslabs_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.177, i64 0, ptr null, ptr @stats_arenas_i_bins_j_nonfull_slabs_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.178, i64 7, ptr @stats_arenas_i_bins_j_mutex_node, ptr null }], align 16
@.str.173 = private unnamed_addr constant [8 x i8] c"curregs\00", align 1
@.str.174 = private unnamed_addr constant [7 x i8] c"nslabs\00", align 1
@.str.175 = private unnamed_addr constant [9 x i8] c"nreslabs\00", align 1
@.str.176 = private unnamed_addr constant [9 x i8] c"curslabs\00", align 1
@.str.177 = private unnamed_addr constant [14 x i8] c"nonfull_slabs\00", align 1
@.str.178 = private unnamed_addr constant [6 x i8] c"mutex\00", align 1
@stats_arenas_i_bins_j_mutex_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.139, i64 0, ptr null, ptr @stats_arenas_i_bins_j_mutex_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.140, i64 0, ptr null, ptr @stats_arenas_i_bins_j_mutex_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.141, i64 0, ptr null, ptr @stats_arenas_i_bins_j_mutex_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.142, i64 0, ptr null, ptr @stats_arenas_i_bins_j_mutex_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.143, i64 0, ptr null, ptr @stats_arenas_i_bins_j_mutex_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.144, i64 0, ptr null, ptr @stats_arenas_i_bins_j_mutex_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.145, i64 0, ptr null, ptr @stats_arenas_i_bins_j_mutex_max_num_thds_ctl }], align 16
@super_stats_arenas_i_lextents_j_node = internal constant [1 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.1, i64 4, ptr @stats_arenas_i_lextents_j_node, ptr null }], align 16
@stats_arenas_i_lextents_j_node = internal constant [4 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.168, i64 0, ptr null, ptr @stats_arenas_i_lextents_j_nmalloc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.169, i64 0, ptr null, ptr @stats_arenas_i_lextents_j_ndalloc_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.170, i64 0, ptr null, ptr @stats_arenas_i_lextents_j_nrequests_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.179, i64 0, ptr null, ptr @stats_arenas_i_lextents_j_curlextents_ctl }], align 16
@.str.179 = private unnamed_addr constant [12 x i8] c"curlextents\00", align 1
@super_stats_arenas_i_extents_j_node = internal constant [1 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.1, i64 6, ptr @stats_arenas_i_extents_j_node, ptr null }], align 16
@stats_arenas_i_extents_j_node = internal constant [6 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.180, i64 0, ptr null, ptr @stats_arenas_i_extents_j_ndirty_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.181, i64 0, ptr null, ptr @stats_arenas_i_extents_j_nmuzzy_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.182, i64 0, ptr null, ptr @stats_arenas_i_extents_j_nretained_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.183, i64 0, ptr null, ptr @stats_arenas_i_extents_j_dirty_bytes_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.184, i64 0, ptr null, ptr @stats_arenas_i_extents_j_muzzy_bytes_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.185, i64 0, ptr null, ptr @stats_arenas_i_extents_j_retained_bytes_ctl }], align 16
@.str.180 = private unnamed_addr constant [7 x i8] c"ndirty\00", align 1
@.str.181 = private unnamed_addr constant [7 x i8] c"nmuzzy\00", align 1
@.str.182 = private unnamed_addr constant [10 x i8] c"nretained\00", align 1
@.str.183 = private unnamed_addr constant [12 x i8] c"dirty_bytes\00", align 1
@.str.184 = private unnamed_addr constant [12 x i8] c"muzzy_bytes\00", align 1
@.str.185 = private unnamed_addr constant [15 x i8] c"retained_bytes\00", align 1
@stats_arenas_i_mutexes_large_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.139, i64 0, ptr null, ptr @stats_arenas_i_mutexes_large_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.140, i64 0, ptr null, ptr @stats_arenas_i_mutexes_large_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.141, i64 0, ptr null, ptr @stats_arenas_i_mutexes_large_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.142, i64 0, ptr null, ptr @stats_arenas_i_mutexes_large_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.143, i64 0, ptr null, ptr @stats_arenas_i_mutexes_large_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.144, i64 0, ptr null, ptr @stats_arenas_i_mutexes_large_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.145, i64 0, ptr null, ptr @stats_arenas_i_mutexes_large_max_num_thds_ctl }], align 16
@stats_arenas_i_mutexes_extent_avail_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.139, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extent_avail_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.140, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extent_avail_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.141, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extent_avail_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.142, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extent_avail_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.143, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extent_avail_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.144, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extent_avail_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.145, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extent_avail_max_num_thds_ctl }], align 16
@.str.186 = private unnamed_addr constant [14 x i8] c"extents_dirty\00", align 1
@stats_arenas_i_mutexes_extents_dirty_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.139, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_dirty_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.140, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_dirty_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.141, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_dirty_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.142, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_dirty_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.143, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_dirty_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.144, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_dirty_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.145, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_dirty_max_num_thds_ctl }], align 16
@.str.187 = private unnamed_addr constant [14 x i8] c"extents_muzzy\00", align 1
@stats_arenas_i_mutexes_extents_muzzy_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.139, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_muzzy_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.140, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_muzzy_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.141, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_muzzy_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.142, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_muzzy_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.143, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_muzzy_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.144, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_muzzy_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.145, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_muzzy_max_num_thds_ctl }], align 16
@.str.188 = private unnamed_addr constant [17 x i8] c"extents_retained\00", align 1
@stats_arenas_i_mutexes_extents_retained_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.139, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_retained_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.140, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_retained_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.141, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_retained_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.142, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_retained_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.143, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_retained_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.144, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_retained_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.145, i64 0, ptr null, ptr @stats_arenas_i_mutexes_extents_retained_max_num_thds_ctl }], align 16
@.str.189 = private unnamed_addr constant [12 x i8] c"decay_dirty\00", align 1
@stats_arenas_i_mutexes_decay_dirty_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.139, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_dirty_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.140, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_dirty_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.141, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_dirty_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.142, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_dirty_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.143, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_dirty_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.144, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_dirty_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.145, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_dirty_max_num_thds_ctl }], align 16
@.str.190 = private unnamed_addr constant [12 x i8] c"decay_muzzy\00", align 1
@stats_arenas_i_mutexes_decay_muzzy_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.139, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_muzzy_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.140, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_muzzy_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.141, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_muzzy_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.142, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_muzzy_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.143, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_muzzy_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.144, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_muzzy_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.145, i64 0, ptr null, ptr @stats_arenas_i_mutexes_decay_muzzy_max_num_thds_ctl }], align 16
@stats_arenas_i_mutexes_base_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.139, i64 0, ptr null, ptr @stats_arenas_i_mutexes_base_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.140, i64 0, ptr null, ptr @stats_arenas_i_mutexes_base_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.141, i64 0, ptr null, ptr @stats_arenas_i_mutexes_base_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.142, i64 0, ptr null, ptr @stats_arenas_i_mutexes_base_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.143, i64 0, ptr null, ptr @stats_arenas_i_mutexes_base_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.144, i64 0, ptr null, ptr @stats_arenas_i_mutexes_base_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.145, i64 0, ptr null, ptr @stats_arenas_i_mutexes_base_max_num_thds_ctl }], align 16
@.str.191 = private unnamed_addr constant [12 x i8] c"tcache_list\00", align 1
@stats_arenas_i_mutexes_tcache_list_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.139, i64 0, ptr null, ptr @stats_arenas_i_mutexes_tcache_list_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.140, i64 0, ptr null, ptr @stats_arenas_i_mutexes_tcache_list_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.141, i64 0, ptr null, ptr @stats_arenas_i_mutexes_tcache_list_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.142, i64 0, ptr null, ptr @stats_arenas_i_mutexes_tcache_list_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.143, i64 0, ptr null, ptr @stats_arenas_i_mutexes_tcache_list_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.144, i64 0, ptr null, ptr @stats_arenas_i_mutexes_tcache_list_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.145, i64 0, ptr null, ptr @stats_arenas_i_mutexes_tcache_list_max_num_thds_ctl }], align 16
@stats_arenas_i_mutexes_hpa_shard_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.139, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.140, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.141, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.142, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.143, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.144, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.145, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_max_num_thds_ctl }], align 16
@.str.192 = private unnamed_addr constant [15 x i8] c"hpa_shard_grow\00", align 1
@stats_arenas_i_mutexes_hpa_shard_grow_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.139, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_grow_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.140, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_grow_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.141, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_grow_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.142, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_grow_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.143, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_grow_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.144, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_grow_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.145, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_shard_grow_max_num_thds_ctl }], align 16
@.str.193 = private unnamed_addr constant [8 x i8] c"hpa_sec\00", align 1
@stats_arenas_i_mutexes_hpa_sec_node = internal constant [7 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.139, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_sec_num_ops_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.140, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_sec_num_wait_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.141, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_sec_num_spin_acq_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.142, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_sec_num_owner_switch_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.143, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_sec_total_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.144, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_sec_max_wait_time_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.145, i64 0, ptr null, ptr @stats_arenas_i_mutexes_hpa_sec_max_num_thds_ctl }], align 16
@.str.194 = private unnamed_addr constant [11 x i8] c"full_slabs\00", align 1
@stats_arenas_i_hpa_shard_full_slabs_node = internal constant [6 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.200, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_full_slabs_npageslabs_nonhuge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.201, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_full_slabs_npageslabs_huge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.202, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_full_slabs_nactive_nonhuge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.203, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_full_slabs_nactive_huge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.204, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_full_slabs_ndirty_nonhuge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.205, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_full_slabs_ndirty_huge_ctl }], align 16
@.str.195 = private unnamed_addr constant [12 x i8] c"empty_slabs\00", align 1
@stats_arenas_i_hpa_shard_empty_slabs_node = internal constant [6 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.200, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_empty_slabs_npageslabs_nonhuge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.201, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_empty_slabs_npageslabs_huge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.202, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_empty_slabs_nactive_nonhuge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.203, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_empty_slabs_nactive_huge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.204, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_empty_slabs_ndirty_nonhuge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.205, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_empty_slabs_ndirty_huge_ctl }], align 16
@stats_arenas_i_hpa_shard_nonfull_slabs_node = internal constant [1 x %struct.ctl_indexed_node_s] [%struct.ctl_indexed_node_s { %struct.ctl_node_s zeroinitializer, ptr @stats_arenas_i_hpa_shard_nonfull_slabs_j_index }], align 16
@.str.196 = private unnamed_addr constant [14 x i8] c"npurge_passes\00", align 1
@.str.197 = private unnamed_addr constant [8 x i8] c"npurges\00", align 1
@.str.198 = private unnamed_addr constant [10 x i8] c"nhugifies\00", align 1
@.str.199 = private unnamed_addr constant [12 x i8] c"ndehugifies\00", align 1
@.str.200 = private unnamed_addr constant [19 x i8] c"npageslabs_nonhuge\00", align 1
@.str.201 = private unnamed_addr constant [16 x i8] c"npageslabs_huge\00", align 1
@.str.202 = private unnamed_addr constant [16 x i8] c"nactive_nonhuge\00", align 1
@.str.203 = private unnamed_addr constant [13 x i8] c"nactive_huge\00", align 1
@.str.204 = private unnamed_addr constant [15 x i8] c"ndirty_nonhuge\00", align 1
@.str.205 = private unnamed_addr constant [12 x i8] c"ndirty_huge\00", align 1
@super_stats_arenas_i_hpa_shard_nonfull_slabs_j_node = internal constant [1 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.1, i64 6, ptr @stats_arenas_i_hpa_shard_nonfull_slabs_j_node, ptr null }], align 16
@stats_arenas_i_hpa_shard_nonfull_slabs_j_node = internal constant [6 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.200, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_nonfull_slabs_j_npageslabs_nonhuge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.201, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_nonfull_slabs_j_npageslabs_huge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.202, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_nonfull_slabs_j_nactive_nonhuge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.203, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_nonfull_slabs_j_nactive_huge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.204, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_nonfull_slabs_j_ndirty_nonhuge_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.205, i64 0, ptr null, ptr @stats_arenas_i_hpa_shard_nonfull_slabs_j_ndirty_huge_ctl }], align 16
@zero_realloc_count = external local_unnamed_addr global %struct.atomic_zu_t, align 8
@.str.206 = private unnamed_addr constant [6 x i8] c"hooks\00", align 1
@experimental_hooks_node = internal constant [5 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.211, i64 0, ptr null, ptr @experimental_hooks_install_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.212, i64 0, ptr null, ptr @experimental_hooks_remove_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.213, i64 0, ptr null, ptr @experimental_hooks_prof_backtrace_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.136, i64 0, ptr null, ptr @experimental_hooks_prof_dump_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.214, i64 0, ptr null, ptr @experimental_hooks_safety_check_abort_ctl }], align 16
@.str.207 = private unnamed_addr constant [12 x i8] c"utilization\00", align 1
@experimental_utilization_node = internal constant [2 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.215, i64 0, ptr null, ptr @experimental_utilization_query_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.216, i64 0, ptr null, ptr @experimental_utilization_batch_query_ctl }], align 16
@experimental_arenas_node = internal constant [1 x %struct.ctl_indexed_node_s] [%struct.ctl_indexed_node_s { %struct.ctl_node_s zeroinitializer, ptr @experimental_arenas_i_index }], align 16
@.str.208 = private unnamed_addr constant [18 x i8] c"arenas_create_ext\00", align 1
@.str.209 = private unnamed_addr constant [12 x i8] c"prof_recent\00", align 1
@experimental_prof_recent_node = internal constant [2 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.218, i64 0, ptr null, ptr @experimental_prof_recent_alloc_max_ctl }, %struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.219, i64 0, ptr null, ptr @experimental_prof_recent_alloc_dump_ctl }], align 16
@.str.210 = private unnamed_addr constant [12 x i8] c"batch_alloc\00", align 1
@experimental_thread_node = internal constant [1 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.220, i64 0, ptr null, ptr @experimental_thread_activity_callback_ctl }], align 16
@.str.211 = private unnamed_addr constant [8 x i8] c"install\00", align 1
@.str.212 = private unnamed_addr constant [7 x i8] c"remove\00", align 1
@.str.213 = private unnamed_addr constant [15 x i8] c"prof_backtrace\00", align 1
@.str.214 = private unnamed_addr constant [19 x i8] c"safety_check_abort\00", align 1
@opt_prof = external local_unnamed_addr global i8, align 1
@.str.215 = private unnamed_addr constant [6 x i8] c"query\00", align 1
@.str.216 = private unnamed_addr constant [12 x i8] c"batch_query\00", align 1
@super_experimental_arenas_i_node = internal constant [1 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.1, i64 1, ptr @experimental_arenas_i_node, ptr null }], align 16
@experimental_arenas_i_node = internal constant [1 x %struct.ctl_named_node_s] [%struct.ctl_named_node_s { %struct.ctl_node_s { i8 1 }, ptr @.str.217, i64 0, ptr null, ptr @experimental_arenas_i_pactivep_ctl }], align 16
@.str.217 = private unnamed_addr constant [9 x i8] c"pactivep\00", align 1
@.str.218 = private unnamed_addr constant [10 x i8] c"alloc_max\00", align 1
@.str.219 = private unnamed_addr constant [11 x i8] c"alloc_dump\00", align 1
@.str.220 = private unnamed_addr constant [18 x i8] c"activity_callback\00", align 1

; Function Attrs: nounwind uwtable
define hidden i32 @ctl_byname(ptr noundef %tsd, ptr noundef %name, ptr noundef %oldp, ptr noundef %oldlenp, ptr noundef %newp, i64 noundef %newlen) local_unnamed_addr #0 {
entry:
  %depth = alloca i64, align 8
  %mib = alloca [7 x i64], align 16
  %node = alloca ptr, align 8
  %.b5 = load i1, ptr @ctl_initialized, align 1
  br i1 %.b5, label %if.end, label %land.lhs.true

land.lhs.true:                                    ; preds = %entry
  %call = tail call fastcc zeroext i1 @ctl_init(ptr noundef %tsd)
  br i1 %call, label %label_return, label %if.end

if.end:                                           ; preds = %land.lhs.true, %entry
  store i64 7, ptr %depth, align 8
  %call2 = call fastcc i32 @ctl_lookup(ptr noundef %tsd, ptr noundef nonnull @super_root_node, ptr noundef %name, ptr noundef nonnull %node, ptr noundef nonnull %mib, ptr noundef nonnull %depth), !range !5
  %cmp.not = icmp eq i32 %call2, 0
  br i1 %cmp.not, label %if.end4, label %label_return

if.end4:                                          ; preds = %if.end
  %0 = load ptr, ptr %node, align 8
  %cmp5.not = icmp eq ptr %0, null
  br i1 %cmp5.not, label %label_return, label %land.lhs.true6

land.lhs.true6:                                   ; preds = %if.end4
  %ctl = getelementptr inbounds %struct.ctl_named_node_s, ptr %0, i64 0, i32 4
  %1 = load ptr, ptr %ctl, align 8
  %tobool7.not = icmp eq ptr %1, null
  br i1 %tobool7.not, label %label_return, label %if.then8

if.then8:                                         ; preds = %land.lhs.true6
  %2 = load i64, ptr %depth, align 8
  %call11 = call i32 %1(ptr noundef %tsd, ptr noundef nonnull %mib, i64 noundef %2, ptr noundef %oldp, ptr noundef %oldlenp, ptr noundef %newp, i64 noundef %newlen) #14
  br label %label_return

label_return:                                     ; preds = %if.end4, %land.lhs.true6, %land.lhs.true, %if.then8, %if.end
  %ret.0 = phi i32 [ %call2, %if.end ], [ %call11, %if.then8 ], [ 11, %land.lhs.true ], [ 2, %land.lhs.true6 ], [ 2, %if.end4 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal fastcc zeroext i1 @ctl_init(ptr noundef %tsd) unnamed_addr #0 {
entry:
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %.b10 = load i1, ptr @ctl_initialized, align 1
  br i1 %.b10, label %label_return, label %if.then

if.then:                                          ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_arenas, align 8
  %cmp = icmp eq ptr %3, null
  br i1 %cmp, label %if.then1, label %if.end6

if.then1:                                         ; preds = %if.then
  %call2 = tail call ptr @b0get() #14
  %call3 = tail call ptr @base_alloc(ptr noundef %tsd, ptr noundef %call2, i64 noundef 32800, i64 noundef 8) #14
  store ptr %call3, ptr @ctl_arenas, align 8
  %cmp4 = icmp eq ptr %call3, null
  br i1 %cmp4, label %label_return, label %if.end6

if.end6:                                          ; preds = %if.then1, %if.then
  %4 = phi ptr [ %call3, %if.then1 ], [ %3, %if.then ]
  %5 = load ptr, ptr @ctl_stats, align 8
  %cmp7 = icmp eq ptr %5, null
  br i1 %cmp7, label %if.then8, label %if.end14

if.then8:                                         ; preds = %if.end6
  %call9 = tail call ptr @b0get() #14
  %call10 = tail call ptr @base_alloc(ptr noundef %tsd, ptr noundef %call9, i64 noundef 720, i64 noundef 8) #14
  store ptr %call10, ptr @ctl_stats, align 8
  %cmp11 = icmp eq ptr %call10, null
  br i1 %cmp11, label %label_return, label %if.then8.if.end14_crit_edge

if.then8.if.end14_crit_edge:                      ; preds = %if.then8
  %.pre = load ptr, ptr @ctl_arenas, align 8
  br label %if.end14

if.end14:                                         ; preds = %if.then8.if.end14_crit_edge, %if.end6
  %6 = phi ptr [ %.pre, %if.then8.if.end14_crit_edge ], [ %4, %if.end6 ]
  %arrayidx.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 0
  %7 = load ptr, ptr %arrayidx.i, align 8
  %cmp.i = icmp eq ptr %7, null
  br i1 %cmp.i, label %if.then.i11, label %if.end18

if.then.i11:                                      ; preds = %if.end14
  %call4.i = tail call ptr @b0get() #14
  %call5.i = tail call ptr @base_alloc(ptr noundef %tsd, ptr noundef %call4.i, i64 noundef 38288, i64 noundef 8) #14
  %cmp6.i = icmp eq ptr %call5.i, null
  br i1 %cmp6.i, label %label_return, label %if.end.i12

if.end.i12:                                       ; preds = %if.then.i11
  %astats.i = getelementptr inbounds %struct.container_s, ptr %call5.i, i64 0, i32 1
  %astats8.i = getelementptr inbounds %struct.ctl_arena_s, ptr %call5.i, i64 0, i32 10
  store ptr %astats.i, ptr %astats8.i, align 8
  store i32 4096, ptr %call5.i, align 8
  %8 = load ptr, ptr @ctl_arenas, align 8
  %arrayidx13.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %8, i64 0, i32 3, i64 0
  store ptr %call5.i, ptr %arrayidx13.i, align 8
  br label %if.end18

if.end18:                                         ; preds = %if.end.i12, %if.end14
  %retval.0.i.ph = phi ptr [ %7, %if.end14 ], [ %call5.i, %if.end.i12 ]
  %initialized = getelementptr inbounds %struct.ctl_arena_s, ptr %retval.0.i.ph, i64 0, i32 1
  store i8 1, ptr %initialized, align 4
  %9 = load ptr, ptr @ctl_arenas, align 8
  %arrayidx.i13 = getelementptr inbounds %struct.ctl_arenas_s, ptr %9, i64 0, i32 3, i64 1
  %10 = load ptr, ptr %arrayidx.i13, align 8
  %cmp.i14 = icmp eq ptr %10, null
  br i1 %cmp.i14, label %if.then.i16, label %if.end22

if.then.i16:                                      ; preds = %if.end18
  %call4.i17 = tail call ptr @b0get() #14
  %call5.i18 = tail call ptr @base_alloc(ptr noundef %tsd, ptr noundef %call4.i17, i64 noundef 38288, i64 noundef 8) #14
  %cmp6.i19 = icmp eq ptr %call5.i18, null
  br i1 %cmp6.i19, label %label_return, label %if.end.i20

if.end.i20:                                       ; preds = %if.then.i16
  %astats.i21 = getelementptr inbounds %struct.container_s, ptr %call5.i18, i64 0, i32 1
  %astats8.i22 = getelementptr inbounds %struct.ctl_arena_s, ptr %call5.i18, i64 0, i32 10
  store ptr %astats.i21, ptr %astats8.i22, align 8
  store i32 4097, ptr %call5.i18, align 8
  %11 = load ptr, ptr @ctl_arenas, align 8
  %arrayidx13.i23 = getelementptr inbounds %struct.ctl_arenas_s, ptr %11, i64 0, i32 3, i64 1
  store ptr %call5.i18, ptr %arrayidx13.i23, align 8
  br label %if.end22

if.end22:                                         ; preds = %if.end.i20, %if.end18
  %retval.0.i15.ph = phi ptr [ %10, %if.end18 ], [ %call5.i18, %if.end.i20 ]
  %nthreads.i = getelementptr inbounds %struct.ctl_arena_s, ptr %retval.0.i15.ph, i64 0, i32 3
  store i32 0, ptr %nthreads.i, align 8
  %12 = load ptr, ptr getelementptr inbounds ([0 x ptr], ptr @dss_prec_names, i64 0, i64 3), align 8
  %dss.i = getelementptr inbounds %struct.ctl_arena_s, ptr %retval.0.i15.ph, i64 0, i32 4
  store ptr %12, ptr %dss.i, align 8
  %dirty_decay_ms.i = getelementptr inbounds %struct.ctl_arena_s, ptr %retval.0.i15.ph, i64 0, i32 5
  %pactive.i = getelementptr inbounds %struct.ctl_arena_s, ptr %retval.0.i15.ph, i64 0, i32 7
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) %dirty_decay_ms.i, i8 -1, i64 16, i1 false)
  %astats.i25 = getelementptr inbounds %struct.ctl_arena_s, ptr %retval.0.i15.ph, i64 0, i32 10
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) %pactive.i, i8 0, i64 24, i1 false)
  %13 = load ptr, ptr %astats.i25, align 8
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(10368) %13, i8 0, i64 10368, i1 false)
  %14 = load ptr, ptr %astats.i25, align 8
  %allocated_small.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %14, i64 0, i32 1
  store i64 0, ptr %allocated_small.i, align 8
  %15 = load ptr, ptr %astats.i25, align 8
  %nmalloc_small.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %15, i64 0, i32 2
  store i64 0, ptr %nmalloc_small.i, align 8
  %16 = load ptr, ptr %astats.i25, align 8
  %ndalloc_small.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %16, i64 0, i32 3
  store i64 0, ptr %ndalloc_small.i, align 8
  %17 = load ptr, ptr %astats.i25, align 8
  %nrequests_small.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %17, i64 0, i32 4
  store i64 0, ptr %nrequests_small.i, align 8
  %18 = load ptr, ptr %astats.i25, align 8
  %nfills_small.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %18, i64 0, i32 5
  store i64 0, ptr %nfills_small.i, align 8
  %19 = load ptr, ptr %astats.i25, align 8
  %nflushes_small.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %19, i64 0, i32 6
  store i64 0, ptr %nflushes_small.i, align 8
  %20 = load ptr, ptr %astats.i25, align 8
  %bstats.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %20, i64 0, i32 7
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(5616) %bstats.i, i8 0, i64 5616, i1 false)
  %21 = load ptr, ptr %astats.i25, align 8
  %lstats.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %21, i64 0, i32 8
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(9408) %lstats.i, i8 0, i64 9408, i1 false)
  %22 = load ptr, ptr %astats.i25, align 8
  %estats.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %22, i64 0, i32 9
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(9552) %estats.i, i8 0, i64 9552, i1 false)
  %23 = load ptr, ptr %astats.i25, align 8
  %hpastats.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %23, i64 0, i32 10
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(3200) %hpastats.i, i8 0, i64 3200, i1 false)
  %24 = load ptr, ptr %astats.i25, align 8
  %secstats.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %24, i64 0, i32 11
  store i64 0, ptr %secstats.i, align 8
  %call23 = tail call i32 @narenas_total_get() #14
  %25 = load ptr, ptr @ctl_arenas, align 8
  %narenas = getelementptr inbounds %struct.ctl_arenas_s, ptr %25, i64 0, i32 1
  store i32 %call23, ptr %narenas, align 8
  %cmp2548.not = icmp eq i32 %call23, 0
  br i1 %cmp2548.not, label %do.body, label %for.body

for.body:                                         ; preds = %if.end22, %for.inc
  %26 = phi ptr [ %32, %for.inc ], [ %25, %if.end22 ]
  %indvars.iv = phi i64 [ %indvars.iv.next, %for.inc ], [ 0, %if.end22 ]
  %27 = trunc i64 %indvars.iv to i32
  switch i32 %27, label %sw.default.i.i [
    i32 4096, label %arenas_i2a_impl.exit.i
    i32 4097, label %sw.bb2.i.i
  ]

sw.bb2.i.i:                                       ; preds = %for.body
  br label %arenas_i2a_impl.exit.i

sw.default.i.i:                                   ; preds = %for.body
  %add.i.i = add nuw nsw i64 %indvars.iv, 2
  %28 = and i64 %add.i.i, 4294967295
  br label %arenas_i2a_impl.exit.i

arenas_i2a_impl.exit.i:                           ; preds = %sw.default.i.i, %sw.bb2.i.i, %for.body
  %a.0.i.i = phi i64 [ %28, %sw.default.i.i ], [ 1, %sw.bb2.i.i ], [ 0, %for.body ]
  %arrayidx.i26 = getelementptr inbounds %struct.ctl_arenas_s, ptr %26, i64 0, i32 3, i64 %a.0.i.i
  %29 = load ptr, ptr %arrayidx.i26, align 8
  %cmp.i27 = icmp eq ptr %29, null
  br i1 %cmp.i27, label %if.then.i29, label %for.inc

if.then.i29:                                      ; preds = %arenas_i2a_impl.exit.i
  %call4.i30 = tail call ptr @b0get() #14
  %call5.i31 = tail call ptr @base_alloc(ptr noundef %tsd, ptr noundef %call4.i30, i64 noundef 38288, i64 noundef 8) #14
  %cmp6.i32 = icmp eq ptr %call5.i31, null
  br i1 %cmp6.i32, label %label_return, label %if.end.i33

if.end.i33:                                       ; preds = %if.then.i29
  %astats.i34 = getelementptr inbounds %struct.container_s, ptr %call5.i31, i64 0, i32 1
  %astats8.i35 = getelementptr inbounds %struct.ctl_arena_s, ptr %call5.i31, i64 0, i32 10
  store ptr %astats.i34, ptr %astats8.i35, align 8
  store i32 %27, ptr %call5.i31, align 8
  %30 = load ptr, ptr @ctl_arenas, align 8
  switch i32 %27, label %sw.default.i12.i [
    i32 4096, label %arenas_i2a_impl.exit20.i
    i32 4097, label %sw.bb2.i10.i
  ]

sw.bb2.i10.i:                                     ; preds = %if.end.i33
  br label %arenas_i2a_impl.exit20.i

sw.default.i12.i:                                 ; preds = %if.end.i33
  %add.i15.i = add nuw nsw i64 %indvars.iv, 2
  %31 = and i64 %add.i15.i, 4294967295
  br label %arenas_i2a_impl.exit20.i

arenas_i2a_impl.exit20.i:                         ; preds = %sw.default.i12.i, %sw.bb2.i10.i, %if.end.i33
  %a.0.i11.i = phi i64 [ %31, %sw.default.i12.i ], [ 1, %sw.bb2.i10.i ], [ 0, %if.end.i33 ]
  %arrayidx13.i36 = getelementptr inbounds %struct.ctl_arenas_s, ptr %30, i64 0, i32 3, i64 %a.0.i11.i
  store ptr %call5.i31, ptr %arrayidx13.i36, align 8
  br label %for.inc

for.inc:                                          ; preds = %arenas_i2a_impl.exit20.i, %arenas_i2a_impl.exit.i
  %32 = phi ptr [ %30, %arenas_i2a_impl.exit20.i ], [ %26, %arenas_i2a_impl.exit.i ]
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %narenas24 = getelementptr inbounds %struct.ctl_arenas_s, ptr %32, i64 0, i32 1
  %33 = load i32, ptr %narenas24, align 8
  %34 = zext i32 %33 to i64
  %cmp25 = icmp ult i64 %indvars.iv.next, %34
  br i1 %cmp25, label %for.body, label %do.body, !llvm.loop !6

do.body:                                          ; preds = %for.inc, %if.end22
  %.lcssa = phi ptr [ %25, %if.end22 ], [ %32, %for.inc ]
  %destroyed = getelementptr inbounds %struct.ctl_arenas_s, ptr %.lcssa, i64 0, i32 2
  store ptr null, ptr %destroyed, align 8
  tail call fastcc void @ctl_refresh(ptr noundef %tsd)
  store i1 true, ptr @ctl_initialized, align 1
  br label %label_return

label_return:                                     ; preds = %if.then.i29, %if.then.i16, %if.then.i11, %malloc_mutex_lock.exit, %do.body, %if.then8, %if.then1
  %ret.0 = phi i1 [ true, %if.then1 ], [ true, %if.then8 ], [ false, %do.body ], [ false, %malloc_mutex_lock.exit ], [ true, %if.then.i11 ], [ true, %if.then.i16 ], [ true, %if.then.i29 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i1 %ret.0
}

; Function Attrs: nounwind uwtable
define internal fastcc i32 @ctl_lookup(ptr noundef %tsdn, ptr noundef %starting_node, ptr noundef %name, ptr noundef writeonly %ending_nodep, ptr noundef %mibp, ptr nocapture noundef %depthp) unnamed_addr #0 {
entry:
  %call = tail call ptr @strchr(ptr noundef nonnull dereferenceable(1) %name, i32 noundef 46) #15
  %cmp.not = icmp eq ptr %call, null
  br i1 %cmp.not, label %cond.false, label %cond.end

cond.false:                                       ; preds = %entry
  %strlen = tail call i64 @strlen(ptr nonnull dereferenceable(1) %name)
  %strchr = getelementptr inbounds i8, ptr %name, i64 %strlen
  br label %cond.end

cond.end:                                         ; preds = %entry, %cond.false
  %cond = phi ptr [ %strchr, %cond.false ], [ %call, %entry ]
  %0 = ptrtoint ptr %cond to i64
  %1 = ptrtoint ptr %name to i64
  %sub = sub i64 %0, %1
  %cmp2 = icmp eq i64 %sub, 0
  br i1 %cmp2, label %label_return, label %for.cond.preheader

for.cond.preheader:                               ; preds = %cond.end
  %2 = load i64, ptr %depthp, align 8
  %cmp367.not = icmp eq i64 %2, 0
  br i1 %cmp367.not, label %for.end61, label %do.end5

do.end5:                                          ; preds = %for.cond.preheader, %cond.end56
  %elm.072 = phi ptr [ %arrayidx49, %cond.end56 ], [ %name, %for.cond.preheader ]
  %dot.071 = phi ptr [ %cond57, %cond.end56 ], [ %cond, %for.cond.preheader ]
  %node.070 = phi ptr [ %node.2, %cond.end56 ], [ %starting_node, %for.cond.preheader ]
  %i.069 = phi i64 [ %inc60, %cond.end56 ], [ 0, %for.cond.preheader ]
  %elen.068 = phi i64 [ %sub58, %cond.end56 ], [ %sub, %for.cond.preheader ]
  %children = getelementptr inbounds %struct.ctl_named_node_s, ptr %node.070, i64 0, i32 3
  %3 = load ptr, ptr %children, align 8
  %4 = load i8, ptr %3, align 1
  %5 = and i8 %4, 1
  %tobool.not.i = icmp eq i8 %5, 0
  br i1 %tobool.not.i, label %if.else, label %for.cond9.preheader

for.cond9.preheader:                              ; preds = %do.end5
  %nchildren = getelementptr inbounds %struct.ctl_named_node_s, ptr %node.070, i64 0, i32 2
  %6 = load i64, ptr %nchildren, align 8
  %cmp1065.not = icmp eq i64 %6, 0
  br i1 %cmp1065.not, label %label_return, label %for.body11

for.body11:                                       ; preds = %for.cond9.preheader, %for.inc
  %j.066 = phi i64 [ %inc, %for.inc ], [ 0, %for.cond9.preheader ]
  %name13 = getelementptr inbounds %struct.ctl_named_node_s, ptr %3, i64 %j.066, i32 1
  %7 = load ptr, ptr %name13, align 8
  %call14 = tail call i64 @strlen(ptr noundef nonnull dereferenceable(1) %7) #15
  %cmp15 = icmp eq i64 %call14, %elen.068
  br i1 %cmp15, label %land.lhs.true, label %for.inc

land.lhs.true:                                    ; preds = %for.body11
  %call17 = tail call i32 @strncmp(ptr noundef %elm.072, ptr noundef %7, i64 noundef %elen.068) #15
  %cmp18 = icmp eq i32 %call17, 0
  br i1 %cmp18, label %for.end, label %for.inc

for.inc:                                          ; preds = %for.body11, %land.lhs.true
  %inc = add nuw i64 %j.066, 1
  %exitcond.not = icmp eq i64 %inc, %6
  br i1 %exitcond.not, label %label_return, label %for.body11, !llvm.loop !8

for.end:                                          ; preds = %land.lhs.true
  %arrayidx.i.le = getelementptr inbounds %struct.ctl_named_node_s, ptr %3, i64 %j.066
  %arrayidx = getelementptr inbounds i64, ptr %mibp, i64 %i.069
  store i64 %j.066, ptr %arrayidx, align 8
  %cmp21 = icmp eq ptr %arrayidx.i.le, %node.070
  br i1 %cmp21, label %label_return, label %if.end37

if.else:                                          ; preds = %do.end5
  %call24 = tail call i64 @malloc_strtoumax(ptr noundef %elm.072, ptr noundef null, i32 noundef 10) #14
  %cmp25 = icmp eq i64 %call24, -1
  br i1 %cmp25, label %label_return, label %if.end28

if.end28:                                         ; preds = %if.else
  %8 = load ptr, ptr %children, align 8
  %9 = load i8, ptr %8, align 1
  %10 = and i8 %9, 1
  %tobool.not.i43 = icmp eq i8 %10, 0
  %cond.i44 = select i1 %tobool.not.i43, ptr %8, ptr null
  %index31 = getelementptr inbounds %struct.ctl_indexed_node_s, ptr %cond.i44, i64 0, i32 1
  %11 = load ptr, ptr %index31, align 8
  %12 = load i64, ptr %depthp, align 8
  %call32 = tail call ptr %11(ptr noundef %tsdn, ptr noundef %mibp, i64 noundef %12, i64 noundef %call24) #14
  %cmp33 = icmp eq ptr %call32, null
  br i1 %cmp33, label %label_return, label %if.end35

if.end35:                                         ; preds = %if.end28
  %arrayidx36 = getelementptr inbounds i64, ptr %mibp, i64 %i.069
  store i64 %call24, ptr %arrayidx36, align 8
  br label %if.end37

if.end37:                                         ; preds = %for.end, %if.end35
  %node.2 = phi ptr [ %arrayidx.i.le, %for.end ], [ %call32, %if.end35 ]
  %ctl = getelementptr inbounds %struct.ctl_named_node_s, ptr %node.2, i64 0, i32 4
  %13 = load ptr, ptr %ctl, align 8
  %cmp38.not = icmp eq ptr %13, null
  %14 = load i8, ptr %dot.071, align 1
  %cmp40 = icmp eq i8 %14, 0
  br i1 %cmp38.not, label %lor.lhs.false39, label %if.then42

lor.lhs.false39:                                  ; preds = %if.end37
  br i1 %cmp40, label %if.end47, label %if.end48

if.then42:                                        ; preds = %if.end37
  br i1 %cmp40, label %if.end47, label %label_return

if.end47:                                         ; preds = %lor.lhs.false39, %if.then42
  %add = add i64 %i.069, 1
  store i64 %add, ptr %depthp, align 8
  br label %for.end61

if.end48:                                         ; preds = %lor.lhs.false39
  %arrayidx49 = getelementptr inbounds i8, ptr %dot.071, i64 1
  %call50 = tail call ptr @strchr(ptr noundef nonnull dereferenceable(1) %arrayidx49, i32 noundef 46) #15
  %cmp51.not = icmp eq ptr %call50, null
  br i1 %cmp51.not, label %cond.false54, label %cond.end56

cond.false54:                                     ; preds = %if.end48
  %strlen40 = tail call i64 @strlen(ptr nonnull dereferenceable(1) %arrayidx49)
  %strchr41 = getelementptr inbounds i8, ptr %arrayidx49, i64 %strlen40
  br label %cond.end56

cond.end56:                                       ; preds = %if.end48, %cond.false54
  %cond57 = phi ptr [ %strchr41, %cond.false54 ], [ %call50, %if.end48 ]
  %15 = ptrtoint ptr %cond57 to i64
  %16 = ptrtoint ptr %arrayidx49 to i64
  %sub58 = sub i64 %15, %16
  %inc60 = add nuw i64 %i.069, 1
  %17 = load i64, ptr %depthp, align 8
  %cmp3 = icmp ult i64 %inc60, %17
  br i1 %cmp3, label %do.end5, label %for.end61, !llvm.loop !9

for.end61:                                        ; preds = %cond.end56, %for.cond.preheader, %if.end47
  %node.3 = phi ptr [ %node.2, %if.end47 ], [ %starting_node, %for.cond.preheader ], [ %node.2, %cond.end56 ]
  %cmp62.not = icmp eq ptr %ending_nodep, null
  br i1 %cmp62.not, label %label_return, label %if.then64

if.then64:                                        ; preds = %for.end61
  store ptr %node.3, ptr %ending_nodep, align 8
  br label %label_return

label_return:                                     ; preds = %if.end28, %if.else, %for.end, %for.cond9.preheader, %for.inc, %for.end61, %if.then64, %if.then42, %cond.end
  %ret.0 = phi i32 [ 2, %cond.end ], [ 2, %if.then42 ], [ 0, %if.then64 ], [ 0, %for.end61 ], [ 2, %for.inc ], [ 2, %for.cond9.preheader ], [ 2, %for.end ], [ 2, %if.else ], [ 2, %if.end28 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define hidden i32 @ctl_nametomib(ptr noundef %tsd, ptr noundef %name, ptr noundef %mibp, ptr nocapture noundef %miblenp) local_unnamed_addr #0 {
entry:
  %.b2 = load i1, ptr @ctl_initialized, align 1
  br i1 %.b2, label %if.end, label %land.lhs.true

land.lhs.true:                                    ; preds = %entry
  %call = tail call fastcc zeroext i1 @ctl_init(ptr noundef %tsd)
  br i1 %call, label %label_return, label %if.end

if.end:                                           ; preds = %land.lhs.true, %entry
  %call2 = tail call fastcc i32 @ctl_lookup(ptr noundef %tsd, ptr noundef nonnull @super_root_node, ptr noundef %name, ptr noundef null, ptr noundef %mibp, ptr noundef %miblenp), !range !5
  br label %label_return

label_return:                                     ; preds = %land.lhs.true, %if.end
  %ret.0 = phi i32 [ %call2, %if.end ], [ 11, %land.lhs.true ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define hidden i32 @ctl_bymib(ptr noundef %tsd, ptr noundef %mib, i64 noundef %miblen, ptr noundef %oldp, ptr noundef %oldlenp, ptr noundef %newp, i64 noundef %newlen) local_unnamed_addr #0 {
entry:
  %.b7 = load i1, ptr @ctl_initialized, align 1
  br i1 %.b7, label %if.end, label %land.lhs.true

land.lhs.true:                                    ; preds = %entry
  %call = tail call fastcc zeroext i1 @ctl_init(ptr noundef %tsd)
  br i1 %call, label %label_return, label %if.end

if.end:                                           ; preds = %land.lhs.true, %entry
  %cmp18.not.i = icmp eq i64 %miblen, 0
  br i1 %cmp18.not.i, label %land.lhs.true6, label %do.end2.i

do.end2.i:                                        ; preds = %if.end, %for.inc.i
  %i.020.i = phi i64 [ %inc.i, %for.inc.i ], [ 0, %if.end ]
  %node.019.i = phi ptr [ %node.1.i, %for.inc.i ], [ @super_root_node, %if.end ]
  %children.i = getelementptr inbounds %struct.ctl_named_node_s, ptr %node.019.i, i64 0, i32 3
  %0 = load ptr, ptr %children.i, align 8
  %1 = load i8, ptr %0, align 1
  %2 = and i8 %1, 1
  %tobool.not.i.i = icmp eq i8 %2, 0
  %arrayidx10.i = getelementptr inbounds i64, ptr %mib, i64 %i.020.i
  %3 = load i64, ptr %arrayidx10.i, align 8
  br i1 %tobool.not.i.i, label %if.else.i, label %if.then.i

if.then.i:                                        ; preds = %do.end2.i
  %nchildren.i = getelementptr inbounds %struct.ctl_named_node_s, ptr %node.019.i, i64 0, i32 2
  %4 = load i64, ptr %nchildren.i, align 8
  %cmp4.not.i = icmp ugt i64 %4, %3
  br i1 %cmp4.not.i, label %if.end.i, label %label_return

if.end.i:                                         ; preds = %if.then.i
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_named_node_s, ptr %0, i64 %3
  br label %for.inc.i

if.else.i:                                        ; preds = %do.end2.i
  %index.i = getelementptr inbounds %struct.ctl_indexed_node_s, ptr %0, i64 0, i32 1
  %5 = load ptr, ptr %index.i, align 8
  %call11.i = tail call ptr %5(ptr noundef %tsd, ptr noundef nonnull %mib, i64 noundef %miblen, i64 noundef %3) #14
  %cmp12.i = icmp eq ptr %call11.i, null
  br i1 %cmp12.i, label %label_return, label %for.inc.i

for.inc.i:                                        ; preds = %if.else.i, %if.end.i
  %node.1.i = phi ptr [ %arrayidx.i.i, %if.end.i ], [ %call11.i, %if.else.i ]
  %inc.i = add nuw i64 %i.020.i, 1
  %exitcond.not.i = icmp eq i64 %inc.i, %miblen
  br i1 %exitcond.not.i, label %if.end4, label %do.end2.i, !llvm.loop !10

if.end4:                                          ; preds = %for.inc.i
  %tobool5.not = icmp eq ptr %node.1.i, null
  br i1 %tobool5.not, label %label_return, label %land.lhs.true6

land.lhs.true6:                                   ; preds = %if.end, %if.end4
  %node.0.ph14 = phi ptr [ %node.1.i, %if.end4 ], [ @super_root_node, %if.end ]
  %ctl = getelementptr inbounds %struct.ctl_named_node_s, ptr %node.0.ph14, i64 0, i32 4
  %6 = load ptr, ptr %ctl, align 8
  %tobool7.not = icmp eq ptr %6, null
  br i1 %tobool7.not, label %label_return, label %if.then8

if.then8:                                         ; preds = %land.lhs.true6
  %call10 = tail call i32 %6(ptr noundef %tsd, ptr noundef %mib, i64 noundef %miblen, ptr noundef %oldp, ptr noundef %oldlenp, ptr noundef %newp, i64 noundef %newlen) #14
  br label %label_return

label_return:                                     ; preds = %if.else.i, %if.then.i, %if.end4, %land.lhs.true6, %land.lhs.true, %if.then8
  %ret.0 = phi i32 [ %call10, %if.then8 ], [ 11, %land.lhs.true ], [ 2, %land.lhs.true6 ], [ 2, %if.end4 ], [ 2, %if.then.i ], [ 2, %if.else.i ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define hidden i32 @ctl_mibnametomib(ptr noundef %tsd, ptr noundef %mib, i64 noundef %miblen, ptr noundef %name, ptr nocapture noundef %miblenp) local_unnamed_addr #0 {
entry:
  %.b10 = load i1, ptr @ctl_initialized, align 1
  br i1 %.b10, label %if.end, label %land.lhs.true

land.lhs.true:                                    ; preds = %entry
  %call = tail call fastcc zeroext i1 @ctl_init(ptr noundef %tsd)
  br i1 %call, label %label_return, label %if.end

if.end:                                           ; preds = %land.lhs.true, %entry
  %cmp18.not.i = icmp eq i64 %miblen, 0
  br i1 %cmp18.not.i, label %lor.lhs.false, label %do.end2.i

do.end2.i:                                        ; preds = %if.end, %for.inc.i
  %i.020.i = phi i64 [ %inc.i, %for.inc.i ], [ 0, %if.end ]
  %node.019.i = phi ptr [ %node.1.i, %for.inc.i ], [ @super_root_node, %if.end ]
  %children.i = getelementptr inbounds %struct.ctl_named_node_s, ptr %node.019.i, i64 0, i32 3
  %0 = load ptr, ptr %children.i, align 8
  %1 = load i8, ptr %0, align 1
  %2 = and i8 %1, 1
  %tobool.not.i.i = icmp eq i8 %2, 0
  %arrayidx10.i = getelementptr inbounds i64, ptr %mib, i64 %i.020.i
  %3 = load i64, ptr %arrayidx10.i, align 8
  br i1 %tobool.not.i.i, label %if.else.i, label %if.then.i

if.then.i:                                        ; preds = %do.end2.i
  %nchildren.i = getelementptr inbounds %struct.ctl_named_node_s, ptr %node.019.i, i64 0, i32 2
  %4 = load i64, ptr %nchildren.i, align 8
  %cmp4.not.i = icmp ugt i64 %4, %3
  br i1 %cmp4.not.i, label %if.end.i, label %label_return

if.end.i:                                         ; preds = %if.then.i
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_named_node_s, ptr %0, i64 %3
  br label %for.inc.i

if.else.i:                                        ; preds = %do.end2.i
  %index.i = getelementptr inbounds %struct.ctl_indexed_node_s, ptr %0, i64 0, i32 1
  %5 = load ptr, ptr %index.i, align 8
  %call11.i = tail call ptr %5(ptr noundef %tsd, ptr noundef nonnull %mib, i64 noundef %miblen, i64 noundef %3) #14
  %cmp12.i = icmp eq ptr %call11.i, null
  br i1 %cmp12.i, label %label_return, label %for.inc.i

for.inc.i:                                        ; preds = %if.else.i, %if.end.i
  %node.1.i = phi ptr [ %arrayidx.i.i, %if.end.i ], [ %call11.i, %if.else.i ]
  %inc.i = add nuw i64 %i.020.i, 1
  %exitcond.not.i = icmp eq i64 %inc.i, %miblen
  br i1 %exitcond.not.i, label %if.end4, label %do.end2.i, !llvm.loop !10

if.end4:                                          ; preds = %for.inc.i
  %cmp5 = icmp eq ptr %node.1.i, null
  br i1 %cmp5, label %label_return, label %lor.lhs.false

lor.lhs.false:                                    ; preds = %if.end, %if.end4
  %node.0.ph18 = phi ptr [ %node.1.i, %if.end4 ], [ @super_root_node, %if.end ]
  %ctl = getelementptr inbounds %struct.ctl_named_node_s, ptr %node.0.ph18, i64 0, i32 4
  %6 = load ptr, ptr %ctl, align 8
  %cmp6.not = icmp eq ptr %6, null
  br i1 %cmp6.not, label %do.end10, label %label_return

do.end10:                                         ; preds = %lor.lhs.false
  %7 = load i64, ptr %miblenp, align 8
  %sub = sub i64 %7, %miblen
  store i64 %sub, ptr %miblenp, align 8
  %add.ptr = getelementptr inbounds i64, ptr %mib, i64 %miblen
  %call12 = tail call fastcc i32 @ctl_lookup(ptr noundef %tsd, ptr noundef nonnull %node.0.ph18, ptr noundef %name, ptr noundef null, ptr noundef %add.ptr, ptr noundef nonnull %miblenp), !range !5
  %8 = load i64, ptr %miblenp, align 8
  %add = add i64 %8, %miblen
  store i64 %add, ptr %miblenp, align 8
  br label %label_return

label_return:                                     ; preds = %if.else.i, %if.then.i, %if.end4, %lor.lhs.false, %land.lhs.true, %do.end10
  %ret.0 = phi i32 [ %call12, %do.end10 ], [ 11, %land.lhs.true ], [ 2, %lor.lhs.false ], [ 2, %if.end4 ], [ 2, %if.then.i ], [ 2, %if.else.i ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define hidden i32 @ctl_bymibname(ptr noundef %tsd, ptr noundef %mib, i64 noundef %miblen, ptr noundef %name, ptr nocapture noundef %miblenp, ptr noundef %oldp, ptr noundef %oldlenp, ptr noundef %newp, i64 noundef %newlen) local_unnamed_addr #0 {
entry:
  %node = alloca ptr, align 8
  %.b16 = load i1, ptr @ctl_initialized, align 1
  br i1 %.b16, label %if.end, label %land.lhs.true

land.lhs.true:                                    ; preds = %entry
  %call = tail call fastcc zeroext i1 @ctl_init(ptr noundef %tsd)
  br i1 %call, label %label_return, label %if.end

if.end:                                           ; preds = %land.lhs.true, %entry
  %cmp18.not.i = icmp eq i64 %miblen, 0
  br i1 %cmp18.not.i, label %if.end4.thread, label %do.end2.i

if.end4.thread:                                   ; preds = %if.end
  store ptr @super_root_node, ptr %node, align 8
  br label %lor.lhs.false

do.end2.i:                                        ; preds = %if.end, %for.inc.i
  %i.020.i = phi i64 [ %inc.i, %for.inc.i ], [ 0, %if.end ]
  %node.019.i = phi ptr [ %node.1.i, %for.inc.i ], [ @super_root_node, %if.end ]
  %children.i = getelementptr inbounds %struct.ctl_named_node_s, ptr %node.019.i, i64 0, i32 3
  %0 = load ptr, ptr %children.i, align 8
  %1 = load i8, ptr %0, align 1
  %2 = and i8 %1, 1
  %tobool.not.i.i = icmp eq i8 %2, 0
  %arrayidx10.i = getelementptr inbounds i64, ptr %mib, i64 %i.020.i
  %3 = load i64, ptr %arrayidx10.i, align 8
  br i1 %tobool.not.i.i, label %if.else.i, label %if.then.i

if.then.i:                                        ; preds = %do.end2.i
  %nchildren.i = getelementptr inbounds %struct.ctl_named_node_s, ptr %node.019.i, i64 0, i32 2
  %4 = load i64, ptr %nchildren.i, align 8
  %cmp4.not.i = icmp ugt i64 %4, %3
  br i1 %cmp4.not.i, label %if.end.i, label %label_return

if.end.i:                                         ; preds = %if.then.i
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_named_node_s, ptr %0, i64 %3
  br label %for.inc.i

if.else.i:                                        ; preds = %do.end2.i
  %index.i = getelementptr inbounds %struct.ctl_indexed_node_s, ptr %0, i64 0, i32 1
  %5 = load ptr, ptr %index.i, align 8
  %call11.i = tail call ptr %5(ptr noundef %tsd, ptr noundef nonnull %mib, i64 noundef %miblen, i64 noundef %3) #14
  %cmp12.i = icmp eq ptr %call11.i, null
  br i1 %cmp12.i, label %label_return, label %for.inc.i

for.inc.i:                                        ; preds = %if.else.i, %if.end.i
  %node.1.i = phi ptr [ %arrayidx.i.i, %if.end.i ], [ %call11.i, %if.else.i ]
  %inc.i = add nuw i64 %i.020.i, 1
  %exitcond.not.i = icmp eq i64 %inc.i, %miblen
  br i1 %exitcond.not.i, label %if.end4, label %do.end2.i, !llvm.loop !10

if.end4:                                          ; preds = %for.inc.i
  store ptr %node.1.i, ptr %node, align 8
  %cmp5 = icmp eq ptr %node.1.i, null
  br i1 %cmp5, label %label_return, label %lor.lhs.false

lor.lhs.false:                                    ; preds = %if.end4.thread, %if.end4
  %node.0.lcssa.i21 = phi ptr [ @super_root_node, %if.end4.thread ], [ %node.1.i, %if.end4 ]
  %ctl = getelementptr inbounds %struct.ctl_named_node_s, ptr %node.0.lcssa.i21, i64 0, i32 4
  %6 = load ptr, ptr %ctl, align 8
  %cmp6.not = icmp eq ptr %6, null
  br i1 %cmp6.not, label %do.end10, label %label_return

do.end10:                                         ; preds = %lor.lhs.false
  %7 = load i64, ptr %miblenp, align 8
  %sub = sub i64 %7, %miblen
  store i64 %sub, ptr %miblenp, align 8
  %add.ptr = getelementptr inbounds i64, ptr %mib, i64 %miblen
  %call12 = call fastcc i32 @ctl_lookup(ptr noundef %tsd, ptr noundef nonnull %node.0.lcssa.i21, ptr noundef %name, ptr noundef nonnull %node, ptr noundef %add.ptr, ptr noundef nonnull %miblenp), !range !5
  %8 = load i64, ptr %miblenp, align 8
  %add = add i64 %8, %miblen
  store i64 %add, ptr %miblenp, align 8
  %cmp13.not = icmp eq i32 %call12, 0
  br i1 %cmp13.not, label %if.end15, label %label_return

if.end15:                                         ; preds = %do.end10
  %9 = load ptr, ptr %node, align 8
  %cmp16.not = icmp eq ptr %9, null
  br i1 %cmp16.not, label %label_return, label %land.lhs.true17

land.lhs.true17:                                  ; preds = %if.end15
  %ctl18 = getelementptr inbounds %struct.ctl_named_node_s, ptr %9, i64 0, i32 4
  %10 = load ptr, ptr %ctl18, align 8
  %tobool19.not = icmp eq ptr %10, null
  br i1 %tobool19.not, label %label_return, label %if.then20

if.then20:                                        ; preds = %land.lhs.true17
  %call22 = call i32 %10(ptr noundef %tsd, ptr noundef %mib, i64 noundef %add, ptr noundef %oldp, ptr noundef %oldlenp, ptr noundef %newp, i64 noundef %newlen) #14
  br label %label_return

label_return:                                     ; preds = %if.else.i, %if.then.i, %if.end15, %land.lhs.true17, %if.end4, %lor.lhs.false, %land.lhs.true, %if.then20, %do.end10
  %ret.0 = phi i32 [ %call12, %do.end10 ], [ %call22, %if.then20 ], [ 11, %land.lhs.true ], [ 2, %lor.lhs.false ], [ 2, %if.end4 ], [ 2, %land.lhs.true17 ], [ 2, %if.end15 ], [ 2, %if.then.i ], [ 2, %if.else.i ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define hidden zeroext i1 @ctl_boot() local_unnamed_addr #0 {
entry:
  %call = tail call zeroext i1 @malloc_mutex_init(ptr noundef nonnull @ctl_mtx, ptr noundef nonnull @.str, i32 noundef 2, i32 noundef 0) #14
  br i1 %call, label %return, label %if.end

if.end:                                           ; preds = %entry
  store i1 false, ptr @ctl_initialized, align 1
  br label %return

return:                                           ; preds = %entry, %if.end
  ret i1 %call
}

declare zeroext i1 @malloc_mutex_init(ptr noundef, ptr noundef, i32 noundef, i32 noundef) local_unnamed_addr #1

; Function Attrs: nounwind uwtable
define hidden void @ctl_prefork(ptr noundef %tsdn) local_unnamed_addr #0 {
entry:
  tail call void @malloc_mutex_prefork(ptr noundef %tsdn, ptr noundef nonnull @ctl_mtx) #14
  ret void
}

declare void @malloc_mutex_prefork(ptr noundef, ptr noundef) local_unnamed_addr #1

; Function Attrs: nounwind uwtable
define hidden void @ctl_postfork_parent(ptr noundef %tsdn) local_unnamed_addr #0 {
entry:
  tail call void @malloc_mutex_postfork_parent(ptr noundef %tsdn, ptr noundef nonnull @ctl_mtx) #14
  ret void
}

declare void @malloc_mutex_postfork_parent(ptr noundef, ptr noundef) local_unnamed_addr #1

; Function Attrs: nounwind uwtable
define hidden void @ctl_postfork_child(ptr noundef %tsdn) local_unnamed_addr #0 {
entry:
  tail call void @malloc_mutex_postfork_child(ptr noundef %tsdn, ptr noundef nonnull @ctl_mtx) #14
  ret void
}

declare void @malloc_mutex_postfork_child(ptr noundef, ptr noundef) local_unnamed_addr #1

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define hidden void @ctl_mtx_assert_held(ptr nocapture noundef readnone %tsdn) local_unnamed_addr #2 {
entry:
  ret void
}

declare ptr @base_alloc(ptr noundef, ptr noundef, i64 noundef, i64 noundef) local_unnamed_addr #1

declare ptr @b0get() local_unnamed_addr #1

declare i32 @narenas_total_get() local_unnamed_addr #1

; Function Attrs: nounwind uwtable
define internal fastcc void @ctl_refresh(ptr noundef %tsdn) unnamed_addr #0 {
entry:
  %0 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %0, i64 0, i32 29
  %1 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %1, 0
  br i1 %cmp6.i.not.i, label %arenas_i.exit, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %entry
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %0, i1 noundef zeroext false) #14
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %entry, %if.then11.i.i
  %2 = load ptr, ptr @ctl_arenas, align 8
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %2, i64 0, i32 3, i64 0
  %3 = load ptr, ptr %arrayidx.i.i, align 8
  %narenas = getelementptr inbounds %struct.ctl_arenas_s, ptr %2, i64 0, i32 1
  %4 = load i32, ptr %narenas, align 8
  %5 = zext i32 %4 to i64
  %vla = alloca ptr, i64 %5, align 16
  %nthreads.i = getelementptr inbounds %struct.ctl_arena_s, ptr %3, i64 0, i32 3
  store i32 0, ptr %nthreads.i, align 8
  %6 = load ptr, ptr getelementptr inbounds ([0 x ptr], ptr @dss_prec_names, i64 0, i64 3), align 8
  %dss.i = getelementptr inbounds %struct.ctl_arena_s, ptr %3, i64 0, i32 4
  store ptr %6, ptr %dss.i, align 8
  %dirty_decay_ms.i = getelementptr inbounds %struct.ctl_arena_s, ptr %3, i64 0, i32 5
  %pactive.i = getelementptr inbounds %struct.ctl_arena_s, ptr %3, i64 0, i32 7
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) %dirty_decay_ms.i, i8 -1, i64 16, i1 false)
  %astats.i = getelementptr inbounds %struct.ctl_arena_s, ptr %3, i64 0, i32 10
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) %pactive.i, i8 0, i64 24, i1 false)
  %7 = load ptr, ptr %astats.i, align 8
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(10368) %7, i8 0, i64 10368, i1 false)
  %8 = load ptr, ptr %astats.i, align 8
  %allocated_small.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %8, i64 0, i32 1
  store i64 0, ptr %allocated_small.i, align 8
  %9 = load ptr, ptr %astats.i, align 8
  %nmalloc_small.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %9, i64 0, i32 2
  store i64 0, ptr %nmalloc_small.i, align 8
  %10 = load ptr, ptr %astats.i, align 8
  %ndalloc_small.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 3
  store i64 0, ptr %ndalloc_small.i, align 8
  %11 = load ptr, ptr %astats.i, align 8
  %nrequests_small.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %11, i64 0, i32 4
  store i64 0, ptr %nrequests_small.i, align 8
  %12 = load ptr, ptr %astats.i, align 8
  %nfills_small.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %12, i64 0, i32 5
  store i64 0, ptr %nfills_small.i, align 8
  %13 = load ptr, ptr %astats.i, align 8
  %nflushes_small.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %13, i64 0, i32 6
  store i64 0, ptr %nflushes_small.i, align 8
  %14 = load ptr, ptr %astats.i, align 8
  %bstats.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %14, i64 0, i32 7
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(5616) %bstats.i, i8 0, i64 5616, i1 false)
  %15 = load ptr, ptr %astats.i, align 8
  %lstats.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %15, i64 0, i32 8
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(9408) %lstats.i, i8 0, i64 9408, i1 false)
  %16 = load ptr, ptr %astats.i, align 8
  %estats.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %16, i64 0, i32 9
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(9552) %estats.i, i8 0, i64 9552, i1 false)
  %17 = load ptr, ptr %astats.i, align 8
  %hpastats.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %17, i64 0, i32 10
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(3200) %hpastats.i, i8 0, i64 3200, i1 false)
  %18 = load ptr, ptr %astats.i, align 8
  %secstats.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %18, i64 0, i32 11
  store i64 0, ptr %secstats.i, align 8
  %19 = load i32, ptr %narenas, align 8
  %cmp38.not = icmp eq i32 %19, 0
  br i1 %cmp38.not, label %for.end19, label %for.body

for.cond3.preheader:                              ; preds = %for.body
  %20 = icmp eq i32 %24, 0
  br i1 %20, label %for.end19, label %for.body6

for.body:                                         ; preds = %arenas_i.exit, %for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %for.body ], [ 0, %arenas_i.exit ]
  %arrayidx.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %indvars.iv
  %21 = load atomic i64, ptr %arrayidx.i acquire, align 8
  %22 = inttoptr i64 %21 to ptr
  %arrayidx = getelementptr inbounds ptr, ptr %vla, i64 %indvars.iv
  store ptr %22, ptr %arrayidx, align 8
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %23 = load ptr, ptr @ctl_arenas, align 8
  %narenas1 = getelementptr inbounds %struct.ctl_arenas_s, ptr %23, i64 0, i32 1
  %24 = load i32, ptr %narenas1, align 8
  %25 = zext i32 %24 to i64
  %cmp = icmp ult i64 %indvars.iv.next, %25
  br i1 %cmp, label %for.body, label %for.cond3.preheader, !llvm.loop !11

for.body6:                                        ; preds = %for.cond3.preheader, %for.inc17
  %26 = phi ptr [ %35, %for.inc17 ], [ %23, %for.cond3.preheader ]
  %indvars.iv44 = phi i64 [ %indvars.iv.next45, %for.inc17 ], [ 0, %for.cond3.preheader ]
  %27 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i28 = icmp eq i8 %27, 0
  br i1 %cmp6.i.not.i28, label %tsd_fetch_impl.exit.i, label %if.then11.i.i29

if.then11.i.i29:                                  ; preds = %for.body6
  %call13.i.i30 = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %0, i1 noundef zeroext false) #14
  %.pre = load ptr, ptr @ctl_arenas, align 8
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i29, %for.body6
  %28 = phi ptr [ %.pre, %if.then11.i.i29 ], [ %26, %for.body6 ]
  %29 = trunc i64 %indvars.iv44 to i32
  switch i32 %29, label %sw.default.i.i.i [
    i32 4096, label %arenas_i.exit32
    i32 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit32

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %28, i64 0, i32 1
  %30 = load i32, ptr %narenas.i.i.i, align 8
  %31 = zext i32 %30 to i64
  %cmp.i.i.i = icmp eq i64 %indvars.iv44, %31
  br i1 %cmp.i.i.i, label %arenas_i.exit32, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add nuw nsw i64 %indvars.iv44, 2
  %32 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit32

arenas_i.exit32:                                  ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %32, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i31 = getelementptr inbounds %struct.ctl_arenas_s, ptr %28, i64 0, i32 3, i64 %a.0.i.i.i
  %33 = load ptr, ptr %arrayidx.i.i31, align 8
  %arrayidx9 = getelementptr inbounds ptr, ptr %vla, i64 %indvars.iv44
  %34 = load ptr, ptr %arrayidx9, align 8
  %cmp10 = icmp ne ptr %34, null
  %frombool = zext i1 %cmp10 to i8
  %initialized12 = getelementptr inbounds %struct.ctl_arena_s, ptr %33, i64 0, i32 1
  store i8 %frombool, ptr %initialized12, align 4
  br i1 %cmp10, label %if.then, label %for.inc17

if.then:                                          ; preds = %arenas_i.exit32
  tail call fastcc void @ctl_arena_refresh(ptr noundef %tsdn, ptr noundef nonnull %34, ptr noundef %3, i32 noundef %29, i1 noundef zeroext false)
  br label %for.inc17

for.inc17:                                        ; preds = %arenas_i.exit32, %if.then
  %indvars.iv.next45 = add nuw nsw i64 %indvars.iv44, 1
  %35 = load ptr, ptr @ctl_arenas, align 8
  %narenas4 = getelementptr inbounds %struct.ctl_arenas_s, ptr %35, i64 0, i32 1
  %36 = load i32, ptr %narenas4, align 8
  %37 = zext i32 %36 to i64
  %cmp5 = icmp ult i64 %indvars.iv.next45, %37
  br i1 %cmp5, label %for.body6, label %for.end19, !llvm.loop !12

for.end19:                                        ; preds = %for.inc17, %arenas_i.exit, %for.cond3.preheader
  %38 = load ptr, ptr %astats.i, align 8
  %allocated_small = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %38, i64 0, i32 1
  %39 = load i64, ptr %allocated_small, align 8
  %allocated_large = getelementptr inbounds %struct.arena_stats_s, ptr %38, i64 0, i32 5
  %40 = load i64, ptr %allocated_large, align 8
  %add = add i64 %40, %39
  %41 = load ptr, ptr @ctl_stats, align 8
  store i64 %add, ptr %41, align 8
  %42 = load i64, ptr %pactive.i, align 8
  %shl = shl i64 %42, 12
  %active = getelementptr inbounds %struct.ctl_stats_s, ptr %41, i64 0, i32 1
  store i64 %shl, ptr %active, align 8
  %43 = load ptr, ptr %astats.i, align 8
  %44 = load i64, ptr %43, align 8
  %internal = getelementptr inbounds %struct.arena_stats_s, ptr %43, i64 0, i32 4
  %45 = load atomic i64, ptr %internal monotonic, align 8
  %add27 = add i64 %45, %44
  %metadata = getelementptr inbounds %struct.ctl_stats_s, ptr %41, i64 0, i32 2
  store i64 %add27, ptr %metadata, align 8
  %46 = load ptr, ptr %astats.i, align 8
  %resident = getelementptr inbounds %struct.arena_stats_s, ptr %46, i64 0, i32 1
  %47 = load i64, ptr %resident, align 8
  %resident30 = getelementptr inbounds %struct.ctl_stats_s, ptr %41, i64 0, i32 4
  store i64 %47, ptr %resident30, align 8
  %48 = load ptr, ptr %astats.i, align 8
  %metadata_thp = getelementptr inbounds %struct.arena_stats_s, ptr %48, i64 0, i32 2
  %49 = load i64, ptr %metadata_thp, align 8
  %metadata_thp33 = getelementptr inbounds %struct.ctl_stats_s, ptr %41, i64 0, i32 3
  store i64 %49, ptr %metadata_thp33, align 8
  %50 = load ptr, ptr %astats.i, align 8
  %mapped = getelementptr inbounds %struct.arena_stats_s, ptr %50, i64 0, i32 3
  %51 = load i64, ptr %mapped, align 8
  %mapped36 = getelementptr inbounds %struct.ctl_stats_s, ptr %41, i64 0, i32 5
  store i64 %51, ptr %mapped36, align 8
  %52 = load ptr, ptr %astats.i, align 8
  %retained = getelementptr inbounds %struct.arena_stats_s, ptr %52, i64 0, i32 11, i32 1, i32 2
  %53 = load i64, ptr %retained, align 8
  %retained39 = getelementptr inbounds %struct.ctl_stats_s, ptr %41, i64 0, i32 6
  store i64 %53, ptr %retained39, align 8
  %background_thread.i = getelementptr inbounds %struct.ctl_stats_s, ptr %41, i64 0, i32 7
  %call.i = tail call zeroext i1 @background_thread_stats_read(ptr noundef %tsdn, ptr noundef nonnull %background_thread.i) #14
  br i1 %call.i, label %if.then.i, label %ctl_background_thread_stats_read.exit

if.then.i:                                        ; preds = %for.end19
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(88) %background_thread.i, i8 0, i64 88, i1 false)
  %run_interval.i = getelementptr inbounds %struct.ctl_stats_s, ptr %41, i64 0, i32 7, i32 2
  tail call void @nstime_copy(ptr noundef nonnull %run_interval.i, ptr noundef nonnull @nstime_zero) #14
  br label %ctl_background_thread_stats_read.exit

ctl_background_thread_stats_read.exit:            ; preds = %for.end19, %if.then.i
  %54 = load ptr, ptr @ctl_stats, align 8
  %arrayidx.i33 = getelementptr inbounds %struct.ctl_stats_s, ptr %54, i64 0, i32 8, i64 1
  %max_counter_per_bg_thd.i = getelementptr inbounds %struct.ctl_stats_s, ptr %41, i64 0, i32 7, i32 3
  tail call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(64) %arrayidx.i33, ptr noundef nonnull align 8 dereferenceable(64) %max_counter_per_bg_thd.i, i64 64, i1 false)
  %n_waiting_thds.i.i = getelementptr inbounds %struct.ctl_stats_s, ptr %54, i64 0, i32 8, i64 1, i32 5
  store atomic i32 0, ptr %n_waiting_thds.i.i monotonic, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i34

if.then.i34:                                      ; preds = %ctl_background_thread_stats_read.exit
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @background_thread_lock) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i34, %ctl_background_thread_stats_read.exit
  %55 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %55, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %56 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %56, %tsdn
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsdn, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %57 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %57, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %58 = load ptr, ptr @ctl_stats, align 8
  %mutex_prof_data = getelementptr inbounds %struct.ctl_stats_s, ptr %58, i64 0, i32 8
  tail call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(64) %mutex_prof_data, ptr noundef nonnull align 8 dereferenceable(64) @background_thread_lock, i64 64, i1 false)
  %n_waiting_thds.i.i35 = getelementptr inbounds %struct.ctl_stats_s, ptr %58, i64 0, i32 8, i64 0, i32 5
  store atomic i32 0, ptr %n_waiting_thds.i.i35 monotonic, align 4
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 1)) #14
  %59 = load ptr, ptr @ctl_stats, align 8
  %arrayidx42 = getelementptr inbounds %struct.ctl_stats_s, ptr %59, i64 0, i32 8, i64 2
  tail call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(64) %arrayidx42, ptr noundef nonnull align 8 dereferenceable(64) @ctl_mtx, i64 64, i1 false)
  %n_waiting_thds.i.i36 = getelementptr inbounds %struct.ctl_stats_s, ptr %59, i64 0, i32 8, i64 2, i32 5
  store atomic i32 0, ptr %n_waiting_thds.i.i36 monotonic, align 4
  %60 = load ptr, ptr @ctl_arenas, align 8
  %61 = load i64, ptr %60, align 8
  %inc43 = add i64 %61, 1
  store i64 %inc43, ptr %60, align 8
  ret void
}

declare void @malloc_mutex_lock_slow(ptr noundef) local_unnamed_addr #1

; Function Attrs: nounwind
declare i32 @pthread_mutex_trylock(ptr noundef) local_unnamed_addr #3

; Function Attrs: mustprogress nocallback nofree nounwind willreturn memory(argmem: write)
declare void @llvm.memset.p0.i64(ptr nocapture writeonly, i8, i64, i1 immarg) #4

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn
declare ptr @llvm.stacksave.p0() #5

; Function Attrs: nounwind uwtable
define internal fastcc void @ctl_arena_refresh(ptr noundef %tsdn, ptr noundef %arena, ptr nocapture noundef %ctl_sdarena, i32 noundef %i, i1 noundef zeroext %destroyed) unnamed_addr #0 {
entry:
  %0 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %0, i64 0, i32 29
  %1 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %1, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %entry
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %0, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %entry
  %2 = load ptr, ptr @ctl_arenas, align 8
  switch i32 %i, label %sw.default.i.i.i [
    i32 4096, label %arenas_i.exit
    i32 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %2, i64 0, i32 1
  %3 = load i32, ptr %narenas.i.i.i, align 8
  %cmp.i.i.i = icmp eq i32 %3, %i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %4 = add i32 %i, 2
  %5 = zext i32 %4 to i64
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %5, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %2, i64 0, i32 3, i64 %a.0.i.i.i
  %6 = load ptr, ptr %arrayidx.i.i, align 8
  %nthreads.i = getelementptr inbounds %struct.ctl_arena_s, ptr %6, i64 0, i32 3
  store i32 0, ptr %nthreads.i, align 8
  %7 = load ptr, ptr getelementptr inbounds ([0 x ptr], ptr @dss_prec_names, i64 0, i64 3), align 8
  %dss.i = getelementptr inbounds %struct.ctl_arena_s, ptr %6, i64 0, i32 4
  store ptr %7, ptr %dss.i, align 8
  %dirty_decay_ms.i = getelementptr inbounds %struct.ctl_arena_s, ptr %6, i64 0, i32 5
  %pactive.i = getelementptr inbounds %struct.ctl_arena_s, ptr %6, i64 0, i32 7
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) %dirty_decay_ms.i, i8 -1, i64 16, i1 false)
  %astats.i = getelementptr inbounds %struct.ctl_arena_s, ptr %6, i64 0, i32 10
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) %pactive.i, i8 0, i64 24, i1 false)
  %8 = load ptr, ptr %astats.i, align 8
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(10368) %8, i8 0, i64 10368, i1 false)
  %9 = load ptr, ptr %astats.i, align 8
  %allocated_small.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %9, i64 0, i32 1
  store i64 0, ptr %allocated_small.i, align 8
  %10 = load ptr, ptr %astats.i, align 8
  %nmalloc_small.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 2
  store i64 0, ptr %nmalloc_small.i, align 8
  %11 = load ptr, ptr %astats.i, align 8
  %ndalloc_small.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %11, i64 0, i32 3
  store i64 0, ptr %ndalloc_small.i, align 8
  %12 = load ptr, ptr %astats.i, align 8
  %nrequests_small.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %12, i64 0, i32 4
  store i64 0, ptr %nrequests_small.i, align 8
  %13 = load ptr, ptr %astats.i, align 8
  %nfills_small.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %13, i64 0, i32 5
  store i64 0, ptr %nfills_small.i, align 8
  %14 = load ptr, ptr %astats.i, align 8
  %nflushes_small.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %14, i64 0, i32 6
  store i64 0, ptr %nflushes_small.i, align 8
  %15 = load ptr, ptr %astats.i, align 8
  %bstats.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %15, i64 0, i32 7
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(5616) %bstats.i, i8 0, i64 5616, i1 false)
  %16 = load ptr, ptr %astats.i, align 8
  %lstats.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %16, i64 0, i32 8
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(9408) %lstats.i, i8 0, i64 9408, i1 false)
  %17 = load ptr, ptr %astats.i, align 8
  %estats.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %17, i64 0, i32 9
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(9552) %estats.i, i8 0, i64 9552, i1 false)
  %18 = load ptr, ptr %astats.i, align 8
  %hpastats.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %18, i64 0, i32 10
  tail call void @llvm.memset.p0.i64(ptr noundef nonnull align 8 dereferenceable(3200) %hpastats.i, i8 0, i64 3200, i1 false)
  %19 = load ptr, ptr %astats.i, align 8
  %secstats.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %19, i64 0, i32 11
  store i64 0, ptr %secstats.i, align 8
  %muzzy_decay_ms.i = getelementptr inbounds %struct.ctl_arena_s, ptr %6, i64 0, i32 6
  %pdirty.i = getelementptr inbounds %struct.ctl_arena_s, ptr %6, i64 0, i32 8
  %pmuzzy.i = getelementptr inbounds %struct.ctl_arena_s, ptr %6, i64 0, i32 9
  %20 = load ptr, ptr %astats.i, align 8
  %bstats.i8 = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %20, i64 0, i32 7
  %lstats.i9 = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %20, i64 0, i32 8
  %estats.i10 = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %20, i64 0, i32 9
  %hpastats.i11 = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %20, i64 0, i32 10
  %secstats.i12 = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %20, i64 0, i32 11
  tail call void @arena_stats_merge(ptr noundef %tsdn, ptr noundef %arena, ptr noundef nonnull %nthreads.i, ptr noundef nonnull %dss.i, ptr noundef nonnull %dirty_decay_ms.i, ptr noundef nonnull %muzzy_decay_ms.i, ptr noundef nonnull %pactive.i, ptr noundef nonnull %pdirty.i, ptr noundef nonnull %pmuzzy.i, ptr noundef %20, ptr noundef nonnull %bstats.i8, ptr noundef nonnull %lstats.i9, ptr noundef nonnull %estats.i10, ptr noundef nonnull %hpastats.i11, ptr noundef nonnull %secstats.i12) #14
  br label %for.body.i

for.body.i:                                       ; preds = %for.body.i, %arenas_i.exit
  %indvars.iv.i = phi i64 [ 0, %arenas_i.exit ], [ %indvars.iv.next.i, %for.body.i ]
  %21 = load ptr, ptr %astats.i, align 8
  %arrayidx.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %21, i64 0, i32 7, i64 %indvars.iv.i
  %curregs.i = getelementptr inbounds %struct.bin_stats_s, ptr %arrayidx.i, i64 0, i32 3
  %22 = load i64, ptr %curregs.i, align 8
  %arrayidx.i.i13 = getelementptr inbounds [235 x i64], ptr @sz_index2size_tab, i64 0, i64 %indvars.iv.i
  %23 = load i64, ptr %arrayidx.i.i13, align 8
  %mul.i = mul i64 %23, %22
  %allocated_small.i14 = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %21, i64 0, i32 1
  %24 = load i64, ptr %allocated_small.i14, align 8
  %add.i = add i64 %24, %mul.i
  store i64 %add.i, ptr %allocated_small.i14, align 8
  %25 = load i64, ptr %arrayidx.i, align 8
  %26 = load ptr, ptr %astats.i, align 8
  %nmalloc_small.i15 = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %26, i64 0, i32 2
  %27 = load i64, ptr %nmalloc_small.i15, align 8
  %add15.i = add i64 %27, %25
  store i64 %add15.i, ptr %nmalloc_small.i15, align 8
  %ndalloc.i = getelementptr inbounds %struct.bin_stats_s, ptr %arrayidx.i, i64 0, i32 1
  %28 = load i64, ptr %ndalloc.i, align 8
  %29 = load ptr, ptr %astats.i, align 8
  %ndalloc_small.i16 = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %29, i64 0, i32 3
  %30 = load i64, ptr %ndalloc_small.i16, align 8
  %add17.i = add i64 %30, %28
  store i64 %add17.i, ptr %ndalloc_small.i16, align 8
  %nrequests.i = getelementptr inbounds %struct.bin_stats_s, ptr %arrayidx.i, i64 0, i32 2
  %31 = load i64, ptr %nrequests.i, align 8
  %32 = load ptr, ptr %astats.i, align 8
  %nrequests_small.i17 = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %32, i64 0, i32 4
  %33 = load i64, ptr %nrequests_small.i17, align 8
  %add19.i = add i64 %33, %31
  store i64 %add19.i, ptr %nrequests_small.i17, align 8
  %nfills.i = getelementptr inbounds %struct.bin_stats_s, ptr %arrayidx.i, i64 0, i32 4
  %34 = load i64, ptr %nfills.i, align 8
  %35 = load ptr, ptr %astats.i, align 8
  %nfills_small.i18 = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %35, i64 0, i32 5
  %36 = load i64, ptr %nfills_small.i18, align 8
  %add21.i = add i64 %36, %34
  store i64 %add21.i, ptr %nfills_small.i18, align 8
  %nflushes.i = getelementptr inbounds %struct.bin_stats_s, ptr %arrayidx.i, i64 0, i32 5
  %37 = load i64, ptr %nflushes.i, align 8
  %38 = load ptr, ptr %astats.i, align 8
  %nflushes_small.i19 = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %38, i64 0, i32 6
  %39 = load i64, ptr %nflushes_small.i19, align 8
  %add23.i = add i64 %39, %37
  store i64 %add23.i, ptr %nflushes_small.i19, align 8
  %indvars.iv.next.i = add nuw nsw i64 %indvars.iv.i, 1
  %exitcond.not.i = icmp eq i64 %indvars.iv.next.i, 39
  br i1 %exitcond.not.i, label %ctl_arena_stats_amerge.exit, label %for.body.i, !llvm.loop !13

ctl_arena_stats_amerge.exit:                      ; preds = %for.body.i
  br i1 %destroyed, label %if.end.thread.i, label %if.then17.i

if.end.thread.i:                                  ; preds = %ctl_arena_stats_amerge.exit
  %astats493.i = getelementptr inbounds %struct.ctl_arena_s, ptr %ctl_sdarena, i64 0, i32 10
  %40 = load ptr, ptr %astats493.i, align 8
  %41 = load ptr, ptr %astats.i, align 8
  br label %if.end34.i

if.then17.i:                                      ; preds = %ctl_arena_stats_amerge.exit
  %42 = load i32, ptr %nthreads.i, align 8
  %nthreads1.i = getelementptr inbounds %struct.ctl_arena_s, ptr %ctl_sdarena, i64 0, i32 3
  %43 = load i32, ptr %nthreads1.i, align 8
  %add.i21 = add i32 %43, %42
  store i32 %add.i21, ptr %nthreads1.i, align 8
  %44 = load i64, ptr %pactive.i, align 8
  %pactive2.i = getelementptr inbounds %struct.ctl_arena_s, ptr %ctl_sdarena, i64 0, i32 7
  %45 = load i64, ptr %pactive2.i, align 8
  %add3.i = add i64 %45, %44
  store i64 %add3.i, ptr %pactive2.i, align 8
  %46 = load i64, ptr %pdirty.i, align 8
  %pdirty4.i = getelementptr inbounds %struct.ctl_arena_s, ptr %ctl_sdarena, i64 0, i32 8
  %47 = load i64, ptr %pdirty4.i, align 8
  %add5.i = add i64 %47, %46
  store i64 %add5.i, ptr %pdirty4.i, align 8
  %48 = load i64, ptr %pmuzzy.i, align 8
  %pmuzzy6.i = getelementptr inbounds %struct.ctl_arena_s, ptr %ctl_sdarena, i64 0, i32 9
  %49 = load i64, ptr %pmuzzy6.i, align 8
  %add7.i = add i64 %49, %48
  store i64 %add7.i, ptr %pmuzzy6.i, align 8
  %astats.i25 = getelementptr inbounds %struct.ctl_arena_s, ptr %ctl_sdarena, i64 0, i32 10
  %50 = load ptr, ptr %astats.i25, align 8
  %51 = load ptr, ptr %astats.i, align 8
  %mapped.i = getelementptr inbounds %struct.arena_stats_s, ptr %51, i64 0, i32 3
  %52 = load i64, ptr %mapped.i, align 8
  %mapped20.i = getelementptr inbounds %struct.arena_stats_s, ptr %50, i64 0, i32 3
  %53 = load i64, ptr %mapped20.i, align 8
  %add21.i26 = add i64 %53, %52
  store i64 %add21.i26, ptr %mapped20.i, align 8
  %pa_shard_stats.i = getelementptr inbounds %struct.arena_stats_s, ptr %51, i64 0, i32 11
  %retained.i = getelementptr inbounds %struct.arena_stats_s, ptr %51, i64 0, i32 11, i32 1, i32 2
  %54 = load i64, ptr %retained.i, align 8
  %pa_shard_stats24.i = getelementptr inbounds %struct.arena_stats_s, ptr %50, i64 0, i32 11
  %retained26.i = getelementptr inbounds %struct.arena_stats_s, ptr %50, i64 0, i32 11, i32 1, i32 2
  %55 = load i64, ptr %retained26.i, align 8
  %add27.i = add i64 %55, %54
  store i64 %add27.i, ptr %retained26.i, align 8
  %56 = load i64, ptr %pa_shard_stats.i, align 8
  %57 = load i64, ptr %pa_shard_stats24.i, align 8
  %add33.i = add i64 %57, %56
  store i64 %add33.i, ptr %pa_shard_stats24.i, align 8
  br label %if.end34.i

if.end34.i:                                       ; preds = %if.then17.i, %if.end.thread.i
  %58 = phi ptr [ %41, %if.end.thread.i ], [ %51, %if.then17.i ]
  %59 = phi ptr [ %40, %if.end.thread.i ], [ %50, %if.then17.i ]
  %pac_stats37.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 11, i32 1
  %pac_stats40.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 11, i32 1
  %60 = load atomic i64, ptr %pac_stats40.i monotonic, align 8
  %61 = load atomic i64, ptr %pac_stats37.i monotonic, align 8
  %add.i.i.i27 = add i64 %61, %60
  store atomic i64 %add.i.i.i27, ptr %pac_stats37.i monotonic, align 8
  %nmadvise.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 11, i32 1, i32 0, i32 1
  %nmadvise51.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 11, i32 1, i32 0, i32 1
  %62 = load atomic i64, ptr %nmadvise51.i monotonic, align 8
  %63 = load atomic i64, ptr %nmadvise.i monotonic, align 8
  %add.i.i170.i = add i64 %63, %62
  store atomic i64 %add.i.i170.i, ptr %nmadvise.i monotonic, align 8
  %purged.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 11, i32 1, i32 0, i32 2
  %purged60.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 11, i32 1, i32 0, i32 2
  %64 = load atomic i64, ptr %purged60.i monotonic, align 8
  %65 = load atomic i64, ptr %purged.i monotonic, align 8
  %add.i.i171.i = add i64 %65, %64
  store atomic i64 %add.i.i171.i, ptr %purged.i monotonic, align 8
  %decay_muzzy.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 11, i32 1, i32 1
  %decay_muzzy68.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 11, i32 1, i32 1
  %66 = load atomic i64, ptr %decay_muzzy68.i monotonic, align 8
  %67 = load atomic i64, ptr %decay_muzzy.i monotonic, align 8
  %add.i.i172.i = add i64 %67, %66
  store atomic i64 %add.i.i172.i, ptr %decay_muzzy.i monotonic, align 8
  %nmadvise74.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 11, i32 1, i32 1, i32 1
  %nmadvise79.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 11, i32 1, i32 1, i32 1
  %68 = load atomic i64, ptr %nmadvise79.i monotonic, align 8
  %69 = load atomic i64, ptr %nmadvise74.i monotonic, align 8
  %add.i.i173.i = add i64 %69, %68
  store atomic i64 %add.i.i173.i, ptr %nmadvise74.i monotonic, align 8
  %purged84.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 11, i32 1, i32 1, i32 2
  %purged89.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 11, i32 1, i32 1, i32 2
  %70 = load atomic i64, ptr %purged89.i monotonic, align 8
  %71 = load atomic i64, ptr %purged84.i monotonic, align 8
  %add.i.i174.i = add i64 %71, %70
  store atomic i64 %add.i.i174.i, ptr %purged84.i monotonic, align 8
  %mutex_prof_data.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14
  %mutex_prof_data92.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14
  tail call void @nstime_add(ptr noundef nonnull %mutex_prof_data.i, ptr noundef nonnull %mutex_prof_data92.i) #14
  %max_wait_time.i.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 0, i32 1
  %max_wait_time2.i.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 0, i32 1
  %call.i.i = tail call i32 @nstime_compare(ptr noundef nonnull %max_wait_time.i.i, ptr noundef nonnull %max_wait_time2.i.i) #14
  %cmp.i.i = icmp slt i32 %call.i.i, 0
  br i1 %cmp.i.i, label %if.then.i.i, label %if.end.i.i

if.then.i.i:                                      ; preds = %if.end34.i
  tail call void @nstime_copy(ptr noundef nonnull %max_wait_time.i.i, ptr noundef nonnull %max_wait_time2.i.i) #14
  br label %if.end.i.i

if.end.i.i:                                       ; preds = %if.then.i.i, %if.end34.i
  %n_wait_times.i.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 0, i32 2
  %72 = load i64, ptr %n_wait_times.i.i, align 8
  %n_wait_times5.i.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 0, i32 2
  %73 = load i64, ptr %n_wait_times5.i.i, align 8
  %add.i.i = add i64 %73, %72
  store i64 %add.i.i, ptr %n_wait_times5.i.i, align 8
  %n_spin_acquired.i.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 0, i32 3
  %74 = load i64, ptr %n_spin_acquired.i.i, align 8
  %n_spin_acquired6.i.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 0, i32 3
  %75 = load i64, ptr %n_spin_acquired6.i.i, align 8
  %add7.i.i = add i64 %75, %74
  store i64 %add7.i.i, ptr %n_spin_acquired6.i.i, align 8
  %max_n_thds.i.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 0, i32 4
  %76 = load i32, ptr %max_n_thds.i.i, align 8
  %max_n_thds8.i.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 0, i32 4
  %77 = load i32, ptr %max_n_thds8.i.i, align 8
  %cmp9.i.i = icmp ult i32 %76, %77
  br i1 %cmp9.i.i, label %if.then10.i.i, label %malloc_mutex_prof_merge.exit.i

if.then10.i.i:                                    ; preds = %if.end.i.i
  store i32 %77, ptr %max_n_thds.i.i, align 8
  br label %malloc_mutex_prof_merge.exit.i

malloc_mutex_prof_merge.exit.i:                   ; preds = %if.then10.i.i, %if.end.i.i
  %n_waiting_thds.i.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 0, i32 5
  %78 = load atomic i32, ptr %n_waiting_thds.i.i monotonic, align 4
  %n_waiting_thds15.i.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 0, i32 5
  %79 = load atomic i32, ptr %n_waiting_thds15.i.i monotonic, align 4
  %add17.i.i = add i32 %79, %78
  store atomic i32 %add17.i.i, ptr %n_waiting_thds.i.i monotonic, align 4
  %n_owner_switches.i.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 0, i32 6
  %80 = load i64, ptr %n_owner_switches.i.i, align 8
  %n_owner_switches19.i.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 0, i32 6
  %81 = load i64, ptr %n_owner_switches19.i.i, align 8
  %add20.i.i = add i64 %81, %80
  store i64 %add20.i.i, ptr %n_owner_switches19.i.i, align 8
  %n_lock_ops.i.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 0, i32 8
  %82 = load i64, ptr %n_lock_ops.i.i, align 8
  %n_lock_ops21.i.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 0, i32 8
  %83 = load i64, ptr %n_lock_ops21.i.i, align 8
  %add22.i.i = add i64 %83, %82
  store i64 %add22.i.i, ptr %n_lock_ops21.i.i, align 8
  %arrayidx96.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 1
  %arrayidx99.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 1
  tail call void @nstime_add(ptr noundef nonnull %arrayidx96.i, ptr noundef nonnull %arrayidx99.i) #14
  %max_wait_time.i175.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 1, i32 1
  %max_wait_time2.i176.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 1, i32 1
  %call.i177.i = tail call i32 @nstime_compare(ptr noundef nonnull %max_wait_time.i175.i, ptr noundef nonnull %max_wait_time2.i176.i) #14
  %cmp.i178.i = icmp slt i32 %call.i177.i, 0
  br i1 %cmp.i178.i, label %if.then.i199.i, label %if.end.i179.i

if.then.i199.i:                                   ; preds = %malloc_mutex_prof_merge.exit.i
  tail call void @nstime_copy(ptr noundef nonnull %max_wait_time.i175.i, ptr noundef nonnull %max_wait_time2.i176.i) #14
  br label %if.end.i179.i

if.end.i179.i:                                    ; preds = %if.then.i199.i, %malloc_mutex_prof_merge.exit.i
  %n_wait_times.i180.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 1, i32 2
  %84 = load i64, ptr %n_wait_times.i180.i, align 8
  %n_wait_times5.i181.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 1, i32 2
  %85 = load i64, ptr %n_wait_times5.i181.i, align 8
  %add.i182.i = add i64 %85, %84
  store i64 %add.i182.i, ptr %n_wait_times5.i181.i, align 8
  %n_spin_acquired.i183.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 1, i32 3
  %86 = load i64, ptr %n_spin_acquired.i183.i, align 8
  %n_spin_acquired6.i184.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 1, i32 3
  %87 = load i64, ptr %n_spin_acquired6.i184.i, align 8
  %add7.i185.i = add i64 %87, %86
  store i64 %add7.i185.i, ptr %n_spin_acquired6.i184.i, align 8
  %max_n_thds.i186.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 1, i32 4
  %88 = load i32, ptr %max_n_thds.i186.i, align 8
  %max_n_thds8.i187.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 1, i32 4
  %89 = load i32, ptr %max_n_thds8.i187.i, align 8
  %cmp9.i188.i = icmp ult i32 %88, %89
  br i1 %cmp9.i188.i, label %if.then10.i198.i, label %malloc_mutex_prof_merge.exit200.i

if.then10.i198.i:                                 ; preds = %if.end.i179.i
  store i32 %89, ptr %max_n_thds.i186.i, align 8
  br label %malloc_mutex_prof_merge.exit200.i

malloc_mutex_prof_merge.exit200.i:                ; preds = %if.then10.i198.i, %if.end.i179.i
  %n_waiting_thds.i189.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 1, i32 5
  %90 = load atomic i32, ptr %n_waiting_thds.i189.i monotonic, align 4
  %n_waiting_thds15.i190.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 1, i32 5
  %91 = load atomic i32, ptr %n_waiting_thds15.i190.i monotonic, align 4
  %add17.i191.i = add i32 %91, %90
  store atomic i32 %add17.i191.i, ptr %n_waiting_thds.i189.i monotonic, align 4
  %n_owner_switches.i192.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 1, i32 6
  %92 = load i64, ptr %n_owner_switches.i192.i, align 8
  %n_owner_switches19.i193.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 1, i32 6
  %93 = load i64, ptr %n_owner_switches19.i193.i, align 8
  %add20.i194.i = add i64 %93, %92
  store i64 %add20.i194.i, ptr %n_owner_switches19.i193.i, align 8
  %n_lock_ops.i195.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 1, i32 8
  %94 = load i64, ptr %n_lock_ops.i195.i, align 8
  %n_lock_ops21.i196.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 1, i32 8
  %95 = load i64, ptr %n_lock_ops21.i196.i, align 8
  %add22.i197.i = add i64 %95, %94
  store i64 %add22.i197.i, ptr %n_lock_ops21.i196.i, align 8
  %arrayidx102.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 2
  %arrayidx105.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 2
  tail call void @nstime_add(ptr noundef nonnull %arrayidx102.i, ptr noundef nonnull %arrayidx105.i) #14
  %max_wait_time.i201.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 2, i32 1
  %max_wait_time2.i202.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 2, i32 1
  %call.i203.i = tail call i32 @nstime_compare(ptr noundef nonnull %max_wait_time.i201.i, ptr noundef nonnull %max_wait_time2.i202.i) #14
  %cmp.i204.i = icmp slt i32 %call.i203.i, 0
  br i1 %cmp.i204.i, label %if.then.i225.i, label %if.end.i205.i

if.then.i225.i:                                   ; preds = %malloc_mutex_prof_merge.exit200.i
  tail call void @nstime_copy(ptr noundef nonnull %max_wait_time.i201.i, ptr noundef nonnull %max_wait_time2.i202.i) #14
  br label %if.end.i205.i

if.end.i205.i:                                    ; preds = %if.then.i225.i, %malloc_mutex_prof_merge.exit200.i
  %n_wait_times.i206.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 2, i32 2
  %96 = load i64, ptr %n_wait_times.i206.i, align 8
  %n_wait_times5.i207.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 2, i32 2
  %97 = load i64, ptr %n_wait_times5.i207.i, align 8
  %add.i208.i = add i64 %97, %96
  store i64 %add.i208.i, ptr %n_wait_times5.i207.i, align 8
  %n_spin_acquired.i209.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 2, i32 3
  %98 = load i64, ptr %n_spin_acquired.i209.i, align 8
  %n_spin_acquired6.i210.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 2, i32 3
  %99 = load i64, ptr %n_spin_acquired6.i210.i, align 8
  %add7.i211.i = add i64 %99, %98
  store i64 %add7.i211.i, ptr %n_spin_acquired6.i210.i, align 8
  %max_n_thds.i212.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 2, i32 4
  %100 = load i32, ptr %max_n_thds.i212.i, align 8
  %max_n_thds8.i213.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 2, i32 4
  %101 = load i32, ptr %max_n_thds8.i213.i, align 8
  %cmp9.i214.i = icmp ult i32 %100, %101
  br i1 %cmp9.i214.i, label %if.then10.i224.i, label %malloc_mutex_prof_merge.exit226.i

if.then10.i224.i:                                 ; preds = %if.end.i205.i
  store i32 %101, ptr %max_n_thds.i212.i, align 8
  br label %malloc_mutex_prof_merge.exit226.i

malloc_mutex_prof_merge.exit226.i:                ; preds = %if.then10.i224.i, %if.end.i205.i
  %n_waiting_thds.i215.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 2, i32 5
  %102 = load atomic i32, ptr %n_waiting_thds.i215.i monotonic, align 4
  %n_waiting_thds15.i216.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 2, i32 5
  %103 = load atomic i32, ptr %n_waiting_thds15.i216.i monotonic, align 4
  %add17.i217.i = add i32 %103, %102
  store atomic i32 %add17.i217.i, ptr %n_waiting_thds.i215.i monotonic, align 4
  %n_owner_switches.i218.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 2, i32 6
  %104 = load i64, ptr %n_owner_switches.i218.i, align 8
  %n_owner_switches19.i219.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 2, i32 6
  %105 = load i64, ptr %n_owner_switches19.i219.i, align 8
  %add20.i220.i = add i64 %105, %104
  store i64 %add20.i220.i, ptr %n_owner_switches19.i219.i, align 8
  %n_lock_ops.i221.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 2, i32 8
  %106 = load i64, ptr %n_lock_ops.i221.i, align 8
  %n_lock_ops21.i222.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 2, i32 8
  %107 = load i64, ptr %n_lock_ops21.i222.i, align 8
  %add22.i223.i = add i64 %107, %106
  store i64 %add22.i223.i, ptr %n_lock_ops21.i222.i, align 8
  %arrayidx108.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 3
  %arrayidx111.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 3
  tail call void @nstime_add(ptr noundef nonnull %arrayidx108.i, ptr noundef nonnull %arrayidx111.i) #14
  %max_wait_time.i227.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 3, i32 1
  %max_wait_time2.i228.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 3, i32 1
  %call.i229.i = tail call i32 @nstime_compare(ptr noundef nonnull %max_wait_time.i227.i, ptr noundef nonnull %max_wait_time2.i228.i) #14
  %cmp.i230.i = icmp slt i32 %call.i229.i, 0
  br i1 %cmp.i230.i, label %if.then.i251.i, label %if.end.i231.i

if.then.i251.i:                                   ; preds = %malloc_mutex_prof_merge.exit226.i
  tail call void @nstime_copy(ptr noundef nonnull %max_wait_time.i227.i, ptr noundef nonnull %max_wait_time2.i228.i) #14
  br label %if.end.i231.i

if.end.i231.i:                                    ; preds = %if.then.i251.i, %malloc_mutex_prof_merge.exit226.i
  %n_wait_times.i232.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 3, i32 2
  %108 = load i64, ptr %n_wait_times.i232.i, align 8
  %n_wait_times5.i233.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 3, i32 2
  %109 = load i64, ptr %n_wait_times5.i233.i, align 8
  %add.i234.i = add i64 %109, %108
  store i64 %add.i234.i, ptr %n_wait_times5.i233.i, align 8
  %n_spin_acquired.i235.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 3, i32 3
  %110 = load i64, ptr %n_spin_acquired.i235.i, align 8
  %n_spin_acquired6.i236.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 3, i32 3
  %111 = load i64, ptr %n_spin_acquired6.i236.i, align 8
  %add7.i237.i = add i64 %111, %110
  store i64 %add7.i237.i, ptr %n_spin_acquired6.i236.i, align 8
  %max_n_thds.i238.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 3, i32 4
  %112 = load i32, ptr %max_n_thds.i238.i, align 8
  %max_n_thds8.i239.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 3, i32 4
  %113 = load i32, ptr %max_n_thds8.i239.i, align 8
  %cmp9.i240.i = icmp ult i32 %112, %113
  br i1 %cmp9.i240.i, label %if.then10.i250.i, label %malloc_mutex_prof_merge.exit252.i

if.then10.i250.i:                                 ; preds = %if.end.i231.i
  store i32 %113, ptr %max_n_thds.i238.i, align 8
  br label %malloc_mutex_prof_merge.exit252.i

malloc_mutex_prof_merge.exit252.i:                ; preds = %if.then10.i250.i, %if.end.i231.i
  %n_waiting_thds.i241.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 3, i32 5
  %114 = load atomic i32, ptr %n_waiting_thds.i241.i monotonic, align 4
  %n_waiting_thds15.i242.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 3, i32 5
  %115 = load atomic i32, ptr %n_waiting_thds15.i242.i monotonic, align 4
  %add17.i243.i = add i32 %115, %114
  store atomic i32 %add17.i243.i, ptr %n_waiting_thds.i241.i monotonic, align 4
  %n_owner_switches.i244.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 3, i32 6
  %116 = load i64, ptr %n_owner_switches.i244.i, align 8
  %n_owner_switches19.i245.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 3, i32 6
  %117 = load i64, ptr %n_owner_switches19.i245.i, align 8
  %add20.i246.i = add i64 %117, %116
  store i64 %add20.i246.i, ptr %n_owner_switches19.i245.i, align 8
  %n_lock_ops.i247.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 3, i32 8
  %118 = load i64, ptr %n_lock_ops.i247.i, align 8
  %n_lock_ops21.i248.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 3, i32 8
  %119 = load i64, ptr %n_lock_ops21.i248.i, align 8
  %add22.i249.i = add i64 %119, %118
  store i64 %add22.i249.i, ptr %n_lock_ops21.i248.i, align 8
  %arrayidx114.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 4
  %arrayidx117.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 4
  tail call void @nstime_add(ptr noundef nonnull %arrayidx114.i, ptr noundef nonnull %arrayidx117.i) #14
  %max_wait_time.i253.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 4, i32 1
  %max_wait_time2.i254.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 4, i32 1
  %call.i255.i = tail call i32 @nstime_compare(ptr noundef nonnull %max_wait_time.i253.i, ptr noundef nonnull %max_wait_time2.i254.i) #14
  %cmp.i256.i = icmp slt i32 %call.i255.i, 0
  br i1 %cmp.i256.i, label %if.then.i277.i, label %if.end.i257.i

if.then.i277.i:                                   ; preds = %malloc_mutex_prof_merge.exit252.i
  tail call void @nstime_copy(ptr noundef nonnull %max_wait_time.i253.i, ptr noundef nonnull %max_wait_time2.i254.i) #14
  br label %if.end.i257.i

if.end.i257.i:                                    ; preds = %if.then.i277.i, %malloc_mutex_prof_merge.exit252.i
  %n_wait_times.i258.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 4, i32 2
  %120 = load i64, ptr %n_wait_times.i258.i, align 8
  %n_wait_times5.i259.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 4, i32 2
  %121 = load i64, ptr %n_wait_times5.i259.i, align 8
  %add.i260.i = add i64 %121, %120
  store i64 %add.i260.i, ptr %n_wait_times5.i259.i, align 8
  %n_spin_acquired.i261.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 4, i32 3
  %122 = load i64, ptr %n_spin_acquired.i261.i, align 8
  %n_spin_acquired6.i262.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 4, i32 3
  %123 = load i64, ptr %n_spin_acquired6.i262.i, align 8
  %add7.i263.i = add i64 %123, %122
  store i64 %add7.i263.i, ptr %n_spin_acquired6.i262.i, align 8
  %max_n_thds.i264.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 4, i32 4
  %124 = load i32, ptr %max_n_thds.i264.i, align 8
  %max_n_thds8.i265.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 4, i32 4
  %125 = load i32, ptr %max_n_thds8.i265.i, align 8
  %cmp9.i266.i = icmp ult i32 %124, %125
  br i1 %cmp9.i266.i, label %if.then10.i276.i, label %malloc_mutex_prof_merge.exit278.i

if.then10.i276.i:                                 ; preds = %if.end.i257.i
  store i32 %125, ptr %max_n_thds.i264.i, align 8
  br label %malloc_mutex_prof_merge.exit278.i

malloc_mutex_prof_merge.exit278.i:                ; preds = %if.then10.i276.i, %if.end.i257.i
  %n_waiting_thds.i267.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 4, i32 5
  %126 = load atomic i32, ptr %n_waiting_thds.i267.i monotonic, align 4
  %n_waiting_thds15.i268.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 4, i32 5
  %127 = load atomic i32, ptr %n_waiting_thds15.i268.i monotonic, align 4
  %add17.i269.i = add i32 %127, %126
  store atomic i32 %add17.i269.i, ptr %n_waiting_thds.i267.i monotonic, align 4
  %n_owner_switches.i270.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 4, i32 6
  %128 = load i64, ptr %n_owner_switches.i270.i, align 8
  %n_owner_switches19.i271.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 4, i32 6
  %129 = load i64, ptr %n_owner_switches19.i271.i, align 8
  %add20.i272.i = add i64 %129, %128
  store i64 %add20.i272.i, ptr %n_owner_switches19.i271.i, align 8
  %n_lock_ops.i273.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 4, i32 8
  %130 = load i64, ptr %n_lock_ops.i273.i, align 8
  %n_lock_ops21.i274.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 4, i32 8
  %131 = load i64, ptr %n_lock_ops21.i274.i, align 8
  %add22.i275.i = add i64 %131, %130
  store i64 %add22.i275.i, ptr %n_lock_ops21.i274.i, align 8
  %arrayidx120.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 5
  %arrayidx123.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 5
  tail call void @nstime_add(ptr noundef nonnull %arrayidx120.i, ptr noundef nonnull %arrayidx123.i) #14
  %max_wait_time.i279.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 5, i32 1
  %max_wait_time2.i280.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 5, i32 1
  %call.i281.i = tail call i32 @nstime_compare(ptr noundef nonnull %max_wait_time.i279.i, ptr noundef nonnull %max_wait_time2.i280.i) #14
  %cmp.i282.i = icmp slt i32 %call.i281.i, 0
  br i1 %cmp.i282.i, label %if.then.i303.i, label %if.end.i283.i

if.then.i303.i:                                   ; preds = %malloc_mutex_prof_merge.exit278.i
  tail call void @nstime_copy(ptr noundef nonnull %max_wait_time.i279.i, ptr noundef nonnull %max_wait_time2.i280.i) #14
  br label %if.end.i283.i

if.end.i283.i:                                    ; preds = %if.then.i303.i, %malloc_mutex_prof_merge.exit278.i
  %n_wait_times.i284.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 5, i32 2
  %132 = load i64, ptr %n_wait_times.i284.i, align 8
  %n_wait_times5.i285.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 5, i32 2
  %133 = load i64, ptr %n_wait_times5.i285.i, align 8
  %add.i286.i = add i64 %133, %132
  store i64 %add.i286.i, ptr %n_wait_times5.i285.i, align 8
  %n_spin_acquired.i287.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 5, i32 3
  %134 = load i64, ptr %n_spin_acquired.i287.i, align 8
  %n_spin_acquired6.i288.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 5, i32 3
  %135 = load i64, ptr %n_spin_acquired6.i288.i, align 8
  %add7.i289.i = add i64 %135, %134
  store i64 %add7.i289.i, ptr %n_spin_acquired6.i288.i, align 8
  %max_n_thds.i290.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 5, i32 4
  %136 = load i32, ptr %max_n_thds.i290.i, align 8
  %max_n_thds8.i291.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 5, i32 4
  %137 = load i32, ptr %max_n_thds8.i291.i, align 8
  %cmp9.i292.i = icmp ult i32 %136, %137
  br i1 %cmp9.i292.i, label %if.then10.i302.i, label %malloc_mutex_prof_merge.exit304.i

if.then10.i302.i:                                 ; preds = %if.end.i283.i
  store i32 %137, ptr %max_n_thds.i290.i, align 8
  br label %malloc_mutex_prof_merge.exit304.i

malloc_mutex_prof_merge.exit304.i:                ; preds = %if.then10.i302.i, %if.end.i283.i
  %n_waiting_thds.i293.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 5, i32 5
  %138 = load atomic i32, ptr %n_waiting_thds.i293.i monotonic, align 4
  %n_waiting_thds15.i294.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 5, i32 5
  %139 = load atomic i32, ptr %n_waiting_thds15.i294.i monotonic, align 4
  %add17.i295.i = add i32 %139, %138
  store atomic i32 %add17.i295.i, ptr %n_waiting_thds.i293.i monotonic, align 4
  %n_owner_switches.i296.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 5, i32 6
  %140 = load i64, ptr %n_owner_switches.i296.i, align 8
  %n_owner_switches19.i297.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 5, i32 6
  %141 = load i64, ptr %n_owner_switches19.i297.i, align 8
  %add20.i298.i = add i64 %141, %140
  store i64 %add20.i298.i, ptr %n_owner_switches19.i297.i, align 8
  %n_lock_ops.i299.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 5, i32 8
  %142 = load i64, ptr %n_lock_ops.i299.i, align 8
  %n_lock_ops21.i300.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 5, i32 8
  %143 = load i64, ptr %n_lock_ops21.i300.i, align 8
  %add22.i301.i = add i64 %143, %142
  store i64 %add22.i301.i, ptr %n_lock_ops21.i300.i, align 8
  %arrayidx126.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 6
  %arrayidx129.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 6
  tail call void @nstime_add(ptr noundef nonnull %arrayidx126.i, ptr noundef nonnull %arrayidx129.i) #14
  %max_wait_time.i305.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 6, i32 1
  %max_wait_time2.i306.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 6, i32 1
  %call.i307.i = tail call i32 @nstime_compare(ptr noundef nonnull %max_wait_time.i305.i, ptr noundef nonnull %max_wait_time2.i306.i) #14
  %cmp.i308.i = icmp slt i32 %call.i307.i, 0
  br i1 %cmp.i308.i, label %if.then.i329.i, label %if.end.i309.i

if.then.i329.i:                                   ; preds = %malloc_mutex_prof_merge.exit304.i
  tail call void @nstime_copy(ptr noundef nonnull %max_wait_time.i305.i, ptr noundef nonnull %max_wait_time2.i306.i) #14
  br label %if.end.i309.i

if.end.i309.i:                                    ; preds = %if.then.i329.i, %malloc_mutex_prof_merge.exit304.i
  %n_wait_times.i310.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 6, i32 2
  %144 = load i64, ptr %n_wait_times.i310.i, align 8
  %n_wait_times5.i311.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 6, i32 2
  %145 = load i64, ptr %n_wait_times5.i311.i, align 8
  %add.i312.i = add i64 %145, %144
  store i64 %add.i312.i, ptr %n_wait_times5.i311.i, align 8
  %n_spin_acquired.i313.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 6, i32 3
  %146 = load i64, ptr %n_spin_acquired.i313.i, align 8
  %n_spin_acquired6.i314.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 6, i32 3
  %147 = load i64, ptr %n_spin_acquired6.i314.i, align 8
  %add7.i315.i = add i64 %147, %146
  store i64 %add7.i315.i, ptr %n_spin_acquired6.i314.i, align 8
  %max_n_thds.i316.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 6, i32 4
  %148 = load i32, ptr %max_n_thds.i316.i, align 8
  %max_n_thds8.i317.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 6, i32 4
  %149 = load i32, ptr %max_n_thds8.i317.i, align 8
  %cmp9.i318.i = icmp ult i32 %148, %149
  br i1 %cmp9.i318.i, label %if.then10.i328.i, label %malloc_mutex_prof_merge.exit330.i

if.then10.i328.i:                                 ; preds = %if.end.i309.i
  store i32 %149, ptr %max_n_thds.i316.i, align 8
  br label %malloc_mutex_prof_merge.exit330.i

malloc_mutex_prof_merge.exit330.i:                ; preds = %if.then10.i328.i, %if.end.i309.i
  %n_waiting_thds.i319.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 6, i32 5
  %150 = load atomic i32, ptr %n_waiting_thds.i319.i monotonic, align 4
  %n_waiting_thds15.i320.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 6, i32 5
  %151 = load atomic i32, ptr %n_waiting_thds15.i320.i monotonic, align 4
  %add17.i321.i = add i32 %151, %150
  store atomic i32 %add17.i321.i, ptr %n_waiting_thds.i319.i monotonic, align 4
  %n_owner_switches.i322.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 6, i32 6
  %152 = load i64, ptr %n_owner_switches.i322.i, align 8
  %n_owner_switches19.i323.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 6, i32 6
  %153 = load i64, ptr %n_owner_switches19.i323.i, align 8
  %add20.i324.i = add i64 %153, %152
  store i64 %add20.i324.i, ptr %n_owner_switches19.i323.i, align 8
  %n_lock_ops.i325.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 6, i32 8
  %154 = load i64, ptr %n_lock_ops.i325.i, align 8
  %n_lock_ops21.i326.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 6, i32 8
  %155 = load i64, ptr %n_lock_ops21.i326.i, align 8
  %add22.i327.i = add i64 %155, %154
  store i64 %add22.i327.i, ptr %n_lock_ops21.i326.i, align 8
  %arrayidx132.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 7
  %arrayidx135.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 7
  tail call void @nstime_add(ptr noundef nonnull %arrayidx132.i, ptr noundef nonnull %arrayidx135.i) #14
  %max_wait_time.i331.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 7, i32 1
  %max_wait_time2.i332.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 7, i32 1
  %call.i333.i = tail call i32 @nstime_compare(ptr noundef nonnull %max_wait_time.i331.i, ptr noundef nonnull %max_wait_time2.i332.i) #14
  %cmp.i334.i = icmp slt i32 %call.i333.i, 0
  br i1 %cmp.i334.i, label %if.then.i355.i, label %if.end.i335.i

if.then.i355.i:                                   ; preds = %malloc_mutex_prof_merge.exit330.i
  tail call void @nstime_copy(ptr noundef nonnull %max_wait_time.i331.i, ptr noundef nonnull %max_wait_time2.i332.i) #14
  br label %if.end.i335.i

if.end.i335.i:                                    ; preds = %if.then.i355.i, %malloc_mutex_prof_merge.exit330.i
  %n_wait_times.i336.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 7, i32 2
  %156 = load i64, ptr %n_wait_times.i336.i, align 8
  %n_wait_times5.i337.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 7, i32 2
  %157 = load i64, ptr %n_wait_times5.i337.i, align 8
  %add.i338.i = add i64 %157, %156
  store i64 %add.i338.i, ptr %n_wait_times5.i337.i, align 8
  %n_spin_acquired.i339.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 7, i32 3
  %158 = load i64, ptr %n_spin_acquired.i339.i, align 8
  %n_spin_acquired6.i340.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 7, i32 3
  %159 = load i64, ptr %n_spin_acquired6.i340.i, align 8
  %add7.i341.i = add i64 %159, %158
  store i64 %add7.i341.i, ptr %n_spin_acquired6.i340.i, align 8
  %max_n_thds.i342.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 7, i32 4
  %160 = load i32, ptr %max_n_thds.i342.i, align 8
  %max_n_thds8.i343.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 7, i32 4
  %161 = load i32, ptr %max_n_thds8.i343.i, align 8
  %cmp9.i344.i = icmp ult i32 %160, %161
  br i1 %cmp9.i344.i, label %if.then10.i354.i, label %malloc_mutex_prof_merge.exit356.i

if.then10.i354.i:                                 ; preds = %if.end.i335.i
  store i32 %161, ptr %max_n_thds.i342.i, align 8
  br label %malloc_mutex_prof_merge.exit356.i

malloc_mutex_prof_merge.exit356.i:                ; preds = %if.then10.i354.i, %if.end.i335.i
  %n_waiting_thds.i345.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 7, i32 5
  %162 = load atomic i32, ptr %n_waiting_thds.i345.i monotonic, align 4
  %n_waiting_thds15.i346.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 7, i32 5
  %163 = load atomic i32, ptr %n_waiting_thds15.i346.i monotonic, align 4
  %add17.i347.i = add i32 %163, %162
  store atomic i32 %add17.i347.i, ptr %n_waiting_thds.i345.i monotonic, align 4
  %n_owner_switches.i348.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 7, i32 6
  %164 = load i64, ptr %n_owner_switches.i348.i, align 8
  %n_owner_switches19.i349.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 7, i32 6
  %165 = load i64, ptr %n_owner_switches19.i349.i, align 8
  %add20.i350.i = add i64 %165, %164
  store i64 %add20.i350.i, ptr %n_owner_switches19.i349.i, align 8
  %n_lock_ops.i351.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 7, i32 8
  %166 = load i64, ptr %n_lock_ops.i351.i, align 8
  %n_lock_ops21.i352.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 7, i32 8
  %167 = load i64, ptr %n_lock_ops21.i352.i, align 8
  %add22.i353.i = add i64 %167, %166
  store i64 %add22.i353.i, ptr %n_lock_ops21.i352.i, align 8
  %arrayidx138.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 8
  %arrayidx141.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 8
  tail call void @nstime_add(ptr noundef nonnull %arrayidx138.i, ptr noundef nonnull %arrayidx141.i) #14
  %max_wait_time.i357.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 8, i32 1
  %max_wait_time2.i358.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 8, i32 1
  %call.i359.i = tail call i32 @nstime_compare(ptr noundef nonnull %max_wait_time.i357.i, ptr noundef nonnull %max_wait_time2.i358.i) #14
  %cmp.i360.i = icmp slt i32 %call.i359.i, 0
  br i1 %cmp.i360.i, label %if.then.i381.i, label %if.end.i361.i

if.then.i381.i:                                   ; preds = %malloc_mutex_prof_merge.exit356.i
  tail call void @nstime_copy(ptr noundef nonnull %max_wait_time.i357.i, ptr noundef nonnull %max_wait_time2.i358.i) #14
  br label %if.end.i361.i

if.end.i361.i:                                    ; preds = %if.then.i381.i, %malloc_mutex_prof_merge.exit356.i
  %n_wait_times.i362.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 8, i32 2
  %168 = load i64, ptr %n_wait_times.i362.i, align 8
  %n_wait_times5.i363.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 8, i32 2
  %169 = load i64, ptr %n_wait_times5.i363.i, align 8
  %add.i364.i = add i64 %169, %168
  store i64 %add.i364.i, ptr %n_wait_times5.i363.i, align 8
  %n_spin_acquired.i365.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 8, i32 3
  %170 = load i64, ptr %n_spin_acquired.i365.i, align 8
  %n_spin_acquired6.i366.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 8, i32 3
  %171 = load i64, ptr %n_spin_acquired6.i366.i, align 8
  %add7.i367.i = add i64 %171, %170
  store i64 %add7.i367.i, ptr %n_spin_acquired6.i366.i, align 8
  %max_n_thds.i368.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 8, i32 4
  %172 = load i32, ptr %max_n_thds.i368.i, align 8
  %max_n_thds8.i369.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 8, i32 4
  %173 = load i32, ptr %max_n_thds8.i369.i, align 8
  %cmp9.i370.i = icmp ult i32 %172, %173
  br i1 %cmp9.i370.i, label %if.then10.i380.i, label %malloc_mutex_prof_merge.exit382.i

if.then10.i380.i:                                 ; preds = %if.end.i361.i
  store i32 %173, ptr %max_n_thds.i368.i, align 8
  br label %malloc_mutex_prof_merge.exit382.i

malloc_mutex_prof_merge.exit382.i:                ; preds = %if.then10.i380.i, %if.end.i361.i
  %n_waiting_thds.i371.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 8, i32 5
  %174 = load atomic i32, ptr %n_waiting_thds.i371.i monotonic, align 4
  %n_waiting_thds15.i372.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 8, i32 5
  %175 = load atomic i32, ptr %n_waiting_thds15.i372.i monotonic, align 4
  %add17.i373.i = add i32 %175, %174
  store atomic i32 %add17.i373.i, ptr %n_waiting_thds.i371.i monotonic, align 4
  %n_owner_switches.i374.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 8, i32 6
  %176 = load i64, ptr %n_owner_switches.i374.i, align 8
  %n_owner_switches19.i375.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 8, i32 6
  %177 = load i64, ptr %n_owner_switches19.i375.i, align 8
  %add20.i376.i = add i64 %177, %176
  store i64 %add20.i376.i, ptr %n_owner_switches19.i375.i, align 8
  %n_lock_ops.i377.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 8, i32 8
  %178 = load i64, ptr %n_lock_ops.i377.i, align 8
  %n_lock_ops21.i378.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 8, i32 8
  %179 = load i64, ptr %n_lock_ops21.i378.i, align 8
  %add22.i379.i = add i64 %179, %178
  store i64 %add22.i379.i, ptr %n_lock_ops21.i378.i, align 8
  %arrayidx144.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 9
  %arrayidx147.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 9
  tail call void @nstime_add(ptr noundef nonnull %arrayidx144.i, ptr noundef nonnull %arrayidx147.i) #14
  %max_wait_time.i383.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 9, i32 1
  %max_wait_time2.i384.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 9, i32 1
  %call.i385.i = tail call i32 @nstime_compare(ptr noundef nonnull %max_wait_time.i383.i, ptr noundef nonnull %max_wait_time2.i384.i) #14
  %cmp.i386.i = icmp slt i32 %call.i385.i, 0
  br i1 %cmp.i386.i, label %if.then.i407.i, label %if.end.i387.i

if.then.i407.i:                                   ; preds = %malloc_mutex_prof_merge.exit382.i
  tail call void @nstime_copy(ptr noundef nonnull %max_wait_time.i383.i, ptr noundef nonnull %max_wait_time2.i384.i) #14
  br label %if.end.i387.i

if.end.i387.i:                                    ; preds = %if.then.i407.i, %malloc_mutex_prof_merge.exit382.i
  %n_wait_times.i388.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 9, i32 2
  %180 = load i64, ptr %n_wait_times.i388.i, align 8
  %n_wait_times5.i389.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 9, i32 2
  %181 = load i64, ptr %n_wait_times5.i389.i, align 8
  %add.i390.i = add i64 %181, %180
  store i64 %add.i390.i, ptr %n_wait_times5.i389.i, align 8
  %n_spin_acquired.i391.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 9, i32 3
  %182 = load i64, ptr %n_spin_acquired.i391.i, align 8
  %n_spin_acquired6.i392.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 9, i32 3
  %183 = load i64, ptr %n_spin_acquired6.i392.i, align 8
  %add7.i393.i = add i64 %183, %182
  store i64 %add7.i393.i, ptr %n_spin_acquired6.i392.i, align 8
  %max_n_thds.i394.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 9, i32 4
  %184 = load i32, ptr %max_n_thds.i394.i, align 8
  %max_n_thds8.i395.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 9, i32 4
  %185 = load i32, ptr %max_n_thds8.i395.i, align 8
  %cmp9.i396.i = icmp ult i32 %184, %185
  br i1 %cmp9.i396.i, label %if.then10.i406.i, label %malloc_mutex_prof_merge.exit408.i

if.then10.i406.i:                                 ; preds = %if.end.i387.i
  store i32 %185, ptr %max_n_thds.i394.i, align 8
  br label %malloc_mutex_prof_merge.exit408.i

malloc_mutex_prof_merge.exit408.i:                ; preds = %if.then10.i406.i, %if.end.i387.i
  %n_waiting_thds.i397.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 9, i32 5
  %186 = load atomic i32, ptr %n_waiting_thds.i397.i monotonic, align 4
  %n_waiting_thds15.i398.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 9, i32 5
  %187 = load atomic i32, ptr %n_waiting_thds15.i398.i monotonic, align 4
  %add17.i399.i = add i32 %187, %186
  store atomic i32 %add17.i399.i, ptr %n_waiting_thds.i397.i monotonic, align 4
  %n_owner_switches.i400.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 9, i32 6
  %188 = load i64, ptr %n_owner_switches.i400.i, align 8
  %n_owner_switches19.i401.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 9, i32 6
  %189 = load i64, ptr %n_owner_switches19.i401.i, align 8
  %add20.i402.i = add i64 %189, %188
  store i64 %add20.i402.i, ptr %n_owner_switches19.i401.i, align 8
  %n_lock_ops.i403.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 9, i32 8
  %190 = load i64, ptr %n_lock_ops.i403.i, align 8
  %n_lock_ops21.i404.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 9, i32 8
  %191 = load i64, ptr %n_lock_ops21.i404.i, align 8
  %add22.i405.i = add i64 %191, %190
  store i64 %add22.i405.i, ptr %n_lock_ops21.i404.i, align 8
  %arrayidx150.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 10
  %arrayidx153.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 10
  tail call void @nstime_add(ptr noundef nonnull %arrayidx150.i, ptr noundef nonnull %arrayidx153.i) #14
  %max_wait_time.i409.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 10, i32 1
  %max_wait_time2.i410.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 10, i32 1
  %call.i411.i = tail call i32 @nstime_compare(ptr noundef nonnull %max_wait_time.i409.i, ptr noundef nonnull %max_wait_time2.i410.i) #14
  %cmp.i412.i = icmp slt i32 %call.i411.i, 0
  br i1 %cmp.i412.i, label %if.then.i433.i, label %if.end.i413.i

if.then.i433.i:                                   ; preds = %malloc_mutex_prof_merge.exit408.i
  tail call void @nstime_copy(ptr noundef nonnull %max_wait_time.i409.i, ptr noundef nonnull %max_wait_time2.i410.i) #14
  br label %if.end.i413.i

if.end.i413.i:                                    ; preds = %if.then.i433.i, %malloc_mutex_prof_merge.exit408.i
  %n_wait_times.i414.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 10, i32 2
  %192 = load i64, ptr %n_wait_times.i414.i, align 8
  %n_wait_times5.i415.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 10, i32 2
  %193 = load i64, ptr %n_wait_times5.i415.i, align 8
  %add.i416.i = add i64 %193, %192
  store i64 %add.i416.i, ptr %n_wait_times5.i415.i, align 8
  %n_spin_acquired.i417.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 10, i32 3
  %194 = load i64, ptr %n_spin_acquired.i417.i, align 8
  %n_spin_acquired6.i418.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 10, i32 3
  %195 = load i64, ptr %n_spin_acquired6.i418.i, align 8
  %add7.i419.i = add i64 %195, %194
  store i64 %add7.i419.i, ptr %n_spin_acquired6.i418.i, align 8
  %max_n_thds.i420.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 10, i32 4
  %196 = load i32, ptr %max_n_thds.i420.i, align 8
  %max_n_thds8.i421.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 10, i32 4
  %197 = load i32, ptr %max_n_thds8.i421.i, align 8
  %cmp9.i422.i = icmp ult i32 %196, %197
  br i1 %cmp9.i422.i, label %if.then10.i432.i, label %malloc_mutex_prof_merge.exit434.i

if.then10.i432.i:                                 ; preds = %if.end.i413.i
  store i32 %197, ptr %max_n_thds.i420.i, align 8
  br label %malloc_mutex_prof_merge.exit434.i

malloc_mutex_prof_merge.exit434.i:                ; preds = %if.then10.i432.i, %if.end.i413.i
  %n_waiting_thds.i423.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 10, i32 5
  %198 = load atomic i32, ptr %n_waiting_thds.i423.i monotonic, align 4
  %n_waiting_thds15.i424.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 10, i32 5
  %199 = load atomic i32, ptr %n_waiting_thds15.i424.i monotonic, align 4
  %add17.i425.i = add i32 %199, %198
  store atomic i32 %add17.i425.i, ptr %n_waiting_thds.i423.i monotonic, align 4
  %n_owner_switches.i426.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 10, i32 6
  %200 = load i64, ptr %n_owner_switches.i426.i, align 8
  %n_owner_switches19.i427.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 10, i32 6
  %201 = load i64, ptr %n_owner_switches19.i427.i, align 8
  %add20.i428.i = add i64 %201, %200
  store i64 %add20.i428.i, ptr %n_owner_switches19.i427.i, align 8
  %n_lock_ops.i429.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 10, i32 8
  %202 = load i64, ptr %n_lock_ops.i429.i, align 8
  %n_lock_ops21.i430.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 10, i32 8
  %203 = load i64, ptr %n_lock_ops21.i430.i, align 8
  %add22.i431.i = add i64 %203, %202
  store i64 %add22.i431.i, ptr %n_lock_ops21.i430.i, align 8
  %arrayidx156.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 11
  %arrayidx159.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 11
  tail call void @nstime_add(ptr noundef nonnull %arrayidx156.i, ptr noundef nonnull %arrayidx159.i) #14
  %max_wait_time.i435.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 11, i32 1
  %max_wait_time2.i436.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 11, i32 1
  %call.i437.i = tail call i32 @nstime_compare(ptr noundef nonnull %max_wait_time.i435.i, ptr noundef nonnull %max_wait_time2.i436.i) #14
  %cmp.i438.i = icmp slt i32 %call.i437.i, 0
  br i1 %cmp.i438.i, label %if.then.i459.i, label %if.end.i439.i

if.then.i459.i:                                   ; preds = %malloc_mutex_prof_merge.exit434.i
  tail call void @nstime_copy(ptr noundef nonnull %max_wait_time.i435.i, ptr noundef nonnull %max_wait_time2.i436.i) #14
  br label %if.end.i439.i

if.end.i439.i:                                    ; preds = %if.then.i459.i, %malloc_mutex_prof_merge.exit434.i
  %n_wait_times.i440.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 11, i32 2
  %204 = load i64, ptr %n_wait_times.i440.i, align 8
  %n_wait_times5.i441.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 11, i32 2
  %205 = load i64, ptr %n_wait_times5.i441.i, align 8
  %add.i442.i = add i64 %205, %204
  store i64 %add.i442.i, ptr %n_wait_times5.i441.i, align 8
  %n_spin_acquired.i443.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 11, i32 3
  %206 = load i64, ptr %n_spin_acquired.i443.i, align 8
  %n_spin_acquired6.i444.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 11, i32 3
  %207 = load i64, ptr %n_spin_acquired6.i444.i, align 8
  %add7.i445.i = add i64 %207, %206
  store i64 %add7.i445.i, ptr %n_spin_acquired6.i444.i, align 8
  %max_n_thds.i446.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 11, i32 4
  %208 = load i32, ptr %max_n_thds.i446.i, align 8
  %max_n_thds8.i447.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 11, i32 4
  %209 = load i32, ptr %max_n_thds8.i447.i, align 8
  %cmp9.i448.i = icmp ult i32 %208, %209
  br i1 %cmp9.i448.i, label %if.then10.i458.i, label %malloc_mutex_prof_merge.exit460.i

if.then10.i458.i:                                 ; preds = %if.end.i439.i
  store i32 %209, ptr %max_n_thds.i446.i, align 8
  br label %malloc_mutex_prof_merge.exit460.i

malloc_mutex_prof_merge.exit460.i:                ; preds = %if.then10.i458.i, %if.end.i439.i
  %n_waiting_thds.i449.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 11, i32 5
  %210 = load atomic i32, ptr %n_waiting_thds.i449.i monotonic, align 4
  %n_waiting_thds15.i450.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 11, i32 5
  %211 = load atomic i32, ptr %n_waiting_thds15.i450.i monotonic, align 4
  %add17.i451.i = add i32 %211, %210
  store atomic i32 %add17.i451.i, ptr %n_waiting_thds.i449.i monotonic, align 4
  %n_owner_switches.i452.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 11, i32 6
  %212 = load i64, ptr %n_owner_switches.i452.i, align 8
  %n_owner_switches19.i453.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 11, i32 6
  %213 = load i64, ptr %n_owner_switches19.i453.i, align 8
  %add20.i454.i = add i64 %213, %212
  store i64 %add20.i454.i, ptr %n_owner_switches19.i453.i, align 8
  %n_lock_ops.i455.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 14, i64 11, i32 8
  %214 = load i64, ptr %n_lock_ops.i455.i, align 8
  %n_lock_ops21.i456.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 14, i64 11, i32 8
  %215 = load i64, ptr %n_lock_ops21.i456.i, align 8
  %add22.i457.i = add i64 %215, %214
  store i64 %add22.i457.i, ptr %n_lock_ops21.i456.i, align 8
  br i1 %destroyed, label %if.end188.i, label %if.then161.i

if.then161.i:                                     ; preds = %malloc_mutex_prof_merge.exit460.i
  %216 = load i64, ptr %58, align 8
  %217 = load i64, ptr %59, align 8
  %add165.i = add i64 %217, %216
  store i64 %add165.i, ptr %59, align 8
  %resident.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 1
  %218 = load i64, ptr %resident.i, align 8
  %resident168.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 1
  %219 = load i64, ptr %resident168.i, align 8
  %add169.i = add i64 %219, %218
  store i64 %add169.i, ptr %resident168.i, align 8
  %metadata_thp.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 2
  %220 = load i64, ptr %metadata_thp.i, align 8
  %metadata_thp172.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 2
  %221 = load i64, ptr %metadata_thp172.i, align 8
  %add173.i = add i64 %221, %220
  store i64 %add173.i, ptr %metadata_thp172.i, align 8
  %internal.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 4
  %internal176.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 4
  %222 = load atomic i64, ptr %internal.i monotonic, align 8
  %223 = load atomic i64, ptr %internal176.i monotonic, align 8
  %add.i461.i = add i64 %223, %222
  store atomic i64 %add.i461.i, ptr %internal.i monotonic, align 8
  %allocated_small.i28 = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 1
  %224 = load i64, ptr %allocated_small.i28, align 8
  %allocated_small183.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 1
  %225 = load i64, ptr %allocated_small183.i, align 8
  %add184.i = add i64 %225, %224
  store i64 %add184.i, ptr %allocated_small183.i, align 8
  br label %if.end188.i

if.end188.i:                                      ; preds = %if.then161.i, %malloc_mutex_prof_merge.exit460.i
  %nmalloc_small.i29 = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 2
  %226 = load i64, ptr %nmalloc_small.i29, align 8
  %nmalloc_small189.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 2
  %227 = load i64, ptr %nmalloc_small189.i, align 8
  %add190.i = add i64 %227, %226
  store i64 %add190.i, ptr %nmalloc_small189.i, align 8
  %ndalloc_small.i30 = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 3
  %228 = load i64, ptr %ndalloc_small.i30, align 8
  %ndalloc_small191.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 3
  %229 = load i64, ptr %ndalloc_small191.i, align 8
  %add192.i = add i64 %229, %228
  store i64 %add192.i, ptr %ndalloc_small191.i, align 8
  %nrequests_small.i31 = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 4
  %230 = load i64, ptr %nrequests_small.i31, align 8
  %nrequests_small193.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 4
  %231 = load i64, ptr %nrequests_small193.i, align 8
  %add194.i = add i64 %231, %230
  store i64 %add194.i, ptr %nrequests_small193.i, align 8
  %nfills_small.i32 = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 5
  %232 = load i64, ptr %nfills_small.i32, align 8
  %nfills_small195.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 5
  %233 = load i64, ptr %nfills_small195.i, align 8
  %add196.i = add i64 %233, %232
  store i64 %add196.i, ptr %nfills_small195.i, align 8
  %nflushes_small.i33 = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 6
  %234 = load i64, ptr %nflushes_small.i33, align 8
  %nflushes_small197.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 6
  %235 = load i64, ptr %nflushes_small197.i, align 8
  %add198.i = add i64 %235, %234
  store i64 %add198.i, ptr %nflushes_small197.i, align 8
  br i1 %destroyed, label %if.end208.i, label %if.then200.i

if.then200.i:                                     ; preds = %if.end188.i
  %allocated_large.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 5
  %236 = load i64, ptr %allocated_large.i, align 8
  %allocated_large203.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 5
  %237 = load i64, ptr %allocated_large203.i, align 8
  %add204.i = add i64 %237, %236
  store i64 %add204.i, ptr %allocated_large203.i, align 8
  br label %if.end208.i

if.end208.i:                                      ; preds = %if.then200.i, %if.end188.i
  %nmalloc_large.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 6
  %238 = load i64, ptr %nmalloc_large.i, align 8
  %nmalloc_large211.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 6
  %239 = load i64, ptr %nmalloc_large211.i, align 8
  %add212.i = add i64 %239, %238
  store i64 %add212.i, ptr %nmalloc_large211.i, align 8
  %ndalloc_large.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 7
  %240 = load i64, ptr %ndalloc_large.i, align 8
  %ndalloc_large215.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 7
  %241 = load i64, ptr %ndalloc_large215.i, align 8
  %add216.i = add i64 %241, %240
  store i64 %add216.i, ptr %ndalloc_large215.i, align 8
  %nrequests_large.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 10
  %242 = load i64, ptr %nrequests_large.i, align 8
  %nrequests_large219.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 10
  %243 = load i64, ptr %nrequests_large219.i, align 8
  %add220.i = add i64 %243, %242
  store i64 %add220.i, ptr %nrequests_large219.i, align 8
  %nflushes_large.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 9
  %244 = load i64, ptr %nflushes_large.i, align 8
  %nflushes_large223.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 9
  %245 = load i64, ptr %nflushes_large223.i, align 8
  %add224.i = add i64 %245, %244
  store i64 %add224.i, ptr %nflushes_large223.i, align 8
  %abandoned_vm.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 11, i32 1, i32 4
  %abandoned_vm231.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 11, i32 1, i32 4
  %246 = load atomic i64, ptr %abandoned_vm.i monotonic, align 8
  %247 = load atomic i64, ptr %abandoned_vm231.i monotonic, align 8
  %add.i462.i = add i64 %247, %246
  store atomic i64 %add.i462.i, ptr %abandoned_vm.i monotonic, align 8
  %tcache_bytes.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 12
  %248 = load i64, ptr %tcache_bytes.i, align 8
  %tcache_bytes234.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 12
  %249 = load i64, ptr %tcache_bytes234.i, align 8
  %add235.i = add i64 %249, %248
  store i64 %add235.i, ptr %tcache_bytes234.i, align 8
  %tcache_stashed_bytes.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 13
  %250 = load i64, ptr %tcache_stashed_bytes.i, align 8
  %tcache_stashed_bytes238.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 13
  %251 = load i64, ptr %tcache_stashed_bytes238.i, align 8
  %add239.i = add i64 %251, %250
  store i64 %add239.i, ptr %tcache_stashed_bytes238.i, align 8
  %252 = load i32, ptr %6, align 8
  %cmp.i = icmp eq i32 %252, 0
  br i1 %cmp.i, label %if.then240.i, label %for.body.i34.preheader

for.body.i34.preheader:                           ; preds = %if.then240.i, %if.end208.i
  br label %for.body.i34

if.then240.i:                                     ; preds = %if.end208.i
  %uptime.i = getelementptr inbounds %struct.arena_stats_s, ptr %59, i64 0, i32 16
  %uptime243.i = getelementptr inbounds %struct.arena_stats_s, ptr %58, i64 0, i32 16
  %253 = load i64, ptr %uptime243.i, align 8
  store i64 %253, ptr %uptime.i, align 8
  br label %for.body.i34.preheader

for.cond294.preheader.i:                          ; preds = %malloc_mutex_prof_merge.exit488.i
  br i1 %destroyed, label %for.body298.us.i, label %for.body298.i

for.body298.us.i:                                 ; preds = %for.cond294.preheader.i, %for.body298.us.i
  %indvars.iv504.i = phi i64 [ %indvars.iv.next505.i, %for.body298.us.i ], [ 0, %for.cond294.preheader.i ]
  %arrayidx300.us.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 8, i64 %indvars.iv504.i
  %arrayidx304.us.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 8, i64 %indvars.iv504.i
  %254 = load atomic i64, ptr %arrayidx304.us.i monotonic, align 8
  %255 = load atomic i64, ptr %arrayidx300.us.i monotonic, align 8
  %add.i.i489.us.i = add i64 %255, %254
  store atomic i64 %add.i.i489.us.i, ptr %arrayidx300.us.i monotonic, align 8
  %ndalloc309.us.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 8, i64 %indvars.iv504.i, i32 1
  %ndalloc313.us.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 8, i64 %indvars.iv504.i, i32 1
  %256 = load atomic i64, ptr %ndalloc313.us.i monotonic, align 8
  %257 = load atomic i64, ptr %ndalloc309.us.i monotonic, align 8
  %add.i.i490.us.i = add i64 %257, %256
  store atomic i64 %add.i.i490.us.i, ptr %ndalloc309.us.i monotonic, align 8
  %nrequests317.us.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 8, i64 %indvars.iv504.i, i32 2
  %nrequests321.us.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 8, i64 %indvars.iv504.i, i32 2
  %258 = load atomic i64, ptr %nrequests321.us.i monotonic, align 8
  %259 = load atomic i64, ptr %nrequests317.us.i monotonic, align 8
  %add.i.i491.us.i = add i64 %259, %258
  store atomic i64 %add.i.i491.us.i, ptr %nrequests317.us.i monotonic, align 8
  %indvars.iv.next505.i = add nuw nsw i64 %indvars.iv504.i, 1
  %exitcond507.not.i = icmp eq i64 %indvars.iv.next505.i, 196
  br i1 %exitcond507.not.i, label %for.body343.i.preheader, label %for.body298.us.i, !llvm.loop !14

for.body.i34:                                     ; preds = %for.body.i34.preheader, %malloc_mutex_prof_merge.exit488.i
  %indvars.iv.i35 = phi i64 [ %indvars.iv.next.i41, %malloc_mutex_prof_merge.exit488.i ], [ 0, %for.body.i34.preheader ]
  %arrayidx248.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 7, i64 %indvars.iv.i35
  %arrayidx251.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 7, i64 %indvars.iv.i35
  %260 = load i64, ptr %arrayidx248.i, align 8
  %261 = load i64, ptr %arrayidx251.i, align 8
  %add254.i = add i64 %261, %260
  store i64 %add254.i, ptr %arrayidx251.i, align 8
  %ndalloc.i36 = getelementptr inbounds %struct.bin_stats_s, ptr %arrayidx248.i, i64 0, i32 1
  %262 = load i64, ptr %ndalloc.i36, align 8
  %ndalloc255.i = getelementptr inbounds %struct.bin_stats_s, ptr %arrayidx251.i, i64 0, i32 1
  %263 = load i64, ptr %ndalloc255.i, align 8
  %add256.i = add i64 %263, %262
  store i64 %add256.i, ptr %ndalloc255.i, align 8
  %nrequests.i37 = getelementptr inbounds %struct.bin_stats_s, ptr %arrayidx248.i, i64 0, i32 2
  %264 = load i64, ptr %nrequests.i37, align 8
  %nrequests257.i = getelementptr inbounds %struct.bin_stats_s, ptr %arrayidx251.i, i64 0, i32 2
  %265 = load i64, ptr %nrequests257.i, align 8
  %add258.i = add i64 %265, %264
  store i64 %add258.i, ptr %nrequests257.i, align 8
  br i1 %destroyed, label %if.end266.i, label %if.then260.i

if.then260.i:                                     ; preds = %for.body.i34
  %curregs.i38 = getelementptr inbounds %struct.bin_stats_s, ptr %arrayidx248.i, i64 0, i32 3
  %266 = load i64, ptr %curregs.i38, align 8
  %curregs261.i = getelementptr inbounds %struct.bin_stats_s, ptr %arrayidx251.i, i64 0, i32 3
  %267 = load i64, ptr %curregs261.i, align 8
  %add262.i = add i64 %267, %266
  store i64 %add262.i, ptr %curregs261.i, align 8
  br label %if.end266.i

if.end266.i:                                      ; preds = %if.then260.i, %for.body.i34
  %nfills.i39 = getelementptr inbounds %struct.bin_stats_s, ptr %arrayidx248.i, i64 0, i32 4
  %268 = load i64, ptr %nfills.i39, align 8
  %nfills267.i = getelementptr inbounds %struct.bin_stats_s, ptr %arrayidx251.i, i64 0, i32 4
  %269 = load i64, ptr %nfills267.i, align 8
  %add268.i = add i64 %269, %268
  store i64 %add268.i, ptr %nfills267.i, align 8
  %nflushes.i40 = getelementptr inbounds %struct.bin_stats_s, ptr %arrayidx248.i, i64 0, i32 5
  %270 = load i64, ptr %nflushes.i40, align 8
  %nflushes269.i = getelementptr inbounds %struct.bin_stats_s, ptr %arrayidx251.i, i64 0, i32 5
  %271 = load i64, ptr %nflushes269.i, align 8
  %add270.i = add i64 %271, %270
  store i64 %add270.i, ptr %nflushes269.i, align 8
  %nslabs.i = getelementptr inbounds %struct.bin_stats_s, ptr %arrayidx248.i, i64 0, i32 6
  %272 = load i64, ptr %nslabs.i, align 8
  %nslabs271.i = getelementptr inbounds %struct.bin_stats_s, ptr %arrayidx251.i, i64 0, i32 6
  %273 = load i64, ptr %nslabs271.i, align 8
  %add272.i = add i64 %273, %272
  store i64 %add272.i, ptr %nslabs271.i, align 8
  %reslabs.i = getelementptr inbounds %struct.bin_stats_s, ptr %arrayidx248.i, i64 0, i32 7
  %274 = load i64, ptr %reslabs.i, align 8
  %reslabs273.i = getelementptr inbounds %struct.bin_stats_s, ptr %arrayidx251.i, i64 0, i32 7
  %275 = load i64, ptr %reslabs273.i, align 8
  %add274.i = add i64 %275, %274
  store i64 %add274.i, ptr %reslabs273.i, align 8
  br i1 %destroyed, label %if.end286.i, label %if.then276.i

if.then276.i:                                     ; preds = %if.end266.i
  %curslabs.i = getelementptr inbounds %struct.bin_stats_s, ptr %arrayidx248.i, i64 0, i32 8
  %276 = load i64, ptr %curslabs.i, align 8
  %curslabs277.i = getelementptr inbounds %struct.bin_stats_s, ptr %arrayidx251.i, i64 0, i32 8
  %277 = load i64, ptr %curslabs277.i, align 8
  %add278.i = add i64 %277, %276
  store i64 %add278.i, ptr %curslabs277.i, align 8
  %nonfull_slabs.i = getelementptr inbounds %struct.bin_stats_s, ptr %arrayidx248.i, i64 0, i32 9
  %278 = load i64, ptr %nonfull_slabs.i, align 8
  %nonfull_slabs279.i = getelementptr inbounds %struct.bin_stats_s, ptr %arrayidx251.i, i64 0, i32 9
  %279 = load i64, ptr %nonfull_slabs279.i, align 8
  %add280.i = add i64 %279, %278
  store i64 %add280.i, ptr %nonfull_slabs279.i, align 8
  br label %if.end286.i

if.end286.i:                                      ; preds = %if.then276.i, %if.end266.i
  %mutex_data.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 7, i64 %indvars.iv.i35, i32 1
  %mutex_data293.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 7, i64 %indvars.iv.i35, i32 1
  tail call void @nstime_add(ptr noundef nonnull %mutex_data.i, ptr noundef nonnull %mutex_data293.i) #14
  %max_wait_time.i463.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 7, i64 %indvars.iv.i35, i32 1, i32 1
  %max_wait_time2.i464.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 7, i64 %indvars.iv.i35, i32 1, i32 1
  %call.i465.i = tail call i32 @nstime_compare(ptr noundef nonnull %max_wait_time.i463.i, ptr noundef nonnull %max_wait_time2.i464.i) #14
  %cmp.i466.i = icmp slt i32 %call.i465.i, 0
  br i1 %cmp.i466.i, label %if.then.i487.i, label %if.end.i467.i

if.then.i487.i:                                   ; preds = %if.end286.i
  tail call void @nstime_copy(ptr noundef nonnull %max_wait_time.i463.i, ptr noundef nonnull %max_wait_time2.i464.i) #14
  br label %if.end.i467.i

if.end.i467.i:                                    ; preds = %if.then.i487.i, %if.end286.i
  %n_wait_times.i468.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 7, i64 %indvars.iv.i35, i32 1, i32 2
  %280 = load i64, ptr %n_wait_times.i468.i, align 8
  %n_wait_times5.i469.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 7, i64 %indvars.iv.i35, i32 1, i32 2
  %281 = load i64, ptr %n_wait_times5.i469.i, align 8
  %add.i470.i = add i64 %281, %280
  store i64 %add.i470.i, ptr %n_wait_times5.i469.i, align 8
  %n_spin_acquired.i471.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 7, i64 %indvars.iv.i35, i32 1, i32 3
  %282 = load i64, ptr %n_spin_acquired.i471.i, align 8
  %n_spin_acquired6.i472.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 7, i64 %indvars.iv.i35, i32 1, i32 3
  %283 = load i64, ptr %n_spin_acquired6.i472.i, align 8
  %add7.i473.i = add i64 %283, %282
  store i64 %add7.i473.i, ptr %n_spin_acquired6.i472.i, align 8
  %max_n_thds.i474.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 7, i64 %indvars.iv.i35, i32 1, i32 4
  %284 = load i32, ptr %max_n_thds.i474.i, align 8
  %max_n_thds8.i475.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 7, i64 %indvars.iv.i35, i32 1, i32 4
  %285 = load i32, ptr %max_n_thds8.i475.i, align 8
  %cmp9.i476.i = icmp ult i32 %284, %285
  br i1 %cmp9.i476.i, label %if.then10.i486.i, label %malloc_mutex_prof_merge.exit488.i

if.then10.i486.i:                                 ; preds = %if.end.i467.i
  store i32 %285, ptr %max_n_thds.i474.i, align 8
  br label %malloc_mutex_prof_merge.exit488.i

malloc_mutex_prof_merge.exit488.i:                ; preds = %if.then10.i486.i, %if.end.i467.i
  %n_waiting_thds.i477.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 7, i64 %indvars.iv.i35, i32 1, i32 5
  %286 = load atomic i32, ptr %n_waiting_thds.i477.i monotonic, align 4
  %n_waiting_thds15.i478.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 7, i64 %indvars.iv.i35, i32 1, i32 5
  %287 = load atomic i32, ptr %n_waiting_thds15.i478.i monotonic, align 4
  %add17.i479.i = add i32 %287, %286
  store atomic i32 %add17.i479.i, ptr %n_waiting_thds.i477.i monotonic, align 4
  %n_owner_switches.i480.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 7, i64 %indvars.iv.i35, i32 1, i32 6
  %288 = load i64, ptr %n_owner_switches.i480.i, align 8
  %n_owner_switches19.i481.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 7, i64 %indvars.iv.i35, i32 1, i32 6
  %289 = load i64, ptr %n_owner_switches19.i481.i, align 8
  %add20.i482.i = add i64 %289, %288
  store i64 %add20.i482.i, ptr %n_owner_switches19.i481.i, align 8
  %n_lock_ops.i483.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 7, i64 %indvars.iv.i35, i32 1, i32 8
  %290 = load i64, ptr %n_lock_ops.i483.i, align 8
  %n_lock_ops21.i484.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 7, i64 %indvars.iv.i35, i32 1, i32 8
  %291 = load i64, ptr %n_lock_ops21.i484.i, align 8
  %add22.i485.i = add i64 %291, %290
  store i64 %add22.i485.i, ptr %n_lock_ops21.i484.i, align 8
  %indvars.iv.next.i41 = add nuw nsw i64 %indvars.iv.i35, 1
  %exitcond.not.i42 = icmp eq i64 %indvars.iv.next.i41, 39
  br i1 %exitcond.not.i42, label %for.cond294.preheader.i, label %for.body.i34, !llvm.loop !15

for.body298.i:                                    ; preds = %for.cond294.preheader.i, %for.body298.i
  %indvars.iv500.i = phi i64 [ %indvars.iv.next501.i, %for.body298.i ], [ 0, %for.cond294.preheader.i ]
  %arrayidx300.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 8, i64 %indvars.iv500.i
  %arrayidx304.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 8, i64 %indvars.iv500.i
  %292 = load atomic i64, ptr %arrayidx304.i monotonic, align 8
  %293 = load atomic i64, ptr %arrayidx300.i monotonic, align 8
  %add.i.i489.i = add i64 %293, %292
  store atomic i64 %add.i.i489.i, ptr %arrayidx300.i monotonic, align 8
  %ndalloc309.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 8, i64 %indvars.iv500.i, i32 1
  %ndalloc313.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 8, i64 %indvars.iv500.i, i32 1
  %294 = load atomic i64, ptr %ndalloc313.i monotonic, align 8
  %295 = load atomic i64, ptr %ndalloc309.i monotonic, align 8
  %add.i.i490.i = add i64 %295, %294
  store atomic i64 %add.i.i490.i, ptr %ndalloc309.i monotonic, align 8
  %nrequests317.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 8, i64 %indvars.iv500.i, i32 2
  %nrequests321.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 8, i64 %indvars.iv500.i, i32 2
  %296 = load atomic i64, ptr %nrequests321.i monotonic, align 8
  %297 = load atomic i64, ptr %nrequests317.i monotonic, align 8
  %add.i.i491.i = add i64 %297, %296
  store atomic i64 %add.i.i491.i, ptr %nrequests317.i monotonic, align 8
  %curlextents.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 8, i64 %indvars.iv500.i, i32 5
  %298 = load i64, ptr %curlextents.i, align 8
  %curlextents330.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 8, i64 %indvars.iv500.i, i32 5
  %299 = load i64, ptr %curlextents330.i, align 8
  %add331.i = add i64 %299, %298
  store i64 %add331.i, ptr %curlextents330.i, align 8
  %indvars.iv.next501.i = add nuw nsw i64 %indvars.iv500.i, 1
  %exitcond503.not.i = icmp eq i64 %indvars.iv.next501.i, 196
  br i1 %exitcond503.not.i, label %for.body343.i.preheader, label %for.body298.i, !llvm.loop !14

for.body343.i.preheader:                          ; preds = %for.body298.i, %for.body298.us.i
  br label %for.body343.i

for.body343.i:                                    ; preds = %for.body343.i.preheader, %for.body343.i
  %indvars.iv508.i = phi i64 [ %indvars.iv.next509.i, %for.body343.i ], [ 0, %for.body343.i.preheader ]
  %arrayidx345.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 9, i64 %indvars.iv508.i
  %300 = load i64, ptr %arrayidx345.i, align 8
  %arrayidx348.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 9, i64 %indvars.iv508.i
  %301 = load i64, ptr %arrayidx348.i, align 8
  %add350.i = add i64 %301, %300
  store i64 %add350.i, ptr %arrayidx348.i, align 8
  %nmuzzy.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 9, i64 %indvars.iv508.i, i32 2
  %302 = load i64, ptr %nmuzzy.i, align 8
  %nmuzzy357.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 9, i64 %indvars.iv508.i, i32 2
  %303 = load i64, ptr %nmuzzy357.i, align 8
  %add358.i = add i64 %303, %302
  store i64 %add358.i, ptr %nmuzzy357.i, align 8
  %nretained.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 9, i64 %indvars.iv508.i, i32 4
  %304 = load i64, ptr %nretained.i, align 8
  %nretained365.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 9, i64 %indvars.iv508.i, i32 4
  %305 = load i64, ptr %nretained365.i, align 8
  %add366.i = add i64 %305, %304
  store i64 %add366.i, ptr %nretained365.i, align 8
  %dirty_bytes.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 9, i64 %indvars.iv508.i, i32 1
  %306 = load i64, ptr %dirty_bytes.i, align 8
  %dirty_bytes373.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 9, i64 %indvars.iv508.i, i32 1
  %307 = load i64, ptr %dirty_bytes373.i, align 8
  %add374.i = add i64 %307, %306
  store i64 %add374.i, ptr %dirty_bytes373.i, align 8
  %muzzy_bytes.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 9, i64 %indvars.iv508.i, i32 3
  %308 = load i64, ptr %muzzy_bytes.i, align 8
  %muzzy_bytes381.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 9, i64 %indvars.iv508.i, i32 3
  %309 = load i64, ptr %muzzy_bytes381.i, align 8
  %add382.i = add i64 %309, %308
  store i64 %add382.i, ptr %muzzy_bytes381.i, align 8
  %retained_bytes.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 9, i64 %indvars.iv508.i, i32 5
  %310 = load i64, ptr %retained_bytes.i, align 8
  %retained_bytes389.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 9, i64 %indvars.iv508.i, i32 5
  %311 = load i64, ptr %retained_bytes389.i, align 8
  %add390.i = add i64 %311, %310
  store i64 %add390.i, ptr %retained_bytes389.i, align 8
  %indvars.iv.next509.i = add nuw nsw i64 %indvars.iv508.i, 1
  %exitcond511.not.i = icmp eq i64 %indvars.iv.next509.i, 199
  br i1 %exitcond511.not.i, label %ctl_arena_stats_sdmerge.exit, label %for.body343.i, !llvm.loop !16

ctl_arena_stats_sdmerge.exit:                     ; preds = %for.body343.i
  %hpastats.i43 = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 10
  %hpastats394.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 10
  tail call void @hpa_shard_stats_accum(ptr noundef nonnull %hpastats.i43, ptr noundef nonnull %hpastats394.i) #14
  %secstats.i44 = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %59, i64 0, i32 11
  %secstats395.i = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %58, i64 0, i32 11
  %secstats395.val.i = load i64, ptr %secstats395.i, align 8
  %312 = load i64, ptr %secstats.i44, align 8
  %add.i492.i = add i64 %312, %secstats395.val.i
  store i64 %add.i492.i, ptr %secstats.i44, align 8
  ret void
}

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn
declare void @llvm.stackrestore.p0(ptr) #5

declare ptr @tsd_fetch_slow(ptr noundef, i1 noundef zeroext) local_unnamed_addr #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare nonnull ptr @llvm.threadlocal.address.p0(ptr nonnull) #6

declare ptr @arena_init(ptr noundef, i32 noundef, ptr noundef) local_unnamed_addr #1

declare void @arena_stats_merge(ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef) local_unnamed_addr #1

; Function Attrs: mustprogress nocallback nofree nounwind willreturn memory(argmem: readwrite)
declare void @llvm.memcpy.p0.p0.i64(ptr noalias nocapture writeonly, ptr noalias nocapture readonly, i64, i1 immarg) #7

declare void @hpa_shard_stats_accum(ptr noundef, ptr noundef) local_unnamed_addr #1

declare void @nstime_add(ptr noundef, ptr noundef) local_unnamed_addr #1

declare i32 @nstime_compare(ptr noundef, ptr noundef) local_unnamed_addr #1

declare void @nstime_copy(ptr noundef, ptr noundef) local_unnamed_addr #1

declare zeroext i1 @background_thread_stats_read(ptr noundef, ptr noundef) local_unnamed_addr #1

; Function Attrs: nounwind
declare i32 @pthread_mutex_unlock(ptr noundef) local_unnamed_addr #3

; Function Attrs: mustprogress nofree nounwind willreturn memory(argmem: read)
declare ptr @strchr(ptr noundef, i32 noundef) local_unnamed_addr #8

; Function Attrs: mustprogress nofree nounwind willreturn memory(argmem: read)
declare i64 @strlen(ptr nocapture noundef) local_unnamed_addr #8

; Function Attrs: mustprogress nofree nounwind willreturn memory(argmem: read)
declare i32 @strncmp(ptr nocapture noundef, ptr nocapture noundef, i64 noundef) local_unnamed_addr #8

declare i64 @malloc_strtoumax(ptr noundef, ptr noundef, i32 noundef) local_unnamed_addr #1

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @version_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca ptr, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store ptr @.str.15, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %0, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store ptr @.str.15, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @epoch_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp.not = icmp eq ptr %newp, null
  br i1 %cmp.not, label %do.body8, label %if.then

if.then:                                          ; preds = %malloc_mutex_lock.exit
  %cmp1.not = icmp eq i64 %newlen, 8
  br i1 %cmp1.not, label %if.then5, label %label_return

if.then5:                                         ; preds = %if.then
  tail call fastcc void @ctl_refresh(ptr noundef %tsd)
  br label %do.body8

do.body8:                                         ; preds = %malloc_mutex_lock.exit, %if.then5
  %cmp9 = icmp ne ptr %oldp, null
  %cmp10 = icmp ne ptr %oldlenp, null
  %or.cond = and i1 %cmp9, %cmp10
  br i1 %or.cond, label %if.then11, label %label_return

if.then11:                                        ; preds = %do.body8
  %3 = load i64, ptr %oldlenp, align 8
  %cmp12.not = icmp eq i64 %3, 8
  br i1 %cmp12.not, label %if.end15, label %if.then13

if.then13:                                        ; preds = %if.then11
  %spec.select = tail call i64 @llvm.umin.i64(i64 %3, i64 8)
  %4 = load ptr, ptr @ctl_arenas, align 8
  tail call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr align 1 %4, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end15:                                         ; preds = %if.then11
  %5 = load ptr, ptr @ctl_arenas, align 8
  %6 = load i64, ptr %5, align 8
  store i64 %6, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end15, %do.body8, %if.then, %if.then13
  %ret.0 = phi i32 [ 22, %if.then13 ], [ 22, %if.then ], [ 0, %do.body8 ], [ 0, %if.end15 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @background_thread_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i8, align 1
  tail call void @background_thread_ctl_init(ptr noundef %tsd) #14
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %call.i.i33 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i34 = icmp eq i32 %call.i.i33, 0
  br i1 %cmp.i.not.i34, label %if.end.i36, label %if.then.i35

if.then.i35:                                      ; preds = %malloc_mutex_lock.exit
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @background_thread_lock) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i36

if.end.i36:                                       ; preds = %if.then.i35, %malloc_mutex_lock.exit
  %3 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i37 = add i64 %3, 1
  store i64 %inc.i.i37, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %4 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i38 = icmp eq ptr %4, %tsd
  br i1 %cmp.not.i.i38, label %malloc_mutex_lock.exit41, label %if.then.i.i39

if.then.i.i39:                                    ; preds = %if.end.i36
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %5 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i40 = add i64 %5, 1
  store i64 %inc2.i.i40, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit41

malloc_mutex_lock.exit41:                         ; preds = %if.end.i36, %if.then.i.i39
  %cmp = icmp eq ptr %newp, null
  br i1 %cmp, label %monotonic.i, label %if.else

monotonic.i:                                      ; preds = %malloc_mutex_lock.exit41
  %6 = load atomic i8, ptr @background_thread_enabled_state monotonic, align 1
  %7 = and i8 %6, 1
  store i8 %7, ptr %oldval, align 1
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond = and i1 %cmp4, %cmp5
  br i1 %or.cond, label %if.then6, label %if.end56

if.then6:                                         ; preds = %monotonic.i
  %8 = load i64, ptr %oldlenp, align 8
  switch i64 %8, label %cond.end [
    i64 1, label %if.end
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then6
  br label %cond.end

cond.end:                                         ; preds = %if.then6, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then6 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end:                                           ; preds = %if.then6
  store i8 %7, ptr %oldp, align 1
  br label %if.end56

if.else:                                          ; preds = %malloc_mutex_lock.exit41
  %cmp12.not = icmp eq i64 %newlen, 1
  br i1 %cmp12.not, label %monotonic.i85, label %label_return

monotonic.i85:                                    ; preds = %if.else
  %9 = load atomic i8, ptr @background_thread_enabled_state monotonic, align 1
  %10 = and i8 %9, 1
  store i8 %10, ptr %oldval, align 1
  %cmp18 = icmp ne ptr %oldp, null
  %cmp20 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp18, %cmp20
  br i1 %or.cond1, label %if.then21, label %do.end34

if.then21:                                        ; preds = %monotonic.i85
  %11 = load i64, ptr %oldlenp, align 8
  switch i64 %11, label %cond.end28 [
    i64 1, label %if.end30
    i64 0, label %cond.false27
  ]

cond.false27:                                     ; preds = %if.then21
  br label %cond.end28

cond.end28:                                       ; preds = %if.then21, %cond.false27
  %cond29 = phi i64 [ 0, %cond.false27 ], [ 1, %if.then21 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond29, i1 false)
  store i64 %cond29, ptr %oldlenp, align 8
  br label %label_return

if.end30:                                         ; preds = %if.then21
  store i8 %10, ptr %oldp, align 1
  br label %do.end34

do.end34:                                         ; preds = %monotonic.i85, %if.end30
  %12 = load i8, ptr %newp, align 1
  %13 = and i8 %12, 1
  %cmp40 = icmp eq i8 %13, %10
  br i1 %cmp40, label %label_return, label %if.end43

if.end43:                                         ; preds = %do.end34
  %tobool35.not = icmp eq i8 %13, 0
  store atomic i8 %13, ptr @background_thread_enabled_state monotonic, align 1
  br i1 %tobool35.not, label %if.else51, label %if.then47

if.then47:                                        ; preds = %if.end43
  %call48 = tail call zeroext i1 @background_threads_enable(ptr noundef %tsd) #14
  br i1 %call48, label %label_return, label %if.end56

if.else51:                                        ; preds = %if.end43
  %call52 = tail call zeroext i1 @background_threads_disable(ptr noundef %tsd) #14
  br i1 %call52, label %label_return, label %if.end56

if.end56:                                         ; preds = %if.then47, %if.else51, %if.end, %monotonic.i
  br label %label_return

label_return:                                     ; preds = %if.else51, %if.then47, %do.end34, %if.else, %if.end56, %cond.end28, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 0, %if.end56 ], [ 22, %cond.end28 ], [ 22, %if.else ], [ 0, %do.end34 ], [ 14, %if.then47 ], [ 14, %if.else51 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 1)) #14
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i42 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @max_background_threads_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  tail call void @background_thread_ctl_init(ptr noundef %tsd) #14
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %call.i.i39 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i40 = icmp eq i32 %call.i.i39, 0
  br i1 %cmp.i.not.i40, label %if.end.i42, label %if.then.i41

if.then.i41:                                      ; preds = %malloc_mutex_lock.exit
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @background_thread_lock) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i42

if.end.i42:                                       ; preds = %if.then.i41, %malloc_mutex_lock.exit
  %3 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i43 = add i64 %3, 1
  store i64 %inc.i.i43, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %4 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i44 = icmp eq ptr %4, %tsd
  br i1 %cmp.not.i.i44, label %malloc_mutex_lock.exit47, label %if.then.i.i45

if.then.i.i45:                                    ; preds = %if.end.i42
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %5 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i46 = add i64 %5, 1
  store i64 %inc2.i.i46, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit47

malloc_mutex_lock.exit47:                         ; preds = %if.end.i42, %if.then.i.i45
  %cmp = icmp eq ptr %newp, null
  br i1 %cmp, label %if.then, label %if.else

if.then:                                          ; preds = %malloc_mutex_lock.exit47
  %6 = load i64, ptr @max_background_threads, align 8
  store i64 %6, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond = and i1 %cmp3, %cmp4
  br i1 %or.cond, label %if.then5, label %if.end47

if.then5:                                         ; preds = %if.then
  %7 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %7, 8
  br i1 %cmp6.not, label %if.end, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %7, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end:                                           ; preds = %if.then5
  store i64 %6, ptr %oldp, align 8
  br label %if.end47

if.else:                                          ; preds = %malloc_mutex_lock.exit47
  %cmp10.not = icmp eq i64 %newlen, 8
  br i1 %cmp10.not, label %if.end12, label %label_return

if.end12:                                         ; preds = %if.else
  %8 = load i64, ptr @max_background_threads, align 8
  store i64 %8, ptr %oldval, align 8
  %cmp14 = icmp ne ptr %oldp, null
  %cmp16 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp14, %cmp16
  br i1 %or.cond1, label %if.then17, label %do.end28

if.then17:                                        ; preds = %if.end12
  %9 = load i64, ptr %oldlenp, align 8
  %cmp18.not = icmp eq i64 %9, 8
  br i1 %cmp18.not, label %if.end26, label %if.then19

if.then19:                                        ; preds = %if.then17
  %spec.select38 = tail call i64 @llvm.umin.i64(i64 %9, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select38, i1 false)
  store i64 %spec.select38, ptr %oldlenp, align 8
  br label %label_return

if.end26:                                         ; preds = %if.then17
  store i64 %8, ptr %oldp, align 8
  br label %do.end28

do.end28:                                         ; preds = %if.end12, %if.end26
  %10 = load i64, ptr %newp, align 8
  %cmp29 = icmp eq i64 %10, %8
  br i1 %cmp29, label %label_return, label %if.end31

if.end31:                                         ; preds = %do.end28
  %11 = load i64, ptr @opt_max_background_threads, align 8
  %cmp32 = icmp ugt i64 %10, %11
  br i1 %cmp32, label %label_return, label %monotonic.i

monotonic.i:                                      ; preds = %if.end31
  %12 = load atomic i8, ptr @background_thread_enabled_state monotonic, align 1
  %13 = and i8 %12, 1
  %tobool.i80.not = icmp eq i8 %13, 0
  br i1 %tobool.i80.not, label %if.else45, label %if.then36

if.then36:                                        ; preds = %monotonic.i
  store atomic i8 0, ptr @background_thread_enabled_state monotonic, align 1
  %call38 = tail call zeroext i1 @background_threads_disable(ptr noundef %tsd) #14
  br i1 %call38, label %label_return, label %if.end40

if.end40:                                         ; preds = %if.then36
  store i64 %10, ptr @max_background_threads, align 8
  store atomic i8 1, ptr @background_thread_enabled_state monotonic, align 1
  %call42 = tail call zeroext i1 @background_threads_enable(ptr noundef %tsd) #14
  br i1 %call42, label %label_return, label %if.end47

if.else45:                                        ; preds = %monotonic.i
  store i64 %10, ptr @max_background_threads, align 8
  br label %if.end47

if.end47:                                         ; preds = %if.else45, %if.end40, %if.end, %if.then
  br label %label_return

label_return:                                     ; preds = %if.end40, %if.then36, %if.end31, %do.end28, %if.else, %if.end47, %if.then19, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 0, %if.end47 ], [ 22, %if.then19 ], [ 22, %if.else ], [ 0, %do.end28 ], [ 22, %if.end31 ], [ 14, %if.then36 ], [ 14, %if.end40 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 1)) #14
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i48 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

declare void @background_thread_ctl_init(ptr noundef) local_unnamed_addr #1

declare zeroext i1 @background_threads_enable(ptr noundef) local_unnamed_addr #1

declare zeroext i1 @background_threads_disable(ptr noundef) local_unnamed_addr #1

; Function Attrs: nounwind uwtable
define internal i32 @thread_arena_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %oldind = alloca i32, align 4
  %call = tail call fastcc ptr @arena_choose(ptr noundef %tsd)
  %cmp = icmp eq ptr %call, null
  br i1 %cmp, label %return, label %if.end

if.end:                                           ; preds = %entry
  %0 = getelementptr i8, ptr %call, i64 78928
  %call.val = load i32, ptr %0, align 8
  store i32 %call.val, ptr %oldind, align 4
  %cmp2.not = icmp eq ptr %newp, null
  br i1 %cmp2.not, label %do.body8, label %if.then3

if.then3:                                         ; preds = %if.end
  %cmp4.not = icmp eq i64 %newlen, 4
  br i1 %cmp4.not, label %if.end6, label %return

if.end6:                                          ; preds = %if.then3
  %1 = load i32, ptr %newp, align 4
  br label %do.body8

do.body8:                                         ; preds = %if.end6, %if.end
  %newind.0 = phi i32 [ %1, %if.end6 ], [ %call.val, %if.end ]
  %cmp9 = icmp ne ptr %oldp, null
  %cmp10 = icmp ne ptr %oldlenp, null
  %or.cond = and i1 %cmp9, %cmp10
  br i1 %or.cond, label %if.then11, label %do.end17

if.then11:                                        ; preds = %do.body8
  %2 = load i64, ptr %oldlenp, align 8
  %cmp12.not = icmp eq i64 %2, 4
  br i1 %cmp12.not, label %if.end15, label %if.then13

if.then13:                                        ; preds = %if.then11
  %spec.select = tail call i64 @llvm.umin.i64(i64 %2, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldind, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %return

if.end15:                                         ; preds = %if.then11
  store i32 %call.val, ptr %oldp, align 4
  br label %do.end17

do.end17:                                         ; preds = %do.body8, %if.end15
  %cmp18.not = icmp eq i32 %newind.0, %call.val
  br i1 %cmp18.not, label %return, label %if.then19

if.then19:                                        ; preds = %do.end17
  %call20 = tail call i32 @narenas_total_get() #14
  %cmp21.not = icmp ult i32 %newind.0, %call20
  br i1 %cmp21.not, label %if.end23, label %return

if.end23:                                         ; preds = %if.then19
  %3 = load i32, ptr @opt_percpu_arena, align 4
  %cmp24 = icmp ugt i32 %3, 2
  br i1 %cmp24, label %if.then25, label %if.end30

if.then25:                                        ; preds = %if.end23
  %cmp.i = icmp eq i32 %3, 4
  %4 = load i32, ptr @ncpus, align 4
  %cmp1.i = icmp ugt i32 %4, 1
  %or.cond1 = and i1 %cmp.i, %cmp1.i
  %rem.i = and i32 %4, 1
  %div3.i29 = lshr i32 %4, 1
  %spec.select32 = add nuw i32 %div3.i29, %rem.i
  %retval.i.0 = select i1 %or.cond1, i32 %spec.select32, i32 %4
  %cmp27 = icmp ult i32 %newind.0, %retval.i.0
  br i1 %cmp27, label %return, label %if.end30

if.end30:                                         ; preds = %if.then25, %if.end23
  %idxprom.i = zext i32 %newind.0 to i64
  %arrayidx.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %idxprom.i
  %5 = load atomic i64, ptr %arrayidx.i acquire, align 8
  %6 = inttoptr i64 %5 to ptr
  %cmp.i31 = icmp eq i64 %5, 0
  br i1 %cmp.i31, label %if.then3.i, label %arena_get.exit

if.then3.i:                                       ; preds = %if.end30
  %call4.i = tail call ptr @arena_init(ptr noundef %tsd, i32 noundef %newind.0, ptr noundef nonnull @arena_config_default) #14
  br label %arena_get.exit

arena_get.exit:                                   ; preds = %if.end30, %if.then3.i
  %ret.0.i = phi ptr [ %call4.i, %if.then3.i ], [ %6, %if.end30 ]
  %cmp33 = icmp eq ptr %ret.0.i, null
  br i1 %cmp33, label %return, label %if.end35

if.end35:                                         ; preds = %arena_get.exit
  tail call void @arena_migrate(ptr noundef %tsd, ptr noundef nonnull %call, ptr noundef nonnull %ret.0.i) #14
  %7 = load i8, ptr %tsd, align 1
  %8 = and i8 %7, 1
  %tobool.i56.not.not = icmp eq i8 %8, 0
  br i1 %tobool.i56.not.not, label %return, label %if.then37

if.then37:                                        ; preds = %if.end35
  %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache_slow.i = getelementptr inbounds %struct.tsd_s, ptr %tsd, i64 0, i32 27
  %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache.i = getelementptr inbounds %struct.tsd_s, ptr %tsd, i64 0, i32 34
  tail call void @tcache_arena_reassociate(ptr noundef nonnull %tsd, ptr noundef nonnull %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache_slow.i, ptr noundef nonnull %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache.i, ptr noundef nonnull %ret.0.i) #14
  br label %return

return:                                           ; preds = %if.then13, %if.then3, %if.then19, %if.then25, %arena_get.exit, %if.end35, %if.then37, %do.end17, %entry
  %retval.0 = phi i32 [ 11, %entry ], [ 22, %if.then13 ], [ 22, %if.then3 ], [ 14, %if.then19 ], [ 1, %if.then25 ], [ 11, %arena_get.exit ], [ 0, %if.end35 ], [ 0, %if.then37 ], [ 0, %do.end17 ]
  ret i32 %retval.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @thread_allocated_ctl(ptr nocapture noundef readonly %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %cant_access_tsd_items_directly_use_a_getter_or_setter_thread_allocated.i = getelementptr inbounds %struct.tsd_s, ptr %tsd, i64 0, i32 30
  %0 = load i64, ptr %cant_access_tsd_items_directly_use_a_getter_or_setter_thread_allocated.i, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @thread_allocatedp_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca ptr, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %cant_access_tsd_items_directly_use_a_getter_or_setter_thread_allocated.i = getelementptr inbounds %struct.tsd_s, ptr %tsd, i64 0, i32 30
  store ptr %cant_access_tsd_items_directly_use_a_getter_or_setter_thread_allocated.i, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %0, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store ptr %cant_access_tsd_items_directly_use_a_getter_or_setter_thread_allocated.i, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @thread_deallocated_ctl(ptr nocapture noundef readonly %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %cant_access_tsd_items_directly_use_a_getter_or_setter_thread_deallocated.i = getelementptr inbounds %struct.tsd_s, ptr %tsd, i64 0, i32 32
  %0 = load i64, ptr %cant_access_tsd_items_directly_use_a_getter_or_setter_thread_deallocated.i, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @thread_deallocatedp_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca ptr, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %cant_access_tsd_items_directly_use_a_getter_or_setter_thread_deallocated.i = getelementptr inbounds %struct.tsd_s, ptr %tsd, i64 0, i32 32
  store ptr %cant_access_tsd_items_directly_use_a_getter_or_setter_thread_deallocated.i, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %0, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store ptr %cant_access_tsd_items_directly_use_a_getter_or_setter_thread_deallocated.i, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @thread_idle_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef readnone %oldp, ptr noundef readnone %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %cmp = icmp ne ptr %oldp, null
  %cmp1 = icmp ne ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  %cmp3 = icmp ne ptr %newp, null
  %or.cond1 = or i1 %or.cond, %cmp3
  %cmp5 = icmp ne i64 %newlen, 0
  %or.cond2 = or i1 %or.cond1, %cmp5
  br i1 %or.cond2, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i8, ptr %tsd, align 1
  %1 = and i8 %0, 1
  %tobool.i.not.not = icmp eq i8 %1, 0
  br i1 %tobool.i.not.not, label %if.end7, label %if.then6

if.then6:                                         ; preds = %do.end
  tail call void @tcache_flush(ptr noundef nonnull %tsd) #14
  br label %if.end7

if.end7:                                          ; preds = %if.then6, %do.end
  %2 = load i32, ptr @opt_narenas, align 4
  %3 = load i32, ptr @ncpus, align 4
  %mul = shl i32 %3, 1
  %cmp8 = icmp ugt i32 %2, %mul
  br i1 %cmp8, label %if.then9, label %label_return

if.then9:                                         ; preds = %if.end7
  %call10 = tail call fastcc ptr @arena_choose(ptr noundef nonnull %tsd)
  %cmp11.not = icmp eq ptr %call10, null
  br i1 %cmp11.not, label %label_return, label %if.then12

if.then12:                                        ; preds = %if.then9
  tail call void @arena_decay(ptr noundef nonnull %tsd, ptr noundef nonnull %call10, i1 noundef zeroext false, i1 noundef zeroext true) #14
  br label %label_return

label_return:                                     ; preds = %if.end7, %if.then12, %if.then9, %entry
  %ret.0 = phi i32 [ 1, %entry ], [ 0, %if.then9 ], [ 0, %if.then12 ], [ 0, %if.end7 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal fastcc ptr @arena_choose(ptr noundef %tsd) unnamed_addr #0 {
entry:
  %cant_access_tsd_items_directly_use_a_getter_or_setter_reentrancy_level.i.i = getelementptr inbounds %struct.tsd_s, ptr %tsd, i64 0, i32 1
  %0 = load i8, ptr %cant_access_tsd_items_directly_use_a_getter_or_setter_reentrancy_level.i.i, align 1
  %cmp1.i = icmp sgt i8 %0, 0
  br i1 %cmp1.i, label %if.then5.i, label %cond.end.i

if.then5.i:                                       ; preds = %entry
  %1 = load atomic i64, ptr @arenas acquire, align 8
  %2 = inttoptr i64 %1 to ptr
  %cmp.i44.i = icmp eq i64 %1, 0
  br i1 %cmp.i44.i, label %if.then3.i.i, label %arena_choose_impl.exit

if.then3.i.i:                                     ; preds = %if.then5.i
  %call4.i.i = tail call ptr @arena_init(ptr noundef nonnull %tsd, i32 noundef 0, ptr noundef nonnull @arena_config_default) #14
  br label %arena_choose_impl.exit

cond.end.i:                                       ; preds = %entry
  %cant_access_tsd_items_directly_use_a_getter_or_setter_arena.i119.i = getelementptr inbounds %struct.tsd_s, ptr %tsd, i64 0, i32 19
  %3 = load ptr, ptr %cant_access_tsd_items_directly_use_a_getter_or_setter_arena.i119.i, align 8
  %cmp13.i = icmp eq ptr %3, null
  br i1 %cmp13.i, label %if.then21.i, label %if.end43.i

if.then21.i:                                      ; preds = %cond.end.i
  %call23.i = tail call ptr @arena_choose_hard(ptr noundef nonnull %tsd, i1 noundef zeroext false) #14
  %4 = load i8, ptr %tsd, align 1
  %5 = and i8 %4, 1
  %tobool.i123.not.not.i = icmp eq i8 %5, 0
  br i1 %tobool.i123.not.not.i, label %if.end43.i, label %if.then25.i

if.then25.i:                                      ; preds = %if.then21.i
  %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache_slow.i.i = getelementptr inbounds %struct.tsd_s, ptr %tsd, i64 0, i32 27
  %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache.i.i = getelementptr inbounds %struct.tsd_s, ptr %tsd, i64 0, i32 34
  %arena28.i = getelementptr inbounds %struct.tsd_s, ptr %tsd, i64 0, i32 27, i32 2
  %6 = load ptr, ptr %arena28.i, align 8
  %cmp29.not.i = icmp eq ptr %6, null
  br i1 %cmp29.not.i, label %if.else.i, label %do.end33.i

do.end33.i:                                       ; preds = %if.then25.i
  %cmp35.not.i = icmp eq ptr %6, %call23.i
  br i1 %cmp35.not.i, label %if.end43.i, label %if.then37.i

if.then37.i:                                      ; preds = %do.end33.i
  tail call void @tcache_arena_reassociate(ptr noundef nonnull %tsd, ptr noundef nonnull %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache_slow.i.i, ptr noundef nonnull %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache.i.i, ptr noundef %call23.i) #14
  br label %if.end43.i

if.else.i:                                        ; preds = %if.then25.i
  tail call void @tcache_arena_associate(ptr noundef nonnull %tsd, ptr noundef nonnull %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache_slow.i.i, ptr noundef nonnull %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache.i.i, ptr noundef %call23.i) #14
  br label %if.end43.i

if.end43.i:                                       ; preds = %if.else.i, %if.then37.i, %do.end33.i, %if.then21.i, %cond.end.i
  %ret.0.i = phi ptr [ %call23.i, %if.then37.i ], [ %call23.i, %do.end33.i ], [ %call23.i, %if.else.i ], [ %call23.i, %if.then21.i ], [ %3, %cond.end.i ]
  %7 = load i32, ptr @opt_percpu_arena, align 4
  %cmp44.i = icmp ugt i32 %7, 2
  br i1 %cmp44.i, label %land.lhs.true47.i, label %arena_choose_impl.exit

land.lhs.true47.i:                                ; preds = %if.end43.i
  %8 = getelementptr i8, ptr %ret.0.i, i64 78928
  %ret.0.val43.i = load i32, ptr %8, align 8
  %cmp.i.i = icmp eq i32 %7, 4
  %9 = load i32, ptr @ncpus, align 4
  %cmp1.i.i = icmp ugt i32 %9, 1
  %or.cond.i = and i1 %cmp.i.i, %cmp1.i.i
  %rem.i.i = and i32 %9, 1
  %div3.i40.i = lshr i32 %9, 1
  %spec.select1.i = add nuw i32 %div3.i40.i, %rem.i.i
  %retval.i.0.i = select i1 %or.cond.i, i32 %spec.select1.i, i32 %9
  %cmp50.i = icmp ult i32 %ret.0.val43.i, %retval.i.0.i
  br i1 %cmp50.i, label %land.lhs.true52.i, label %arena_choose_impl.exit

land.lhs.true52.i:                                ; preds = %land.lhs.true47.i
  %last_thd.i = getelementptr inbounds %struct.arena_s, ptr %ret.0.i, i64 0, i32 2
  %10 = load ptr, ptr %last_thd.i, align 8
  %cmp54.not.i = icmp eq ptr %10, %tsd
  br i1 %cmp54.not.i, label %arena_choose_impl.exit, label %if.then56.i

if.then56.i:                                      ; preds = %land.lhs.true52.i
  %call.i120.i = tail call i32 @sched_getcpu() #14
  %11 = load i32, ptr @opt_percpu_arena, align 4
  %cmp.i90.i = icmp eq i32 %11, 3
  br i1 %cmp.i90.i, label %percpu_arena_choose.exit.i, label %lor.lhs.false.i.i

lor.lhs.false.i.i:                                ; preds = %if.then56.i
  %12 = load i32, ptr @ncpus, align 4
  %div.i9142.i = lshr i32 %12, 1
  %cmp3.i.i = icmp ult i32 %call.i120.i, %div.i9142.i
  %sub.i.i = select i1 %cmp3.i.i, i32 0, i32 %div.i9142.i
  %spec.select.i = sub i32 %call.i120.i, %sub.i.i
  br label %percpu_arena_choose.exit.i

percpu_arena_choose.exit.i:                       ; preds = %lor.lhs.false.i.i, %if.then56.i
  %arena_ind.i.0.i = phi i32 [ %call.i120.i, %if.then56.i ], [ %spec.select.i, %lor.lhs.false.i.i ]
  %ret.0.val.i = load i32, ptr %8, align 8
  %cmp59.not.i = icmp eq i32 %ret.0.val.i, %arena_ind.i.0.i
  br i1 %cmp59.not.i, label %if.end63.i, label %if.then61.i

if.then61.i:                                      ; preds = %percpu_arena_choose.exit.i
  %13 = load ptr, ptr %cant_access_tsd_items_directly_use_a_getter_or_setter_arena.i119.i, align 8
  %14 = getelementptr i8, ptr %13, i64 78928
  %.val.i.i = load i32, ptr %14, align 8
  %cmp.not.i.i = icmp eq i32 %.val.i.i, %arena_ind.i.0.i
  br i1 %cmp.not.i.i, label %percpu_arena_update.exit.i, label %if.then.i45.i

if.then.i45.i:                                    ; preds = %if.then61.i
  %idxprom.i.i.i = zext i32 %arena_ind.i.0.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %idxprom.i.i.i
  %15 = load atomic i64, ptr %arrayidx.i.i.i acquire, align 8
  %16 = inttoptr i64 %15 to ptr
  %cmp.i.i.i = icmp eq i64 %15, 0
  br i1 %cmp.i.i.i, label %if.then3.i.i.i, label %arena_get.exit.i.i

if.then3.i.i.i:                                   ; preds = %if.then.i45.i
  %call4.i.i.i = tail call ptr @arena_init(ptr noundef nonnull %tsd, i32 noundef %arena_ind.i.0.i, ptr noundef nonnull @arena_config_default) #14
  br label %arena_get.exit.i.i

arena_get.exit.i.i:                               ; preds = %if.then3.i.i.i, %if.then.i45.i
  %ret.0.i.i.i = phi ptr [ %call4.i.i.i, %if.then3.i.i.i ], [ %16, %if.then.i45.i ]
  tail call void @arena_migrate(ptr noundef nonnull %tsd, ptr noundef nonnull %13, ptr noundef %ret.0.i.i.i) #14
  %17 = load i8, ptr %tsd, align 1
  %18 = and i8 %17, 1
  %tobool.i.not.not.i.i = icmp eq i8 %18, 0
  br i1 %tobool.i.not.not.i.i, label %percpu_arena_update.exit.i, label %if.then10.i.i

if.then10.i.i:                                    ; preds = %arena_get.exit.i.i
  %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %tsd, i64 0, i32 34
  %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache_slow.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %tsd, i64 0, i32 27
  tail call void @tcache_arena_reassociate(ptr noundef nonnull %tsd, ptr noundef nonnull %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache_slow.i.i.i, ptr noundef nonnull %cant_access_tsd_items_directly_use_a_getter_or_setter_tcache.i.i.i, ptr noundef %ret.0.i.i.i) #14
  br label %percpu_arena_update.exit.i

percpu_arena_update.exit.i:                       ; preds = %if.then10.i.i, %arena_get.exit.i.i, %if.then61.i
  %19 = load ptr, ptr %cant_access_tsd_items_directly_use_a_getter_or_setter_arena.i119.i, align 8
  br label %if.end63.i

if.end63.i:                                       ; preds = %percpu_arena_update.exit.i, %percpu_arena_choose.exit.i
  %ret.1.i = phi ptr [ %19, %percpu_arena_update.exit.i ], [ %ret.0.i, %percpu_arena_choose.exit.i ]
  %last_thd65.i = getelementptr inbounds %struct.arena_s, ptr %ret.1.i, i64 0, i32 2
  store ptr %tsd, ptr %last_thd65.i, align 8
  br label %arena_choose_impl.exit

arena_choose_impl.exit:                           ; preds = %if.then5.i, %if.then3.i.i, %if.end43.i, %land.lhs.true47.i, %land.lhs.true52.i, %if.end63.i
  %retval.0.i = phi ptr [ %ret.1.i, %if.end63.i ], [ %ret.0.i, %land.lhs.true52.i ], [ %ret.0.i, %land.lhs.true47.i ], [ %ret.0.i, %if.end43.i ], [ %call4.i.i, %if.then3.i.i ], [ %2, %if.then5.i ]
  ret ptr %retval.0.i
}

declare void @arena_migrate(ptr noundef, ptr noundef, ptr noundef) local_unnamed_addr #1

declare void @tcache_arena_reassociate(ptr noundef, ptr noundef, ptr noundef, ptr noundef) local_unnamed_addr #1

declare ptr @arena_choose_hard(ptr noundef, i1 noundef zeroext) local_unnamed_addr #1

declare void @tcache_arena_associate(ptr noundef, ptr noundef, ptr noundef, ptr noundef) local_unnamed_addr #1

; Function Attrs: nounwind
declare i32 @sched_getcpu() local_unnamed_addr #3

; Function Attrs: nounwind uwtable
define internal i32 @thread_tcache_enabled_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i8, align 1
  %tsd.val = load i8, ptr %tsd, align 1
  %0 = and i8 %tsd.val, 1
  store i8 %0, ptr %oldval, align 1
  %cmp.not = icmp eq ptr %newp, null
  br i1 %cmp.not, label %do.body, label %if.then

if.then:                                          ; preds = %entry
  %cmp1.not = icmp eq i64 %newlen, 1
  br i1 %cmp1.not, label %if.end, label %label_return

if.end:                                           ; preds = %if.then
  %1 = load i8, ptr %newp, align 1
  %2 = and i8 %1, 1
  %tobool = icmp ne i8 %2, 0
  %tobool.i.not.i = icmp eq i8 %0, 0
  %brmerge.demorgan.i = and i1 %tobool.i.not.i, %tobool
  br i1 %brmerge.demorgan.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %if.end
  %call3.i = tail call zeroext i1 @tsd_tcache_data_init(ptr noundef nonnull %tsd) #14
  br label %tcache_enabled_set.exit

if.else.i:                                        ; preds = %if.end
  %brmerge10.i = or i1 %tobool.i.not.i, %tobool
  br i1 %brmerge10.i, label %tcache_enabled_set.exit, label %if.then7.i

if.then7.i:                                       ; preds = %if.else.i
  tail call void @tcache_cleanup(ptr noundef nonnull %tsd) #14
  br label %tcache_enabled_set.exit

tcache_enabled_set.exit:                          ; preds = %if.then.i, %if.else.i, %if.then7.i
  store i8 %2, ptr %tsd, align 1
  tail call void @tsd_slow_update(ptr noundef nonnull %tsd) #14
  br label %do.body

do.body:                                          ; preds = %entry, %tcache_enabled_set.exit
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond = and i1 %cmp4, %cmp5
  br i1 %or.cond, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.body
  %3 = load i64, ptr %oldlenp, align 8
  switch i64 %3, label %cond.end [
    i64 1, label %if.end10
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then6
  br label %cond.end

cond.end:                                         ; preds = %if.then6, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then6 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i8 %0, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.body, %if.then, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 22, %if.then ], [ 0, %do.body ], [ 0, %if.end10 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @thread_tcache_flush_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef readnone %oldp, ptr noundef readnone %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %0 = load i8, ptr %tsd, align 1
  %1 = and i8 %0, 1
  %tobool.i.not.not = icmp eq i8 %1, 0
  br i1 %tobool.i.not.not, label %label_return, label %do.body

do.body:                                          ; preds = %entry
  %cmp = icmp ne ptr %oldp, null
  %cmp1 = icmp ne ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  %cmp3 = icmp ne ptr %newp, null
  %or.cond1 = or i1 %or.cond, %cmp3
  %cmp5 = icmp ne i64 %newlen, 0
  %or.cond2 = or i1 %or.cond1, %cmp5
  br i1 %or.cond2, label %label_return, label %do.end

do.end:                                           ; preds = %do.body
  tail call void @tcache_flush(ptr noundef nonnull %tsd) #14
  br label %label_return

label_return:                                     ; preds = %do.body, %entry, %do.end
  %ret.0 = phi i32 [ 0, %do.end ], [ 14, %entry ], [ 1, %do.body ]
  ret i32 %ret.0
}

declare zeroext i1 @tsd_tcache_data_init(ptr noundef) local_unnamed_addr #1

declare void @tcache_cleanup(ptr noundef) local_unnamed_addr #1

declare void @tsd_slow_update(ptr noundef) local_unnamed_addr #1

declare void @tcache_flush(ptr noundef) local_unnamed_addr #1

; Function Attrs: nounwind uwtable
define internal i32 @thread_peak_read_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %result = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  tail call void @peak_event_update(ptr noundef %tsd) #14
  %call = tail call i64 @peak_event_max(ptr noundef %tsd) #14
  store i64 %call, ptr %result, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %0, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %result, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %call, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @thread_peak_reset_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef readnone %oldp, ptr noundef readnone %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %cmp = icmp ne ptr %oldp, null
  %cmp1 = icmp ne ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  %cmp3 = icmp ne ptr %newp, null
  %or.cond1 = or i1 %or.cond, %cmp3
  %cmp5 = icmp ne i64 %newlen, 0
  %or.cond2 = or i1 %or.cond1, %cmp5
  br i1 %or.cond2, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  tail call void @peak_event_zero(ptr noundef %tsd) #14
  br label %label_return

label_return:                                     ; preds = %entry, %do.end
  %ret.0 = phi i32 [ 0, %do.end ], [ 1, %entry ]
  ret i32 %ret.0
}

declare void @peak_event_update(ptr noundef) local_unnamed_addr #1

declare i64 @peak_event_max(ptr noundef) local_unnamed_addr #1

declare void @peak_event_zero(ptr noundef) local_unnamed_addr #1

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @thread_prof_name_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @thread_prof_active_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

declare void @arena_decay(ptr noundef, ptr noundef, i1 noundef zeroext, i1 noundef zeroext) local_unnamed_addr #1

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @config_cache_oblivious_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i8 0, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  switch i64 %0, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 0, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @config_debug_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i8 0, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  switch i64 %0, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 0, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @config_fill_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i8 1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  switch i64 %0, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @config_lazy_lock_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i8 0, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  switch i64 %0, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 0, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @config_malloc_conf_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca ptr, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store ptr @.str.1, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %0, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store ptr @.str.1, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @config_opt_safety_checks_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i8 0, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  switch i64 %0, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 0, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @config_prof_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i8 0, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  switch i64 %0, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 0, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @config_prof_libgcc_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i8 0, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  switch i64 %0, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 0, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @config_prof_libunwind_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i8 0, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  switch i64 %0, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 0, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @config_stats_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i8 1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  switch i64 %0, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @config_utrace_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i8 0, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  switch i64 %0, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 0, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @config_xmalloc_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i8 0, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  switch i64 %0, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 0, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_abort_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i8, ptr @opt_abort, align 1
  %1 = and i8 %0, 1
  store i8 %1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  switch i64 %2, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 %1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_abort_conf_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i8, ptr @opt_abort_conf, align 1
  %1 = and i8 %0, 1
  store i8 %1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  switch i64 %2, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 %1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_cache_oblivious_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i8, ptr @opt_cache_oblivious, align 1
  %1 = and i8 %0, 1
  store i8 %1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  switch i64 %2, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 %1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_trust_madvise_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i8, ptr @opt_trust_madvise, align 1
  %1 = and i8 %0, 1
  store i8 %1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  switch i64 %2, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 %1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_confirm_conf_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i8, ptr @opt_confirm_conf, align 1
  %1 = and i8 %0, 1
  store i8 %1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  switch i64 %2, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 %1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_hpa_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i8, ptr @opt_hpa, align 1
  %1 = and i8 %0, 1
  store i8 %1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  switch i64 %2, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 %1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_hpa_slab_max_alloc_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @opt_hpa_opts, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_hpa_hugification_threshold_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.hpa_shard_opts_s, ptr @opt_hpa_opts, i64 0, i32 1), align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_hpa_hugify_delay_ms_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.hpa_shard_opts_s, ptr @opt_hpa_opts, i64 0, i32 4), align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_hpa_min_purge_interval_ms_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.hpa_shard_opts_s, ptr @opt_hpa_opts, i64 0, i32 5), align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_hpa_dirty_mult_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i32, ptr getelementptr inbounds (%struct.hpa_shard_opts_s, ptr @opt_hpa_opts, i64 0, i32 2), align 8
  store i32 %0, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %0, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_hpa_sec_nshards_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @opt_hpa_sec_opts, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_hpa_sec_max_alloc_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.sec_opts_s, ptr @opt_hpa_sec_opts, i64 0, i32 1), align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_hpa_sec_max_bytes_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.sec_opts_s, ptr @opt_hpa_sec_opts, i64 0, i32 2), align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_hpa_sec_bytes_after_flush_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.sec_opts_s, ptr @opt_hpa_sec_opts, i64 0, i32 3), align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_hpa_sec_batch_fill_extra_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.sec_opts_s, ptr @opt_hpa_sec_opts, i64 0, i32 4), align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_metadata_thp_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca ptr, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i32, ptr @opt_metadata_thp, align 4
  %idxprom = zext i32 %0 to i64
  %arrayidx = getelementptr inbounds [0 x ptr], ptr @metadata_thp_mode_names, i64 0, i64 %idxprom
  %1 = load ptr, ptr %arrayidx, align 8
  store ptr %1, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %2, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %2, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store ptr %1, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_retain_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i8, ptr @opt_retain, align 1
  %1 = and i8 %0, 1
  store i8 %1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  switch i64 %2, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 %1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_dss_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca ptr, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load ptr, ptr @opt_dss, align 8
  store ptr %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store ptr %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_narenas_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i32, ptr @opt_narenas, align 4
  store i32 %0, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %0, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_percpu_arena_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca ptr, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i32, ptr @opt_percpu_arena, align 4
  %idxprom = zext i32 %0 to i64
  %arrayidx = getelementptr inbounds [0 x ptr], ptr @percpu_arena_mode_names, i64 0, i64 %idxprom
  %1 = load ptr, ptr %arrayidx, align 8
  store ptr %1, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %2, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %2, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store ptr %1, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_oversize_threshold_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @opt_oversize_threshold, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_mutex_max_spin_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @opt_mutex_max_spin, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_background_thread_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i8, ptr @opt_background_thread, align 1
  %1 = and i8 %0, 1
  store i8 %1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  switch i64 %2, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 %1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_max_background_threads_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @opt_max_background_threads, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_dirty_decay_ms_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @opt_dirty_decay_ms, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_muzzy_decay_ms_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @opt_muzzy_decay_ms, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_stats_print_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i8, ptr @opt_stats_print, align 1
  %1 = and i8 %0, 1
  store i8 %1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  switch i64 %2, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 %1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @opt_stats_print_opts_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca ptr, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store ptr @opt_stats_print_opts, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %0, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store ptr @opt_stats_print_opts, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_stats_interval_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @opt_stats_interval, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @opt_stats_interval_opts_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca ptr, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store ptr @opt_stats_interval_opts, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %0, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store ptr @opt_stats_interval_opts, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_junk_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca ptr, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load ptr, ptr @opt_junk, align 8
  store ptr %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store ptr %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_zero_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i8, ptr @opt_zero, align 1
  %1 = and i8 %0, 1
  store i8 %1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  switch i64 %2, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 %1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_utrace_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_xmalloc_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_experimental_infallible_new_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_tcache_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i8, ptr @opt_tcache, align 1
  %1 = and i8 %0, 1
  store i8 %1, ptr %oldval, align 1
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  switch i64 %2, label %cond.end [
    i64 1, label %if.end9
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then5
  br label %cond.end

cond.end:                                         ; preds = %if.then5, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then5 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %oldval, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i8 %1, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_tcache_max_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @opt_tcache_max, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_tcache_nslots_small_min_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i32, ptr @opt_tcache_nslots_small_min, align 4
  store i32 %0, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %0, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_tcache_nslots_small_max_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i32, ptr @opt_tcache_nslots_small_max, align 4
  store i32 %0, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %0, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_tcache_nslots_large_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i32, ptr @opt_tcache_nslots_large, align 4
  store i32 %0, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %0, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_lg_tcache_nslots_mul_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @opt_lg_tcache_nslots_mul, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_tcache_gc_incr_bytes_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @opt_tcache_gc_incr_bytes, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_tcache_gc_delay_bytes_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @opt_tcache_gc_delay_bytes, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_lg_tcache_flush_small_div_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i32, ptr @opt_lg_tcache_flush_small_div, align 4
  store i32 %0, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %0, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_lg_tcache_flush_large_div_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i32, ptr @opt_lg_tcache_flush_large_div, align 4
  store i32 %0, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %0, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_thp_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca ptr, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i32, ptr @opt_thp, align 4
  %idxprom = zext i32 %0 to i64
  %arrayidx = getelementptr inbounds [0 x ptr], ptr @thp_mode_names, i64 0, i64 %idxprom
  %1 = load ptr, ptr %arrayidx, align 8
  store ptr %1, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %2, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %2, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store ptr %1, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_lg_extent_max_active_fit_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @opt_lg_extent_max_active_fit, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_prefix_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_active_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_thread_active_init_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_lg_prof_sample_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_lg_prof_interval_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_gdump_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_final_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_leak_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_leak_error_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_accum_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_recent_alloc_max_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_stats_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_sys_thread_name_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_prof_time_res_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @opt_lg_san_uaf_align_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @opt_zero_realloc_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca ptr, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i32, ptr @opt_zero_realloc_action, align 4
  %idxprom = zext i32 %0 to i64
  %arrayidx = getelementptr inbounds [0 x ptr], ptr @zero_realloc_mode_names, i64 0, i64 %idxprom
  %1 = load ptr, ptr %arrayidx, align 8
  store ptr %1, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %2, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %2, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store ptr %1, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @tcache_create_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %tcache_ind = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.body2

do.body2:                                         ; preds = %entry
  %cmp3 = icmp eq ptr %oldp, null
  %cmp5 = icmp eq ptr %oldlenp, null
  %or.cond1 = or i1 %cmp3, %cmp5
  br i1 %or.cond1, label %if.then8, label %lor.lhs.false6

lor.lhs.false6:                                   ; preds = %do.body2
  %0 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %0, 4
  br i1 %cmp7.not, label %do.end10, label %if.then8

if.then8:                                         ; preds = %lor.lhs.false6, %do.body2
  store i64 0, ptr %oldlenp, align 8
  br label %label_return

do.end10:                                         ; preds = %lor.lhs.false6
  %call = tail call ptr @b0get() #14
  %call11 = call zeroext i1 @tcaches_create(ptr noundef %tsd, ptr noundef %call, ptr noundef nonnull %tcache_ind) #14
  br i1 %call11, label %label_return, label %if.then17

if.then17:                                        ; preds = %do.end10
  %1 = load i64, ptr %oldlenp, align 8
  %cmp18.not = icmp eq i64 %1, 4
  br i1 %cmp18.not, label %if.end21, label %if.then19

if.then19:                                        ; preds = %if.then17
  %spec.select = call i64 @llvm.umin.i64(i64 %1, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %tcache_ind, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end21:                                         ; preds = %if.then17
  %2 = load i32, ptr %tcache_ind, align 4
  store i32 %2, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %do.end10, %entry, %if.end21, %if.then19, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 22, %if.then19 ], [ 0, %if.end21 ], [ 1, %entry ], [ 14, %do.end10 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @tcache_flush_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef readnone %oldp, ptr noundef readnone %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %cmp = icmp ne ptr %oldp, null
  %cmp1 = icmp ne ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.body2

do.body2:                                         ; preds = %entry
  %cmp3 = icmp eq ptr %newp, null
  %cmp5 = icmp ne i64 %newlen, 4
  %or.cond1 = or i1 %cmp3, %cmp5
  br i1 %or.cond1, label %label_return, label %if.end7

if.end7:                                          ; preds = %do.body2
  %0 = load i32, ptr %newp, align 4
  tail call void @tcaches_flush(ptr noundef %tsd, i32 noundef %0) #14
  br label %label_return

label_return:                                     ; preds = %do.body2, %entry, %if.end7
  %ret.0 = phi i32 [ 0, %if.end7 ], [ 1, %entry ], [ 22, %do.body2 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @tcache_destroy_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef readnone %oldp, ptr noundef readnone %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %cmp = icmp ne ptr %oldp, null
  %cmp1 = icmp ne ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.body2

do.body2:                                         ; preds = %entry
  %cmp3 = icmp eq ptr %newp, null
  %cmp5 = icmp ne i64 %newlen, 4
  %or.cond1 = or i1 %cmp3, %cmp5
  br i1 %or.cond1, label %label_return, label %if.end7

if.end7:                                          ; preds = %do.body2
  %0 = load i32, ptr %newp, align 4
  tail call void @tcaches_destroy(ptr noundef %tsd, i32 noundef %0) #14
  br label %label_return

label_return:                                     ; preds = %do.body2, %entry, %if.end7
  %ret.0 = phi i32 [ 0, %if.end7 ], [ 1, %entry ], [ 22, %do.body2 ]
  ret i32 %ret.0
}

declare zeroext i1 @tcaches_create(ptr noundef, ptr noundef, ptr noundef) local_unnamed_addr #1

declare void @tcaches_flush(ptr noundef, i32 noundef) local_unnamed_addr #1

declare void @tcaches_destroy(ptr noundef, i32 noundef) local_unnamed_addr #1

; Function Attrs: nounwind uwtable
define internal ptr @arena_i_index(ptr noundef %tsdn, ptr nocapture readnone %mib, i64 %miblen, i64 noundef %i) #0 {
entry:
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsdn
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsdn, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %3 = and i64 %i, -2
  %switch = icmp eq i64 %3, 4096
  br i1 %switch, label %sw.epilog, label %sw.default

sw.default:                                       ; preds = %malloc_mutex_lock.exit
  %4 = load ptr, ptr @ctl_arenas, align 8
  %narenas = getelementptr inbounds %struct.ctl_arenas_s, ptr %4, i64 0, i32 1
  %5 = load i32, ptr %narenas, align 8
  %conv = zext i32 %5 to i64
  %cmp = icmp ult i64 %conv, %i
  br i1 %cmp, label %label_return, label %sw.epilog

sw.epilog:                                        ; preds = %malloc_mutex_lock.exit, %sw.default
  br label %label_return

label_return:                                     ; preds = %sw.default, %sw.epilog
  %ret.0 = phi ptr [ @super_arena_i_node, %sw.epilog ], [ null, %sw.default ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret ptr %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @arena_i_initialized_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %initialized = alloca i8, align 1
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.body2

do.body2:                                         ; preds = %entry
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 1
  %0 = load i64, ptr %arrayidx, align 8
  %cmp3 = icmp ugt i64 %0, 4294967295
  br i1 %cmp3, label %label_return, label %if.end5

if.end5:                                          ; preds = %do.body2
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %if.end5
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %if.end5
  %1 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %1, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %2 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %2, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %3 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %3, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %malloc_mutex_lock.exit
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %malloc_mutex_lock.exit
  %6 = load ptr, ptr @ctl_arenas, align 8
  %trunc = trunc i64 %0 to i32
  switch i32 %trunc, label %sw.default.i.i.i [
    i32 4096, label %arenas_i.exit
    i32 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %0, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add nuw nsw i64 %0, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %initialized10 = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 1
  %10 = load i8, ptr %initialized10, align 4
  %11 = and i8 %10, 1
  store i8 %11, ptr %initialized, align 1
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp12 = icmp ne ptr %oldp, null
  %cmp14 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp12, %cmp14
  br i1 %or.cond1, label %if.then16, label %label_return

if.then16:                                        ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  switch i64 %12, label %cond.end [
    i64 1, label %if.end22
    i64 0, label %cond.false
  ]

cond.false:                                       ; preds = %if.then16
  br label %cond.end

cond.end:                                         ; preds = %if.then16, %cond.false
  %cond = phi i64 [ 0, %cond.false ], [ 1, %if.then16 ]
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 1 %initialized, i64 %cond, i1 false)
  store i64 %cond, ptr %oldlenp, align 8
  br label %label_return

if.end22:                                         ; preds = %if.then16
  store i8 %11, ptr %oldp, align 1
  br label %label_return

label_return:                                     ; preds = %if.end22, %arenas_i.exit, %do.body2, %entry, %cond.end
  %ret.0 = phi i32 [ 22, %cond.end ], [ 1, %entry ], [ 14, %do.body2 ], [ 0, %arenas_i.exit ], [ 0, %if.end22 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @arena_i_decay_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef readnone %oldp, ptr noundef readnone %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %cmp = icmp ne ptr %oldp, null
  %cmp1 = icmp ne ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  %cmp3 = icmp ne ptr %newp, null
  %or.cond1 = or i1 %or.cond, %cmp3
  %cmp5 = icmp ne i64 %newlen, 0
  %or.cond2 = or i1 %or.cond1, %cmp5
  br i1 %or.cond2, label %label_return, label %do.body6

do.body6:                                         ; preds = %entry
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 1
  %0 = load i64, ptr %arrayidx, align 8
  %cmp7 = icmp ugt i64 %0, 4294967295
  br i1 %cmp7, label %label_return, label %if.end9

if.end9:                                          ; preds = %do.body6
  %conv = trunc i64 %0 to i32
  tail call fastcc void @arena_i_decay(ptr noundef %tsd, i32 noundef %conv, i1 noundef zeroext false)
  br label %label_return

label_return:                                     ; preds = %do.body6, %entry, %if.end9
  %ret.0 = phi i32 [ 0, %if.end9 ], [ 1, %entry ], [ 14, %do.body6 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @arena_i_purge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef readnone %oldp, ptr noundef readnone %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %cmp = icmp ne ptr %oldp, null
  %cmp1 = icmp ne ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  %cmp3 = icmp ne ptr %newp, null
  %or.cond1 = or i1 %or.cond, %cmp3
  %cmp5 = icmp ne i64 %newlen, 0
  %or.cond2 = or i1 %or.cond1, %cmp5
  br i1 %or.cond2, label %label_return, label %do.body6

do.body6:                                         ; preds = %entry
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 1
  %0 = load i64, ptr %arrayidx, align 8
  %cmp7 = icmp ugt i64 %0, 4294967295
  br i1 %cmp7, label %label_return, label %if.end9

if.end9:                                          ; preds = %do.body6
  %conv = trunc i64 %0 to i32
  tail call fastcc void @arena_i_decay(ptr noundef %tsd, i32 noundef %conv, i1 noundef zeroext true)
  br label %label_return

label_return:                                     ; preds = %do.body6, %entry, %if.end9
  %ret.0 = phi i32 [ 0, %if.end9 ], [ 1, %entry ], [ 14, %do.body6 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @arena_i_reset_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef readnone %oldp, ptr noundef readnone %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %cmp.i = icmp ne ptr %oldp, null
  %cmp1.i = icmp ne ptr %oldlenp, null
  %or.cond.i = or i1 %cmp.i, %cmp1.i
  %cmp3.i = icmp ne ptr %newp, null
  %or.cond1.i = or i1 %or.cond.i, %cmp3.i
  %cmp5.i = icmp ne i64 %newlen, 0
  %or.cond2.i = or i1 %or.cond1.i, %cmp5.i
  br i1 %or.cond2.i, label %return, label %do.body6.i

do.body6.i:                                       ; preds = %entry
  %arrayidx.i = getelementptr inbounds i64, ptr %mib, i64 1
  %0 = load i64, ptr %arrayidx.i, align 8
  %cmp7.i = icmp ugt i64 %0, 4294967295
  br i1 %cmp7.i, label %return, label %if.end9.i

if.end9.i:                                        ; preds = %do.body6.i
  %conv.i = trunc i64 %0 to i32
  %arrayidx.i.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %0
  %1 = load atomic i64, ptr %arrayidx.i.i acquire, align 8
  %2 = inttoptr i64 %1 to ptr
  %cmp13.i = icmp eq i64 %1, 0
  br i1 %cmp13.i, label %return, label %lor.lhs.false15.i

lor.lhs.false15.i:                                ; preds = %if.end9.i
  %3 = getelementptr i8, ptr %2, i64 78928
  %call12.val.i = load i32, ptr %3, align 8
  %4 = load i32, ptr @manual_arena_base, align 4
  %cmp.i7.i = icmp ugt i32 %4, %call12.val.i
  br i1 %cmp.i7.i, label %return, label %if.end

if.end:                                           ; preds = %lor.lhs.false15.i
  tail call fastcc void @arena_reset_prepare_background_thread(ptr noundef %tsd, i32 noundef %conv.i)
  tail call void @arena_reset(ptr noundef %tsd, ptr noundef nonnull %2) #14
  %5 = load atomic i8, ptr @background_thread_enabled_state monotonic, align 1
  %6 = and i8 %5, 1
  %tobool.i.not.i = icmp eq i8 %6, 0
  br i1 %tobool.i.not.i, label %arena_reset_finish_background_thread.exit, label %if.then.i

if.then.i:                                        ; preds = %if.end
  %7 = load ptr, ptr @background_thread_info, align 8
  %8 = load i64, ptr @max_background_threads, align 8
  %rem.i.i = urem i64 %0, %8
  %mtx.i = getelementptr inbounds %struct.background_thread_info_s, ptr %7, i64 %rem.i.i, i32 2
  %lock.i.i.i = getelementptr inbounds %struct.anon, ptr %mtx.i, i64 0, i32 1
  %call.i.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull %lock.i.i.i) #14
  %cmp.i.not.i.i = icmp eq i32 %call.i.i.i, 0
  br i1 %cmp.i.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.then.i
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull %mtx.i) #14
  %locked.i.i = getelementptr inbounds %struct.anon, ptr %mtx.i, i64 0, i32 2
  store atomic i8 1, ptr %locked.i.i monotonic, align 1
  br label %if.end.i.i

if.end.i.i:                                       ; preds = %if.then.i.i, %if.then.i
  %n_lock_ops.i.i.i = getelementptr inbounds %struct.mutex_prof_data_t, ptr %mtx.i, i64 0, i32 8
  %9 = load i64, ptr %n_lock_ops.i.i.i, align 8
  %inc.i.i.i = add i64 %9, 1
  store i64 %inc.i.i.i, ptr %n_lock_ops.i.i.i, align 8
  %prev_owner.i.i.i = getelementptr inbounds %struct.mutex_prof_data_t, ptr %mtx.i, i64 0, i32 7
  %10 = load ptr, ptr %prev_owner.i.i.i, align 8
  %cmp.not.i.i.i = icmp eq ptr %10, %tsd
  br i1 %cmp.not.i.i.i, label %malloc_mutex_lock.exit.i, label %if.then.i.i.i

if.then.i.i.i:                                    ; preds = %if.end.i.i
  store ptr %tsd, ptr %prev_owner.i.i.i, align 8
  %n_owner_switches.i.i.i = getelementptr inbounds %struct.mutex_prof_data_t, ptr %mtx.i, i64 0, i32 6
  %11 = load i64, ptr %n_owner_switches.i.i.i, align 8
  %inc2.i.i.i = add i64 %11, 1
  store i64 %inc2.i.i.i, ptr %n_owner_switches.i.i.i, align 8
  br label %malloc_mutex_lock.exit.i

malloc_mutex_lock.exit.i:                         ; preds = %if.then.i.i.i, %if.end.i.i
  %state.i = getelementptr inbounds %struct.background_thread_info_s, ptr %7, i64 %rem.i.i, i32 3
  store i32 1, ptr %state.i, align 8
  %locked.i5.i = getelementptr inbounds %struct.anon, ptr %mtx.i, i64 0, i32 2
  store atomic i8 0, ptr %locked.i5.i monotonic, align 1
  %call1.i.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull %lock.i.i.i) #14
  br label %arena_reset_finish_background_thread.exit

arena_reset_finish_background_thread.exit:        ; preds = %if.end, %malloc_mutex_lock.exit.i
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i6.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 1)) #14
  br label %return

return:                                           ; preds = %lor.lhs.false15.i, %if.end9.i, %do.body6.i, %entry, %arena_reset_finish_background_thread.exit
  %ret.0.i12 = phi i32 [ 0, %arena_reset_finish_background_thread.exit ], [ 14, %if.end9.i ], [ 14, %do.body6.i ], [ 1, %entry ], [ 14, %lor.lhs.false15.i ]
  ret i32 %ret.0.i12
}

; Function Attrs: nounwind uwtable
define internal i32 @arena_i_destroy_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef readnone %oldp, ptr noundef readnone %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp.i = icmp ne ptr %oldp, null
  %cmp1.i = icmp ne ptr %oldlenp, null
  %or.cond.i = or i1 %cmp.i, %cmp1.i
  %cmp3.i = icmp ne ptr %newp, null
  %or.cond1.i = or i1 %or.cond.i, %cmp3.i
  %cmp5.i = icmp ne i64 %newlen, 0
  %or.cond2.i = or i1 %or.cond1.i, %cmp5.i
  br i1 %or.cond2.i, label %label_return, label %do.body6.i

do.body6.i:                                       ; preds = %malloc_mutex_lock.exit
  %arrayidx.i = getelementptr inbounds i64, ptr %mib, i64 1
  %3 = load i64, ptr %arrayidx.i, align 8
  %cmp7.i = icmp ugt i64 %3, 4294967295
  br i1 %cmp7.i, label %label_return, label %if.end9.i

if.end9.i:                                        ; preds = %do.body6.i
  %conv.i = trunc i64 %3 to i32
  %arrayidx.i.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %3
  %4 = load atomic i64, ptr %arrayidx.i.i acquire, align 8
  %5 = inttoptr i64 %4 to ptr
  %cmp13.i = icmp eq i64 %4, 0
  br i1 %cmp13.i, label %label_return, label %lor.lhs.false15.i

lor.lhs.false15.i:                                ; preds = %if.end9.i
  %6 = getelementptr i8, ptr %5, i64 78928
  %call12.val.i = load i32, ptr %6, align 8
  %7 = load i32, ptr @manual_arena_base, align 4
  %cmp.i7.i = icmp ugt i32 %7, %call12.val.i
  br i1 %cmp.i7.i, label %label_return, label %if.end

if.end:                                           ; preds = %lor.lhs.false15.i
  %call2 = tail call i32 @arena_nthreads_get(ptr noundef nonnull %5, i1 noundef zeroext false) #14
  %cmp3.not = icmp eq i32 %call2, 0
  br i1 %cmp3.not, label %lor.lhs.false, label %label_return

lor.lhs.false:                                    ; preds = %if.end
  %call4 = tail call i32 @arena_nthreads_get(ptr noundef nonnull %5, i1 noundef zeroext true) #14
  %cmp5.not = icmp eq i32 %call4, 0
  br i1 %cmp5.not, label %if.end7, label %label_return

if.end7:                                          ; preds = %lor.lhs.false
  tail call fastcc void @arena_reset_prepare_background_thread(ptr noundef %tsd, i32 noundef %conv.i)
  tail call void @arena_reset(ptr noundef %tsd, ptr noundef nonnull %5) #14
  tail call void @arena_decay(ptr noundef %tsd, ptr noundef nonnull %5, i1 noundef zeroext false, i1 noundef zeroext true) #14
  %8 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %8, i64 0, i32 29
  %9 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %9, 0
  br i1 %cmp6.i.not.i, label %arenas_i.exit, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %if.end7
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %8, i1 noundef zeroext false) #14
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %if.end7, %if.then11.i.i
  %10 = load ptr, ptr @ctl_arenas, align 8
  %arrayidx.i.i22 = getelementptr inbounds %struct.ctl_arenas_s, ptr %10, i64 0, i32 3, i64 1
  %11 = load ptr, ptr %arrayidx.i.i22, align 8
  %initialized = getelementptr inbounds %struct.ctl_arena_s, ptr %11, i64 0, i32 1
  store i8 1, ptr %initialized, align 4
  tail call fastcc void @ctl_arena_refresh(ptr noundef %tsd, ptr noundef nonnull %5, ptr noundef %11, i32 noundef %conv.i, i1 noundef zeroext true)
  tail call void @arena_destroy(ptr noundef %tsd, ptr noundef nonnull %5) #14
  %12 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i24 = icmp eq i8 %12, 0
  br i1 %cmp6.i.not.i24, label %tsd_fetch_impl.exit.i, label %if.then11.i.i25

if.then11.i.i25:                                  ; preds = %arenas_i.exit
  %call13.i.i26 = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %8, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i25, %arenas_i.exit
  %13 = load ptr, ptr @ctl_arenas, align 8
  switch i32 %conv.i, label %sw.default.i.i.i [
    i32 4096, label %arenas_i.exit28
    i32 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit28

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %13, i64 0, i32 1
  %14 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %14 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit28, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add nuw nsw i64 %3, 2
  %15 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit28

arenas_i.exit28:                                  ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %15, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i27 = getelementptr inbounds %struct.ctl_arenas_s, ptr %13, i64 0, i32 3, i64 %a.0.i.i.i
  %16 = load ptr, ptr %arrayidx.i.i27, align 8
  %initialized12 = getelementptr inbounds %struct.ctl_arena_s, ptr %16, i64 0, i32 1
  store i8 0, ptr %initialized12, align 4
  %destroyed_link = getelementptr inbounds %struct.ctl_arena_s, ptr %16, i64 0, i32 2
  store ptr %16, ptr %destroyed_link, align 8
  %qre_prev = getelementptr inbounds %struct.ctl_arena_s, ptr %16, i64 0, i32 2, i32 1
  store ptr %16, ptr %qre_prev, align 8
  %destroyed = getelementptr inbounds %struct.ctl_arenas_s, ptr %13, i64 0, i32 2
  %17 = load ptr, ptr %destroyed, align 8
  %cmp15 = icmp eq ptr %17, null
  br i1 %cmp15, label %if.end52, label %do.body18

do.body18:                                        ; preds = %arenas_i.exit28
  %qre_prev22 = getelementptr inbounds %struct.ctl_arena_s, ptr %17, i64 0, i32 2, i32 1
  %18 = load ptr, ptr %qre_prev22, align 8
  store ptr %18, ptr %destroyed_link, align 8
  %19 = load ptr, ptr %destroyed, align 8
  %qre_prev32 = getelementptr inbounds %struct.ctl_arena_s, ptr %19, i64 0, i32 2, i32 1
  store ptr %16, ptr %qre_prev32, align 8
  %20 = load ptr, ptr %qre_prev, align 8
  %destroyed_link35 = getelementptr inbounds %struct.ctl_arena_s, ptr %20, i64 0, i32 2
  %21 = load ptr, ptr %destroyed_link35, align 8
  store ptr %21, ptr %qre_prev, align 8
  %22 = load ptr, ptr %destroyed, align 8
  %qre_prev44 = getelementptr inbounds %struct.ctl_arena_s, ptr %22, i64 0, i32 2, i32 1
  %23 = load ptr, ptr %qre_prev44, align 8
  %destroyed_link45 = getelementptr inbounds %struct.ctl_arena_s, ptr %23, i64 0, i32 2
  store ptr %22, ptr %destroyed_link45, align 8
  %24 = load ptr, ptr %qre_prev, align 8
  %destroyed_link49 = getelementptr inbounds %struct.ctl_arena_s, ptr %24, i64 0, i32 2
  store ptr %16, ptr %destroyed_link49, align 8
  %.pre = load ptr, ptr %destroyed_link, align 8
  br label %if.end52

if.end52:                                         ; preds = %do.body18, %arenas_i.exit28
  %25 = phi ptr [ %.pre, %do.body18 ], [ %16, %arenas_i.exit28 ]
  store ptr %25, ptr %destroyed, align 8
  %26 = load atomic i8, ptr @background_thread_enabled_state monotonic, align 1
  %27 = and i8 %26, 1
  %tobool.i.not.i = icmp eq i8 %27, 0
  br i1 %tobool.i.not.i, label %arena_reset_finish_background_thread.exit, label %if.then.i29

if.then.i29:                                      ; preds = %if.end52
  %28 = load ptr, ptr @background_thread_info, align 8
  %29 = load i64, ptr @max_background_threads, align 8
  %rem.i.i = urem i64 %3, %29
  %mtx.i = getelementptr inbounds %struct.background_thread_info_s, ptr %28, i64 %rem.i.i, i32 2
  %lock.i.i.i = getelementptr inbounds %struct.anon, ptr %mtx.i, i64 0, i32 1
  %call.i.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull %lock.i.i.i) #14
  %cmp.i.not.i.i = icmp eq i32 %call.i.i.i, 0
  br i1 %cmp.i.not.i.i, label %if.end.i.i, label %if.then.i.i31

if.then.i.i31:                                    ; preds = %if.then.i29
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull %mtx.i) #14
  %locked.i.i = getelementptr inbounds %struct.anon, ptr %mtx.i, i64 0, i32 2
  store atomic i8 1, ptr %locked.i.i monotonic, align 1
  br label %if.end.i.i

if.end.i.i:                                       ; preds = %if.then.i.i31, %if.then.i29
  %n_lock_ops.i.i.i = getelementptr inbounds %struct.mutex_prof_data_t, ptr %mtx.i, i64 0, i32 8
  %30 = load i64, ptr %n_lock_ops.i.i.i, align 8
  %inc.i.i.i = add i64 %30, 1
  store i64 %inc.i.i.i, ptr %n_lock_ops.i.i.i, align 8
  %prev_owner.i.i.i = getelementptr inbounds %struct.mutex_prof_data_t, ptr %mtx.i, i64 0, i32 7
  %31 = load ptr, ptr %prev_owner.i.i.i, align 8
  %cmp.not.i.i.i = icmp eq ptr %31, %tsd
  br i1 %cmp.not.i.i.i, label %malloc_mutex_lock.exit.i, label %if.then.i.i.i

if.then.i.i.i:                                    ; preds = %if.end.i.i
  store ptr %tsd, ptr %prev_owner.i.i.i, align 8
  %n_owner_switches.i.i.i = getelementptr inbounds %struct.mutex_prof_data_t, ptr %mtx.i, i64 0, i32 6
  %32 = load i64, ptr %n_owner_switches.i.i.i, align 8
  %inc2.i.i.i = add i64 %32, 1
  store i64 %inc2.i.i.i, ptr %n_owner_switches.i.i.i, align 8
  br label %malloc_mutex_lock.exit.i

malloc_mutex_lock.exit.i:                         ; preds = %if.then.i.i.i, %if.end.i.i
  %state.i = getelementptr inbounds %struct.background_thread_info_s, ptr %28, i64 %rem.i.i, i32 3
  store i32 1, ptr %state.i, align 8
  %locked.i5.i = getelementptr inbounds %struct.anon, ptr %mtx.i, i64 0, i32 2
  store atomic i8 0, ptr %locked.i5.i monotonic, align 1
  %call1.i.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull %lock.i.i.i) #14
  br label %arena_reset_finish_background_thread.exit

arena_reset_finish_background_thread.exit:        ; preds = %if.end52, %malloc_mutex_lock.exit.i
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i6.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 1)) #14
  br label %label_return

label_return:                                     ; preds = %lor.lhs.false15.i, %if.end9.i, %do.body6.i, %malloc_mutex_lock.exit, %if.end, %lor.lhs.false, %arena_reset_finish_background_thread.exit
  %ret.0 = phi i32 [ 0, %arena_reset_finish_background_thread.exit ], [ 14, %lor.lhs.false ], [ 14, %if.end ], [ 14, %if.end9.i ], [ 14, %do.body6.i ], [ 1, %malloc_mutex_lock.exit ], [ 14, %lor.lhs.false15.i ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @arena_i_dss_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %dss = alloca ptr, align 8
  store ptr null, ptr %dss, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp.not = icmp eq ptr %newp, null
  br i1 %cmp.not, label %do.body4.thread, label %if.then

if.then:                                          ; preds = %malloc_mutex_lock.exit
  %cmp1.not = icmp eq i64 %newlen, 8
  br i1 %cmp1.not, label %do.body4, label %label_return

do.body4:                                         ; preds = %if.then
  %3 = load ptr, ptr %newp, align 8
  store ptr %3, ptr %dss, align 8
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 1
  %4 = load i64, ptr %arrayidx, align 8
  %cmp5 = icmp ugt i64 %4, 4294967295
  br i1 %cmp5, label %label_return, label %if.end7

do.body4.thread:                                  ; preds = %malloc_mutex_lock.exit
  %arrayidx32 = getelementptr inbounds i64, ptr %mib, i64 1
  %5 = load i64, ptr %arrayidx32, align 8
  %cmp533 = icmp ugt i64 %5, 4294967295
  br i1 %cmp533, label %label_return, label %if.end23

if.end7:                                          ; preds = %do.body4
  %cmp10.not = icmp eq ptr %3, null
  br i1 %cmp10.not, label %if.end23, label %for.body

for.body:                                         ; preds = %if.end7, %for.inc
  %indvars.iv = phi i64 [ %indvars.iv.next, %for.inc ], [ 0, %if.end7 ]
  %arrayidx15 = getelementptr inbounds [0 x ptr], ptr @dss_prec_names, i64 0, i64 %indvars.iv
  %6 = load ptr, ptr %arrayidx15, align 8
  %call16 = tail call i32 @strcmp(ptr noundef nonnull dereferenceable(1) %6, ptr noundef nonnull dereferenceable(1) %3) #15
  %cmp17 = icmp eq i32 %call16, 0
  br i1 %cmp17, label %if.end23.loopexit, label %for.inc

for.inc:                                          ; preds = %for.body
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, 3
  br i1 %exitcond.not, label %label_return, label %for.body, !llvm.loop !17

if.end23.loopexit:                                ; preds = %for.body
  %7 = trunc i64 %indvars.iv to i32
  br label %if.end23

if.end23:                                         ; preds = %do.body4.thread, %if.end23.loopexit, %if.end7
  %8 = phi i64 [ %4, %if.end7 ], [ %4, %if.end23.loopexit ], [ %5, %do.body4.thread ]
  %dss_prec.1 = phi i32 [ 3, %if.end7 ], [ %7, %if.end23.loopexit ], [ 3, %do.body4.thread ]
  %conv38 = trunc i64 %8 to i32
  %cmp24 = icmp eq i32 %conv38, 4096
  br i1 %cmp24, label %if.then28, label %lor.lhs.false

lor.lhs.false:                                    ; preds = %if.end23
  %9 = load ptr, ptr @ctl_arenas, align 8
  %narenas = getelementptr inbounds %struct.ctl_arenas_s, ptr %9, i64 0, i32 1
  %10 = load i32, ptr %narenas, align 8
  %cmp26 = icmp eq i32 %10, %conv38
  br i1 %cmp26, label %if.then28, label %if.else

if.then28:                                        ; preds = %lor.lhs.false, %if.end23
  %cmp29.not = icmp eq i32 %dss_prec.1, 3
  br i1 %cmp29.not, label %if.end34, label %land.lhs.true

land.lhs.true:                                    ; preds = %if.then28
  %call31 = tail call zeroext i1 @extent_dss_prec_set(i32 noundef %dss_prec.1) #14
  br i1 %call31, label %label_return, label %if.end34

if.end34:                                         ; preds = %land.lhs.true, %if.then28
  %call35 = tail call i32 @extent_dss_prec_get() #14
  br label %if.end49

if.else:                                          ; preds = %lor.lhs.false
  %arrayidx.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %8
  %11 = load atomic i64, ptr %arrayidx.i acquire, align 8
  %12 = inttoptr i64 %11 to ptr
  %cmp38 = icmp eq i64 %11, 0
  br i1 %cmp38, label %label_return, label %lor.lhs.false40

lor.lhs.false40:                                  ; preds = %if.else
  %cmp41.not = icmp eq i32 %dss_prec.1, 3
  br i1 %cmp41.not, label %if.end47, label %land.lhs.true43

land.lhs.true43:                                  ; preds = %lor.lhs.false40
  %call44 = tail call zeroext i1 @arena_dss_prec_set(ptr noundef nonnull %12, i32 noundef %dss_prec.1) #14
  br i1 %call44, label %label_return, label %if.end47

if.end47:                                         ; preds = %land.lhs.true43, %lor.lhs.false40
  %call48 = tail call i32 @arena_dss_prec_get(ptr noundef nonnull %12) #14
  br label %if.end49

if.end49:                                         ; preds = %if.end47, %if.end34
  %dss_prec_old.0 = phi i32 [ %call35, %if.end34 ], [ %call48, %if.end47 ]
  %idxprom50 = zext i32 %dss_prec_old.0 to i64
  %arrayidx51 = getelementptr inbounds [0 x ptr], ptr @dss_prec_names, i64 0, i64 %idxprom50
  %13 = load ptr, ptr %arrayidx51, align 8
  store ptr %13, ptr %dss, align 8
  %cmp53 = icmp ne ptr %oldp, null
  %cmp56 = icmp ne ptr %oldlenp, null
  %or.cond = and i1 %cmp53, %cmp56
  br i1 %or.cond, label %if.then58, label %label_return

if.then58:                                        ; preds = %if.end49
  %14 = load i64, ptr %oldlenp, align 8
  %cmp59.not = icmp eq i64 %14, 8
  br i1 %cmp59.not, label %if.end64, label %if.then61

if.then61:                                        ; preds = %if.then58
  %spec.select = tail call i64 @llvm.umin.i64(i64 %14, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %dss, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end64:                                         ; preds = %if.then58
  store ptr %13, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %for.inc, %do.body4.thread, %if.end64, %if.end49, %if.else, %land.lhs.true43, %land.lhs.true, %do.body4, %if.then, %if.then61
  %ret.0 = phi i32 [ 22, %if.then61 ], [ 22, %if.then ], [ 14, %do.body4 ], [ 14, %land.lhs.true ], [ 14, %land.lhs.true43 ], [ 14, %if.else ], [ 0, %if.end49 ], [ 0, %if.end64 ], [ 14, %do.body4.thread ], [ 22, %for.inc ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nounwind willreturn memory(readwrite, inaccessiblemem: none) uwtable
define internal i32 @arena_i_oversize_threshold_ctl(ptr nocapture readnone %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #11 {
entry:
  %oldval = alloca i64, align 8
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 1
  %0 = load i64, ptr %arrayidx, align 8
  %cmp = icmp ugt i64 %0, 4294967295
  br i1 %cmp, label %label_return, label %if.end

if.end:                                           ; preds = %entry
  %arrayidx.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %0
  %1 = load atomic i64, ptr %arrayidx.i acquire, align 8
  %2 = inttoptr i64 %1 to ptr
  %cmp3 = icmp eq i64 %1, 0
  br i1 %cmp3, label %label_return, label %if.end6

if.end6:                                          ; preds = %if.end
  %cmp7 = icmp ne ptr %oldp, null
  %cmp9 = icmp ne ptr %oldlenp, null
  %or.cond = and i1 %cmp7, %cmp9
  br i1 %or.cond, label %if.then11, label %if.end28

if.then11:                                        ; preds = %if.end6
  %oversize_threshold = getelementptr inbounds %struct.arena_s, ptr %2, i64 0, i32 10, i32 4, i32 10
  %3 = load atomic i64, ptr %oversize_threshold monotonic, align 8
  store i64 %3, ptr %oldval, align 8
  %4 = load i64, ptr %oldlenp, align 8
  %cmp20.not = icmp eq i64 %4, 8
  br i1 %cmp20.not, label %if.end25, label %if.then22

if.then22:                                        ; preds = %if.then11
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end25:                                         ; preds = %if.then11
  store i64 %3, ptr %oldp, align 8
  br label %if.end28

if.end28:                                         ; preds = %if.end25, %if.end6
  %cmp29.not = icmp eq ptr %newp, null
  br i1 %cmp29.not, label %label_return, label %if.then31

if.then31:                                        ; preds = %if.end28
  %cmp32.not = icmp eq i64 %newlen, 8
  br i1 %cmp32.not, label %if.end35, label %label_return

if.end35:                                         ; preds = %if.then31
  %oversize_threshold38 = getelementptr inbounds %struct.arena_s, ptr %2, i64 0, i32 10, i32 4, i32 10
  %5 = load i64, ptr %newp, align 8
  store atomic i64 %5, ptr %oversize_threshold38 monotonic, align 8
  br label %label_return

label_return:                                     ; preds = %if.end28, %if.end35, %if.then31, %if.end, %entry, %if.then22
  %ret.0 = phi i32 [ 22, %if.then22 ], [ 14, %entry ], [ 14, %if.end ], [ 22, %if.then31 ], [ 0, %if.end35 ], [ 0, %if.end28 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @arena_i_dirty_decay_ms_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %oldval.i = alloca i64, align 8
  %0 = getelementptr i8, ptr %mib, i64 8
  %mib.val = load i64, ptr %0, align 8
  call void @llvm.lifetime.start.p0(i64 8, ptr nonnull %oldval.i)
  %cmp.i = icmp ugt i64 %mib.val, 4294967295
  br i1 %cmp.i, label %arena_i_decay_ms_ctl_impl.exit, label %if.end.i

if.end.i:                                         ; preds = %entry
  %conv.i = trunc i64 %mib.val to i32
  %arrayidx.i.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %mib.val
  %1 = load atomic i64, ptr %arrayidx.i.i acquire, align 8
  %2 = inttoptr i64 %1 to ptr
  %cmp3.i = icmp eq i64 %1, 0
  br i1 %cmp3.i, label %arena_i_decay_ms_ctl_impl.exit, label %if.end6.i

if.end6.i:                                        ; preds = %if.end.i
  %cmp8.i = icmp ne ptr %oldp, null
  %cmp10.i = icmp ne ptr %oldlenp, null
  %or.cond.i = and i1 %cmp8.i, %cmp10.i
  br i1 %or.cond.i, label %if.then12.i, label %if.end30.i

if.then12.i:                                      ; preds = %if.end6.i
  %call13.i = tail call i64 @arena_decay_ms_get(ptr noundef nonnull %2, i32 noundef 1) #14
  store i64 %call13.i, ptr %oldval.i, align 8
  %3 = load i64, ptr %oldlenp, align 8
  %cmp21.not.i = icmp eq i64 %3, 8
  br i1 %cmp21.not.i, label %if.end27.i, label %if.then23.i

if.then23.i:                                      ; preds = %if.then12.i
  %spec.select.i = tail call i64 @llvm.umin.i64(i64 %3, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval.i, i64 %spec.select.i, i1 false)
  store i64 %spec.select.i, ptr %oldlenp, align 8
  br label %arena_i_decay_ms_ctl_impl.exit

if.end27.i:                                       ; preds = %if.then12.i
  store i64 %call13.i, ptr %oldp, align 8
  br label %if.end30.i

if.end30.i:                                       ; preds = %if.end27.i, %if.end6.i
  %cmp31.not.i = icmp eq ptr %newp, null
  br i1 %cmp31.not.i, label %if.end52.i, label %if.then33.i

if.then33.i:                                      ; preds = %if.end30.i
  %cmp34.not.i = icmp eq i64 %newlen, 8
  br i1 %cmp34.not.i, label %if.end37.i, label %arena_i_decay_ms_ctl_impl.exit

if.end37.i:                                       ; preds = %if.then33.i
  %call38.i = tail call zeroext i1 @arena_is_huge(i32 noundef %conv.i) #14
  %.pre1.i = load i64, ptr %newp, align 8
  %cmp41.i = icmp sgt i64 %.pre1.i, 0
  %or.cond2.i = select i1 %call38.i, i1 %cmp41.i, i1 false
  br i1 %or.cond2.i, label %if.then43.i, label %if.end47.i

if.then43.i:                                      ; preds = %if.end37.i
  %call44.i = tail call zeroext i1 @background_thread_create(ptr noundef %tsd, i32 noundef %conv.i) #14
  br i1 %call44.i, label %arena_i_decay_ms_ctl_impl.exit, label %if.then43.if.end47_crit_edge.i

if.then43.if.end47_crit_edge.i:                   ; preds = %if.then43.i
  %.pre.i = load i64, ptr %newp, align 8
  br label %if.end47.i

if.end47.i:                                       ; preds = %if.then43.if.end47_crit_edge.i, %if.end37.i
  %4 = phi i64 [ %.pre.i, %if.then43.if.end47_crit_edge.i ], [ %.pre1.i, %if.end37.i ]
  %call49.i = tail call zeroext i1 @arena_decay_ms_set(ptr noundef %tsd, ptr noundef nonnull %2, i32 noundef 1, i64 noundef %4) #14
  br i1 %call49.i, label %arena_i_decay_ms_ctl_impl.exit, label %if.end52.i

if.end52.i:                                       ; preds = %if.end47.i, %if.end30.i
  br label %arena_i_decay_ms_ctl_impl.exit

arena_i_decay_ms_ctl_impl.exit:                   ; preds = %entry, %if.end.i, %if.then23.i, %if.then33.i, %if.then43.i, %if.end47.i, %if.end52.i
  %ret.0.i = phi i32 [ 22, %if.then23.i ], [ 0, %if.end52.i ], [ 14, %entry ], [ 14, %if.end.i ], [ 22, %if.then33.i ], [ 14, %if.then43.i ], [ 14, %if.end47.i ]
  call void @llvm.lifetime.end.p0(i64 8, ptr nonnull %oldval.i)
  ret i32 %ret.0.i
}

; Function Attrs: nounwind uwtable
define internal i32 @arena_i_muzzy_decay_ms_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %oldval.i = alloca i64, align 8
  %0 = getelementptr i8, ptr %mib, i64 8
  %mib.val = load i64, ptr %0, align 8
  call void @llvm.lifetime.start.p0(i64 8, ptr nonnull %oldval.i)
  %cmp.i = icmp ugt i64 %mib.val, 4294967295
  br i1 %cmp.i, label %arena_i_decay_ms_ctl_impl.exit, label %if.end.i

if.end.i:                                         ; preds = %entry
  %conv.i = trunc i64 %mib.val to i32
  %arrayidx.i.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %mib.val
  %1 = load atomic i64, ptr %arrayidx.i.i acquire, align 8
  %2 = inttoptr i64 %1 to ptr
  %cmp3.i = icmp eq i64 %1, 0
  br i1 %cmp3.i, label %arena_i_decay_ms_ctl_impl.exit, label %if.end6.i

if.end6.i:                                        ; preds = %if.end.i
  %cmp8.i = icmp ne ptr %oldp, null
  %cmp10.i = icmp ne ptr %oldlenp, null
  %or.cond.i = and i1 %cmp8.i, %cmp10.i
  br i1 %or.cond.i, label %if.then12.i, label %if.end30.i

if.then12.i:                                      ; preds = %if.end6.i
  %call13.i = tail call i64 @arena_decay_ms_get(ptr noundef nonnull %2, i32 noundef 2) #14
  store i64 %call13.i, ptr %oldval.i, align 8
  %3 = load i64, ptr %oldlenp, align 8
  %cmp21.not.i = icmp eq i64 %3, 8
  br i1 %cmp21.not.i, label %if.end27.i, label %if.then23.i

if.then23.i:                                      ; preds = %if.then12.i
  %spec.select.i = tail call i64 @llvm.umin.i64(i64 %3, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval.i, i64 %spec.select.i, i1 false)
  store i64 %spec.select.i, ptr %oldlenp, align 8
  br label %arena_i_decay_ms_ctl_impl.exit

if.end27.i:                                       ; preds = %if.then12.i
  store i64 %call13.i, ptr %oldp, align 8
  br label %if.end30.i

if.end30.i:                                       ; preds = %if.end27.i, %if.end6.i
  %cmp31.not.i = icmp eq ptr %newp, null
  br i1 %cmp31.not.i, label %if.end52.i, label %if.then33.i

if.then33.i:                                      ; preds = %if.end30.i
  %cmp34.not.i = icmp eq i64 %newlen, 8
  br i1 %cmp34.not.i, label %if.end37.i, label %arena_i_decay_ms_ctl_impl.exit

if.end37.i:                                       ; preds = %if.then33.i
  %call38.i = tail call zeroext i1 @arena_is_huge(i32 noundef %conv.i) #14
  %.pre1.i = load i64, ptr %newp, align 8
  %cmp41.i = icmp sgt i64 %.pre1.i, 0
  %or.cond2.i = select i1 %call38.i, i1 %cmp41.i, i1 false
  br i1 %or.cond2.i, label %if.then43.i, label %if.end47.i

if.then43.i:                                      ; preds = %if.end37.i
  %call44.i = tail call zeroext i1 @background_thread_create(ptr noundef %tsd, i32 noundef %conv.i) #14
  br i1 %call44.i, label %arena_i_decay_ms_ctl_impl.exit, label %if.then43.if.end47_crit_edge.i

if.then43.if.end47_crit_edge.i:                   ; preds = %if.then43.i
  %.pre.i = load i64, ptr %newp, align 8
  br label %if.end47.i

if.end47.i:                                       ; preds = %if.then43.if.end47_crit_edge.i, %if.end37.i
  %4 = phi i64 [ %.pre.i, %if.then43.if.end47_crit_edge.i ], [ %.pre1.i, %if.end37.i ]
  %call49.i = tail call zeroext i1 @arena_decay_ms_set(ptr noundef %tsd, ptr noundef nonnull %2, i32 noundef 2, i64 noundef %4) #14
  br i1 %call49.i, label %arena_i_decay_ms_ctl_impl.exit, label %if.end52.i

if.end52.i:                                       ; preds = %if.end47.i, %if.end30.i
  br label %arena_i_decay_ms_ctl_impl.exit

arena_i_decay_ms_ctl_impl.exit:                   ; preds = %entry, %if.end.i, %if.then23.i, %if.then33.i, %if.then43.i, %if.end47.i, %if.end52.i
  %ret.0.i = phi i32 [ 22, %if.then23.i ], [ 0, %if.end52.i ], [ 14, %entry ], [ 14, %if.end.i ], [ 22, %if.then33.i ], [ 14, %if.then43.i ], [ 14, %if.end47.i ]
  call void @llvm.lifetime.end.p0(i64 8, ptr nonnull %oldval.i)
  ret i32 %ret.0.i
}

; Function Attrs: nounwind uwtable
define internal i32 @arena_i_extent_hooks_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %old_extent_hooks = alloca ptr, align 8
  %config = alloca %struct.arena_config_s, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 1
  %3 = load i64, ptr %arrayidx, align 8
  %cmp = icmp ugt i64 %3, 4294967295
  br i1 %cmp, label %label_return, label %if.end

if.end:                                           ; preds = %malloc_mutex_lock.exit
  %conv = trunc i64 %3 to i32
  %call2 = tail call i32 @narenas_total_get() #14
  %cmp3 = icmp ugt i32 %call2, %conv
  br i1 %cmp3, label %if.then5, label %label_return

if.then5:                                         ; preds = %if.end
  %arrayidx.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %3
  %4 = load atomic i64, ptr %arrayidx.i acquire, align 8
  %5 = inttoptr i64 %4 to ptr
  %cmp8 = icmp eq i64 %4, 0
  br i1 %cmp8, label %if.then10, label %if.else

if.then10:                                        ; preds = %if.then5
  %6 = load i32, ptr @narenas_auto, align 4
  %cmp11.not = icmp ugt i32 %6, %conv
  br i1 %cmp11.not, label %if.end14, label %label_return

if.end14:                                         ; preds = %if.then10
  store ptr @ehooks_default_extent_hooks, ptr %old_extent_hooks, align 8
  %cmp16 = icmp ne ptr %oldp, null
  %cmp18 = icmp ne ptr %oldlenp, null
  %or.cond = and i1 %cmp16, %cmp18
  br i1 %or.cond, label %if.then20, label %do.end28

if.then20:                                        ; preds = %if.end14
  %7 = load i64, ptr %oldlenp, align 8
  %cmp21.not = icmp eq i64 %7, 8
  br i1 %cmp21.not, label %if.end26, label %if.then23

if.then23:                                        ; preds = %if.then20
  %spec.select = tail call i64 @llvm.umin.i64(i64 %7, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %old_extent_hooks, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end26:                                         ; preds = %if.then20
  store ptr @ehooks_default_extent_hooks, ptr %oldp, align 8
  br label %do.end28

do.end28:                                         ; preds = %if.end14, %if.end26
  %cmp29.not = icmp eq ptr %newp, null
  br i1 %cmp29.not, label %if.end110, label %if.then35

if.then35:                                        ; preds = %do.end28
  %cmp36.not = icmp eq i64 %newlen, 8
  br i1 %cmp36.not, label %do.end41, label %label_return

do.end41:                                         ; preds = %if.then35
  %8 = load ptr, ptr %newp, align 8
  call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) %config, ptr noundef nonnull align 8 dereferenceable(16) @arena_config_default, i64 16, i1 false)
  store ptr %8, ptr %config, align 8
  %call43 = call ptr @arena_init(ptr noundef %tsd, i32 noundef %conv, ptr noundef nonnull %config) #14
  %cmp44 = icmp eq ptr %call43, null
  br i1 %cmp44, label %label_return, label %if.end110

if.else:                                          ; preds = %if.then5
  %cmp49.not = icmp eq ptr %newp, null
  br i1 %cmp49.not, label %if.else84, label %if.then56

if.then56:                                        ; preds = %if.else
  %cmp57.not = icmp eq i64 %newlen, 8
  br i1 %cmp57.not, label %do.end62, label %label_return

do.end62:                                         ; preds = %if.then56
  %9 = load ptr, ptr %newp, align 8
  %call63 = tail call ptr @arena_set_extent_hooks(ptr noundef %tsd, ptr noundef nonnull %5, ptr noundef %9) #14
  store ptr %call63, ptr %old_extent_hooks, align 8
  %cmp65 = icmp ne ptr %oldp, null
  %cmp68 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp65, %cmp68
  br i1 %or.cond1, label %if.then70, label %if.end110

if.then70:                                        ; preds = %do.end62
  %10 = load i64, ptr %oldlenp, align 8
  %cmp71.not = icmp eq i64 %10, 8
  br i1 %cmp71.not, label %if.end110.sink.split, label %if.then73

if.then73:                                        ; preds = %if.then70
  %spec.select50 = tail call i64 @llvm.umin.i64(i64 %10, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %old_extent_hooks, i64 %spec.select50, i1 false)
  store i64 %spec.select50, ptr %oldlenp, align 8
  br label %label_return

if.else84:                                        ; preds = %if.else
  %call85 = tail call ptr @arena_get_ehooks(ptr noundef nonnull %5) #14
  %ptr.i = getelementptr inbounds %struct.ehooks_s, ptr %call85, i64 0, i32 1
  %11 = load atomic i64, ptr %ptr.i acquire, align 8
  %12 = inttoptr i64 %11 to ptr
  store ptr %12, ptr %old_extent_hooks, align 8
  %cmp88 = icmp ne ptr %oldp, null
  %cmp91 = icmp ne ptr %oldlenp, null
  %or.cond2 = and i1 %cmp88, %cmp91
  br i1 %or.cond2, label %if.then93, label %if.end110

if.then93:                                        ; preds = %if.else84
  %13 = load i64, ptr %oldlenp, align 8
  %cmp94.not = icmp eq i64 %13, 8
  br i1 %cmp94.not, label %if.end110.sink.split, label %if.then96

if.then96:                                        ; preds = %if.then93
  %spec.select51 = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %old_extent_hooks, i64 %spec.select51, i1 false)
  store i64 %spec.select51, ptr %oldlenp, align 8
  br label %label_return

if.end110.sink.split:                             ; preds = %if.then93, %if.then70
  %.sink = phi ptr [ %call63, %if.then70 ], [ %12, %if.then93 ]
  store ptr %.sink, ptr %oldp, align 8
  br label %if.end110

if.end110:                                        ; preds = %if.end110.sink.split, %do.end41, %do.end28, %if.else84, %do.end62
  br label %label_return

label_return:                                     ; preds = %if.end, %if.then56, %do.end41, %if.then35, %if.then10, %malloc_mutex_lock.exit, %if.end110, %if.then96, %if.then73, %if.then23
  %ret.0 = phi i32 [ 22, %if.then23 ], [ 0, %if.end110 ], [ 22, %if.then73 ], [ 22, %if.then96 ], [ 14, %malloc_mutex_lock.exit ], [ 14, %if.then10 ], [ 22, %if.then35 ], [ 14, %do.end41 ], [ 22, %if.then56 ], [ 14, %if.end ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @arena_i_retain_grow_limit_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %old_limit = alloca i64, align 8
  %new_limit = alloca i64, align 8
  %0 = load i8, ptr @opt_retain, align 1
  %1 = and i8 %0, 1
  %tobool.not = icmp eq i8 %1, 0
  br i1 %tobool.not, label %return, label %if.end

if.end:                                           ; preds = %entry
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %if.end
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %if.end
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %2, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %3 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %3, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %4 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %4, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 1
  %5 = load i64, ptr %arrayidx, align 8
  %cmp = icmp ugt i64 %5, 4294967295
  br i1 %cmp, label %label_return, label %if.end2

if.end2:                                          ; preds = %malloc_mutex_lock.exit
  %conv = trunc i64 %5 to i32
  %call4 = tail call i32 @narenas_total_get() #14
  %cmp5 = icmp ugt i32 %call4, %conv
  br i1 %cmp5, label %land.lhs.true, label %label_return

land.lhs.true:                                    ; preds = %if.end2
  %arrayidx.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %5
  %6 = load atomic i64, ptr %arrayidx.i acquire, align 8
  %7 = inttoptr i64 %6 to ptr
  %cmp9.not = icmp eq i64 %6, 0
  br i1 %cmp9.not, label %label_return, label %if.then11

if.then11:                                        ; preds = %land.lhs.true
  %cmp12.not = icmp eq ptr %newp, null
  br i1 %cmp12.not, label %if.end25, label %if.then18

if.then18:                                        ; preds = %if.then11
  %cmp19.not = icmp eq i64 %newlen, 8
  br i1 %cmp19.not, label %if.end22, label %label_return

if.end22:                                         ; preds = %if.then18
  %8 = load i64, ptr %newp, align 8
  store i64 %8, ptr %new_limit, align 8
  br label %if.end25

if.end25:                                         ; preds = %if.end22, %if.then11
  %new_limit. = phi ptr [ %new_limit, %if.end22 ], [ null, %if.then11 ]
  %call28 = call zeroext i1 @arena_retain_grow_limit_get_set(ptr noundef %tsd, ptr noundef nonnull %7, ptr noundef nonnull %old_limit, ptr noundef %new_limit.) #14
  br i1 %call28, label %label_return, label %do.body31

do.body31:                                        ; preds = %if.end25
  %cmp32 = icmp ne ptr %oldp, null
  %cmp35 = icmp ne ptr %oldlenp, null
  %or.cond = and i1 %cmp32, %cmp35
  br i1 %or.cond, label %if.then37, label %label_return

if.then37:                                        ; preds = %do.body31
  %9 = load i64, ptr %oldlenp, align 8
  %cmp38.not = icmp eq i64 %9, 8
  br i1 %cmp38.not, label %if.end47, label %if.then40

if.then40:                                        ; preds = %if.then37
  %spec.select = call i64 @llvm.umin.i64(i64 %9, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %old_limit, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end47:                                         ; preds = %if.then37
  %10 = load i64, ptr %old_limit, align 8
  store i64 %10, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end2, %land.lhs.true, %if.end25, %if.end47, %do.body31, %if.then18, %malloc_mutex_lock.exit, %if.then40
  %ret.0 = phi i32 [ 22, %if.then40 ], [ 14, %malloc_mutex_lock.exit ], [ 22, %if.then18 ], [ 0, %do.body31 ], [ 0, %if.end47 ], [ 14, %if.end25 ], [ 14, %land.lhs.true ], [ 14, %if.end2 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  br label %return

return:                                           ; preds = %entry, %label_return
  %retval.0 = phi i32 [ %ret.0, %label_return ], [ 2, %entry ]
  ret i32 %retval.0
}

; Function Attrs: nounwind uwtable
define internal fastcc void @arena_i_decay(ptr noundef %tsdn, i32 noundef %arena_ind, i1 noundef zeroext %all) unnamed_addr #0 {
entry:
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsdn
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsdn, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %3 = load ptr, ptr @ctl_arenas, align 8
  %narenas1 = getelementptr inbounds %struct.ctl_arenas_s, ptr %3, i64 0, i32 1
  %4 = load i32, ptr %narenas1, align 8
  %cmp = icmp eq i32 %arena_ind, 4096
  %cmp2 = icmp eq i32 %4, %arena_ind
  %or.cond = select i1 %cmp, i1 true, i1 %cmp2
  br i1 %or.cond, label %if.then, label %do.end

if.then:                                          ; preds = %malloc_mutex_lock.exit
  %5 = zext i32 %4 to i64
  %6 = tail call ptr @llvm.stacksave.p0()
  %vla = alloca ptr, i64 %5, align 16
  %cmp326.not = icmp eq i32 %4, 0
  br i1 %cmp326.not, label %for.end.thread, label %for.body

for.end.thread:                                   ; preds = %if.then
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i36 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  br label %for.end15

for.body:                                         ; preds = %if.then, %for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %for.body ], [ 0, %if.then ]
  %arrayidx.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %indvars.iv
  %7 = load atomic i64, ptr %arrayidx.i acquire, align 8
  %8 = inttoptr i64 %7 to ptr
  %arrayidx = getelementptr inbounds ptr, ptr %vla, i64 %indvars.iv
  store ptr %8, ptr %arrayidx, align 8
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, %5
  br i1 %exitcond.not, label %for.end, label %for.body, !llvm.loop !18

for.end:                                          ; preds = %for.body
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  br i1 %cmp326.not, label %for.end15, label %for.body6

for.body6:                                        ; preds = %for.end, %for.inc13
  %indvars.iv31 = phi i64 [ %indvars.iv.next32, %for.inc13 ], [ 0, %for.end ]
  %arrayidx8 = getelementptr inbounds ptr, ptr %vla, i64 %indvars.iv31
  %9 = load ptr, ptr %arrayidx8, align 8
  %cmp9.not = icmp eq ptr %9, null
  br i1 %cmp9.not, label %for.inc13, label %if.then10

if.then10:                                        ; preds = %for.body6
  tail call void @arena_decay(ptr noundef %tsdn, ptr noundef nonnull %9, i1 noundef zeroext false, i1 noundef zeroext %all) #14
  br label %for.inc13

for.inc13:                                        ; preds = %for.body6, %if.then10
  %indvars.iv.next32 = add nuw nsw i64 %indvars.iv31, 1
  %exitcond35.not = icmp eq i64 %indvars.iv.next32, %5
  br i1 %exitcond35.not, label %for.end15, label %for.body6, !llvm.loop !19

for.end15:                                        ; preds = %for.inc13, %for.end.thread, %for.end
  tail call void @llvm.stackrestore.p0(ptr %6)
  br label %if.end21

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %idxprom.i22 = zext i32 %arena_ind to i64
  %arrayidx.i23 = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %idxprom.i22
  %10 = load atomic i64, ptr %arrayidx.i23 acquire, align 8
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i25 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp17.not = icmp eq i64 %10, 0
  br i1 %cmp17.not, label %if.end21, label %if.then18

if.then18:                                        ; preds = %do.end
  %11 = inttoptr i64 %10 to ptr
  tail call void @arena_decay(ptr noundef %tsdn, ptr noundef nonnull %11, i1 noundef zeroext false, i1 noundef zeroext %all) #14
  br label %if.end21

if.end21:                                         ; preds = %do.end, %if.then18, %for.end15
  ret void
}

; Function Attrs: nounwind uwtable
define internal fastcc void @arena_reset_prepare_background_thread(ptr noundef %tsd, i32 noundef %arena_ind) unnamed_addr #0 {
entry:
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @background_thread_lock) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %3 = load atomic i8, ptr @background_thread_enabled_state monotonic, align 1
  %4 = and i8 %3, 1
  %tobool.i.not = icmp eq i8 %4, 0
  br i1 %tobool.i.not, label %if.end, label %if.then

if.then:                                          ; preds = %malloc_mutex_lock.exit
  %conv = zext i32 %arena_ind to i64
  %5 = load ptr, ptr @background_thread_info, align 8
  %6 = load i64, ptr @max_background_threads, align 8
  %rem.i = urem i64 %conv, %6
  %mtx = getelementptr inbounds %struct.background_thread_info_s, ptr %5, i64 %rem.i, i32 2
  %lock.i.i = getelementptr inbounds %struct.anon, ptr %mtx, i64 0, i32 1
  %call.i.i5 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull %lock.i.i) #14
  %cmp.i.not.i6 = icmp eq i32 %call.i.i5, 0
  br i1 %cmp.i.not.i6, label %if.end.i8, label %if.then.i7

if.then.i7:                                       ; preds = %if.then
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull %mtx) #14
  %locked.i = getelementptr inbounds %struct.anon, ptr %mtx, i64 0, i32 2
  store atomic i8 1, ptr %locked.i monotonic, align 1
  br label %if.end.i8

if.end.i8:                                        ; preds = %if.then.i7, %if.then
  %n_lock_ops.i.i = getelementptr inbounds %struct.mutex_prof_data_t, ptr %mtx, i64 0, i32 8
  %7 = load i64, ptr %n_lock_ops.i.i, align 8
  %inc.i.i9 = add i64 %7, 1
  store i64 %inc.i.i9, ptr %n_lock_ops.i.i, align 8
  %prev_owner.i.i = getelementptr inbounds %struct.mutex_prof_data_t, ptr %mtx, i64 0, i32 7
  %8 = load ptr, ptr %prev_owner.i.i, align 8
  %cmp.not.i.i10 = icmp eq ptr %8, %tsd
  br i1 %cmp.not.i.i10, label %malloc_mutex_lock.exit13, label %if.then.i.i11

if.then.i.i11:                                    ; preds = %if.end.i8
  store ptr %tsd, ptr %prev_owner.i.i, align 8
  %n_owner_switches.i.i = getelementptr inbounds %struct.mutex_prof_data_t, ptr %mtx, i64 0, i32 6
  %9 = load i64, ptr %n_owner_switches.i.i, align 8
  %inc2.i.i12 = add i64 %9, 1
  store i64 %inc2.i.i12, ptr %n_owner_switches.i.i, align 8
  br label %malloc_mutex_lock.exit13

malloc_mutex_lock.exit13:                         ; preds = %if.end.i8, %if.then.i.i11
  %state = getelementptr inbounds %struct.background_thread_info_s, ptr %5, i64 %rem.i, i32 3
  store i32 2, ptr %state, align 8
  %locked.i14 = getelementptr inbounds %struct.anon, ptr %mtx, i64 0, i32 2
  store atomic i8 0, ptr %locked.i14 monotonic, align 1
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull %lock.i.i) #14
  br label %if.end

if.end:                                           ; preds = %malloc_mutex_lock.exit13, %malloc_mutex_lock.exit
  ret void
}

declare void @arena_reset(ptr noundef, ptr noundef) local_unnamed_addr #1

declare i32 @arena_nthreads_get(ptr noundef, i1 noundef zeroext) local_unnamed_addr #1

declare void @arena_destroy(ptr noundef, ptr noundef) local_unnamed_addr #1

; Function Attrs: mustprogress nofree nounwind willreturn memory(argmem: read)
declare i32 @strcmp(ptr nocapture noundef, ptr nocapture noundef) local_unnamed_addr #8

declare zeroext i1 @extent_dss_prec_set(i32 noundef) local_unnamed_addr #1

declare i32 @extent_dss_prec_get() local_unnamed_addr #1

declare zeroext i1 @arena_dss_prec_set(ptr noundef, i32 noundef) local_unnamed_addr #1

declare i32 @arena_dss_prec_get(ptr noundef) local_unnamed_addr #1

declare i64 @arena_decay_ms_get(ptr noundef, i32 noundef) local_unnamed_addr #1

declare zeroext i1 @arena_is_huge(i32 noundef) local_unnamed_addr #1

declare zeroext i1 @background_thread_create(ptr noundef, i32 noundef) local_unnamed_addr #1

declare zeroext i1 @arena_decay_ms_set(ptr noundef, ptr noundef, i32 noundef, i64 noundef) local_unnamed_addr #1

declare ptr @arena_set_extent_hooks(ptr noundef, ptr noundef, ptr noundef) local_unnamed_addr #1

declare ptr @arena_get_ehooks(ptr noundef) local_unnamed_addr #1

declare zeroext i1 @arena_retain_grow_limit_get_set(ptr noundef, ptr noundef, ptr noundef, ptr noundef) local_unnamed_addr #1

; Function Attrs: nounwind uwtable
define internal i32 @arenas_narenas_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %narenas = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_arenas, align 8
  %narenas2 = getelementptr inbounds %struct.ctl_arenas_s, ptr %3, i64 0, i32 1
  %4 = load i32, ptr %narenas2, align 8
  store i32 %4, ptr %narenas, align 4
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %5, 4
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %narenas, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i32 %4, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @arenas_dirty_decay_ms_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %oldval.i = alloca i64, align 8
  call void @llvm.lifetime.start.p0(i64 8, ptr nonnull %oldval.i)
  %cmp.i = icmp ne ptr %oldp, null
  %cmp1.i = icmp ne ptr %oldlenp, null
  %or.cond.i = and i1 %cmp.i, %cmp1.i
  br i1 %or.cond.i, label %if.then.i, label %if.end15.i

if.then.i:                                        ; preds = %entry
  %call.i = tail call i64 @arena_dirty_decay_ms_default_get() #14
  store i64 %call.i, ptr %oldval.i, align 8
  %0 = load i64, ptr %oldlenp, align 8
  %cmp7.not.i = icmp eq i64 %0, 8
  br i1 %cmp7.not.i, label %if.end.i, label %if.then8.i

if.then8.i:                                       ; preds = %if.then.i
  %spec.select.i = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval.i, i64 %spec.select.i, i1 false)
  store i64 %spec.select.i, ptr %oldlenp, align 8
  br label %arenas_decay_ms_ctl_impl.exit

if.end.i:                                         ; preds = %if.then.i
  store i64 %call.i, ptr %oldp, align 8
  br label %if.end15.i

if.end15.i:                                       ; preds = %if.end.i, %entry
  %cmp16.not.i = icmp eq ptr %newp, null
  br i1 %cmp16.not.i, label %if.end28.i, label %if.then17.i

if.then17.i:                                      ; preds = %if.end15.i
  %cmp18.not.i = icmp eq i64 %newlen, 8
  br i1 %cmp18.not.i, label %if.end20.i, label %arenas_decay_ms_ctl_impl.exit

if.end20.i:                                       ; preds = %if.then17.i
  %1 = load i64, ptr %newp, align 8
  %call23.i = tail call zeroext i1 @arena_dirty_decay_ms_default_set(i64 noundef %1) #14
  br i1 %call23.i, label %arenas_decay_ms_ctl_impl.exit, label %if.end28.i

if.end28.i:                                       ; preds = %if.end20.i, %if.end15.i
  br label %arenas_decay_ms_ctl_impl.exit

arenas_decay_ms_ctl_impl.exit:                    ; preds = %if.then8.i, %if.then17.i, %if.end20.i, %if.end28.i
  %ret.0.i = phi i32 [ 22, %if.then8.i ], [ 0, %if.end28.i ], [ 22, %if.then17.i ], [ 14, %if.end20.i ]
  call void @llvm.lifetime.end.p0(i64 8, ptr nonnull %oldval.i)
  ret i32 %ret.0.i
}

; Function Attrs: nounwind uwtable
define internal i32 @arenas_muzzy_decay_ms_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %oldval.i = alloca i64, align 8
  call void @llvm.lifetime.start.p0(i64 8, ptr nonnull %oldval.i)
  %cmp.i = icmp ne ptr %oldp, null
  %cmp1.i = icmp ne ptr %oldlenp, null
  %or.cond.i = and i1 %cmp.i, %cmp1.i
  br i1 %or.cond.i, label %if.then.i, label %if.end15.i

if.then.i:                                        ; preds = %entry
  %call2.i = tail call i64 @arena_muzzy_decay_ms_default_get() #14
  store i64 %call2.i, ptr %oldval.i, align 8
  %0 = load i64, ptr %oldlenp, align 8
  %cmp7.not.i = icmp eq i64 %0, 8
  br i1 %cmp7.not.i, label %if.end.i, label %if.then8.i

if.then8.i:                                       ; preds = %if.then.i
  %spec.select.i = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval.i, i64 %spec.select.i, i1 false)
  store i64 %spec.select.i, ptr %oldlenp, align 8
  br label %arenas_decay_ms_ctl_impl.exit

if.end.i:                                         ; preds = %if.then.i
  store i64 %call2.i, ptr %oldp, align 8
  br label %if.end15.i

if.end15.i:                                       ; preds = %if.end.i, %entry
  %cmp16.not.i = icmp eq ptr %newp, null
  br i1 %cmp16.not.i, label %if.end28.i, label %if.then17.i

if.then17.i:                                      ; preds = %if.end15.i
  %cmp18.not.i = icmp eq i64 %newlen, 8
  br i1 %cmp18.not.i, label %if.end20.i, label %arenas_decay_ms_ctl_impl.exit

if.end20.i:                                       ; preds = %if.then17.i
  %1 = load i64, ptr %newp, align 8
  %call25.i = tail call zeroext i1 @arena_muzzy_decay_ms_default_set(i64 noundef %1) #14
  br i1 %call25.i, label %arenas_decay_ms_ctl_impl.exit, label %if.end28.i

if.end28.i:                                       ; preds = %if.end20.i, %if.end15.i
  br label %arenas_decay_ms_ctl_impl.exit

arenas_decay_ms_ctl_impl.exit:                    ; preds = %if.then8.i, %if.then17.i, %if.end20.i, %if.end28.i
  %ret.0.i = phi i32 [ 22, %if.then8.i ], [ 0, %if.end28.i ], [ 22, %if.then17.i ], [ 14, %if.end20.i ]
  call void @llvm.lifetime.end.p0(i64 8, ptr nonnull %oldval.i)
  ret i32 %ret.0.i
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @arenas_quantum_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i64 8, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %0, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 8, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @arenas_page_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i64 4096, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %0, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 4096, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @arenas_tcache_max_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i64, ptr @tcache_maxclass, align 8
  store i64 %0, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %0, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @arenas_nbins_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i32 39, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %0, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 39, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @arenas_nhbins_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %0 = load i32, ptr @nhbins, align 4
  store i32 %0, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %1 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %1, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %0, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @arenas_nlextents_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #9 {
entry:
  %oldval = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  store i32 196, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %0 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %0, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 196, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %entry, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end9 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @arenas_create_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %arena_ind = alloca i32, align 4
  %config = alloca %struct.arena_config_s, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp eq ptr %oldp, null
  %cmp1 = icmp eq ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %if.then, label %lor.lhs.false2

lor.lhs.false2:                                   ; preds = %malloc_mutex_lock.exit
  %3 = load i64, ptr %oldlenp, align 8
  %cmp3.not = icmp eq i64 %3, 4
  br i1 %cmp3.not, label %do.end, label %if.then

if.then:                                          ; preds = %lor.lhs.false2, %malloc_mutex_lock.exit
  store i64 0, ptr %oldlenp, align 8
  br label %label_return

do.end:                                           ; preds = %lor.lhs.false2
  call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) %config, ptr noundef nonnull align 8 dereferenceable(16) @arena_config_default, i64 16, i1 false)
  %cmp5.not = icmp eq ptr %newp, null
  br i1 %cmp5.not, label %do.end11, label %if.then6

if.then6:                                         ; preds = %do.end
  %cmp7.not = icmp eq i64 %newlen, 8
  br i1 %cmp7.not, label %if.end9, label %label_return

if.end9:                                          ; preds = %if.then6
  %4 = load ptr, ptr %newp, align 8
  store ptr %4, ptr %config, align 8
  br label %do.end11

do.end11:                                         ; preds = %do.end, %if.end9
  %call12 = call fastcc i32 @ctl_arena_init(ptr noundef %tsd, ptr noundef nonnull %config)
  store i32 %call12, ptr %arena_ind, align 4
  %cmp13 = icmp eq i32 %call12, -1
  br i1 %cmp13, label %label_return, label %if.then19

if.then19:                                        ; preds = %do.end11
  %5 = load i64, ptr %oldlenp, align 8
  %cmp20.not = icmp eq i64 %5, 4
  br i1 %cmp20.not, label %if.end23, label %if.then21

if.then21:                                        ; preds = %if.then19
  %spec.select = call i64 @llvm.umin.i64(i64 %5, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %arena_ind, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end23:                                         ; preds = %if.then19
  store i32 %call12, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %do.end11, %if.then6, %if.end23, %if.then21, %if.then
  %ret.0 = phi i32 [ 22, %if.then ], [ 22, %if.then21 ], [ 0, %if.end23 ], [ 22, %if.then6 ], [ 11, %do.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @arenas_lookup_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %rtree_ctx_fallback.i = alloca %struct.rtree_ctx_s, align 8
  %tmp.i = alloca %struct.rtree_contents_s, align 8
  %arena_ind = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i18

if.then.i18:                                      ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i18, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp.not = icmp eq ptr %newp, null
  br i1 %cmp.not, label %do.end, label %if.then

if.then:                                          ; preds = %malloc_mutex_lock.exit
  %cmp1.not = icmp eq i64 %newlen, 8
  br i1 %cmp1.not, label %if.end, label %label_return

if.end:                                           ; preds = %if.then
  %3 = load ptr, ptr %newp, align 8
  %4 = ptrtoint ptr %3 to i64
  br label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit, %if.end
  %ptr.0 = phi i64 [ %4, %if.end ], [ 0, %malloc_mutex_lock.exit ]
  %cmp.i.i = icmp eq ptr %tsd, null
  br i1 %cmp.i.i, label %if.then.i, label %if.end.i.split

if.then.i:                                        ; preds = %do.end
  call void @rtree_ctx_data_init(ptr noundef nonnull %rtree_ctx_fallback.i) #14
  call fastcc void @rtree_read(ptr noalias nonnull align 8 %tmp.i, ptr noundef null, ptr noundef nonnull %rtree_ctx_fallback.i, i64 noundef %ptr.0)
  br label %tsdn_rtree_ctx.exit

if.end.i.split:                                   ; preds = %do.end
  %cant_access_tsd_items_directly_use_a_getter_or_setter_rtree_ctx.i = getelementptr inbounds %struct.tsd_s, ptr %tsd, i64 0, i32 28
  call fastcc void @rtree_read(ptr noalias nonnull align 8 %tmp.i, ptr noundef nonnull %tsd, ptr noundef nonnull %cant_access_tsd_items_directly_use_a_getter_or_setter_rtree_ctx.i, i64 noundef %ptr.0)
  br label %tsdn_rtree_ctx.exit

tsdn_rtree_ctx.exit:                              ; preds = %if.end.i.split, %if.then.i
  %5 = load ptr, ptr %tmp.i, align 8
  %cmp6 = icmp eq ptr %5, null
  br i1 %cmp6, label %label_return, label %if.end8

if.end8:                                          ; preds = %tsdn_rtree_ctx.exit
  %edata.val.i = load i64, ptr %5, align 8
  %conv.i.i = and i64 %edata.val.i, 4095
  %arrayidx.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %conv.i.i
  %6 = load atomic i64, ptr %arrayidx.i monotonic, align 8
  %cmp10 = icmp eq i64 %6, 0
  br i1 %cmp10, label %label_return, label %if.end12

if.end12:                                         ; preds = %if.end8
  %7 = inttoptr i64 %6 to ptr
  %8 = getelementptr i8, ptr %7, i64 78928
  %call9.val = load i32, ptr %8, align 8
  store i32 %call9.val, ptr %arena_ind, align 4
  %cmp15 = icmp ne ptr %oldp, null
  %cmp16 = icmp ne ptr %oldlenp, null
  %or.cond = and i1 %cmp15, %cmp16
  br i1 %or.cond, label %if.then17, label %label_return

if.then17:                                        ; preds = %if.end12
  %9 = load i64, ptr %oldlenp, align 8
  %cmp18.not = icmp eq i64 %9, 4
  br i1 %cmp18.not, label %if.end21, label %if.then19

if.then19:                                        ; preds = %if.then17
  %spec.select = call i64 @llvm.umin.i64(i64 %9, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %arena_ind, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end21:                                         ; preds = %if.then17
  store i32 %call9.val, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end21, %if.end12, %if.then, %if.end8, %tsdn_rtree_ctx.exit, %if.then19
  %ret.0 = phi i32 [ 22, %tsdn_rtree_ctx.exit ], [ 22, %if.end8 ], [ 22, %if.then19 ], [ 22, %if.then ], [ 0, %if.end12 ], [ 0, %if.end21 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

declare i64 @arena_dirty_decay_ms_default_get() local_unnamed_addr #1

declare i64 @arena_muzzy_decay_ms_default_get() local_unnamed_addr #1

declare zeroext i1 @arena_dirty_decay_ms_default_set(i64 noundef) local_unnamed_addr #1

declare zeroext i1 @arena_muzzy_decay_ms_default_set(i64 noundef) local_unnamed_addr #1

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal ptr @arenas_bin_i_index(ptr nocapture readnone %tsdn, ptr nocapture readnone %mib, i64 %miblen, i64 noundef %i) #2 {
entry:
  %cmp = icmp ugt i64 %i, 39
  %.super_arenas_bin_i_node = select i1 %cmp, ptr null, ptr @super_arenas_bin_i_node
  ret ptr %.super_arenas_bin_i_node
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @arenas_bin_i_size_ctl(ptr nocapture readnone %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %0 = load i64, ptr %arrayidx, align 8
  %arrayidx2 = getelementptr inbounds [39 x %struct.bin_info_s], ptr @bin_infos, i64 0, i64 %0
  %1 = load i64, ptr %arrayidx2, align 8
  store i64 %1, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %2, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %2, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %1, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %entry, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end10 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @arenas_bin_i_nregs_ctl(ptr nocapture readnone %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %0 = load i64, ptr %arrayidx, align 8
  %nregs = getelementptr inbounds [39 x %struct.bin_info_s], ptr @bin_infos, i64 0, i64 %0, i32 2
  %1 = load i32, ptr %nregs, align 8
  store i32 %1, ptr %oldval, align 4
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %2, 4
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %2, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i32 %1, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %entry, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end10 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @arenas_bin_i_slab_size_ctl(ptr nocapture readnone %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %0 = load i64, ptr %arrayidx, align 8
  %slab_size = getelementptr inbounds [39 x %struct.bin_info_s], ptr @bin_infos, i64 0, i64 %0, i32 1
  %1 = load i64, ptr %slab_size, align 8
  store i64 %1, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %2, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %2, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %1, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %entry, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end10 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @arenas_bin_i_nshards_ctl(ptr nocapture readnone %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i32, align 4
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %0 = load i64, ptr %arrayidx, align 8
  %n_shards = getelementptr inbounds [39 x %struct.bin_info_s], ptr @bin_infos, i64 0, i64 %0, i32 3
  %1 = load i32, ptr %n_shards, align 4
  store i32 %1, ptr %oldval, align 4
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %2, 4
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %2, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i32 %1, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %entry, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end10 ]
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal ptr @arenas_lextent_i_index(ptr nocapture readnone %tsdn, ptr nocapture readnone %mib, i64 %miblen, i64 noundef %i) #2 {
entry:
  %cmp = icmp ugt i64 %i, 196
  %.super_arenas_lextent_i_node = select i1 %cmp, ptr null, ptr @super_arenas_lextent_i_node
  ret ptr %.super_arenas_lextent_i_node
}

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define internal i32 @arenas_lextent_i_size_ctl(ptr nocapture readnone %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #10 {
entry:
  %oldval = alloca i64, align 8
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %0 = load i64, ptr %arrayidx, align 8
  %conv3 = add i64 %0, 39
  %idxprom.i = and i64 %conv3, 4294967295
  %arrayidx.i = getelementptr inbounds [235 x i64], ptr @sz_index2size_tab, i64 0, i64 %idxprom.i
  %1 = load i64, ptr %arrayidx.i, align 8
  store i64 %1, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp7
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %do.end
  %2 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %2, 8
  br i1 %cmp10.not, label %if.end15, label %if.then12

if.then12:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %2, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end15:                                         ; preds = %if.then9
  store i64 %1, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end15, %do.end, %entry, %if.then12
  %ret.0 = phi i32 [ 22, %if.then12 ], [ 1, %entry ], [ 0, %do.end ], [ 0, %if.end15 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal fastcc i32 @ctl_arena_init(ptr noundef %tsd, ptr noundef %config) unnamed_addr #0 {
entry:
  %0 = load ptr, ptr @ctl_arenas, align 8
  %destroyed = getelementptr inbounds %struct.ctl_arenas_s, ptr %0, i64 0, i32 2
  %1 = load ptr, ptr %destroyed, align 8
  %cmp = icmp eq ptr %1, null
  br i1 %cmp, label %if.else57, label %cond.end

cond.end:                                         ; preds = %entry
  %qre_prev = getelementptr inbounds %struct.ctl_arena_s, ptr %1, i64 0, i32 2, i32 1
  %2 = load ptr, ptr %qre_prev, align 8
  %cmp3.not = icmp eq ptr %2, null
  br i1 %cmp3.not, label %if.else57, label %do.body

do.body:                                          ; preds = %cond.end
  %cmp6 = icmp eq ptr %1, %2
  br i1 %cmp6, label %if.then7, label %if.end

if.then7:                                         ; preds = %do.body
  %destroyed_link10 = getelementptr inbounds %struct.ctl_arena_s, ptr %1, i64 0, i32 2
  %3 = load ptr, ptr %destroyed_link10, align 8
  store ptr %3, ptr %destroyed, align 8
  br label %if.end

if.end:                                           ; preds = %if.then7, %do.body
  %4 = phi ptr [ %3, %if.then7 ], [ %1, %do.body ]
  %cmp15.not = icmp eq ptr %4, %2
  br i1 %cmp15.not, label %do.body50, label %do.body17

do.body17:                                        ; preds = %if.end
  %destroyed_link18 = getelementptr inbounds %struct.ctl_arena_s, ptr %2, i64 0, i32 2
  %5 = load ptr, ptr %destroyed_link18, align 8
  %qre_prev21 = getelementptr inbounds %struct.ctl_arena_s, ptr %5, i64 0, i32 2, i32 1
  %6 = load ptr, ptr %qre_prev21, align 8
  %qre_prev23 = getelementptr inbounds %struct.ctl_arena_s, ptr %2, i64 0, i32 2, i32 1
  %7 = load ptr, ptr %qre_prev23, align 8
  %destroyed_link24 = getelementptr inbounds %struct.ctl_arena_s, ptr %7, i64 0, i32 2
  store ptr %6, ptr %destroyed_link24, align 8
  %8 = load ptr, ptr %qre_prev23, align 8
  %9 = load ptr, ptr %destroyed_link18, align 8
  %qre_prev31 = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 2, i32 1
  store ptr %8, ptr %qre_prev31, align 8
  %destroyed_link34 = getelementptr inbounds %struct.ctl_arena_s, ptr %8, i64 0, i32 2
  %10 = load ptr, ptr %destroyed_link34, align 8
  store ptr %10, ptr %qre_prev23, align 8
  %11 = load ptr, ptr %destroyed_link18, align 8
  %qre_prev43 = getelementptr inbounds %struct.ctl_arena_s, ptr %11, i64 0, i32 2, i32 1
  %12 = load ptr, ptr %qre_prev43, align 8
  %destroyed_link44 = getelementptr inbounds %struct.ctl_arena_s, ptr %12, i64 0, i32 2
  store ptr %11, ptr %destroyed_link44, align 8
  %13 = load ptr, ptr %qre_prev23, align 8
  %destroyed_link48 = getelementptr inbounds %struct.ctl_arena_s, ptr %13, i64 0, i32 2
  store ptr %2, ptr %destroyed_link48, align 8
  br label %if.end58

do.body50:                                        ; preds = %if.end
  store ptr null, ptr %destroyed, align 8
  br label %if.end58

if.else57:                                        ; preds = %entry, %cond.end
  %narenas = getelementptr inbounds %struct.ctl_arenas_s, ptr %0, i64 0, i32 1
  br label %if.end58

if.end58:                                         ; preds = %do.body50, %do.body17, %if.else57
  %arena_ind.0.in = phi ptr [ %narenas, %if.else57 ], [ %2, %do.body17 ], [ %2, %do.body50 ]
  %arena_ind.0 = load i32, ptr %arena_ind.0.in, align 8
  %conv = zext i32 %arena_ind.0 to i64
  switch i32 %arena_ind.0, label %sw.default.i.i [
    i32 4096, label %arenas_i2a_impl.exit.i
    i32 4097, label %sw.bb2.i.i
  ]

sw.bb2.i.i:                                       ; preds = %if.end58
  br label %arenas_i2a_impl.exit.i

sw.default.i.i:                                   ; preds = %if.end58
  %add.i.i = add nuw nsw i64 %conv, 2
  %14 = and i64 %add.i.i, 4294967295
  br label %arenas_i2a_impl.exit.i

arenas_i2a_impl.exit.i:                           ; preds = %sw.default.i.i, %sw.bb2.i.i, %if.end58
  %a.0.i.i = phi i64 [ %14, %sw.default.i.i ], [ 1, %sw.bb2.i.i ], [ 0, %if.end58 ]
  %arrayidx.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %0, i64 0, i32 3, i64 %a.0.i.i
  %15 = load ptr, ptr %arrayidx.i, align 8
  %cmp.i = icmp eq ptr %15, null
  br i1 %cmp.i, label %if.then.i, label %if.end62

if.then.i:                                        ; preds = %arenas_i2a_impl.exit.i
  %call4.i = tail call ptr @b0get() #14
  %call5.i = tail call ptr @base_alloc(ptr noundef %tsd, ptr noundef %call4.i, i64 noundef 38288, i64 noundef 8) #14
  %cmp6.i = icmp eq ptr %call5.i, null
  br i1 %cmp6.i, label %return, label %if.end.i

if.end.i:                                         ; preds = %if.then.i
  %astats.i = getelementptr inbounds %struct.container_s, ptr %call5.i, i64 0, i32 1
  %astats8.i = getelementptr inbounds %struct.ctl_arena_s, ptr %call5.i, i64 0, i32 10
  store ptr %astats.i, ptr %astats8.i, align 8
  store i32 %arena_ind.0, ptr %call5.i, align 8
  %16 = load ptr, ptr @ctl_arenas, align 8
  switch i32 %arena_ind.0, label %sw.default.i12.i [
    i32 4096, label %arenas_i2a_impl.exit20.i
    i32 4097, label %sw.bb2.i10.i
  ]

sw.bb2.i10.i:                                     ; preds = %if.end.i
  br label %arenas_i2a_impl.exit20.i

sw.default.i12.i:                                 ; preds = %if.end.i
  %add.i15.i = add nuw nsw i64 %conv, 2
  %17 = and i64 %add.i15.i, 4294967295
  br label %arenas_i2a_impl.exit20.i

arenas_i2a_impl.exit20.i:                         ; preds = %sw.default.i12.i, %sw.bb2.i10.i, %if.end.i
  %a.0.i11.i = phi i64 [ %17, %sw.default.i12.i ], [ 1, %sw.bb2.i10.i ], [ 0, %if.end.i ]
  %arrayidx13.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %16, i64 0, i32 3, i64 %a.0.i11.i
  store ptr %call5.i, ptr %arrayidx13.i, align 8
  br label %if.end62

if.end62:                                         ; preds = %arenas_i2a_impl.exit20.i, %arenas_i2a_impl.exit.i
  %call64 = tail call ptr @arena_init(ptr noundef %tsd, i32 noundef %arena_ind.0, ptr noundef %config) #14
  %cmp65 = icmp eq ptr %call64, null
  br i1 %cmp65, label %return, label %if.end68

if.end68:                                         ; preds = %if.end62
  %18 = load ptr, ptr @ctl_arenas, align 8
  %narenas69 = getelementptr inbounds %struct.ctl_arenas_s, ptr %18, i64 0, i32 1
  %19 = load i32, ptr %narenas69, align 8
  %cmp70 = icmp eq i32 %arena_ind.0, %19
  br i1 %cmp70, label %if.then72, label %return

if.then72:                                        ; preds = %if.end68
  %inc = add i32 %arena_ind.0, 1
  store i32 %inc, ptr %narenas69, align 8
  br label %return

return:                                           ; preds = %if.then.i, %if.end68, %if.then72, %if.end62
  %retval.0 = phi i32 [ -1, %if.end62 ], [ %arena_ind.0, %if.then72 ], [ %arena_ind.0, %if.end68 ], [ -1, %if.then.i ]
  ret i32 %retval.0
}

; Function Attrs: nounwind uwtable
define internal fastcc void @rtree_read(ptr noalias nocapture writeonly align 8 %agg.result, ptr noundef %tsdn, ptr noundef %rtree_ctx, i64 noundef %key) unnamed_addr #0 {
entry:
  %shr.i = lshr i64 %key, 30
  %and.i = and i64 %shr.i, 15
  %and.i10 = and i64 %key, -1073741824
  %arrayidx.i = getelementptr inbounds [16 x %struct.rtree_ctx_cache_elm_s], ptr %rtree_ctx, i64 0, i64 %and.i
  %0 = load i64, ptr %arrayidx.i, align 8
  %cmp.i = icmp eq i64 %0, %and.i10
  br i1 %cmp.i, label %if.then.i, label %if.end.i

if.then.i:                                        ; preds = %entry
  %leaf11.i = getelementptr inbounds [16 x %struct.rtree_ctx_cache_elm_s], ptr %rtree_ctx, i64 0, i64 %and.i, i32 1
  %1 = load ptr, ptr %leaf11.i, align 8
  %shr.i18 = lshr i64 %key, 12
  %and.i19 = and i64 %shr.i18, 262143
  %arrayidx15.i = getelementptr inbounds %struct.rtree_leaf_elm_s, ptr %1, i64 %and.i19
  br label %monotonic.i.i

if.end.i:                                         ; preds = %entry
  %l2_cache.i = getelementptr inbounds %struct.rtree_ctx_s, ptr %rtree_ctx, i64 0, i32 1
  %2 = load i64, ptr %l2_cache.i, align 8
  %cmp19.i = icmp eq i64 %2, %and.i10
  br i1 %cmp19.i, label %if.then27.i, label %for.body.i

if.then27.i:                                      ; preds = %if.end.i
  %leaf31.i = getelementptr inbounds %struct.rtree_ctx_s, ptr %rtree_ctx, i64 0, i32 1, i64 0, i32 1
  %3 = load ptr, ptr %leaf31.i, align 8
  store i64 %0, ptr %l2_cache.i, align 8
  %leaf42.i = getelementptr inbounds [16 x %struct.rtree_ctx_cache_elm_s], ptr %rtree_ctx, i64 0, i64 %and.i, i32 1
  %4 = load ptr, ptr %leaf42.i, align 8
  store ptr %4, ptr %leaf31.i, align 8
  store i64 %and.i10, ptr %arrayidx.i, align 8
  store ptr %3, ptr %leaf42.i, align 8
  %shr.i37 = lshr i64 %key, 12
  %and.i38 = and i64 %shr.i37, 262143
  %arrayidx54.i = getelementptr inbounds %struct.rtree_leaf_elm_s, ptr %3, i64 %and.i38
  br label %monotonic.i.i

for.body.i:                                       ; preds = %if.end.i, %if.end137.i
  %indvars.iv = phi i64 [ %indvars.iv.next, %if.end137.i ], [ 1, %if.end.i ]
  %arrayidx61.i = getelementptr inbounds %struct.rtree_ctx_s, ptr %rtree_ctx, i64 0, i32 1, i64 %indvars.iv
  %5 = load i64, ptr %arrayidx61.i, align 8
  %cmp63.i = icmp eq i64 %5, %and.i10
  br i1 %cmp63.i, label %if.then71.i, label %if.end137.i

if.then71.i:                                      ; preds = %for.body.i
  %leaf76.i = getelementptr inbounds %struct.rtree_ctx_s, ptr %rtree_ctx, i64 0, i32 1, i64 %indvars.iv, i32 1
  %6 = load ptr, ptr %leaf76.i, align 8
  %sub.i = add nuw i64 %indvars.iv, 4294967295
  %idxprom83.i = and i64 %sub.i, 4294967295
  %arrayidx84.i = getelementptr inbounds %struct.rtree_ctx_s, ptr %rtree_ctx, i64 0, i32 1, i64 %idxprom83.i
  %7 = load i64, ptr %arrayidx84.i, align 8
  store i64 %7, ptr %arrayidx61.i, align 8
  %leaf94.i = getelementptr inbounds %struct.rtree_ctx_s, ptr %rtree_ctx, i64 0, i32 1, i64 %idxprom83.i, i32 1
  %8 = load ptr, ptr %leaf94.i, align 8
  store ptr %8, ptr %leaf76.i, align 8
  store i64 %0, ptr %arrayidx84.i, align 8
  %leaf109.i = getelementptr inbounds [16 x %struct.rtree_ctx_cache_elm_s], ptr %rtree_ctx, i64 0, i64 %and.i, i32 1
  %9 = load ptr, ptr %leaf109.i, align 8
  store ptr %9, ptr %leaf94.i, align 8
  store i64 %and.i10, ptr %arrayidx.i, align 8
  store ptr %6, ptr %leaf109.i, align 8
  %shr.i56 = lshr i64 %key, 12
  %and.i57 = and i64 %shr.i56, 262143
  %arrayidx136.i = getelementptr inbounds %struct.rtree_leaf_elm_s, ptr %6, i64 %and.i57
  br label %monotonic.i.i

if.end137.i:                                      ; preds = %for.body.i
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, 8
  br i1 %exitcond.not, label %for.end.i, label %for.body.i, !llvm.loop !20

for.end.i:                                        ; preds = %if.end137.i
  %call141.i = tail call ptr @rtree_leaf_elm_lookup_hard(ptr noundef %tsdn, ptr noundef nonnull @arena_emap_global, ptr noundef nonnull %rtree_ctx, i64 noundef %key, i1 noundef zeroext true, i1 noundef zeroext false) #14
  br label %monotonic.i.i

monotonic.i.i:                                    ; preds = %if.then.i, %if.then27.i, %if.then71.i, %for.end.i
  %retval.i.0 = phi ptr [ %arrayidx15.i, %if.then.i ], [ %arrayidx54.i, %if.then27.i ], [ %arrayidx136.i, %if.then71.i ], [ %call141.i, %for.end.i ]
  %10 = load atomic i64, ptr %retval.i.0 monotonic, align 8, !noalias !21
  %shr.i69 = lshr i64 %10, 48
  %conv.i70 = trunc i64 %shr.i69 to i32
  %metadata.i = getelementptr inbounds %struct.rtree_contents_s, ptr %agg.result, i64 0, i32 1
  store i32 %conv.i70, ptr %metadata.i, align 8, !alias.scope !24
  %slab.i = getelementptr inbounds %struct.rtree_contents_s, ptr %agg.result, i64 0, i32 1, i32 3
  %11 = trunc i64 %10 to i8
  %frombool.i73 = and i8 %11, 1
  store i8 %frombool.i73, ptr %slab.i, align 1, !alias.scope !24
  %is_head.i = getelementptr inbounds %struct.rtree_contents_s, ptr %agg.result, i64 0, i32 1, i32 2
  %12 = lshr i8 %11, 1
  %frombool5.i = and i8 %12, 1
  store i8 %frombool5.i, ptr %is_head.i, align 8, !alias.scope !24
  %13 = trunc i64 %10 to i32
  %14 = lshr i32 %13, 2
  %conv8.i = and i32 %14, 7
  %state.i = getelementptr inbounds %struct.rtree_contents_s, ptr %agg.result, i64 0, i32 1, i32 1
  store i32 %conv8.i, ptr %state.i, align 4, !alias.scope !24
  %shl.i74 = shl i64 %10, 16
  %shr10.i = ashr exact i64 %shl.i74, 16
  %and11.i = and i64 %shr10.i, -128
  %15 = inttoptr i64 %and11.i to ptr
  store ptr %15, ptr %agg.result, align 8, !alias.scope !24
  ret void
}

declare void @rtree_ctx_data_init(ptr noundef) local_unnamed_addr #1

declare ptr @rtree_leaf_elm_lookup_hard(ptr noundef, ptr noundef, ptr noundef, i64 noundef, i1 noundef zeroext, i1 noundef zeroext) local_unnamed_addr #1

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @prof_thread_active_init_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @prof_active_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @prof_dump_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @prof_gdump_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @prof_prefix_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @prof_reset_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @prof_interval_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @lg_prof_sample_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @prof_log_start_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @prof_log_stop_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal noalias ptr @prof_stats_bins_i_index(ptr nocapture readnone %tsdn, ptr nocapture readnone %mib, i64 %miblen, i64 %i) #2 {
entry:
  ret ptr null
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal noalias ptr @prof_stats_lextents_i_index(ptr nocapture readnone %tsdn, ptr nocapture readnone %mib, i64 %miblen, i64 %i) #2 {
entry:
  ret ptr null
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_allocated_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %4 = load i64, ptr %3, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_active_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %active = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 1
  %4 = load i64, ptr %active, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_metadata_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %metadata = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 2
  %4 = load i64, ptr %metadata, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_metadata_thp_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %metadata_thp = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 3
  %4 = load i64, ptr %metadata_thp, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_resident_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %resident = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 4
  %4 = load i64, ptr %resident, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mapped_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %mapped = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 5
  %4 = load i64, ptr %mapped, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_retained_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %retained = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 6
  %4 = load i64, ptr %retained, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_zero_reallocs_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %monotonic.i

monotonic.i:                                      ; preds = %malloc_mutex_lock.exit
  %3 = load atomic i64, ptr @zero_realloc_count monotonic, align 8
  store i64 %3, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %monotonic.i
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %3, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %monotonic.i, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %monotonic.i ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_background_thread_num_threads_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %background_thread = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 7
  %4 = load i64, ptr %background_thread, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_background_thread_num_runs_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %num_runs = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 7, i32 1
  %4 = load i64, ptr %num_runs, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_background_thread_run_interval_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %run_interval = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 7, i32 2
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %run_interval) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

declare i64 @nstime_ns(ptr noundef) local_unnamed_addr #1

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_reset_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #0 {
entry:
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  tail call void @malloc_mutex_prof_data_reset(ptr noundef %tsd, ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %call.i.i74 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i75 = icmp eq i32 %call.i.i74, 0
  br i1 %cmp.i.not.i75, label %if.end.i77, label %if.then.i76

if.then.i76:                                      ; preds = %malloc_mutex_lock.exit
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @background_thread_lock) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i77

if.end.i77:                                       ; preds = %if.then.i76, %malloc_mutex_lock.exit
  %3 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i78 = add i64 %3, 1
  store i64 %inc.i.i78, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %4 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i79 = icmp eq ptr %4, %tsd
  br i1 %cmp.not.i.i79, label %malloc_mutex_lock.exit82, label %if.then.i.i80

if.then.i.i80:                                    ; preds = %if.end.i77
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %5 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i81 = add i64 %5, 1
  store i64 %inc2.i.i81, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit82

malloc_mutex_lock.exit82:                         ; preds = %if.end.i77, %if.then.i.i80
  tail call void @malloc_mutex_prof_data_reset(ptr noundef %tsd, ptr noundef nonnull @background_thread_lock) #14
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i83 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @background_thread_lock, i64 0, i32 0, i32 0, i32 1)) #14
  %call1 = tail call i32 @narenas_total_get() #14
  %cmp254.not = icmp eq i32 %call1, 0
  br i1 %cmp254.not, label %for.end88, label %for.body.preheader

for.body.preheader:                               ; preds = %malloc_mutex_lock.exit82
  %wide.trip.count = zext i32 %call1 to i64
  br label %for.body

for.body:                                         ; preds = %for.body.preheader, %for.inc86
  %indvars.iv260 = phi i64 [ 0, %for.body.preheader ], [ %indvars.iv.next261, %for.inc86 ]
  %arrayidx.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %indvars.iv260
  %6 = load atomic i64, ptr %arrayidx.i acquire, align 8
  %7 = inttoptr i64 %6 to ptr
  %tobool.not = icmp eq i64 %6, 0
  br i1 %tobool.not, label %for.inc86, label %if.end

if.end:                                           ; preds = %for.body
  %large_mtx = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 9
  %lock.i.i = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 9, i32 0, i32 0, i32 1
  %call.i.i84 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull %lock.i.i) #14
  %cmp.i.not.i85 = icmp eq i32 %call.i.i84, 0
  br i1 %cmp.i.not.i85, label %if.end.i87, label %if.then.i86

if.then.i86:                                      ; preds = %if.end
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull %large_mtx) #14
  %locked.i = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 9, i32 0, i32 0, i32 2
  store atomic i8 1, ptr %locked.i monotonic, align 1
  br label %if.end.i87

if.end.i87:                                       ; preds = %if.then.i86, %if.end
  %n_lock_ops.i.i = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 9, i32 0, i32 0, i32 0, i32 8
  %8 = load i64, ptr %n_lock_ops.i.i, align 8
  %inc.i.i88 = add i64 %8, 1
  store i64 %inc.i.i88, ptr %n_lock_ops.i.i, align 8
  %prev_owner.i.i = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 9, i32 0, i32 0, i32 0, i32 7
  %9 = load ptr, ptr %prev_owner.i.i, align 8
  %cmp.not.i.i89 = icmp eq ptr %9, %tsd
  br i1 %cmp.not.i.i89, label %malloc_mutex_lock.exit92, label %if.then.i.i90

if.then.i.i90:                                    ; preds = %if.end.i87
  store ptr %tsd, ptr %prev_owner.i.i, align 8
  %n_owner_switches.i.i = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 9, i32 0, i32 0, i32 0, i32 6
  %10 = load i64, ptr %n_owner_switches.i.i, align 8
  %inc2.i.i91 = add i64 %10, 1
  store i64 %inc2.i.i91, ptr %n_owner_switches.i.i, align 8
  br label %malloc_mutex_lock.exit92

malloc_mutex_lock.exit92:                         ; preds = %if.end.i87, %if.then.i.i90
  tail call void @malloc_mutex_prof_data_reset(ptr noundef %tsd, ptr noundef nonnull %large_mtx) #14
  %locked.i93 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 9, i32 0, i32 0, i32 2
  store atomic i8 0, ptr %locked.i93 monotonic, align 1
  %call1.i94 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull %lock.i.i) #14
  %mtx = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 7, i32 2
  %lock.i.i95 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 7, i32 2, i32 0, i32 0, i32 1
  %call.i.i96 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull %lock.i.i95) #14
  %cmp.i.not.i97 = icmp eq i32 %call.i.i96, 0
  br i1 %cmp.i.not.i97, label %if.end.i100, label %if.then.i98

if.then.i98:                                      ; preds = %malloc_mutex_lock.exit92
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull %mtx) #14
  %locked.i99 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 7, i32 2, i32 0, i32 0, i32 2
  store atomic i8 1, ptr %locked.i99 monotonic, align 1
  br label %if.end.i100

if.end.i100:                                      ; preds = %if.then.i98, %malloc_mutex_lock.exit92
  %n_lock_ops.i.i101 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 7, i32 2, i32 0, i32 0, i32 0, i32 8
  %11 = load i64, ptr %n_lock_ops.i.i101, align 8
  %inc.i.i102 = add i64 %11, 1
  store i64 %inc.i.i102, ptr %n_lock_ops.i.i101, align 8
  %prev_owner.i.i103 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 7, i32 2, i32 0, i32 0, i32 0, i32 7
  %12 = load ptr, ptr %prev_owner.i.i103, align 8
  %cmp.not.i.i104 = icmp eq ptr %12, %tsd
  br i1 %cmp.not.i.i104, label %malloc_mutex_lock.exit108, label %if.then.i.i105

if.then.i.i105:                                   ; preds = %if.end.i100
  store ptr %tsd, ptr %prev_owner.i.i103, align 8
  %n_owner_switches.i.i106 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 7, i32 2, i32 0, i32 0, i32 0, i32 6
  %13 = load i64, ptr %n_owner_switches.i.i106, align 8
  %inc2.i.i107 = add i64 %13, 1
  store i64 %inc2.i.i107, ptr %n_owner_switches.i.i106, align 8
  br label %malloc_mutex_lock.exit108

malloc_mutex_lock.exit108:                        ; preds = %if.end.i100, %if.then.i.i105
  tail call void @malloc_mutex_prof_data_reset(ptr noundef %tsd, ptr noundef nonnull %mtx) #14
  %locked.i109 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 7, i32 2, i32 0, i32 0, i32 2
  store atomic i8 0, ptr %locked.i109 monotonic, align 1
  %call1.i111 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull %lock.i.i95) #14
  %ecache_dirty = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 1
  %lock.i.i112 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 1, i32 0, i32 0, i32 0, i32 1
  %call.i.i113 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull %lock.i.i112) #14
  %cmp.i.not.i114 = icmp eq i32 %call.i.i113, 0
  br i1 %cmp.i.not.i114, label %if.end.i117, label %if.then.i115

if.then.i115:                                     ; preds = %malloc_mutex_lock.exit108
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull %ecache_dirty) #14
  %locked.i116 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 1, i32 0, i32 0, i32 0, i32 2
  store atomic i8 1, ptr %locked.i116 monotonic, align 1
  br label %if.end.i117

if.end.i117:                                      ; preds = %if.then.i115, %malloc_mutex_lock.exit108
  %n_lock_ops.i.i118 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 1, i32 0, i32 0, i32 0, i32 0, i32 8
  %14 = load i64, ptr %n_lock_ops.i.i118, align 8
  %inc.i.i119 = add i64 %14, 1
  store i64 %inc.i.i119, ptr %n_lock_ops.i.i118, align 8
  %prev_owner.i.i120 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 1, i32 0, i32 0, i32 0, i32 0, i32 7
  %15 = load ptr, ptr %prev_owner.i.i120, align 8
  %cmp.not.i.i121 = icmp eq ptr %15, %tsd
  br i1 %cmp.not.i.i121, label %malloc_mutex_lock.exit125, label %if.then.i.i122

if.then.i.i122:                                   ; preds = %if.end.i117
  store ptr %tsd, ptr %prev_owner.i.i120, align 8
  %n_owner_switches.i.i123 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 1, i32 0, i32 0, i32 0, i32 0, i32 6
  %16 = load i64, ptr %n_owner_switches.i.i123, align 8
  %inc2.i.i124 = add i64 %16, 1
  store i64 %inc2.i.i124, ptr %n_owner_switches.i.i123, align 8
  br label %malloc_mutex_lock.exit125

malloc_mutex_lock.exit125:                        ; preds = %if.end.i117, %if.then.i.i122
  tail call void @malloc_mutex_prof_data_reset(ptr noundef %tsd, ptr noundef nonnull %ecache_dirty) #14
  %locked.i126 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 1, i32 0, i32 0, i32 0, i32 2
  store atomic i8 0, ptr %locked.i126 monotonic, align 1
  %call1.i128 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull %lock.i.i112) #14
  %ecache_muzzy = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 2
  %lock.i.i129 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 2, i32 0, i32 0, i32 0, i32 1
  %call.i.i130 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull %lock.i.i129) #14
  %cmp.i.not.i131 = icmp eq i32 %call.i.i130, 0
  br i1 %cmp.i.not.i131, label %if.end.i134, label %if.then.i132

if.then.i132:                                     ; preds = %malloc_mutex_lock.exit125
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull %ecache_muzzy) #14
  %locked.i133 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 2, i32 0, i32 0, i32 0, i32 2
  store atomic i8 1, ptr %locked.i133 monotonic, align 1
  br label %if.end.i134

if.end.i134:                                      ; preds = %if.then.i132, %malloc_mutex_lock.exit125
  %n_lock_ops.i.i135 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 2, i32 0, i32 0, i32 0, i32 0, i32 8
  %17 = load i64, ptr %n_lock_ops.i.i135, align 8
  %inc.i.i136 = add i64 %17, 1
  store i64 %inc.i.i136, ptr %n_lock_ops.i.i135, align 8
  %prev_owner.i.i137 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 2, i32 0, i32 0, i32 0, i32 0, i32 7
  %18 = load ptr, ptr %prev_owner.i.i137, align 8
  %cmp.not.i.i138 = icmp eq ptr %18, %tsd
  br i1 %cmp.not.i.i138, label %malloc_mutex_lock.exit142, label %if.then.i.i139

if.then.i.i139:                                   ; preds = %if.end.i134
  store ptr %tsd, ptr %prev_owner.i.i137, align 8
  %n_owner_switches.i.i140 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 2, i32 0, i32 0, i32 0, i32 0, i32 6
  %19 = load i64, ptr %n_owner_switches.i.i140, align 8
  %inc2.i.i141 = add i64 %19, 1
  store i64 %inc2.i.i141, ptr %n_owner_switches.i.i140, align 8
  br label %malloc_mutex_lock.exit142

malloc_mutex_lock.exit142:                        ; preds = %if.end.i134, %if.then.i.i139
  tail call void @malloc_mutex_prof_data_reset(ptr noundef %tsd, ptr noundef nonnull %ecache_muzzy) #14
  %locked.i143 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 2, i32 0, i32 0, i32 0, i32 2
  store atomic i8 0, ptr %locked.i143 monotonic, align 1
  %call1.i145 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull %lock.i.i129) #14
  %ecache_retained = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 3
  %lock.i.i146 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 3, i32 0, i32 0, i32 0, i32 1
  %call.i.i147 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull %lock.i.i146) #14
  %cmp.i.not.i148 = icmp eq i32 %call.i.i147, 0
  br i1 %cmp.i.not.i148, label %if.end.i151, label %if.then.i149

if.then.i149:                                     ; preds = %malloc_mutex_lock.exit142
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull %ecache_retained) #14
  %locked.i150 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 3, i32 0, i32 0, i32 0, i32 2
  store atomic i8 1, ptr %locked.i150 monotonic, align 1
  br label %if.end.i151

if.end.i151:                                      ; preds = %if.then.i149, %malloc_mutex_lock.exit142
  %n_lock_ops.i.i152 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 3, i32 0, i32 0, i32 0, i32 0, i32 8
  %20 = load i64, ptr %n_lock_ops.i.i152, align 8
  %inc.i.i153 = add i64 %20, 1
  store i64 %inc.i.i153, ptr %n_lock_ops.i.i152, align 8
  %prev_owner.i.i154 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 3, i32 0, i32 0, i32 0, i32 0, i32 7
  %21 = load ptr, ptr %prev_owner.i.i154, align 8
  %cmp.not.i.i155 = icmp eq ptr %21, %tsd
  br i1 %cmp.not.i.i155, label %malloc_mutex_lock.exit159, label %if.then.i.i156

if.then.i.i156:                                   ; preds = %if.end.i151
  store ptr %tsd, ptr %prev_owner.i.i154, align 8
  %n_owner_switches.i.i157 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 3, i32 0, i32 0, i32 0, i32 0, i32 6
  %22 = load i64, ptr %n_owner_switches.i.i157, align 8
  %inc2.i.i158 = add i64 %22, 1
  store i64 %inc2.i.i158, ptr %n_owner_switches.i.i157, align 8
  br label %malloc_mutex_lock.exit159

malloc_mutex_lock.exit159:                        ; preds = %if.end.i151, %if.then.i.i156
  tail call void @malloc_mutex_prof_data_reset(ptr noundef %tsd, ptr noundef nonnull %ecache_retained) #14
  %locked.i160 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 3, i32 0, i32 0, i32 0, i32 2
  store atomic i8 0, ptr %locked.i160 monotonic, align 1
  %call1.i162 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull %lock.i.i146) #14
  %decay_dirty = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 11
  %lock.i.i163 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 11, i32 0, i32 0, i32 0, i32 1
  %call.i.i164 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull %lock.i.i163) #14
  %cmp.i.not.i165 = icmp eq i32 %call.i.i164, 0
  br i1 %cmp.i.not.i165, label %if.end.i168, label %if.then.i166

if.then.i166:                                     ; preds = %malloc_mutex_lock.exit159
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull %decay_dirty) #14
  %locked.i167 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 11, i32 0, i32 0, i32 0, i32 2
  store atomic i8 1, ptr %locked.i167 monotonic, align 1
  br label %if.end.i168

if.end.i168:                                      ; preds = %if.then.i166, %malloc_mutex_lock.exit159
  %n_lock_ops.i.i169 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 11, i32 0, i32 0, i32 0, i32 0, i32 8
  %23 = load i64, ptr %n_lock_ops.i.i169, align 8
  %inc.i.i170 = add i64 %23, 1
  store i64 %inc.i.i170, ptr %n_lock_ops.i.i169, align 8
  %prev_owner.i.i171 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 11, i32 0, i32 0, i32 0, i32 0, i32 7
  %24 = load ptr, ptr %prev_owner.i.i171, align 8
  %cmp.not.i.i172 = icmp eq ptr %24, %tsd
  br i1 %cmp.not.i.i172, label %malloc_mutex_lock.exit176, label %if.then.i.i173

if.then.i.i173:                                   ; preds = %if.end.i168
  store ptr %tsd, ptr %prev_owner.i.i171, align 8
  %n_owner_switches.i.i174 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 11, i32 0, i32 0, i32 0, i32 0, i32 6
  %25 = load i64, ptr %n_owner_switches.i.i174, align 8
  %inc2.i.i175 = add i64 %25, 1
  store i64 %inc2.i.i175, ptr %n_owner_switches.i.i174, align 8
  br label %malloc_mutex_lock.exit176

malloc_mutex_lock.exit176:                        ; preds = %if.end.i168, %if.then.i.i173
  tail call void @malloc_mutex_prof_data_reset(ptr noundef %tsd, ptr noundef nonnull %decay_dirty) #14
  %locked.i177 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 11, i32 0, i32 0, i32 0, i32 2
  store atomic i8 0, ptr %locked.i177 monotonic, align 1
  %call1.i179 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull %lock.i.i163) #14
  %decay_muzzy = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 12
  %lock.i.i180 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 12, i32 0, i32 0, i32 0, i32 1
  %call.i.i181 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull %lock.i.i180) #14
  %cmp.i.not.i182 = icmp eq i32 %call.i.i181, 0
  br i1 %cmp.i.not.i182, label %if.end.i185, label %if.then.i183

if.then.i183:                                     ; preds = %malloc_mutex_lock.exit176
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull %decay_muzzy) #14
  %locked.i184 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 12, i32 0, i32 0, i32 0, i32 2
  store atomic i8 1, ptr %locked.i184 monotonic, align 1
  br label %if.end.i185

if.end.i185:                                      ; preds = %if.then.i183, %malloc_mutex_lock.exit176
  %n_lock_ops.i.i186 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 12, i32 0, i32 0, i32 0, i32 0, i32 8
  %26 = load i64, ptr %n_lock_ops.i.i186, align 8
  %inc.i.i187 = add i64 %26, 1
  store i64 %inc.i.i187, ptr %n_lock_ops.i.i186, align 8
  %prev_owner.i.i188 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 12, i32 0, i32 0, i32 0, i32 0, i32 7
  %27 = load ptr, ptr %prev_owner.i.i188, align 8
  %cmp.not.i.i189 = icmp eq ptr %27, %tsd
  br i1 %cmp.not.i.i189, label %malloc_mutex_lock.exit193, label %if.then.i.i190

if.then.i.i190:                                   ; preds = %if.end.i185
  store ptr %tsd, ptr %prev_owner.i.i188, align 8
  %n_owner_switches.i.i191 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 12, i32 0, i32 0, i32 0, i32 0, i32 6
  %28 = load i64, ptr %n_owner_switches.i.i191, align 8
  %inc2.i.i192 = add i64 %28, 1
  store i64 %inc2.i.i192, ptr %n_owner_switches.i.i191, align 8
  br label %malloc_mutex_lock.exit193

malloc_mutex_lock.exit193:                        ; preds = %if.end.i185, %if.then.i.i190
  tail call void @malloc_mutex_prof_data_reset(ptr noundef %tsd, ptr noundef nonnull %decay_muzzy) #14
  %locked.i194 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 10, i32 4, i32 12, i32 0, i32 0, i32 0, i32 2
  store atomic i8 0, ptr %locked.i194 monotonic, align 1
  %call1.i196 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull %lock.i.i180) #14
  %tcache_ql_mtx = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 6
  %lock.i.i197 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 6, i32 0, i32 0, i32 1
  %call.i.i198 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull %lock.i.i197) #14
  %cmp.i.not.i199 = icmp eq i32 %call.i.i198, 0
  br i1 %cmp.i.not.i199, label %if.end.i202, label %if.then.i200

if.then.i200:                                     ; preds = %malloc_mutex_lock.exit193
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull %tcache_ql_mtx) #14
  %locked.i201 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 6, i32 0, i32 0, i32 2
  store atomic i8 1, ptr %locked.i201 monotonic, align 1
  br label %if.end.i202

if.end.i202:                                      ; preds = %if.then.i200, %malloc_mutex_lock.exit193
  %n_lock_ops.i.i203 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 6, i32 0, i32 0, i32 0, i32 8
  %29 = load i64, ptr %n_lock_ops.i.i203, align 8
  %inc.i.i204 = add i64 %29, 1
  store i64 %inc.i.i204, ptr %n_lock_ops.i.i203, align 8
  %prev_owner.i.i205 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 6, i32 0, i32 0, i32 0, i32 7
  %30 = load ptr, ptr %prev_owner.i.i205, align 8
  %cmp.not.i.i206 = icmp eq ptr %30, %tsd
  br i1 %cmp.not.i.i206, label %malloc_mutex_lock.exit210, label %if.then.i.i207

if.then.i.i207:                                   ; preds = %if.end.i202
  store ptr %tsd, ptr %prev_owner.i.i205, align 8
  %n_owner_switches.i.i208 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 6, i32 0, i32 0, i32 0, i32 6
  %31 = load i64, ptr %n_owner_switches.i.i208, align 8
  %inc2.i.i209 = add i64 %31, 1
  store i64 %inc2.i.i209, ptr %n_owner_switches.i.i208, align 8
  br label %malloc_mutex_lock.exit210

malloc_mutex_lock.exit210:                        ; preds = %if.end.i202, %if.then.i.i207
  tail call void @malloc_mutex_prof_data_reset(ptr noundef %tsd, ptr noundef nonnull %tcache_ql_mtx) #14
  %locked.i211 = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 6, i32 0, i32 0, i32 2
  store atomic i8 0, ptr %locked.i211 monotonic, align 1
  %call1.i213 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull %lock.i.i197) #14
  %base = getelementptr inbounds %struct.arena_s, ptr %7, i64 0, i32 12
  %32 = load ptr, ptr %base, align 8
  %lock.i.i214 = getelementptr inbounds %struct.base_s, ptr %32, i64 0, i32 2, i32 0, i32 0, i32 1
  %call.i.i215 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull %lock.i.i214) #14
  %cmp.i.not.i216 = icmp eq i32 %call.i.i215, 0
  br i1 %cmp.i.not.i216, label %if.end.i219, label %if.then.i217

if.then.i217:                                     ; preds = %malloc_mutex_lock.exit210
  %mtx67 = getelementptr inbounds %struct.base_s, ptr %32, i64 0, i32 2
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull %mtx67) #14
  %locked.i218 = getelementptr inbounds %struct.base_s, ptr %32, i64 0, i32 2, i32 0, i32 0, i32 2
  store atomic i8 1, ptr %locked.i218 monotonic, align 1
  br label %if.end.i219

if.end.i219:                                      ; preds = %if.then.i217, %malloc_mutex_lock.exit210
  %n_lock_ops.i.i220 = getelementptr inbounds %struct.base_s, ptr %32, i64 0, i32 2, i32 0, i32 0, i32 0, i32 8
  %33 = load i64, ptr %n_lock_ops.i.i220, align 8
  %inc.i.i221 = add i64 %33, 1
  store i64 %inc.i.i221, ptr %n_lock_ops.i.i220, align 8
  %prev_owner.i.i222 = getelementptr inbounds %struct.base_s, ptr %32, i64 0, i32 2, i32 0, i32 0, i32 0, i32 7
  %34 = load ptr, ptr %prev_owner.i.i222, align 8
  %cmp.not.i.i223 = icmp eq ptr %34, %tsd
  br i1 %cmp.not.i.i223, label %malloc_mutex_lock.exit227, label %if.then.i.i224

if.then.i.i224:                                   ; preds = %if.end.i219
  store ptr %tsd, ptr %prev_owner.i.i222, align 8
  %n_owner_switches.i.i225 = getelementptr inbounds %struct.base_s, ptr %32, i64 0, i32 2, i32 0, i32 0, i32 0, i32 6
  %35 = load i64, ptr %n_owner_switches.i.i225, align 8
  %inc2.i.i226 = add i64 %35, 1
  store i64 %inc2.i.i226, ptr %n_owner_switches.i.i225, align 8
  br label %malloc_mutex_lock.exit227

malloc_mutex_lock.exit227:                        ; preds = %if.end.i219, %if.then.i.i224
  %36 = load ptr, ptr %base, align 8
  %mtx69 = getelementptr inbounds %struct.base_s, ptr %36, i64 0, i32 2
  tail call void @malloc_mutex_prof_data_reset(ptr noundef %tsd, ptr noundef nonnull %mtx69) #14
  %37 = load ptr, ptr %base, align 8
  %locked.i228 = getelementptr inbounds %struct.base_s, ptr %37, i64 0, i32 2, i32 0, i32 0, i32 2
  store atomic i8 0, ptr %locked.i228 monotonic, align 1
  %lock.i229 = getelementptr inbounds %struct.base_s, ptr %37, i64 0, i32 2, i32 0, i32 0, i32 1
  %call1.i230 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull %lock.i229) #14
  br label %for.cond76.preheader

for.cond76.preheader:                             ; preds = %malloc_mutex_lock.exit227, %for.inc83
  %indvars.iv257 = phi i64 [ 0, %malloc_mutex_lock.exit227 ], [ %indvars.iv.next258, %for.inc83 ]
  %n_shards = getelementptr inbounds [39 x %struct.bin_info_s], ptr @bin_infos, i64 0, i64 %indvars.iv257, i32 3
  %38 = load i32, ptr %n_shards, align 4
  %cmp77250.not = icmp eq i32 %38, 0
  br i1 %cmp77250.not, label %for.inc83, label %for.body79.lr.ph

for.body79.lr.ph:                                 ; preds = %for.cond76.preheader
  %arrayidx.i232 = getelementptr inbounds [39 x i32], ptr @arena_bin_offsets, i64 0, i64 %indvars.iv257
  br label %for.body79

for.body79:                                       ; preds = %for.body79.lr.ph, %malloc_mutex_lock.exit246
  %indvars.iv = phi i64 [ 0, %for.body79.lr.ph ], [ %indvars.iv.next, %malloc_mutex_lock.exit246 ]
  %39 = load i32, ptr %arrayidx.i232, align 4
  %conv.i = zext i32 %39 to i64
  %add.i = add i64 %6, %conv.i
  %40 = inttoptr i64 %add.i to ptr
  %add.ptr.i = getelementptr inbounds %struct.bin_s, ptr %40, i64 %indvars.iv
  %lock.i.i233 = getelementptr inbounds %struct.anon, ptr %add.ptr.i, i64 0, i32 1
  %call.i.i234 = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull %lock.i.i233) #14
  %cmp.i.not.i235 = icmp eq i32 %call.i.i234, 0
  br i1 %cmp.i.not.i235, label %if.end.i238, label %if.then.i236

if.then.i236:                                     ; preds = %for.body79
  tail call void @malloc_mutex_lock_slow(ptr noundef %add.ptr.i) #14
  %locked.i237 = getelementptr inbounds %struct.anon, ptr %add.ptr.i, i64 0, i32 2
  store atomic i8 1, ptr %locked.i237 monotonic, align 1
  br label %if.end.i238

if.end.i238:                                      ; preds = %if.then.i236, %for.body79
  %n_lock_ops.i.i239 = getelementptr inbounds %struct.mutex_prof_data_t, ptr %add.ptr.i, i64 0, i32 8
  %41 = load i64, ptr %n_lock_ops.i.i239, align 8
  %inc.i.i240 = add i64 %41, 1
  store i64 %inc.i.i240, ptr %n_lock_ops.i.i239, align 8
  %prev_owner.i.i241 = getelementptr inbounds %struct.mutex_prof_data_t, ptr %add.ptr.i, i64 0, i32 7
  %42 = load ptr, ptr %prev_owner.i.i241, align 8
  %cmp.not.i.i242 = icmp eq ptr %42, %tsd
  br i1 %cmp.not.i.i242, label %malloc_mutex_lock.exit246, label %if.then.i.i243

if.then.i.i243:                                   ; preds = %if.end.i238
  store ptr %tsd, ptr %prev_owner.i.i241, align 8
  %n_owner_switches.i.i244 = getelementptr inbounds %struct.mutex_prof_data_t, ptr %add.ptr.i, i64 0, i32 6
  %43 = load i64, ptr %n_owner_switches.i.i244, align 8
  %inc2.i.i245 = add i64 %43, 1
  store i64 %inc2.i.i245, ptr %n_owner_switches.i.i244, align 8
  br label %malloc_mutex_lock.exit246

malloc_mutex_lock.exit246:                        ; preds = %if.end.i238, %if.then.i.i243
  tail call void @malloc_mutex_prof_data_reset(ptr noundef %tsd, ptr noundef nonnull %add.ptr.i) #14
  %locked.i247 = getelementptr inbounds %struct.anon, ptr %add.ptr.i, i64 0, i32 2
  store atomic i8 0, ptr %locked.i247 monotonic, align 1
  %call1.i249 = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull %lock.i.i233) #14
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %44 = load i32, ptr %n_shards, align 4
  %45 = zext i32 %44 to i64
  %cmp77 = icmp ult i64 %indvars.iv.next, %45
  br i1 %cmp77, label %for.body79, label %for.inc83, !llvm.loop !27

for.inc83:                                        ; preds = %malloc_mutex_lock.exit246, %for.cond76.preheader
  %indvars.iv.next258 = add nuw nsw i64 %indvars.iv257, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next258, 39
  br i1 %exitcond.not, label %for.inc86, label %for.cond76.preheader, !llvm.loop !28

for.inc86:                                        ; preds = %for.inc83, %for.body
  %indvars.iv.next261 = add nuw nsw i64 %indvars.iv260, 1
  %exitcond263.not = icmp eq i64 %indvars.iv.next261, %wide.trip.count
  br i1 %exitcond263.not, label %for.end88, label %for.body, !llvm.loop !29

for.end88:                                        ; preds = %for.inc86, %malloc_mutex_lock.exit82
  ret i32 0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_background_thread_num_ops_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_lock_ops = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 0, i32 8
  %4 = load i64, ptr %n_lock_ops, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_background_thread_num_wait_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_wait_times = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 0, i32 2
  %4 = load i64, ptr %n_wait_times, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_background_thread_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_spin_acquired = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 0, i32 3
  %4 = load i64, ptr %n_spin_acquired, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_background_thread_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_owner_switches = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 0, i32 6
  %4 = load i64, ptr %n_owner_switches, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_background_thread_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %mutex_prof_data = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %mutex_prof_data) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_background_thread_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_wait_time = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 0, i32 1
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_background_thread_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_n_thds = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 0, i32 4
  %4 = load i32, ptr %max_n_thds, align 8
  store i32 %4, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %4, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_max_per_bg_thd_num_ops_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_lock_ops = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 1, i32 8
  %4 = load i64, ptr %n_lock_ops, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_max_per_bg_thd_num_wait_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_wait_times = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 1, i32 2
  %4 = load i64, ptr %n_wait_times, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_max_per_bg_thd_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_spin_acquired = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 1, i32 3
  %4 = load i64, ptr %n_spin_acquired, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_max_per_bg_thd_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_owner_switches = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 1, i32 6
  %4 = load i64, ptr %n_owner_switches, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_max_per_bg_thd_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %arrayidx = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 1
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_max_per_bg_thd_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_wait_time = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 1, i32 1
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_max_per_bg_thd_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_n_thds = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 1, i32 4
  %4 = load i32, ptr %max_n_thds, align 8
  store i32 %4, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %4, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_ctl_num_ops_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_lock_ops = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 2, i32 8
  %4 = load i64, ptr %n_lock_ops, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_ctl_num_wait_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_wait_times = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 2, i32 2
  %4 = load i64, ptr %n_wait_times, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_ctl_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_spin_acquired = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 2, i32 3
  %4 = load i64, ptr %n_spin_acquired, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_ctl_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_owner_switches = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 2, i32 6
  %4 = load i64, ptr %n_owner_switches, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_ctl_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %arrayidx = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 2
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_ctl_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_wait_time = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 2, i32 1
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_ctl_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_n_thds = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 2, i32 4
  %4 = load i32, ptr %max_n_thds, align 8
  store i32 %4, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %4, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_num_ops_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_lock_ops = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 3, i32 8
  %4 = load i64, ptr %n_lock_ops, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_num_wait_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_wait_times = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 3, i32 2
  %4 = load i64, ptr %n_wait_times, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_spin_acquired = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 3, i32 3
  %4 = load i64, ptr %n_spin_acquired, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_owner_switches = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 3, i32 6
  %4 = load i64, ptr %n_owner_switches, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %arrayidx = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 3
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_wait_time = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 3, i32 1
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_n_thds = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 3, i32 4
  %4 = load i32, ptr %max_n_thds, align 8
  store i32 %4, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %4, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_thds_data_num_ops_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_lock_ops = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 4, i32 8
  %4 = load i64, ptr %n_lock_ops, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_thds_data_num_wait_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_wait_times = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 4, i32 2
  %4 = load i64, ptr %n_wait_times, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_thds_data_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_spin_acquired = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 4, i32 3
  %4 = load i64, ptr %n_spin_acquired, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_thds_data_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_owner_switches = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 4, i32 6
  %4 = load i64, ptr %n_owner_switches, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_thds_data_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %arrayidx = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 4
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_thds_data_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_wait_time = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 4, i32 1
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_thds_data_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_n_thds = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 4, i32 4
  %4 = load i32, ptr %max_n_thds, align 8
  store i32 %4, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %4, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_dump_num_ops_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_lock_ops = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 5, i32 8
  %4 = load i64, ptr %n_lock_ops, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_dump_num_wait_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_wait_times = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 5, i32 2
  %4 = load i64, ptr %n_wait_times, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_dump_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_spin_acquired = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 5, i32 3
  %4 = load i64, ptr %n_spin_acquired, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_dump_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_owner_switches = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 5, i32 6
  %4 = load i64, ptr %n_owner_switches, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_dump_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %arrayidx = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 5
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_dump_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_wait_time = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 5, i32 1
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_dump_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_n_thds = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 5, i32 4
  %4 = load i32, ptr %max_n_thds, align 8
  store i32 %4, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %4, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_alloc_num_ops_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_lock_ops = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 6, i32 8
  %4 = load i64, ptr %n_lock_ops, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_alloc_num_wait_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_wait_times = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 6, i32 2
  %4 = load i64, ptr %n_wait_times, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_alloc_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_spin_acquired = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 6, i32 3
  %4 = load i64, ptr %n_spin_acquired, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_alloc_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_owner_switches = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 6, i32 6
  %4 = load i64, ptr %n_owner_switches, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_alloc_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %arrayidx = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 6
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_alloc_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_wait_time = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 6, i32 1
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_alloc_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_n_thds = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 6, i32 4
  %4 = load i32, ptr %max_n_thds, align 8
  store i32 %4, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %4, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_dump_num_ops_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_lock_ops = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 7, i32 8
  %4 = load i64, ptr %n_lock_ops, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_dump_num_wait_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_wait_times = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 7, i32 2
  %4 = load i64, ptr %n_wait_times, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_dump_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_spin_acquired = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 7, i32 3
  %4 = load i64, ptr %n_spin_acquired, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_dump_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_owner_switches = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 7, i32 6
  %4 = load i64, ptr %n_owner_switches, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_dump_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %arrayidx = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 7
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_dump_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_wait_time = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 7, i32 1
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_recent_dump_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_n_thds = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 7, i32 4
  %4 = load i32, ptr %max_n_thds, align 8
  store i32 %4, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %4, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_stats_num_ops_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_lock_ops = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 8, i32 8
  %4 = load i64, ptr %n_lock_ops, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_stats_num_wait_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_wait_times = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 8, i32 2
  %4 = load i64, ptr %n_wait_times, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_stats_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_spin_acquired = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 8, i32 3
  %4 = load i64, ptr %n_spin_acquired, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_stats_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %n_owner_switches = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 8, i32 6
  %4 = load i64, ptr %n_owner_switches, align 8
  store i64 %4, ptr %oldval, align 8
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 8
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i64 %4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_stats_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %arrayidx = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 8
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_stats_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_wait_time = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 8, i32 1
  %call2 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call2, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %do.end
  %4 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %4, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %4, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %call2, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %do.end, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_mutexes_prof_stats_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_stats, align 8
  %max_n_thds = getelementptr inbounds %struct.ctl_stats_s, ptr %3, i64 0, i32 8, i64 8, i32 4
  %4 = load i32, ptr %max_n_thds, align 8
  store i32 %4, ptr %oldval, align 4
  %cmp3 = icmp ne ptr %oldp, null
  %cmp4 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp3, %cmp4
  br i1 %or.cond1, label %if.then5, label %label_return

if.then5:                                         ; preds = %do.end
  %5 = load i64, ptr %oldlenp, align 8
  %cmp6.not = icmp eq i64 %5, 4
  br i1 %cmp6.not, label %if.end9, label %if.then7

if.then7:                                         ; preds = %if.then5
  %spec.select = tail call i64 @llvm.umin.i64(i64 %5, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end9:                                          ; preds = %if.then5
  store i32 %4, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end9, %do.end, %malloc_mutex_lock.exit, %if.then7
  %ret.0 = phi i32 [ 22, %if.then7 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %do.end ], [ 0, %if.end9 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

declare void @malloc_mutex_prof_data_reset(ptr noundef, ptr noundef) local_unnamed_addr #1

; Function Attrs: nounwind uwtable
define internal ptr @stats_arenas_i_index(ptr noundef %tsdn, ptr nocapture readnone %mib, i64 %miblen, i64 noundef %i) #0 {
entry:
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsdn
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsdn, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  switch i64 %i, label %sw.default.i.i [
    i64 4096, label %ctl_arenas_i_verify.exit
    i64 4097, label %sw.bb2.i.i
  ]

sw.bb2.i.i:                                       ; preds = %malloc_mutex_lock.exit
  br label %ctl_arenas_i_verify.exit

sw.default.i.i:                                   ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_arenas, align 8
  %narenas.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %3, i64 0, i32 1
  %4 = load i32, ptr %narenas.i.i, align 8
  %conv.i.i = zext i32 %4 to i64
  %cmp.i.i = icmp eq i64 %conv.i.i, %i
  br i1 %cmp.i.i, label %ctl_arenas_i_verify.exit, label %if.else.i.i

if.else.i.i:                                      ; preds = %sw.default.i.i
  %cmp9.not.i.i = icmp ule i64 %conv.i.i, %i
  %conv13.i.i = trunc i64 %i to i32
  %add.i.i = add i32 %conv13.i.i, 2
  %cmp.i = icmp eq i32 %add.i.i, -1
  %or.cond.i = or i1 %cmp.i, %cmp9.not.i.i
  br i1 %or.cond.i, label %ctl_arenas_i_verify.exit.thread, label %ctl_arenas_i_verify.exit

ctl_arenas_i_verify.exit:                         ; preds = %malloc_mutex_lock.exit, %sw.bb2.i.i, %sw.default.i.i, %if.else.i.i
  %a.0.i4.i = phi i32 [ 0, %sw.default.i.i ], [ 0, %malloc_mutex_lock.exit ], [ 1, %sw.bb2.i.i ], [ %add.i.i, %if.else.i.i ]
  %conv.i = zext i32 %a.0.i4.i to i64
  %5 = load ptr, ptr @ctl_arenas, align 8
  %arrayidx.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %5, i64 0, i32 3, i64 %conv.i
  %6 = load ptr, ptr %arrayidx.i, align 8
  %initialized.i = getelementptr inbounds %struct.ctl_arena_s, ptr %6, i64 0, i32 1
  %7 = load i8, ptr %initialized.i, align 4
  %.fr4 = freeze i8 %7
  %8 = and i8 %.fr4, 1
  %tobool.not.i = icmp eq i8 %8, 0
  br i1 %tobool.not.i, label %ctl_arenas_i_verify.exit.thread, label %9

ctl_arenas_i_verify.exit.thread:                  ; preds = %if.else.i.i, %ctl_arenas_i_verify.exit
  br label %9

9:                                                ; preds = %ctl_arenas_i_verify.exit, %ctl_arenas_i_verify.exit.thread
  %10 = phi ptr [ null, %ctl_arenas_i_verify.exit.thread ], [ @super_stats_arenas_i_node, %ctl_arenas_i_verify.exit ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret ptr %10
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_nthreads_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %nthreads = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 3
  %10 = load i32, ptr %nthreads, align 8
  store i32 %10, ptr %oldval, align 4
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %11, 4
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i32 %10, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_uptime_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %uptime = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 16
  %call4 = tail call i64 @nstime_ns(ptr noundef nonnull %uptime) #14
  store i64 %call4, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %11, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %call4, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_dss_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca ptr, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %dss = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 4
  %10 = load ptr, ptr %dss, align 8
  store ptr %10, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %11, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store ptr %10, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_dirty_decay_ms_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %dirty_decay_ms = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 5
  %10 = load i64, ptr %dirty_decay_ms, align 8
  store i64 %10, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %11, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %10, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_muzzy_decay_ms_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %muzzy_decay_ms = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 6
  %10 = load i64, ptr %muzzy_decay_ms, align 8
  store i64 %10, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %11, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %10, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_pactive_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %pactive = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 7
  %10 = load i64, ptr %pactive, align 8
  store i64 %10, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %11, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %10, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_pdirty_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %pdirty = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 8
  %10 = load i64, ptr %pdirty, align 8
  store i64 %10, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %11, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %10, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_pmuzzy_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %pmuzzy = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 9
  %10 = load i64, ptr %pmuzzy, align 8
  store i64 %10, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %11, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %10, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mapped_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %mapped = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 3
  %11 = load i64, ptr %mapped, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_retained_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %retained = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 11, i32 1, i32 2
  %11 = load i64, ptr %retained, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_extent_avail_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %pa_shard_stats = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 11
  %11 = load i64, ptr %pa_shard_stats, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_dirty_npurge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %pac_stats = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 11, i32 1
  %11 = load atomic i64, ptr %pac_stats monotonic, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_dirty_nmadvise_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %nmadvise = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 11, i32 1, i32 0, i32 1
  %11 = load atomic i64, ptr %nmadvise monotonic, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_dirty_purged_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %purged = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 11, i32 1, i32 0, i32 2
  %11 = load atomic i64, ptr %purged monotonic, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_muzzy_npurge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %decay_muzzy = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 11, i32 1, i32 1
  %11 = load atomic i64, ptr %decay_muzzy monotonic, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_muzzy_nmadvise_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %nmadvise = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 11, i32 1, i32 1, i32 1
  %11 = load atomic i64, ptr %nmadvise monotonic, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_muzzy_purged_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %purged = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 11, i32 1, i32 1, i32 2
  %11 = load atomic i64, ptr %purged monotonic, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_base_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %11 = load i64, ptr %10, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_internal_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %internal = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 4
  %11 = load atomic i64, ptr %internal monotonic, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_metadata_thp_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %metadata_thp = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 2
  %11 = load i64, ptr %metadata_thp, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_tcache_bytes_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %tcache_bytes = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 12
  %11 = load i64, ptr %tcache_bytes, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_tcache_stashed_bytes_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %tcache_stashed_bytes = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 13
  %11 = load i64, ptr %tcache_stashed_bytes, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_resident_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %resident = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 1
  %11 = load i64, ptr %resident, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_abandoned_vm_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %abandoned_vm = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 11, i32 1, i32 4
  %11 = load atomic i64, ptr %abandoned_vm monotonic, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_sec_bytes_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %secstats = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 11
  %11 = load i64, ptr %secstats, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %12, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_small_allocated_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %allocated_small = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 1
  %11 = load i64, ptr %allocated_small, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %12, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_small_nmalloc_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %nmalloc_small = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 2
  %11 = load i64, ptr %nmalloc_small, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %12, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_small_ndalloc_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %ndalloc_small = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 3
  %11 = load i64, ptr %ndalloc_small, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %12, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_small_nrequests_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %nrequests_small = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 4
  %11 = load i64, ptr %nrequests_small, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %12, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_small_nfills_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %nfills_small = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 5
  %11 = load i64, ptr %nfills_small, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %12, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_small_nflushes_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %nflushes_small = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 6
  %11 = load i64, ptr %nflushes_small, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %12, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_large_allocated_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %allocated_large = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 5
  %11 = load i64, ptr %allocated_large, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_large_nmalloc_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %nmalloc_large = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 6
  %11 = load i64, ptr %nmalloc_large, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_large_ndalloc_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %ndalloc_large = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 7
  %11 = load i64, ptr %ndalloc_large, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_large_nrequests_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %nrequests_large = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 10
  %11 = load i64, ptr %nrequests_large, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_large_nfills_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %nmalloc_large = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 6
  %11 = load i64, ptr %nmalloc_large, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_large_nflushes_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %nflushes_large = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 9
  %11 = load i64, ptr %nflushes_large, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal ptr @stats_arenas_i_bins_j_index(ptr nocapture readnone %tsdn, ptr nocapture readnone %mib, i64 %miblen, i64 noundef %j) #2 {
entry:
  %cmp = icmp ugt i64 %j, 39
  %.super_stats_arenas_i_bins_j_node = select i1 %cmp, ptr null, ptr @super_stats_arenas_i_bins_j_node
  ret ptr %.super_stats_arenas_i_bins_j_node
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_nmalloc_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %arrayidx4 = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 7, i64 %11
  %12 = load i64, ptr %arrayidx4, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_ndalloc_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %ndalloc = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 7, i64 %11, i32 0, i32 1
  %12 = load i64, ptr %ndalloc, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_nrequests_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %nrequests = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 7, i64 %11, i32 0, i32 2
  %12 = load i64, ptr %nrequests, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_curregs_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %curregs = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 7, i64 %11, i32 0, i32 3
  %12 = load i64, ptr %curregs, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_nfills_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %nfills = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 7, i64 %11, i32 0, i32 4
  %12 = load i64, ptr %nfills, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_nflushes_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %nflushes = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 7, i64 %11, i32 0, i32 5
  %12 = load i64, ptr %nflushes, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_nslabs_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %nslabs = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 7, i64 %11, i32 0, i32 6
  %12 = load i64, ptr %nslabs, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_nreslabs_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %reslabs = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 7, i64 %11, i32 0, i32 7
  %12 = load i64, ptr %reslabs, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_curslabs_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %curslabs = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 7, i64 %11, i32 0, i32 8
  %12 = load i64, ptr %curslabs, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_nonfull_slabs_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %nonfull_slabs = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 7, i64 %11, i32 0, i32 9
  %12 = load i64, ptr %nonfull_slabs, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_mutex_num_ops_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %n_lock_ops = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 7, i64 %11, i32 1, i32 8
  %12 = load i64, ptr %n_lock_ops, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_mutex_num_wait_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %n_wait_times = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 7, i64 %11, i32 1, i32 2
  %12 = load i64, ptr %n_wait_times, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_mutex_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %n_spin_acquired = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 7, i64 %11, i32 1, i32 3
  %12 = load i64, ptr %n_spin_acquired, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_mutex_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %n_owner_switches = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 7, i64 %11, i32 1, i32 6
  %12 = load i64, ptr %n_owner_switches, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_mutex_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %mutex_data = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 7, i64 %11, i32 1
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %mutex_data) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %12, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_mutex_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %max_wait_time = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 7, i64 %11, i32 1, i32 1
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %12, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_bins_j_mutex_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %max_n_thds = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 7, i64 %11, i32 1, i32 4
  %12 = load i32, ptr %max_n_thds, align 8
  store i32 %12, ptr %oldval, align 4
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 4
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i32 %12, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal ptr @stats_arenas_i_lextents_j_index(ptr nocapture readnone %tsdn, ptr nocapture readnone %mib, i64 %miblen, i64 noundef %j) #2 {
entry:
  %cmp = icmp ugt i64 %j, 196
  %.super_stats_arenas_i_lextents_j_node = select i1 %cmp, ptr null, ptr @super_stats_arenas_i_lextents_j_node
  ret ptr %.super_stats_arenas_i_lextents_j_node
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_lextents_j_nmalloc_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %arrayidx4 = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 8, i64 %11
  %12 = load atomic i64, ptr %arrayidx4 monotonic, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %13, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_lextents_j_ndalloc_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %ndalloc = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 8, i64 %11, i32 1
  %12 = load atomic i64, ptr %ndalloc monotonic, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %13, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_lextents_j_nrequests_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %nrequests = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 8, i64 %11, i32 2
  %12 = load atomic i64, ptr %nrequests monotonic, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %13, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_lextents_j_curlextents_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %curlextents = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 8, i64 %11, i32 5
  %12 = load i64, ptr %curlextents, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal ptr @stats_arenas_i_extents_j_index(ptr nocapture readnone %tsdn, ptr nocapture readnone %mib, i64 %miblen, i64 noundef %j) #2 {
entry:
  %cmp = icmp ugt i64 %j, 198
  %.super_stats_arenas_i_extents_j_node = select i1 %cmp, ptr null, ptr @super_stats_arenas_i_extents_j_node
  ret ptr %.super_stats_arenas_i_extents_j_node
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_extents_j_ndirty_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %arrayidx4 = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 9, i64 %11
  %12 = load i64, ptr %arrayidx4, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_extents_j_nmuzzy_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %nmuzzy = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 9, i64 %11, i32 2
  %12 = load i64, ptr %nmuzzy, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_extents_j_nretained_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %nretained = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 9, i64 %11, i32 4
  %12 = load i64, ptr %nretained, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_extents_j_dirty_bytes_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %dirty_bytes = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 9, i64 %11, i32 1
  %12 = load i64, ptr %dirty_bytes, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_extents_j_muzzy_bytes_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %muzzy_bytes = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 9, i64 %11, i32 3
  %12 = load i64, ptr %muzzy_bytes, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_extents_j_retained_bytes_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 4
  %11 = load i64, ptr %arrayidx3, align 8
  %retained_bytes = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 9, i64 %11, i32 5
  %12 = load i64, ptr %retained_bytes, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %13, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_large_num_ops_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_lock_ops = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 0, i32 8
  %11 = load i64, ptr %n_lock_ops, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_large_num_wait_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_wait_times = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 0, i32 2
  %11 = load i64, ptr %n_wait_times, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_large_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_spin_acquired = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 0, i32 3
  %11 = load i64, ptr %n_spin_acquired, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_large_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_owner_switches = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 0, i32 6
  %11 = load i64, ptr %n_owner_switches, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_large_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %mutex_prof_data = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %mutex_prof_data) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_large_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %max_wait_time = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 0, i32 1
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_large_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %max_n_thds = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 0, i32 4
  %11 = load i32, ptr %max_n_thds, align 8
  store i32 %11, ptr %oldval, align 4
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 4
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i32 %11, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extent_avail_num_ops_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_lock_ops = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 1, i32 8
  %11 = load i64, ptr %n_lock_ops, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extent_avail_num_wait_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_wait_times = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 1, i32 2
  %11 = load i64, ptr %n_wait_times, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extent_avail_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_spin_acquired = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 1, i32 3
  %11 = load i64, ptr %n_spin_acquired, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extent_avail_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_owner_switches = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 1, i32 6
  %11 = load i64, ptr %n_owner_switches, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extent_avail_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx4 = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 1
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx4) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extent_avail_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %max_wait_time = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 1, i32 1
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extent_avail_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %max_n_thds = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 1, i32 4
  %11 = load i32, ptr %max_n_thds, align 8
  store i32 %11, ptr %oldval, align 4
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 4
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i32 %11, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_dirty_num_ops_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_lock_ops = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 2, i32 8
  %11 = load i64, ptr %n_lock_ops, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_dirty_num_wait_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_wait_times = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 2, i32 2
  %11 = load i64, ptr %n_wait_times, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_dirty_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_spin_acquired = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 2, i32 3
  %11 = load i64, ptr %n_spin_acquired, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_dirty_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_owner_switches = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 2, i32 6
  %11 = load i64, ptr %n_owner_switches, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_dirty_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx4 = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 2
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx4) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_dirty_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %max_wait_time = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 2, i32 1
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_dirty_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %max_n_thds = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 2, i32 4
  %11 = load i32, ptr %max_n_thds, align 8
  store i32 %11, ptr %oldval, align 4
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 4
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i32 %11, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_muzzy_num_ops_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_lock_ops = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 3, i32 8
  %11 = load i64, ptr %n_lock_ops, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_muzzy_num_wait_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_wait_times = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 3, i32 2
  %11 = load i64, ptr %n_wait_times, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_muzzy_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_spin_acquired = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 3, i32 3
  %11 = load i64, ptr %n_spin_acquired, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_muzzy_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_owner_switches = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 3, i32 6
  %11 = load i64, ptr %n_owner_switches, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_muzzy_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx4 = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 3
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx4) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_muzzy_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %max_wait_time = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 3, i32 1
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_muzzy_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %max_n_thds = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 3, i32 4
  %11 = load i32, ptr %max_n_thds, align 8
  store i32 %11, ptr %oldval, align 4
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 4
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i32 %11, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_retained_num_ops_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_lock_ops = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 4, i32 8
  %11 = load i64, ptr %n_lock_ops, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_retained_num_wait_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_wait_times = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 4, i32 2
  %11 = load i64, ptr %n_wait_times, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_retained_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_spin_acquired = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 4, i32 3
  %11 = load i64, ptr %n_spin_acquired, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_retained_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_owner_switches = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 4, i32 6
  %11 = load i64, ptr %n_owner_switches, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_retained_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx4 = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 4
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx4) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_retained_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %max_wait_time = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 4, i32 1
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_extents_retained_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %max_n_thds = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 4, i32 4
  %11 = load i32, ptr %max_n_thds, align 8
  store i32 %11, ptr %oldval, align 4
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 4
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i32 %11, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_dirty_num_ops_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_lock_ops = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 5, i32 8
  %11 = load i64, ptr %n_lock_ops, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_dirty_num_wait_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_wait_times = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 5, i32 2
  %11 = load i64, ptr %n_wait_times, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_dirty_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_spin_acquired = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 5, i32 3
  %11 = load i64, ptr %n_spin_acquired, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_dirty_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_owner_switches = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 5, i32 6
  %11 = load i64, ptr %n_owner_switches, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_dirty_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx4 = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 5
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx4) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_dirty_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %max_wait_time = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 5, i32 1
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_dirty_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %max_n_thds = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 5, i32 4
  %11 = load i32, ptr %max_n_thds, align 8
  store i32 %11, ptr %oldval, align 4
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 4
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i32 %11, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_muzzy_num_ops_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_lock_ops = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 6, i32 8
  %11 = load i64, ptr %n_lock_ops, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_muzzy_num_wait_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_wait_times = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 6, i32 2
  %11 = load i64, ptr %n_wait_times, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_muzzy_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_spin_acquired = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 6, i32 3
  %11 = load i64, ptr %n_spin_acquired, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_muzzy_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_owner_switches = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 6, i32 6
  %11 = load i64, ptr %n_owner_switches, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_muzzy_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx4 = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 6
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx4) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_muzzy_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %max_wait_time = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 6, i32 1
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_decay_muzzy_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %max_n_thds = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 6, i32 4
  %11 = load i32, ptr %max_n_thds, align 8
  store i32 %11, ptr %oldval, align 4
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 4
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i32 %11, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_base_num_ops_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_lock_ops = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 7, i32 8
  %11 = load i64, ptr %n_lock_ops, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_base_num_wait_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_wait_times = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 7, i32 2
  %11 = load i64, ptr %n_wait_times, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_base_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_spin_acquired = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 7, i32 3
  %11 = load i64, ptr %n_spin_acquired, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_base_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_owner_switches = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 7, i32 6
  %11 = load i64, ptr %n_owner_switches, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_base_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx4 = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 7
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx4) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_base_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %max_wait_time = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 7, i32 1
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_base_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %max_n_thds = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 7, i32 4
  %11 = load i32, ptr %max_n_thds, align 8
  store i32 %11, ptr %oldval, align 4
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 4
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i32 %11, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_tcache_list_num_ops_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_lock_ops = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 8, i32 8
  %11 = load i64, ptr %n_lock_ops, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_tcache_list_num_wait_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_wait_times = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 8, i32 2
  %11 = load i64, ptr %n_wait_times, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_tcache_list_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_spin_acquired = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 8, i32 3
  %11 = load i64, ptr %n_spin_acquired, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_tcache_list_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_owner_switches = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 8, i32 6
  %11 = load i64, ptr %n_owner_switches, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_tcache_list_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx4 = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 8
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx4) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_tcache_list_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %max_wait_time = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 8, i32 1
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_tcache_list_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %max_n_thds = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 8, i32 4
  %11 = load i32, ptr %max_n_thds, align 8
  store i32 %11, ptr %oldval, align 4
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 4
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i32 %11, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_num_ops_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_lock_ops = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 9, i32 8
  %11 = load i64, ptr %n_lock_ops, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_num_wait_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_wait_times = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 9, i32 2
  %11 = load i64, ptr %n_wait_times, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_spin_acquired = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 9, i32 3
  %11 = load i64, ptr %n_spin_acquired, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_owner_switches = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 9, i32 6
  %11 = load i64, ptr %n_owner_switches, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx4 = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 9
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx4) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %max_wait_time = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 9, i32 1
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %max_n_thds = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 9, i32 4
  %11 = load i32, ptr %max_n_thds, align 8
  store i32 %11, ptr %oldval, align 4
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 4
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i32 %11, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_grow_num_ops_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_lock_ops = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 10, i32 8
  %11 = load i64, ptr %n_lock_ops, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_grow_num_wait_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_wait_times = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 10, i32 2
  %11 = load i64, ptr %n_wait_times, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_grow_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_spin_acquired = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 10, i32 3
  %11 = load i64, ptr %n_spin_acquired, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_grow_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_owner_switches = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 10, i32 6
  %11 = load i64, ptr %n_owner_switches, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_grow_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx4 = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 10
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx4) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_grow_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %max_wait_time = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 10, i32 1
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_shard_grow_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %max_n_thds = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 10, i32 4
  %11 = load i32, ptr %max_n_thds, align 8
  store i32 %11, ptr %oldval, align 4
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 4
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i32 %11, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_sec_num_ops_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_lock_ops = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 11, i32 8
  %11 = load i64, ptr %n_lock_ops, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_sec_num_wait_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_wait_times = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 11, i32 2
  %11 = load i64, ptr %n_wait_times, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_sec_num_spin_acq_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_spin_acquired = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 11, i32 3
  %11 = load i64, ptr %n_spin_acquired, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_sec_num_owner_switch_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %n_owner_switches = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 11, i32 6
  %11 = load i64, ptr %n_owner_switches, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 8
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_sec_total_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx4 = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 11
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %arrayidx4) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_sec_max_wait_time_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %max_wait_time = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 11, i32 1
  %call5 = tail call i64 @nstime_ns(ptr noundef nonnull %max_wait_time) #14
  store i64 %call5, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %11 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %11, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %11, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %call5, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_mutexes_hpa_sec_max_num_thds_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i32, align 4
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %max_n_thds = getelementptr inbounds %struct.arena_stats_s, ptr %10, i64 0, i32 14, i64 11, i32 4
  %11 = load i32, ptr %max_n_thds, align 8
  store i32 %11, ptr %oldval, align 4
  %cmp6 = icmp ne ptr %oldp, null
  %cmp7 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp6, %cmp7
  br i1 %or.cond1, label %if.then8, label %label_return

if.then8:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp9.not = icmp eq i64 %12, 4
  br i1 %cmp9.not, label %if.end12, label %if.then10

if.then10:                                        ; preds = %if.then8
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end12:                                         ; preds = %if.then8
  store i32 %11, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %if.end12, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then10
  %ret.0 = phi i32 [ 22, %if.then10 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_npurge_passes_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %nonderived_stats = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 10, i32 1
  %11 = load i64, ptr %nonderived_stats, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %12, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_npurges_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %npurges = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 10, i32 1, i32 1
  %11 = load i64, ptr %npurges, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %12, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_nhugifies_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %nhugifies = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 10, i32 1, i32 2
  %11 = load i64, ptr %nhugifies, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %12, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_ndehugifies_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %ndehugifies = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 10, i32 1, i32 3
  %11 = load i64, ptr %ndehugifies, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp4 = icmp ne ptr %oldp, null
  %cmp5 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp4, %cmp5
  br i1 %or.cond1, label %if.then6, label %label_return

if.then6:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp7.not = icmp eq i64 %12, 8
  br i1 %cmp7.not, label %if.end10, label %if.then8

if.then8:                                         ; preds = %if.then6
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end10:                                         ; preds = %if.then6
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end10, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then8
  %ret.0 = phi i32 [ 22, %if.then8 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end10 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_full_slabs_npageslabs_nonhuge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %full_slabs = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 10, i32 0, i32 1
  %11 = load i64, ptr %full_slabs, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_full_slabs_npageslabs_huge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 10, i32 0, i32 1, i64 1
  %11 = load i64, ptr %arrayidx3, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_full_slabs_nactive_nonhuge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %nactive = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 10, i32 0, i32 1, i64 0, i32 1
  %11 = load i64, ptr %nactive, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_full_slabs_nactive_huge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %nactive = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 10, i32 0, i32 1, i64 1, i32 1
  %11 = load i64, ptr %nactive, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_full_slabs_ndirty_nonhuge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %ndirty = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 10, i32 0, i32 1, i64 0, i32 2
  %11 = load i64, ptr %ndirty, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_full_slabs_ndirty_huge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %ndirty = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 10, i32 0, i32 1, i64 1, i32 2
  %11 = load i64, ptr %ndirty, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_empty_slabs_npageslabs_nonhuge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %empty_slabs = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 10, i32 0, i32 2
  %11 = load i64, ptr %empty_slabs, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_empty_slabs_npageslabs_huge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %arrayidx3 = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 10, i32 0, i32 2, i64 1
  %11 = load i64, ptr %arrayidx3, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_empty_slabs_nactive_nonhuge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %nactive = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 10, i32 0, i32 2, i64 0, i32 1
  %11 = load i64, ptr %nactive, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_empty_slabs_nactive_huge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %nactive = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 10, i32 0, i32 2, i64 1, i32 1
  %11 = load i64, ptr %nactive, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_empty_slabs_ndirty_nonhuge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %ndirty = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 10, i32 0, i32 2, i64 0, i32 2
  %11 = load i64, ptr %ndirty, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_empty_slabs_ndirty_huge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %ndirty = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 10, i32 0, i32 2, i64 1, i32 2
  %11 = load i64, ptr %ndirty, align 8
  store i64 %11, ptr %oldval, align 8
  %cmp5 = icmp ne ptr %oldp, null
  %cmp6 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp5, %cmp6
  br i1 %or.cond1, label %if.then7, label %label_return

if.then7:                                         ; preds = %arenas_i.exit
  %12 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %12, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %12, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store i64 %11, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end11, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal ptr @stats_arenas_i_hpa_shard_nonfull_slabs_j_index(ptr nocapture readnone %tsdn, ptr nocapture readnone %mib, i64 %miblen, i64 noundef %j) #2 {
entry:
  %cmp = icmp ugt i64 %j, 63
  %.super_stats_arenas_i_hpa_shard_nonfull_slabs_j_node = select i1 %cmp, ptr null, ptr @super_stats_arenas_i_hpa_shard_nonfull_slabs_j_node
  ret ptr %.super_stats_arenas_i_hpa_shard_nonfull_slabs_j_node
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_nonfull_slabs_j_npageslabs_nonhuge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %hpastats = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 10
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 5
  %11 = load i64, ptr %arrayidx3, align 8
  %arrayidx4 = getelementptr inbounds [64 x [2 x %struct.psset_bin_stats_s]], ptr %hpastats, i64 0, i64 %11
  %12 = load i64, ptr %arrayidx4, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %13, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_nonfull_slabs_j_npageslabs_huge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %hpastats = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 10
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 5
  %11 = load i64, ptr %arrayidx3, align 8
  %arrayidx5 = getelementptr inbounds [64 x [2 x %struct.psset_bin_stats_s]], ptr %hpastats, i64 0, i64 %11, i64 1
  %12 = load i64, ptr %arrayidx5, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %13, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_nonfull_slabs_j_nactive_nonhuge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %hpastats = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 10
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 5
  %11 = load i64, ptr %arrayidx3, align 8
  %nactive = getelementptr inbounds [64 x [2 x %struct.psset_bin_stats_s]], ptr %hpastats, i64 0, i64 %11, i64 0, i32 1
  %12 = load i64, ptr %nactive, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %13, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_nonfull_slabs_j_nactive_huge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %hpastats = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 10
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 5
  %11 = load i64, ptr %arrayidx3, align 8
  %nactive = getelementptr inbounds [64 x [2 x %struct.psset_bin_stats_s]], ptr %hpastats, i64 0, i64 %11, i64 1, i32 1
  %12 = load i64, ptr %nactive, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %13, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_nonfull_slabs_j_ndirty_nonhuge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %hpastats = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 10
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 5
  %11 = load i64, ptr %arrayidx3, align 8
  %ndirty = getelementptr inbounds [64 x [2 x %struct.psset_bin_stats_s]], ptr %hpastats, i64 0, i64 %11, i64 0, i32 2
  %12 = load i64, ptr %ndirty, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %13, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @stats_arenas_i_hpa_shard_nonfull_slabs_j_ndirty_huge_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %oldval = alloca i64, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp = icmp ne ptr %newp, null
  %cmp1 = icmp ne i64 %newlen, 0
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %3 = load i64, ptr %arrayidx, align 8
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @tsd_tls)
  %state.i.i.i = getelementptr inbounds %struct.tsd_s, ptr %4, i64 0, i32 29
  %5 = load i8, ptr %state.i.i.i, align 8
  %cmp6.i.not.i = icmp eq i8 %5, 0
  br i1 %cmp6.i.not.i, label %tsd_fetch_impl.exit.i, label %if.then11.i.i

if.then11.i.i:                                    ; preds = %do.end
  %call13.i.i = tail call ptr @tsd_fetch_slow(ptr noundef nonnull %4, i1 noundef zeroext false) #14
  br label %tsd_fetch_impl.exit.i

tsd_fetch_impl.exit.i:                            ; preds = %if.then11.i.i, %do.end
  %6 = load ptr, ptr @ctl_arenas, align 8
  switch i64 %3, label %sw.default.i.i.i [
    i64 4096, label %arenas_i.exit
    i64 4097, label %sw.bb2.i.i.i
  ]

sw.bb2.i.i.i:                                     ; preds = %tsd_fetch_impl.exit.i
  br label %arenas_i.exit

sw.default.i.i.i:                                 ; preds = %tsd_fetch_impl.exit.i
  %narenas.i.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 1
  %7 = load i32, ptr %narenas.i.i.i, align 8
  %conv.i.i.i = zext i32 %7 to i64
  %cmp.i.i.i = icmp eq i64 %3, %conv.i.i.i
  br i1 %cmp.i.i.i, label %arenas_i.exit, label %if.else.i.i.i

if.else.i.i.i:                                    ; preds = %sw.default.i.i.i
  %add.i.i.i = add i64 %3, 2
  %8 = and i64 %add.i.i.i, 4294967295
  br label %arenas_i.exit

arenas_i.exit:                                    ; preds = %tsd_fetch_impl.exit.i, %sw.bb2.i.i.i, %sw.default.i.i.i, %if.else.i.i.i
  %a.0.i.i.i = phi i64 [ %8, %if.else.i.i.i ], [ 1, %sw.bb2.i.i.i ], [ 0, %tsd_fetch_impl.exit.i ], [ 0, %sw.default.i.i.i ]
  %arrayidx.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %6, i64 0, i32 3, i64 %a.0.i.i.i
  %9 = load ptr, ptr %arrayidx.i.i, align 8
  %astats = getelementptr inbounds %struct.ctl_arena_s, ptr %9, i64 0, i32 10
  %10 = load ptr, ptr %astats, align 8
  %hpastats = getelementptr inbounds %struct.ctl_arena_stats_s, ptr %10, i64 0, i32 10
  %arrayidx3 = getelementptr inbounds i64, ptr %mib, i64 5
  %11 = load i64, ptr %arrayidx3, align 8
  %ndirty = getelementptr inbounds [64 x [2 x %struct.psset_bin_stats_s]], ptr %hpastats, i64 0, i64 %11, i64 1, i32 2
  %12 = load i64, ptr %ndirty, align 8
  store i64 %12, ptr %oldval, align 8
  %cmp7 = icmp ne ptr %oldp, null
  %cmp8 = icmp ne ptr %oldlenp, null
  %or.cond1 = and i1 %cmp7, %cmp8
  br i1 %or.cond1, label %if.then9, label %label_return

if.then9:                                         ; preds = %arenas_i.exit
  %13 = load i64, ptr %oldlenp, align 8
  %cmp10.not = icmp eq i64 %13, 8
  br i1 %cmp10.not, label %if.end13, label %if.then11

if.then11:                                        ; preds = %if.then9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %13, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %oldval, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end13:                                         ; preds = %if.then9
  store i64 %12, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end13, %arenas_i.exit, %malloc_mutex_lock.exit, %if.then11
  %ret.0 = phi i32 [ 22, %if.then11 ], [ 1, %malloc_mutex_lock.exit ], [ 0, %arenas_i.exit ], [ 0, %if.end13 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @experimental_arenas_create_ext_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %arena_ind = alloca i32, align 4
  %config = alloca %struct.arena_config_s, align 8
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) %config, ptr noundef nonnull align 8 dereferenceable(16) @arena_config_default, i64 16, i1 false)
  %cmp = icmp eq ptr %oldp, null
  %cmp1 = icmp eq ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %if.then, label %lor.lhs.false2

lor.lhs.false2:                                   ; preds = %malloc_mutex_lock.exit
  %3 = load i64, ptr %oldlenp, align 8
  %cmp3.not = icmp eq i64 %3, 4
  br i1 %cmp3.not, label %do.body4, label %if.then

if.then:                                          ; preds = %lor.lhs.false2, %malloc_mutex_lock.exit
  store i64 0, ptr %oldlenp, align 8
  br label %label_return

do.body4:                                         ; preds = %lor.lhs.false2
  %cmp5.not = icmp eq ptr %newp, null
  br i1 %cmp5.not, label %do.end11, label %if.then6

if.then6:                                         ; preds = %do.body4
  %cmp7.not = icmp eq i64 %newlen, 16
  br i1 %cmp7.not, label %if.end9, label %label_return

if.end9:                                          ; preds = %if.then6
  call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) %config, ptr noundef nonnull align 8 dereferenceable(16) %newp, i64 16, i1 false)
  br label %do.end11

do.end11:                                         ; preds = %do.body4, %if.end9
  %call12 = call fastcc i32 @ctl_arena_init(ptr noundef %tsd, ptr noundef nonnull %config)
  store i32 %call12, ptr %arena_ind, align 4
  %cmp13 = icmp eq i32 %call12, -1
  br i1 %cmp13, label %label_return, label %if.then19

if.then19:                                        ; preds = %do.end11
  %4 = load i64, ptr %oldlenp, align 8
  %cmp20.not = icmp eq i64 %4, 4
  br i1 %cmp20.not, label %if.end23, label %if.then21

if.then21:                                        ; preds = %if.then19
  %spec.select = call i64 @llvm.umin.i64(i64 %4, i64 4)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 4 %arena_ind, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end23:                                         ; preds = %if.then19
  store i32 %call12, ptr %oldp, align 4
  br label %label_return

label_return:                                     ; preds = %do.end11, %if.then6, %if.end23, %if.then21, %if.then
  %ret.0 = phi i32 [ 22, %if.then ], [ 22, %if.then21 ], [ 0, %if.end23 ], [ 22, %if.then6 ], [ 11, %do.end11 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @experimental_batch_alloc_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %filled = alloca i64, align 8
  %cmp = icmp eq ptr %oldp, null
  %cmp1 = icmp eq ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %if.then, label %lor.lhs.false2

lor.lhs.false2:                                   ; preds = %entry
  %0 = load i64, ptr %oldlenp, align 8
  %cmp3.not = icmp eq i64 %0, 8
  br i1 %cmp3.not, label %do.body4, label %if.then

if.then:                                          ; preds = %lor.lhs.false2, %entry
  store i64 0, ptr %oldlenp, align 8
  br label %label_return

do.body4:                                         ; preds = %lor.lhs.false2
  %cmp5 = icmp eq ptr %newp, null
  %cmp7 = icmp ne i64 %newlen, 32
  %or.cond1 = or i1 %cmp5, %cmp7
  br i1 %or.cond1, label %label_return, label %if.end9

if.end9:                                          ; preds = %do.body4
  %batch_alloc_packet.sroa.0.0.copyload = load ptr, ptr %newp, align 8
  %batch_alloc_packet.sroa.2.0..sroa_idx = getelementptr inbounds i8, ptr %newp, i64 8
  %batch_alloc_packet.sroa.2.0.copyload = load i64, ptr %batch_alloc_packet.sroa.2.0..sroa_idx, align 8
  %batch_alloc_packet.sroa.3.0..sroa_idx = getelementptr inbounds i8, ptr %newp, i64 16
  %batch_alloc_packet.sroa.3.0.copyload = load i64, ptr %batch_alloc_packet.sroa.3.0..sroa_idx, align 8
  %batch_alloc_packet.sroa.4.0..sroa_idx = getelementptr inbounds i8, ptr %newp, i64 24
  %batch_alloc_packet.sroa.4.0.copyload = load i32, ptr %batch_alloc_packet.sroa.4.0..sroa_idx, align 8
  %call = tail call i64 @batch_alloc(ptr noundef %batch_alloc_packet.sroa.0.0.copyload, i64 noundef %batch_alloc_packet.sroa.2.0.copyload, i64 noundef %batch_alloc_packet.sroa.3.0.copyload, i32 noundef %batch_alloc_packet.sroa.4.0.copyload) #14
  store i64 %call, ptr %filled, align 8
  %1 = load i64, ptr %oldlenp, align 8
  %cmp15.not = icmp eq i64 %1, 8
  br i1 %cmp15.not, label %if.end18, label %if.then16

if.then16:                                        ; preds = %if.end9
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %filled, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end18:                                         ; preds = %if.end9
  store i64 %call, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %do.body4, %if.end18, %if.then16, %if.then
  %ret.0 = phi i32 [ 22, %if.then ], [ 22, %if.then16 ], [ 0, %if.end18 ], [ 22, %do.body4 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @experimental_hooks_install_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %hooks = alloca %struct.hooks_s, align 8
  %handle = alloca ptr, align 8
  %cmp = icmp eq ptr %oldp, null
  %cmp1 = icmp eq ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  %cmp3 = icmp eq ptr %newp, null
  %or.cond1 = or i1 %or.cond, %cmp3
  %cmp6.not = icmp ne i64 %newlen, 32
  %or.cond15.not = or i1 %cmp6.not, %or.cond1
  br i1 %or.cond15.not, label %label_return, label %if.end8

if.end8:                                          ; preds = %entry
  call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) %hooks, ptr noundef nonnull align 8 dereferenceable(32) %newp, i64 32, i1 false)
  %call10 = call ptr @hook_install(ptr noundef %tsd, ptr noundef nonnull %hooks) #14
  store ptr %call10, ptr %handle, align 8
  %cmp11 = icmp eq ptr %call10, null
  br i1 %cmp11, label %label_return, label %if.then17

if.then17:                                        ; preds = %if.end8
  %0 = load i64, ptr %oldlenp, align 8
  %cmp18.not = icmp eq i64 %0, 8
  br i1 %cmp18.not, label %if.end21, label %if.then19

if.then19:                                        ; preds = %if.then17
  %spec.select = call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %handle, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end21:                                         ; preds = %if.then17
  store ptr %call10, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end8, %entry, %if.end21, %if.then19
  %ret.0 = phi i32 [ 22, %if.then19 ], [ 0, %if.end21 ], [ 22, %entry ], [ 11, %if.end8 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @experimental_hooks_remove_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef readnone %oldp, ptr noundef readnone %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %cmp = icmp ne ptr %oldp, null
  %cmp1 = icmp ne ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %cmp3.not = icmp ne ptr %newp, null
  %cmp5.not = icmp eq i64 %newlen, 8
  %or.cond5 = and i1 %cmp3.not, %cmp5.not
  br i1 %or.cond5, label %do.end9, label %label_return

do.end9:                                          ; preds = %do.end
  %0 = load ptr, ptr %newp, align 8
  %cmp10 = icmp eq ptr %0, null
  br i1 %cmp10, label %label_return, label %if.end12

if.end12:                                         ; preds = %do.end9
  tail call void @hook_remove(ptr noundef %tsd, ptr noundef nonnull %0) #14
  br label %label_return

label_return:                                     ; preds = %do.end, %do.end9, %entry, %if.end12
  %ret.0 = phi i32 [ 0, %if.end12 ], [ 1, %entry ], [ 22, %do.end9 ], [ 22, %do.end ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @experimental_hooks_prof_backtrace_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %old_hook = alloca ptr, align 8
  %cmp = icmp eq ptr %oldp, null
  %cmp1 = icmp eq ptr %newp, null
  %or.cond = and i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %if.end

if.end:                                           ; preds = %entry
  br i1 %cmp, label %if.end13, label %if.then3

if.then3:                                         ; preds = %if.end
  %call = tail call ptr (...) @prof_backtrace_hook_get() #14
  store ptr %call, ptr %old_hook, align 8
  %cmp6.not = icmp eq ptr %oldlenp, null
  br i1 %cmp6.not, label %if.end13, label %if.then7

if.then7:                                         ; preds = %if.then3
  %0 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %0, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %old_hook, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store ptr %call, ptr %oldp, align 8
  br label %if.end13

if.end13:                                         ; preds = %if.end11, %if.then3, %if.end
  br i1 %cmp1, label %label_return, label %if.then15

if.then15:                                        ; preds = %if.end13
  %1 = load i8, ptr @opt_prof, align 1
  %2 = and i8 %1, 1
  %tobool.not = icmp eq i8 %2, 0
  br i1 %tobool.not, label %label_return, label %if.then20

if.then20:                                        ; preds = %if.then15
  %cmp21.not = icmp eq i64 %newlen, 8
  br i1 %cmp21.not, label %if.end23, label %label_return

if.end23:                                         ; preds = %if.then20
  %3 = load ptr, ptr %newp, align 8
  %cmp26 = icmp eq ptr %3, null
  br i1 %cmp26, label %label_return, label %if.end28

if.end28:                                         ; preds = %if.end23
  tail call void @prof_backtrace_hook_set(ptr noundef nonnull %3) #14
  br label %label_return

label_return:                                     ; preds = %if.end13, %if.end28, %if.end23, %if.then20, %if.then15, %entry, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 22, %entry ], [ 2, %if.then15 ], [ 22, %if.then20 ], [ 22, %if.end23 ], [ 0, %if.end28 ], [ 0, %if.end13 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @experimental_hooks_prof_dump_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %old_hook = alloca ptr, align 8
  %cmp = icmp eq ptr %oldp, null
  %cmp1 = icmp eq ptr %newp, null
  %or.cond = and i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %if.end

if.end:                                           ; preds = %entry
  br i1 %cmp, label %if.end13, label %if.then3

if.then3:                                         ; preds = %if.end
  %call = tail call ptr (...) @prof_dump_hook_get() #14
  store ptr %call, ptr %old_hook, align 8
  %cmp6.not = icmp eq ptr %oldlenp, null
  br i1 %cmp6.not, label %if.end13, label %if.then7

if.then7:                                         ; preds = %if.then3
  %0 = load i64, ptr %oldlenp, align 8
  %cmp8.not = icmp eq i64 %0, 8
  br i1 %cmp8.not, label %if.end11, label %if.then9

if.then9:                                         ; preds = %if.then7
  %spec.select = tail call i64 @llvm.umin.i64(i64 %0, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %old_hook, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end11:                                         ; preds = %if.then7
  store ptr %call, ptr %oldp, align 8
  br label %if.end13

if.end13:                                         ; preds = %if.end11, %if.then3, %if.end
  br i1 %cmp1, label %label_return, label %if.then15

if.then15:                                        ; preds = %if.end13
  %1 = load i8, ptr @opt_prof, align 1
  %2 = and i8 %1, 1
  %tobool.not = icmp eq i8 %2, 0
  br i1 %tobool.not, label %label_return, label %if.then20

if.then20:                                        ; preds = %if.then15
  %cmp21.not = icmp eq i64 %newlen, 8
  br i1 %cmp21.not, label %do.end25, label %label_return

do.end25:                                         ; preds = %if.then20
  %3 = load ptr, ptr %newp, align 8
  tail call void @prof_dump_hook_set(ptr noundef %3) #14
  br label %label_return

label_return:                                     ; preds = %if.end13, %do.end25, %if.then20, %if.then15, %entry, %if.then9
  %ret.0 = phi i32 [ 22, %if.then9 ], [ 22, %entry ], [ 2, %if.then15 ], [ 22, %if.then20 ], [ 0, %do.end25 ], [ 0, %if.end13 ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @experimental_hooks_safety_check_abort_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef readnone %oldp, ptr noundef readnone %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %cmp = icmp ne ptr %oldp, null
  %cmp1 = icmp ne ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %do.end

do.end:                                           ; preds = %entry
  %cmp2.not = icmp eq ptr %newp, null
  br i1 %cmp2.not, label %label_return, label %if.then3

if.then3:                                         ; preds = %do.end
  %cmp4.not = icmp eq i64 %newlen, 8
  br i1 %cmp4.not, label %do.end14, label %label_return

do.end14:                                         ; preds = %if.then3
  %0 = load ptr, ptr %newp, align 8
  tail call void @safety_check_set_abort(ptr noundef %0) #14
  br label %label_return

label_return:                                     ; preds = %do.end, %do.end14, %if.then3, %entry
  %ret.0 = phi i32 [ 1, %entry ], [ 22, %if.then3 ], [ 0, %do.end14 ], [ 0, %do.end ]
  ret i32 %ret.0
}

declare ptr @hook_install(ptr noundef, ptr noundef) local_unnamed_addr #1

declare void @hook_remove(ptr noundef, ptr noundef) local_unnamed_addr #1

declare ptr @prof_backtrace_hook_get(...) local_unnamed_addr #1

declare void @prof_backtrace_hook_set(ptr noundef) local_unnamed_addr #1

declare ptr @prof_dump_hook_get(...) local_unnamed_addr #1

declare void @prof_dump_hook_set(ptr noundef) local_unnamed_addr #1

declare void @safety_check_set_abort(ptr noundef) local_unnamed_addr #1

; Function Attrs: nounwind uwtable
define internal i32 @experimental_utilization_query_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef %oldp, ptr noundef readonly %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %cmp = icmp eq ptr %oldp, null
  %cmp1 = icmp eq ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %label_return, label %lor.lhs.false2

lor.lhs.false2:                                   ; preds = %entry
  %0 = load i64, ptr %oldlenp, align 8
  %cmp3 = icmp eq i64 %0, 48
  %cmp5 = icmp ne ptr %newp, null
  %or.cond1.not12 = and i1 %cmp5, %cmp3
  %cmp9.not = icmp eq i64 %newlen, 8
  %or.cond11 = and i1 %cmp9.not, %or.cond1.not12
  br i1 %or.cond11, label %do.end13, label %label_return

do.end13:                                         ; preds = %lor.lhs.false2
  %1 = load ptr, ptr %newp, align 8
  %nfree = getelementptr inbounds %struct.inspect_extent_util_stats_verbose_s, ptr %oldp, i64 0, i32 1
  %nregs = getelementptr inbounds %struct.inspect_extent_util_stats_verbose_s, ptr %oldp, i64 0, i32 2
  %size = getelementptr inbounds %struct.inspect_extent_util_stats_verbose_s, ptr %oldp, i64 0, i32 3
  %bin_nfree = getelementptr inbounds %struct.inspect_extent_util_stats_verbose_s, ptr %oldp, i64 0, i32 4
  %bin_nregs = getelementptr inbounds %struct.inspect_extent_util_stats_verbose_s, ptr %oldp, i64 0, i32 5
  tail call void @inspect_extent_util_stats_verbose_get(ptr noundef %tsd, ptr noundef %1, ptr noundef nonnull %nfree, ptr noundef nonnull %nregs, ptr noundef nonnull %size, ptr noundef nonnull %bin_nfree, ptr noundef nonnull %bin_nregs, ptr noundef nonnull %oldp) #14
  br label %label_return

label_return:                                     ; preds = %entry, %lor.lhs.false2, %do.end13
  %ret.0 = phi i32 [ 0, %do.end13 ], [ 22, %lor.lhs.false2 ], [ 22, %entry ]
  ret i32 %ret.0
}

; Function Attrs: nounwind uwtable
define internal i32 @experimental_utilization_batch_query_ctl(ptr noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef %oldp, ptr noundef readonly %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #0 {
entry:
  %div17 = lshr i64 %newlen, 3
  %cmp = icmp eq ptr %oldp, null
  %cmp1 = icmp eq ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  %cmp3 = icmp eq ptr %newp, null
  %or.cond1 = or i1 %or.cond, %cmp3
  %cmp5 = icmp eq i64 %newlen, 0
  %or.cond2 = or i1 %or.cond1, %cmp5
  %0 = and i64 %newlen, 7
  %cmp7.not = icmp ne i64 %0, 0
  %or.cond18.not = or i1 %cmp7.not, %or.cond2
  br i1 %or.cond18.not, label %label_return, label %lor.lhs.false8

lor.lhs.false8:                                   ; preds = %entry
  %1 = load i64, ptr %oldlenp, align 8
  %mul9 = mul i64 %div17, 24
  %cmp10.not = icmp eq i64 %1, %mul9
  br i1 %cmp10.not, label %for.cond.preheader, label %label_return

for.cond.preheader:                               ; preds = %lor.lhs.false8
  %cmp1119.not = icmp ult i64 %newlen, 8
  br i1 %cmp1119.not, label %label_return, label %for.body

for.body:                                         ; preds = %for.cond.preheader, %for.body
  %i.020 = phi i64 [ %inc, %for.body ], [ 0, %for.cond.preheader ]
  %arrayidx = getelementptr inbounds ptr, ptr %newp, i64 %i.020
  %2 = load ptr, ptr %arrayidx, align 8
  %arrayidx12 = getelementptr inbounds %struct.inspect_extent_util_stats_s, ptr %oldp, i64 %i.020
  %nregs = getelementptr inbounds %struct.inspect_extent_util_stats_s, ptr %oldp, i64 %i.020, i32 1
  %size = getelementptr inbounds %struct.inspect_extent_util_stats_s, ptr %oldp, i64 %i.020, i32 2
  tail call void @inspect_extent_util_stats_get(ptr noundef %tsd, ptr noundef %2, ptr noundef %arrayidx12, ptr noundef nonnull %nregs, ptr noundef nonnull %size) #14
  %inc = add nuw nsw i64 %i.020, 1
  %exitcond.not = icmp eq i64 %inc, %div17
  br i1 %exitcond.not, label %label_return, label %for.body, !llvm.loop !30

label_return:                                     ; preds = %for.body, %for.cond.preheader, %entry, %lor.lhs.false8
  %ret.0 = phi i32 [ 22, %lor.lhs.false8 ], [ 22, %entry ], [ 0, %for.cond.preheader ], [ 0, %for.body ]
  ret i32 %ret.0
}

declare void @inspect_extent_util_stats_verbose_get(ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef) local_unnamed_addr #1

declare void @inspect_extent_util_stats_get(ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef) local_unnamed_addr #1

; Function Attrs: nounwind uwtable
define internal ptr @experimental_arenas_i_index(ptr noundef %tsdn, ptr nocapture readnone %mib, i64 %miblen, i64 noundef %i) #0 {
entry:
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %entry
  %0 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %0, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %1 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %1, %tsdn
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsdn, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %2 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %2, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  switch i64 %i, label %sw.default.i.i [
    i64 4096, label %ctl_arenas_i_verify.exit
    i64 4097, label %sw.bb2.i.i
  ]

sw.bb2.i.i:                                       ; preds = %malloc_mutex_lock.exit
  br label %ctl_arenas_i_verify.exit

sw.default.i.i:                                   ; preds = %malloc_mutex_lock.exit
  %3 = load ptr, ptr @ctl_arenas, align 8
  %narenas.i.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %3, i64 0, i32 1
  %4 = load i32, ptr %narenas.i.i, align 8
  %conv.i.i = zext i32 %4 to i64
  %cmp.i.i = icmp eq i64 %conv.i.i, %i
  br i1 %cmp.i.i, label %ctl_arenas_i_verify.exit, label %if.else.i.i

if.else.i.i:                                      ; preds = %sw.default.i.i
  %cmp9.not.i.i = icmp ule i64 %conv.i.i, %i
  %conv13.i.i = trunc i64 %i to i32
  %add.i.i = add i32 %conv13.i.i, 2
  %cmp.i = icmp eq i32 %add.i.i, -1
  %or.cond.i = or i1 %cmp.i, %cmp9.not.i.i
  br i1 %or.cond.i, label %ctl_arenas_i_verify.exit.thread, label %ctl_arenas_i_verify.exit

ctl_arenas_i_verify.exit:                         ; preds = %malloc_mutex_lock.exit, %sw.bb2.i.i, %sw.default.i.i, %if.else.i.i
  %a.0.i4.i = phi i32 [ 0, %sw.default.i.i ], [ 0, %malloc_mutex_lock.exit ], [ 1, %sw.bb2.i.i ], [ %add.i.i, %if.else.i.i ]
  %conv.i = zext i32 %a.0.i4.i to i64
  %5 = load ptr, ptr @ctl_arenas, align 8
  %arrayidx.i = getelementptr inbounds %struct.ctl_arenas_s, ptr %5, i64 0, i32 3, i64 %conv.i
  %6 = load ptr, ptr %arrayidx.i, align 8
  %initialized.i = getelementptr inbounds %struct.ctl_arena_s, ptr %6, i64 0, i32 1
  %7 = load i8, ptr %initialized.i, align 4
  %.fr4 = freeze i8 %7
  %8 = and i8 %.fr4, 1
  %tobool.not.i = icmp eq i8 %8, 0
  br i1 %tobool.not.i, label %ctl_arenas_i_verify.exit.thread, label %9

ctl_arenas_i_verify.exit.thread:                  ; preds = %if.else.i.i, %ctl_arenas_i_verify.exit
  br label %9

9:                                                ; preds = %ctl_arenas_i_verify.exit, %ctl_arenas_i_verify.exit.thread
  %10 = phi ptr [ null, %ctl_arenas_i_verify.exit.thread ], [ @super_experimental_arenas_i_node, %ctl_arenas_i_verify.exit ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  ret ptr %10
}

; Function Attrs: nounwind uwtable
define internal i32 @experimental_arenas_i_pactivep_ctl(ptr noundef %tsd, ptr nocapture noundef readonly %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readnone %newp, i64 noundef %newlen) #0 {
entry:
  %pactivep = alloca ptr, align 8
  %cmp = icmp eq ptr %oldp, null
  %cmp1 = icmp eq ptr %oldlenp, null
  %or.cond = or i1 %cmp, %cmp1
  br i1 %or.cond, label %return, label %lor.lhs.false2

lor.lhs.false2:                                   ; preds = %entry
  %0 = load i64, ptr %oldlenp, align 8
  %cmp3.not = icmp eq i64 %0, 8
  br i1 %cmp3.not, label %if.end, label %return

if.end:                                           ; preds = %lor.lhs.false2
  %call.i.i = tail call i32 @pthread_mutex_trylock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  %cmp.i.not.i = icmp eq i32 %call.i.i, 0
  br i1 %cmp.i.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %if.end
  tail call void @malloc_mutex_lock_slow(ptr noundef nonnull @ctl_mtx) #14
  store atomic i8 1, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  br label %if.end.i

if.end.i:                                         ; preds = %if.then.i, %if.end
  %1 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %inc.i.i = add i64 %1, 1
  store i64 %inc.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 8), align 8
  %2 = load ptr, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %cmp.not.i.i = icmp eq ptr %2, %tsd
  br i1 %cmp.not.i.i, label %malloc_mutex_lock.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.end.i
  store ptr %tsd, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 7), align 8
  %3 = load i64, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  %inc2.i.i = add i64 %3, 1
  store i64 %inc2.i.i, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 0, i32 6), align 8
  br label %malloc_mutex_lock.exit

malloc_mutex_lock.exit:                           ; preds = %if.end.i, %if.then.i.i
  %cmp4 = icmp ne ptr %newp, null
  %cmp6 = icmp ne i64 %newlen, 0
  %or.cond1 = or i1 %cmp4, %cmp6
  br i1 %or.cond1, label %label_return, label %do.body9

do.body9:                                         ; preds = %malloc_mutex_lock.exit
  %arrayidx = getelementptr inbounds i64, ptr %mib, i64 2
  %4 = load i64, ptr %arrayidx, align 8
  %cmp10 = icmp ugt i64 %4, 4294967295
  br i1 %cmp10, label %label_return, label %if.end12

if.end12:                                         ; preds = %do.body9
  %conv = trunc i64 %4 to i32
  %call15 = tail call i32 @narenas_total_get() #14
  %cmp16 = icmp ugt i32 %call15, %conv
  br i1 %cmp16, label %land.lhs.true, label %label_return

land.lhs.true:                                    ; preds = %if.end12
  %arrayidx.i = getelementptr inbounds [0 x %struct.atomic_p_t], ptr @arenas, i64 0, i64 %4
  %5 = load atomic i64, ptr %arrayidx.i acquire, align 8
  %cmp20.not = icmp eq i64 %5, 0
  br i1 %cmp20.not, label %label_return, label %if.then22

if.then22:                                        ; preds = %land.lhs.true
  %6 = inttoptr i64 %5 to ptr
  %nactive = getelementptr inbounds %struct.arena_s, ptr %6, i64 0, i32 10, i32 1
  store ptr %nactive, ptr %pactivep, align 8
  %7 = load i64, ptr %oldlenp, align 8
  %cmp30.not = icmp eq i64 %7, 8
  br i1 %cmp30.not, label %if.end35, label %if.then32

if.then32:                                        ; preds = %if.then22
  %spec.select = tail call i64 @llvm.umin.i64(i64 %7, i64 8)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 8 %pactivep, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end35:                                         ; preds = %if.then22
  store ptr %nactive, ptr %oldp, align 8
  br label %label_return

label_return:                                     ; preds = %if.end12, %land.lhs.true, %do.body9, %malloc_mutex_lock.exit, %if.end35, %if.then32
  %ret.0 = phi i32 [ 22, %if.then32 ], [ 0, %if.end35 ], [ 1, %malloc_mutex_lock.exit ], [ 14, %do.body9 ], [ 14, %land.lhs.true ], [ 14, %if.end12 ]
  store atomic i8 0, ptr getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 2) monotonic, align 8
  %call1.i = tail call i32 @pthread_mutex_unlock(ptr noundef nonnull getelementptr inbounds (%struct.malloc_mutex_s, ptr @ctl_mtx, i64 0, i32 0, i32 0, i32 1)) #14
  br label %return

return:                                           ; preds = %entry, %lor.lhs.false2, %label_return
  %retval.0 = phi i32 [ %ret.0, %label_return ], [ 22, %lor.lhs.false2 ], [ 22, %entry ]
  ret i32 %retval.0
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @experimental_prof_recent_alloc_max_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable
define internal i32 @experimental_prof_recent_alloc_dump_ctl(ptr nocapture readnone %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr nocapture readnone %oldp, ptr nocapture readnone %oldlenp, ptr nocapture readnone %newp, i64 %newlen) #2 {
entry:
  ret i32 2
}

declare i64 @batch_alloc(ptr noundef, i64 noundef, i64 noundef, i32 noundef) local_unnamed_addr #1

; Function Attrs: mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable
define internal i32 @experimental_thread_activity_callback_ctl(ptr nocapture noundef %tsd, ptr nocapture readnone %mib, i64 %miblen, ptr noundef writeonly %oldp, ptr noundef %oldlenp, ptr noundef readonly %newp, i64 noundef %newlen) #9 {
entry:
  %t_old.sroa.0 = alloca <2 x ptr>, align 16
  %cant_access_tsd_items_directly_use_a_getter_or_setter_activity_callback_thunk.i = getelementptr inbounds %struct.tsd_s, ptr %tsd, i64 0, i32 26
  %0 = load <2 x ptr>, ptr %cant_access_tsd_items_directly_use_a_getter_or_setter_activity_callback_thunk.i, align 8
  store <2 x ptr> %0, ptr %t_old.sroa.0, align 16
  %cmp = icmp ne ptr %oldp, null
  %cmp1 = icmp ne ptr %oldlenp, null
  %or.cond = and i1 %cmp, %cmp1
  br i1 %or.cond, label %if.then, label %do.end

if.then:                                          ; preds = %entry
  %1 = load i64, ptr %oldlenp, align 8
  %cmp2.not = icmp eq i64 %1, 16
  br i1 %cmp2.not, label %if.end, label %if.then3

if.then3:                                         ; preds = %if.then
  %spec.select = tail call i64 @llvm.umin.i64(i64 %1, i64 16)
  call void @llvm.memcpy.p0.p0.i64(ptr nonnull align 1 %oldp, ptr nonnull align 16 %t_old.sroa.0, i64 %spec.select, i1 false)
  store i64 %spec.select, ptr %oldlenp, align 8
  br label %label_return

if.end:                                           ; preds = %if.then
  %t_old.sroa.0.0.t_old.sroa.0.0.copyload = load <2 x ptr>, ptr %t_old.sroa.0, align 16
  store <2 x ptr> %t_old.sroa.0.0.t_old.sroa.0.0.copyload, ptr %oldp, align 8
  br label %do.end

do.end:                                           ; preds = %entry, %if.end
  %cmp6.not = icmp eq ptr %newp, null
  br i1 %cmp6.not, label %label_return, label %if.then10

if.then10:                                        ; preds = %do.end
  %cmp11.not = icmp eq i64 %newlen, 16
  br i1 %cmp11.not, label %do.end15, label %label_return

do.end15:                                         ; preds = %if.then10
  %2 = load <2 x ptr>, ptr %newp, align 8
  store <2 x ptr> %2, ptr %cant_access_tsd_items_directly_use_a_getter_or_setter_activity_callback_thunk.i, align 8
  br label %label_return

label_return:                                     ; preds = %do.end, %do.end15, %if.then10, %if.then3
  %ret.0 = phi i32 [ 22, %if.then3 ], [ 22, %if.then10 ], [ 0, %do.end15 ], [ 0, %do.end ]
  ret i32 %ret.0
}

; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare i64 @llvm.umin.i64(i64, i64) #12

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(argmem: readwrite)
declare void @llvm.lifetime.start.p0(i64 immarg, ptr nocapture) #13

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(argmem: readwrite)
declare void @llvm.lifetime.end.p0(i64 immarg, ptr nocapture) #13

attributes #0 = { nounwind uwtable "frame-pointer"="all" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #1 = { "frame-pointer"="all" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #2 = { mustprogress nofree norecurse nosync nounwind willreturn memory(none) uwtable "frame-pointer"="all" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #3 = { nounwind "frame-pointer"="all" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #4 = { mustprogress nocallback nofree nounwind willreturn memory(argmem: write) }
attributes #5 = { mustprogress nocallback nofree nosync nounwind willreturn }
attributes #6 = { mustprogress nocallback nofree nosync nounwind speculatable willreturn memory(none) }
attributes #7 = { mustprogress nocallback nofree nounwind willreturn memory(argmem: readwrite) }
attributes #8 = { mustprogress nofree nounwind willreturn memory(argmem: read) "frame-pointer"="all" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #9 = { mustprogress nofree nosync nounwind willreturn memory(argmem: readwrite) uwtable "frame-pointer"="all" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #10 = { mustprogress nofree nosync nounwind willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable "frame-pointer"="all" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #11 = { mustprogress nofree nounwind willreturn memory(readwrite, inaccessiblemem: none) uwtable "frame-pointer"="all" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #12 = { nocallback nofree nosync nounwind speculatable willreturn memory(none) }
attributes #13 = { nocallback nofree nosync nounwind willreturn memory(argmem: readwrite) }
attributes #14 = { nounwind }
attributes #15 = { nounwind willreturn memory(read) }

!llvm.module.flags = !{!0, !1, !2, !3, !4}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 8, !"PIC Level", i32 2}
!2 = !{i32 7, !"PIE Level", i32 2}
!3 = !{i32 7, !"uwtable", i32 2}
!4 = !{i32 7, !"frame-pointer", i32 2}
!5 = !{i32 0, i32 3}
!6 = distinct !{!6, !7}
!7 = !{!"llvm.loop.mustprogress"}
!8 = distinct !{!8, !7}
!9 = distinct !{!9, !7}
!10 = distinct !{!10, !7}
!11 = distinct !{!11, !7}
!12 = distinct !{!12, !7}
!13 = distinct !{!13, !7}
!14 = distinct !{!14, !7}
!15 = distinct !{!15, !7}
!16 = distinct !{!16, !7}
!17 = distinct !{!17, !7}
!18 = distinct !{!18, !7}
!19 = distinct !{!19, !7}
!20 = distinct !{!20, !7}
!21 = !{!22}
!22 = distinct !{!22, !23, !"rtree_leaf_elm_read: %agg.result"}
!23 = distinct !{!23, !"rtree_leaf_elm_read"}
!24 = !{!25}
!25 = distinct !{!25, !26, !"rtree_leaf_elm_bits_decode: %agg.result"}
!26 = distinct !{!26, !"rtree_leaf_elm_bits_decode"}
!27 = distinct !{!27, !7}
!28 = distinct !{!28, !7}
!29 = distinct !{!29, !7}
!30 = distinct !{!30, !7}
