; ModuleID = 'bench/qemu/original/accel_tcg_user-exec.c.ll'
source_filename = "bench/qemu/original/accel_tcg_user-exec.c.ll"
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

%struct.RBRootLeftCached = type { %struct.RBRoot, ptr }
%struct.RBRoot = type { ptr }
%struct.IntervalTreeNode = type { %struct.RBNode, i64, i64, i64 }
%struct.RBNode = type { i64, ptr, ptr }
%struct.PageFlagsNode = type { %struct.rcu_head, %struct.IntervalTreeNode, i32 }
%struct.rcu_head = type { ptr, ptr }
%struct.CPUState = type { %struct.DeviceState, ptr, i32, i32, ptr, i32, i8, i8, ptr, i8, i8, i8, i8, i8, i8, i8, i8, i32, i32, i32, i32, i64, i64, i64, [1 x %struct.__jmp_buf_tag], %struct.QemuMutex, %struct.anon, ptr, i32, ptr, ptr, ptr, ptr, i32, i32, %union.anon, %union.anon.0, %union.anon.1, ptr, ptr, i64, i32, ptr, ptr, ptr, i32, i64, i32, %struct.QemuLockCnt, [1 x i64], ptr, i32, i32, i32, i32, i32, ptr, i8, i8, i64, i8, i8, ptr, [8 x i8], [0 x i8], %struct.CPUNegativeOffsetState }
%struct.DeviceState = type { %struct.Object, ptr, ptr, i8, i8, i64, ptr, i32, i8, ptr, %struct.NamedGPIOListHead, %struct.NamedClockListHead, %struct.BusStateHead, i32, i32, i32, %struct.ResettableState, ptr, %struct.MemReentrancyGuard }
%struct.Object = type { ptr, ptr, ptr, i32, ptr }
%struct.NamedGPIOListHead = type { ptr }
%struct.NamedClockListHead = type { ptr }
%struct.BusStateHead = type { ptr }
%struct.ResettableState = type { i32, i8, i8 }
%struct.MemReentrancyGuard = type { i8 }
%struct.__jmp_buf_tag = type { [8 x i64], i32, %struct.__sigset_t }
%struct.__sigset_t = type { [16 x i64] }
%struct.QemuMutex = type { %union.pthread_mutex_t, i8 }
%union.pthread_mutex_t = type { %struct.__pthread_mutex_s }
%struct.__pthread_mutex_s = type { i32, i32, i32, i32, i32, i16, i16, %struct.__pthread_internal_list }
%struct.__pthread_internal_list = type { ptr, ptr }
%struct.anon = type { ptr, ptr }
%union.anon = type { %struct.QTailQLink }
%struct.QTailQLink = type { ptr, ptr }
%union.anon.0 = type { %struct.QTailQLink }
%union.anon.1 = type { %struct.QTailQLink }
%struct.QemuLockCnt = type { i32 }
%struct.CPUNegativeOffsetState = type { %struct.CPUTLB, %union.IcountDecr, i8, [11 x i8] }
%struct.CPUTLB = type { %struct.CPUTLBCommon, [16 x %struct.CPUTLBDesc], [16 x %struct.CPUTLBDescFast] }
%struct.CPUTLBCommon = type { %struct.QemuSpin, i16, i64, i64, i64 }
%struct.QemuSpin = type { i32 }
%struct.CPUTLBDesc = type { i64, i64, i64, i64, i64, i64, [8 x %union.CPUTLBEntry], [8 x %struct.CPUTLBEntryFull], ptr }
%union.CPUTLBEntry = type { %struct.anon.2 }
%struct.anon.2 = type { i64, i64, i64, i64 }
%struct.CPUTLBEntryFull = type { i64, i64, %struct.MemTxAttrs, i8, i8, [3 x i8], %union.anon.3 }
%struct.MemTxAttrs = type { i32 }
%union.anon.3 = type { %struct.anon.4 }
%struct.anon.4 = type { i8, i8, i8 }
%struct.CPUTLBDescFast = type { i64, ptr }
%union.IcountDecr = type { i32 }

@helper_retaddr = dso_local thread_local global i64 0, align 8
@.str = private unnamed_addr constant [30 x i8] c"../qemu/accel/tcg/user-exec.c\00", align 1
@__func__.handle_sigsegv_accerr_write = private unnamed_addr constant [28 x i8] c"handle_sigsegv_accerr_write\00", align 1
@pageflags_root = internal global %struct.RBRootLeftCached zeroinitializer, align 8
@.str.1 = private unnamed_addr constant [19 x i8] c"%-*s %-*s %-*s %s\0A\00", align 1
@.str.2 = private unnamed_addr constant [6 x i8] c"start\00", align 1
@.str.3 = private unnamed_addr constant [4 x i8] c"end\00", align 1
@.str.4 = private unnamed_addr constant [5 x i8] c"size\00", align 1
@.str.5 = private unnamed_addr constant [5 x i8] c"prot\00", align 1
@.str.6 = private unnamed_addr constant [14 x i8] c"start <= last\00", align 1
@__PRETTY_FUNCTION__.page_set_flags = private unnamed_addr constant [53 x i8] c"void page_set_flags(target_ulong, target_ulong, int)\00", align 1
@reserved_va = external local_unnamed_addr global i64, align 8
@.str.7 = private unnamed_addr constant [23 x i8] c"last <= GUEST_ADDR_MAX\00", align 1
@.str.8 = private unnamed_addr constant [45 x i8] c"!(flags & PAGE_ANON) || (flags & PAGE_RESET)\00", align 1
@.str.9 = private unnamed_addr constant [14 x i8] c"last >= start\00", align 1
@__PRETTY_FUNCTION__.page_check_range_empty = private unnamed_addr constant [57 x i8] c"_Bool page_check_range_empty(target_ulong, target_ulong)\00", align 1
@.str.10 = private unnamed_addr constant [11 x i8] c"min <= max\00", align 1
@__PRETTY_FUNCTION__.page_find_range_empty = private unnamed_addr constant [91 x i8] c"target_ulong page_find_range_empty(target_ulong, target_ulong, target_ulong, target_ulong)\00", align 1
@.str.11 = private unnamed_addr constant [22 x i8] c"max <= GUEST_ADDR_MAX\00", align 1
@.str.12 = private unnamed_addr constant [9 x i8] c"len != 0\00", align 1
@.str.13 = private unnamed_addr constant [21 x i8] c"is_power_of_2(align)\00", align 1
@qemu_host_page_size = external local_unnamed_addr global i64, align 8
@qemu_host_page_mask = external local_unnamed_addr global i64, align 8
@.str.14 = private unnamed_addr constant [39 x i8] c"TARGET_PAGE_SIZE < qemu_host_page_size\00", align 1
@__PRETTY_FUNCTION__.page_protect = private unnamed_addr constant [34 x i8] c"void page_protect(tb_page_addr_t)\00", align 1
@__func__.probe_access_flags = private unnamed_addr constant [19 x i8] c"probe_access_flags\00", align 1
@.str.15 = private unnamed_addr constant [35 x i8] c"-(addr | TARGET_PAGE_MASK) >= size\00", align 1
@__func__.probe_access = private unnamed_addr constant [13 x i8] c"probe_access\00", align 1
@.str.16 = private unnamed_addr constant [25 x i8] c"(flags & ~TLB_MMIO) == 0\00", align 1
@__func__.get_page_addr_code_hostp = private unnamed_addr constant [25 x i8] c"get_page_addr_code_hostp\00", align 1
@.str.17 = private unnamed_addr constant [11 x i8] c"flags == 0\00", align 1
@.str.18 = private unnamed_addr constant [38 x i8] c"../qemu/accel/tcg/atomic_common.c.inc\00", align 1
@__func__.helper_nonatomic_cmpxchgo = private unnamed_addr constant [26 x i8] c"helper_nonatomic_cmpxchgo\00", align 1
@.str.19 = private unnamed_addr constant [29 x i8] c"%016lx-%016lx %016lx %c%c%c\0A\00", align 1
@guest_base = external local_unnamed_addr global i64, align 8
@__func__.probe_access_internal = private unnamed_addr constant [22 x i8] c"probe_access_internal\00", align 1
@cpuinfo = external local_unnamed_addr global i32, align 4
@.str.20 = private unnamed_addr constant [39 x i8] c"../qemu/accel/tcg/ldst_atomicity.c.inc\00", align 1
@__func__.load_atom_2 = private unnamed_addr constant [12 x i8] c"load_atom_2\00", align 1
@__func__.required_atomicity = private unnamed_addr constant [19 x i8] c"required_atomicity\00", align 1
@.str.21 = private unnamed_addr constant [13 x i8] c"h2g_valid(p)\00", align 1
@__PRETTY_FUNCTION__.load_atomic16_or_exit = private unnamed_addr constant [60 x i8] c"Int128 load_atomic16_or_exit(CPUState *, uintptr_t, void *)\00", align 1
@__func__.load_atom_4 = private unnamed_addr constant [12 x i8] c"load_atom_4\00", align 1
@__func__.load_atom_16 = private unnamed_addr constant [13 x i8] c"load_atom_16\00", align 1
@__func__.store_atom_2 = private unnamed_addr constant [13 x i8] c"store_atom_2\00", align 1
@__func__.store_atom_4 = private unnamed_addr constant [13 x i8] c"store_atom_4\00", align 1
@__func__.store_atom_8 = private unnamed_addr constant [13 x i8] c"store_atom_8\00", align 1
@__func__.store_atom_16 = private unnamed_addr constant [14 x i8] c"store_atom_16\00", align 1

; Function Attrs: mustprogress nofree nosync nounwind sspstrong willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable
define dso_local i32 @adjust_signal_pc(ptr nocapture noundef %pc, i1 noundef zeroext %is_write) local_unnamed_addr #0 {
entry:
  %0 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  %1 = load i64, ptr %0, align 8
  switch i64 %1, label %sw.epilog [
    i64 0, label %sw.bb
    i64 1, label %sw.bb1
  ]

sw.bb:                                            ; preds = %entry
  %2 = load i64, ptr %pc, align 8
  %add = add i64 %2, 2
  br label %sw.epilog

sw.bb1:                                           ; preds = %entry
  store i64 0, ptr %pc, align 8
  br label %return

sw.epilog:                                        ; preds = %entry, %sw.bb
  %storemerge = phi i64 [ %add, %sw.bb ], [ %1, %entry ]
  store i64 %storemerge, ptr %pc, align 8
  %cond = zext i1 %is_write to i32
  br label %return

return:                                           ; preds = %sw.epilog, %sw.bb1
  %retval.0 = phi i32 [ %cond, %sw.epilog ], [ 2, %sw.bb1 ]
  ret i32 %retval.0
}

; Function Attrs: mustprogress nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare nonnull ptr @llvm.threadlocal.address.p0(ptr nonnull) #1

; Function Attrs: nounwind sspstrong uwtable
define dso_local zeroext i1 @handle_sigsegv_accerr_write(ptr noundef %cpu, ptr noundef %old_set, i64 noundef %host_pc, i64 noundef %guest_addr) local_unnamed_addr #2 {
entry:
  %call = tail call i32 @page_unprotect(i64 noundef %guest_addr, i64 noundef %host_pc), !range !5
  switch i32 %call, label %do.body [
    i32 0, label %sw.epilog
    i32 1, label %sw.bb1
    i32 2, label %sw.bb2
  ]

sw.bb1:                                           ; preds = %entry
  br label %sw.epilog

sw.bb2:                                           ; preds = %entry
  %call3 = tail call i32 @sigprocmask(i32 noundef 2, ptr noundef %old_set, ptr noundef null) #16
  tail call void @cpu_loop_exit_noexc(ptr noundef %cpu) #17
  unreachable

do.body:                                          ; preds = %entry
  tail call void @g_assertion_message_expr(ptr noundef null, ptr noundef nonnull @.str, i32 noundef 136, ptr noundef nonnull @__func__.handle_sigsegv_accerr_write, ptr noundef null) #17
  unreachable

sw.epilog:                                        ; preds = %entry, %sw.bb1
  %retval.0 = phi i1 [ true, %sw.bb1 ], [ false, %entry ]
  ret i1 %retval.0
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @page_unprotect(i64 noundef %address, i64 noundef %pc) local_unnamed_addr #2 {
entry:
  tail call void @mmap_lock() #16
  %call.i = tail call ptr @interval_tree_iter_first(ptr noundef nonnull @pageflags_root, i64 noundef %address, i64 noundef %address) #16
  %tobool.not.i = icmp eq ptr %call.i, null
  %add.ptr.i = getelementptr i8, ptr %call.i, i64 -16
  %tobool.not32 = icmp eq ptr %add.ptr.i, null
  %tobool.not = or i1 %tobool.not.i, %tobool.not32
  br i1 %tobool.not, label %return, label %lor.lhs.false

lor.lhs.false:                                    ; preds = %entry
  %flags = getelementptr i8, ptr %call.i, i64 48
  %0 = load i32, ptr %flags, align 8
  %and = and i32 %0, 16
  %tobool1.not = icmp eq i32 %and, 0
  br i1 %tobool1.not, label %return, label %if.end

if.end:                                           ; preds = %lor.lhs.false
  %and3 = and i32 %0, 2
  %tobool4.not = icmp eq i32 %and3, 0
  br i1 %tobool4.not, label %if.else, label %return

if.else:                                          ; preds = %if.end
  %1 = load i64, ptr @qemu_host_page_size, align 8
  %cmp = icmp ult i64 %1, 4097
  br i1 %cmp, label %if.then6, label %if.else11

if.then6:                                         ; preds = %if.else
  %and7 = and i64 %address, -4096
  %or = or disjoint i32 %0, 2
  %sub = or i64 %address, 4095
  %call9 = tail call fastcc zeroext i1 @pageflags_set_clear(i64 noundef %and7, i64 noundef %sub, i32 noundef 2, i32 noundef 0)
  %call10 = tail call zeroext i1 @tb_invalidate_phys_page_unwind(i64 noundef %and7, i64 noundef %pc) #16
  %frombool = zext i1 %call10 to i8
  br label %if.end37

if.else11:                                        ; preds = %if.else
  %2 = load i64, ptr @qemu_host_page_mask, align 8
  %and12 = and i64 %2, %address
  br label %for.body

for.body:                                         ; preds = %if.else11, %if.end29
  %prot.036 = phi i32 [ 0, %if.else11 ], [ %prot.1, %if.end29 ]
  %i.035 = phi i64 [ 0, %if.else11 ], [ %add36, %if.end29 ]
  %current_tb_invalidated.034 = phi i8 [ 0, %if.else11 ], [ %5, %if.end29 ]
  %add14 = add i64 %i.035, %and12
  %call.i28 = tail call ptr @interval_tree_iter_first(ptr noundef nonnull @pageflags_root, i64 noundef %add14, i64 noundef %add14) #16
  %tobool.not.i29 = icmp eq ptr %call.i28, null
  %add.ptr.i30 = getelementptr i8, ptr %call.i28, i64 -16
  %tobool16.not33 = icmp eq ptr %add.ptr.i30, null
  %tobool16.not = or i1 %tobool.not.i29, %tobool16.not33
  br i1 %tobool16.not, label %if.end29, label %if.then17

if.then17:                                        ; preds = %for.body
  %flags18 = getelementptr i8, ptr %call.i28, i64 48
  %3 = load i32, ptr %flags18, align 8
  %or19 = or i32 %3, %prot.036
  %and21 = and i32 %3, 16
  %tobool22.not = icmp eq i32 %and21, 0
  br i1 %tobool22.not, label %if.end29, label %if.then23

if.then23:                                        ; preds = %if.then17
  %or24 = or i32 %or19, 2
  %sub26 = add i64 %add14, 4095
  %call27 = tail call fastcc zeroext i1 @pageflags_set_clear(i64 noundef %add14, i64 noundef %sub26, i32 noundef 2, i32 noundef 0)
  br label %if.end29

if.end29:                                         ; preds = %if.then17, %if.then23, %for.body
  %prot.1 = phi i32 [ %or24, %if.then23 ], [ %or19, %if.then17 ], [ %prot.036, %for.body ]
  %call30 = tail call zeroext i1 @tb_invalidate_phys_page_unwind(i64 noundef %add14, i64 noundef %pc) #16
  %4 = zext i1 %call30 to i8
  %5 = or i8 %current_tb_invalidated.034, %4
  %add36 = add i64 %i.035, 4096
  %cmp13 = icmp ult i64 %add36, %1
  br i1 %cmp13, label %for.body, label %if.end37, !llvm.loop !6

if.end37:                                         ; preds = %if.end29, %if.then6
  %current_tb_invalidated.1 = phi i8 [ %frombool, %if.then6 ], [ %5, %if.end29 ]
  %start.0 = phi i64 [ %and7, %if.then6 ], [ %and12, %if.end29 ]
  %len.0 = phi i64 [ 4096, %if.then6 ], [ %1, %if.end29 ]
  %prot.2 = phi i32 [ %or, %if.then6 ], [ %prot.1, %if.end29 ]
  %and38 = and i32 %prot.2, 4
  %tobool39.not = icmp eq i32 %and38, 0
  %and41 = and i32 %prot.2, 2
  %or42 = or disjoint i32 %and41, 1
  %prot.3 = select i1 %tobool39.not, i32 %prot.2, i32 %or42
  %6 = load i64, ptr @guest_base, align 8
  %add.i = add i64 %6, %start.0
  %7 = inttoptr i64 %add.i to ptr
  %and45 = and i32 %prot.3, 7
  %call46 = tail call i32 @mprotect(ptr noundef %7, i64 noundef %len.0, i32 noundef %and45) #16
  %8 = and i8 %current_tb_invalidated.1, 1
  %.not = icmp eq i8 %8, 0
  %9 = select i1 %.not, i32 1, i32 2
  br label %return

return:                                           ; preds = %if.end37, %if.end, %entry, %lor.lhs.false
  %retval.0 = phi i32 [ 0, %lor.lhs.false ], [ 0, %entry ], [ 1, %if.end ], [ %9, %if.end37 ]
  tail call void @mmap_unlock() #16
  ret i32 %retval.0
}

; Function Attrs: nounwind
declare i32 @sigprocmask(i32 noundef, ptr noundef, ptr noundef) local_unnamed_addr #3

; Function Attrs: noreturn
declare void @cpu_loop_exit_noexc(ptr noundef) local_unnamed_addr #4

; Function Attrs: noreturn
declare void @g_assertion_message_expr(ptr noundef, ptr noundef, i32 noundef, ptr noundef, ptr noundef) local_unnamed_addr #4

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @walk_memory_regions(ptr noundef %priv, ptr nocapture noundef readonly %fn) local_unnamed_addr #2 {
entry:
  tail call void @mmap_lock() #16
  %call = tail call ptr @interval_tree_iter_first(ptr noundef nonnull @pageflags_root, i64 noundef 0, i64 noundef -1) #16
  %cmp.not6 = icmp eq ptr %call, null
  br i1 %cmp.not6, label %for.end, label %for.body

for.body:                                         ; preds = %entry, %for.inc
  %n.07 = phi ptr [ %call4, %for.inc ], [ %call, %entry ]
  %start = getelementptr inbounds %struct.IntervalTreeNode, ptr %n.07, i64 0, i32 1
  %0 = load i64, ptr %start, align 8
  %last = getelementptr inbounds %struct.IntervalTreeNode, ptr %n.07, i64 0, i32 2
  %1 = load i64, ptr %last, align 8
  %add = add i64 %1, 1
  %flags = getelementptr i8, ptr %n.07, i64 48
  %2 = load i32, ptr %flags, align 8
  %conv = sext i32 %2 to i64
  %call1 = tail call i32 %fn(ptr noundef %priv, i64 noundef %0, i64 noundef %add, i64 noundef %conv) #16
  %cmp2.not = icmp eq i32 %call1, 0
  br i1 %cmp2.not, label %for.inc, label %for.end

for.inc:                                          ; preds = %for.body
  %call4 = tail call ptr @interval_tree_iter_next(ptr noundef nonnull %n.07, i64 noundef 0, i64 noundef -1) #16
  %cmp.not = icmp eq ptr %call4, null
  br i1 %cmp.not, label %for.end, label %for.body, !llvm.loop !8

for.end:                                          ; preds = %for.inc, %for.body, %entry
  %rc.1 = phi i32 [ 0, %entry ], [ %call1, %for.body ], [ 0, %for.inc ]
  tail call void @mmap_unlock() #16
  ret i32 %rc.1
}

declare void @mmap_lock() local_unnamed_addr #5

declare ptr @interval_tree_iter_first(ptr noundef, i64 noundef, i64 noundef) local_unnamed_addr #5

declare ptr @interval_tree_iter_next(ptr noundef, i64 noundef, i64 noundef) local_unnamed_addr #5

declare void @mmap_unlock() local_unnamed_addr #5

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @page_dump(ptr nocapture noundef %f) local_unnamed_addr #2 {
entry:
  %call = tail call i32 (ptr, ptr, ...) @fprintf(ptr noundef %f, ptr noundef nonnull @.str.1, i32 noundef 16, ptr noundef nonnull @.str.2, i32 noundef 16, ptr noundef nonnull @.str.3, i32 noundef 16, ptr noundef nonnull @.str.4, ptr noundef nonnull @.str.5)
  tail call void @mmap_lock() #16
  %call.i = tail call ptr @interval_tree_iter_first(ptr noundef nonnull @pageflags_root, i64 noundef 0, i64 noundef -1) #16
  %cmp.not6.i = icmp eq ptr %call.i, null
  br i1 %cmp.not6.i, label %walk_memory_regions.exit, label %for.inc.i

for.inc.i:                                        ; preds = %entry, %for.inc.i
  %n.07.i = phi ptr [ %call4.i, %for.inc.i ], [ %call.i, %entry ]
  %start.i = getelementptr inbounds %struct.IntervalTreeNode, ptr %n.07.i, i64 0, i32 1
  %0 = load i64, ptr %start.i, align 8
  %last.i = getelementptr inbounds %struct.IntervalTreeNode, ptr %n.07.i, i64 0, i32 2
  %1 = load i64, ptr %last.i, align 8
  %add.i = add i64 %1, 1
  %flags.i = getelementptr i8, ptr %n.07.i, i64 48
  %2 = load i32, ptr %flags.i, align 8
  %conv.i3 = zext i32 %2 to i64
  %sub.i = sub i64 %add.i, %0
  %and.i = and i64 %conv.i3, 1
  %tobool.not.i = icmp eq i64 %and.i, 0
  %cond.i = select i1 %tobool.not.i, i32 45, i32 114
  %and1.i = and i64 %conv.i3, 2
  %tobool2.not.i = icmp eq i64 %and1.i, 0
  %cond3.i = select i1 %tobool2.not.i, i32 45, i32 119
  %and4.i = and i64 %conv.i3, 4
  %tobool5.not.i = icmp eq i64 %and4.i, 0
  %cond6.i = select i1 %tobool5.not.i, i32 45, i32 120
  %call.i2 = tail call i32 (ptr, ptr, ...) @fprintf(ptr noundef %f, ptr noundef nonnull @.str.19, i64 noundef %0, i64 noundef %add.i, i64 noundef %sub.i, i32 noundef %cond.i, i32 noundef %cond3.i, i32 noundef %cond6.i)
  %call4.i = tail call ptr @interval_tree_iter_next(ptr noundef nonnull %n.07.i, i64 noundef 0, i64 noundef -1) #16
  %cmp.not.i = icmp eq ptr %call4.i, null
  br i1 %cmp.not.i, label %walk_memory_regions.exit, label %for.inc.i, !llvm.loop !8

walk_memory_regions.exit:                         ; preds = %for.inc.i, %entry
  tail call void @mmap_unlock() #16
  ret void
}

; Function Attrs: nofree nounwind
declare noundef i32 @fprintf(ptr nocapture noundef, ptr nocapture noundef readonly, ...) local_unnamed_addr #6

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @page_get_flags(i64 noundef %address) local_unnamed_addr #2 {
entry:
  %call.i = tail call ptr @interval_tree_iter_first(ptr noundef nonnull @pageflags_root, i64 noundef %address, i64 noundef %address) #16
  %tobool.not.i = icmp eq ptr %call.i, null
  %add.ptr.i = getelementptr i8, ptr %call.i, i64 -16
  %tobool.not11 = icmp eq ptr %add.ptr.i, null
  %tobool.not = or i1 %tobool.not.i, %tobool.not11
  br i1 %tobool.not, label %if.end, label %return.sink.split

if.end:                                           ; preds = %entry
  %call1 = tail call zeroext i1 @have_mmap_lock() #16
  br i1 %call1, label %return, label %if.end3

if.end3:                                          ; preds = %if.end
  tail call void @mmap_lock() #16
  %call.i7 = tail call ptr @interval_tree_iter_first(ptr noundef nonnull @pageflags_root, i64 noundef %address, i64 noundef %address) #16
  %tobool.not.i8 = icmp eq ptr %call.i7, null
  %add.ptr.i9 = getelementptr i8, ptr %call.i7, i64 -16
  tail call void @mmap_unlock() #16
  %tobool5.not12 = icmp eq ptr %add.ptr.i9, null
  %tobool5.not = or i1 %tobool.not.i8, %tobool5.not12
  br i1 %tobool5.not, label %return, label %return.sink.split

return.sink.split:                                ; preds = %if.end3, %entry
  %call.i7.sink = phi ptr [ %call.i, %entry ], [ %call.i7, %if.end3 ]
  %flags6 = getelementptr i8, ptr %call.i7.sink, i64 48
  %0 = load i32, ptr %flags6, align 8
  br label %return

return:                                           ; preds = %return.sink.split, %if.end3, %if.end
  %retval.0 = phi i32 [ 0, %if.end ], [ 0, %if.end3 ], [ %0, %return.sink.split ]
  ret i32 %retval.0
}

declare zeroext i1 @have_mmap_lock() local_unnamed_addr #5

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @page_set_flags(i64 noundef %start, i64 noundef %last, i32 noundef %flags) local_unnamed_addr #2 {
entry:
  %cmp.not = icmp ugt i64 %start, %last
  br i1 %cmp.not, label %if.else, label %if.end

if.else:                                          ; preds = %entry
  tail call void @__assert_fail(ptr noundef nonnull @.str.6, ptr noundef nonnull @.str, i32 noundef 492, ptr noundef nonnull @__PRETTY_FUNCTION__.page_set_flags) #17
  unreachable

if.end:                                           ; preds = %entry
  %0 = load i64, ptr @reserved_va, align 8
  %tobool.not = icmp ne i64 %0, 0
  %cmp1.not24 = icmp ult i64 %0, %last
  %cmp1.not = and i1 %tobool.not, %cmp1.not24
  br i1 %cmp1.not, label %if.else3, label %if.end4

if.else3:                                         ; preds = %if.end
  tail call void @__assert_fail(ptr noundef nonnull @.str.7, ptr noundef nonnull @.str, i32 noundef 493, ptr noundef nonnull @__PRETTY_FUNCTION__.page_set_flags) #17
  unreachable

if.end4:                                          ; preds = %if.end
  %1 = and i32 %flags, 192
  %or.cond = icmp eq i32 %1, 128
  br i1 %or.cond, label %if.else9, label %do.body

if.else9:                                         ; preds = %if.end4
  tail call void @__assert_fail(ptr noundef nonnull @.str.8, ptr noundef nonnull @.str, i32 noundef 495, ptr noundef nonnull @__PRETTY_FUNCTION__.page_set_flags) #17
  unreachable

do.body:                                          ; preds = %if.end4
  %call = tail call zeroext i1 @have_mmap_lock() #16
  tail call void @llvm.assume(i1 %call)
  %and13 = and i64 %start, -4096
  %or = or i64 %last, 4095
  %and14 = and i32 %flags, 8
  %tobool15.not = icmp eq i32 %and14, 0
  br i1 %tobool15.not, label %if.then30, label %if.end26

if.end26:                                         ; preds = %do.body
  %and20 = and i32 %flags, -65
  %and21 = shl i32 %flags, 3
  %2 = and i32 %and21, 16
  %spec.select = or i32 %2, %and20
  %tobool27.not = icmp eq i32 %spec.select, 0
  %tobool27.not.not = xor i1 %tobool27.not, true
  %3 = and i32 %flags, 64
  %tobool29.not = icmp eq i32 %3, 0
  %or.cond25 = and i1 %tobool29.not, %tobool27.not.not
  br i1 %or.cond25, label %if.then39.thread, label %if.then30

if.then30:                                        ; preds = %do.body, %if.end26
  %tobool29.not36 = phi i1 [ %tobool29.not, %if.end26 ], [ true, %do.body ]
  %tobool27.not34 = phi i1 [ %tobool27.not, %if.end26 ], [ true, %do.body ]
  %flags.addr.032 = phi i32 [ %spec.select, %if.end26 ], [ 0, %do.body ]
  %call.i30.i = tail call ptr @interval_tree_iter_first(ptr noundef nonnull @pageflags_root, i64 noundef %and13, i64 noundef %or) #16
  %tobool.not.i31.i = icmp eq ptr %call.i30.i, null
  %add.ptr.i32.i = getelementptr i8, ptr %call.i30.i, i64 -16
  %tobool.not2033.i = icmp eq ptr %add.ptr.i32.i, null
  %tobool.not34.i = or i1 %tobool.not.i31.i, %tobool.not2033.i
  br i1 %tobool.not34.i, label %pageflags_unset.exit, label %if.end.lr.ph.i

if.end.lr.ph.i:                                   ; preds = %if.then30
  %sub.i = add i64 %and13, -1
  br label %if.end.i

if.end.i:                                         ; preds = %if.end24.i, %if.end.lr.ph.i
  %add.ptr.i37.i = phi ptr [ %add.ptr.i32.i, %if.end.lr.ph.i ], [ %add.ptr.i.i, %if.end24.i ]
  %call.i36.i = phi ptr [ %call.i30.i, %if.end.lr.ph.i ], [ %call.i.i, %if.end24.i ]
  %inval_tb.035.i = phi i8 [ 0, %if.end.lr.ph.i ], [ %spec.select.i, %if.end24.i ]
  %flags.i = getelementptr i8, ptr %call.i36.i, i64 48
  %4 = load i32, ptr %flags.i, align 8
  %and.i = and i32 %4, 4
  %tobool1.not.i = icmp eq i32 %and.i, 0
  %spec.select.i = select i1 %tobool1.not.i, i8 %inval_tb.035.i, i8 1
  tail call void @interval_tree_remove(ptr noundef nonnull %call.i36.i, ptr noundef nonnull @pageflags_root) #16
  %last5.i = getelementptr i8, ptr %call.i36.i, i64 32
  %5 = load i64, ptr %last5.i, align 8
  %start7.i = getelementptr i8, ptr %call.i36.i, i64 24
  %6 = load i64, ptr %start7.i, align 8
  %cmp.i = icmp ult i64 %6, %and13
  br i1 %cmp.i, label %if.then8.i, label %if.else.i

if.then8.i:                                       ; preds = %if.end.i
  store i64 %sub.i, ptr %last5.i, align 8
  tail call void @interval_tree_insert(ptr noundef nonnull %call.i36.i, ptr noundef nonnull @pageflags_root) #16
  %cmp12.i = icmp ugt i64 %5, %or
  br i1 %cmp12.i, label %if.then13.i, label %if.end24.i

if.then13.i:                                      ; preds = %if.then8.i
  %flags.i.le = getelementptr i8, ptr %call.i36.i, i64 48
  %add.i = add nuw i64 %or, 1
  %7 = load i32, ptr %flags.i.le, align 8
  %call.i19.i = tail call noalias dereferenceable_or_null(72) ptr @g_malloc_n(i64 noundef 1, i64 noundef 72) #18
  %itree.i.i = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i19.i, i64 0, i32 1
  %start1.i.i = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i19.i, i64 0, i32 1, i32 1
  store i64 %add.i, ptr %start1.i.i, align 8
  %last3.i.i = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i19.i, i64 0, i32 1, i32 2
  store i64 %5, ptr %last3.i.i, align 8
  %flags4.i.i = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i19.i, i64 0, i32 2
  store i32 %7, ptr %flags4.i.i, align 8
  br label %while.end.sink.split.i

if.else.i:                                        ; preds = %if.end.i
  %cmp16.not.i = icmp ugt i64 %5, %or
  br i1 %cmp16.not.i, label %if.else18.i, label %if.then17.i

if.then17.i:                                      ; preds = %if.else.i
  tail call void @call_rcu1(ptr noundef nonnull %add.ptr.i37.i, ptr noundef nonnull @g_free) #16
  br label %if.end24.i

if.else18.i:                                      ; preds = %if.else.i
  %start7.i.le = getelementptr i8, ptr %call.i36.i, i64 24
  %add19.i = add nuw i64 %or, 1
  store i64 %add19.i, ptr %start7.i.le, align 8
  br label %while.end.sink.split.i

if.end24.i:                                       ; preds = %if.then17.i, %if.then8.i
  %call.i.i = tail call ptr @interval_tree_iter_first(ptr noundef nonnull @pageflags_root, i64 noundef %and13, i64 noundef %or) #16
  %tobool.not.i.i = icmp eq ptr %call.i.i, null
  %add.ptr.i.i = getelementptr i8, ptr %call.i.i, i64 -16
  %tobool.not20.i = icmp eq ptr %add.ptr.i.i, null
  %tobool.not.i = or i1 %tobool.not.i.i, %tobool.not20.i
  br i1 %tobool.not.i, label %pageflags_unset.exit, label %if.end.i

while.end.sink.split.i:                           ; preds = %if.else18.i, %if.then13.i
  %call.i36.lcssa54.sink.i = phi ptr [ %call.i36.i, %if.else18.i ], [ %itree.i.i, %if.then13.i ]
  tail call void @interval_tree_insert(ptr noundef nonnull %call.i36.lcssa54.sink.i, ptr noundef nonnull @pageflags_root) #16
  br label %pageflags_unset.exit

pageflags_unset.exit:                             ; preds = %if.end24.i, %if.then30, %while.end.sink.split.i
  %inval_tb.2.i = phi i8 [ 0, %if.then30 ], [ %spec.select.i, %while.end.sink.split.i ], [ %spec.select.i, %if.end24.i ]
  %8 = and i8 %inval_tb.2.i, 1
  br i1 %tobool27.not34, label %if.end50, label %if.then39

if.then39:                                        ; preds = %pageflags_unset.exit
  %spec.select83 = select i1 %tobool29.not36, i32 -2177, i32 -1
  br label %if.then39.thread

if.then39.thread:                                 ; preds = %if.then39, %if.end26
  %flags.addr.0333968 = phi i32 [ %spec.select, %if.end26 ], [ %flags.addr.032, %if.then39 ]
  %inval_tb.04066 = phi i8 [ 0, %if.end26 ], [ %8, %if.then39 ]
  %9 = phi i32 [ -2177, %if.end26 ], [ %spec.select83, %if.then39 ]
  %call43 = tail call fastcc zeroext i1 @pageflags_set_clear(i64 noundef %and13, i64 noundef %or, i32 noundef %flags.addr.0333968, i32 noundef %9)
  %10 = zext i1 %call43 to i8
  %11 = or i8 %inval_tb.04066, %10
  br label %if.end50

if.end50:                                         ; preds = %pageflags_unset.exit, %if.then39.thread
  %inval_tb.1 = phi i8 [ %11, %if.then39.thread ], [ %8, %pageflags_unset.exit ]
  %tobool51.not = icmp eq i8 %inval_tb.1, 0
  br i1 %tobool51.not, label %if.end53, label %if.then52

if.then52:                                        ; preds = %if.end50
  tail call void @tb_invalidate_phys_range(i64 noundef %and13, i64 noundef %or) #16
  br label %if.end53

if.end53:                                         ; preds = %if.then52, %if.end50
  ret void
}

; Function Attrs: noreturn nounwind
declare void @__assert_fail(ptr noundef, ptr noundef, i32 noundef, ptr noundef) local_unnamed_addr #7

; Function Attrs: mustprogress nofree norecurse nosync nounwind sspstrong willreturn memory(none) uwtable
define dso_local void @page_reset_target_data(i64 noundef %start, i64 noundef %last) local_unnamed_addr #8 {
entry:
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define internal fastcc zeroext i1 @pageflags_set_clear(i64 noundef %start, i64 noundef %last, i32 noundef %set_flags, i32 noundef %clear_flags) unnamed_addr #2 {
entry:
  %call.i203214 = tail call ptr @interval_tree_iter_first(ptr noundef nonnull @pageflags_root, i64 noundef %start, i64 noundef %last) #16
  %tobool.not.i204215 = icmp eq ptr %call.i203214, null
  %add.ptr.i205216 = getelementptr i8, ptr %call.i203214, i64 -16
  %tobool.not130206217 = icmp eq ptr %add.ptr.i205216, null
  %tobool.not207218 = or i1 %tobool.not.i204215, %tobool.not130206217
  br i1 %tobool.not207218, label %if.then, label %if.end3.lr.ph.lr.ph

if.end3.lr.ph.lr.ph:                              ; preds = %entry
  %not = xor i32 %clear_flags, -1
  %and12 = and i32 %set_flags, 2
  %tobool51 = icmp ne i32 %set_flags, 0
  br label %if.end3.lr.ph

if.end3.lr.ph:                                    ; preds = %if.end3.lr.ph.lr.ph, %restart.outer.backedge
  %add.ptr.i205222 = phi ptr [ %add.ptr.i205216, %if.end3.lr.ph.lr.ph ], [ %add.ptr.i205, %restart.outer.backedge ]
  %call.i203221 = phi ptr [ %call.i203214, %if.end3.lr.ph.lr.ph ], [ %call.i203, %restart.outer.backedge ]
  %start.addr.0.ph220 = phi i64 [ %start, %if.end3.lr.ph.lr.ph ], [ %start.addr.0.ph.be, %restart.outer.backedge ]
  %inval_tb.0.ph219 = phi i8 [ 0, %if.end3.lr.ph.lr.ph ], [ %inval_tb.1, %restart.outer.backedge ]
  %sub99 = add i64 %start.addr.0.ph220, -1
  br label %if.end3

if.then:                                          ; preds = %restart.outer.backedge, %restart.backedge, %entry
  %start.addr.0.ph.lcssa194 = phi i64 [ %start, %entry ], [ %start.addr.0.ph220, %restart.backedge ], [ %start.addr.0.ph.be, %restart.outer.backedge ]
  %inval_tb.0.lcssa = phi i8 [ 0, %entry ], [ %inval_tb.1, %restart.backedge ], [ %inval_tb.1, %restart.outer.backedge ]
  %tobool1.not = icmp eq i32 %set_flags, 0
  br i1 %tobool1.not, label %done, label %if.then2

if.then2:                                         ; preds = %if.then
  %cmp.not.i = icmp eq i64 %start.addr.0.ph.lcssa194, 0
  br i1 %cmp.not.i, label %if.end7.i, label %if.then.i

if.then.i:                                        ; preds = %if.then2
  %sub.i = add i64 %start.addr.0.ph.lcssa194, -1
  %call.i.i = tail call ptr @interval_tree_iter_first(ptr noundef nonnull @pageflags_root, i64 noundef %sub.i, i64 noundef %sub.i) #16
  %tobool.not.i.i = icmp eq ptr %call.i.i, null
  %add.ptr.i.i = getelementptr i8, ptr %call.i.i, i64 -16
  %tobool.not30.i = icmp eq ptr %add.ptr.i.i, null
  %tobool.not.i103 = or i1 %tobool.not.i.i, %tobool.not30.i
  br i1 %tobool.not.i103, label %if.end7.i, label %if.then2.i

if.then2.i:                                       ; preds = %if.then.i
  %flags3.i = getelementptr i8, ptr %call.i.i, i64 48
  %0 = load i32, ptr %flags3.i, align 8
  %cmp4.i = icmp eq i32 %0, %set_flags
  br i1 %cmp4.i, label %if.then5.i, label %if.end7.i

if.then5.i:                                       ; preds = %if.then2.i
  tail call void @interval_tree_remove(ptr noundef nonnull %call.i.i, ptr noundef nonnull @pageflags_root) #16
  br label %if.end7.i

if.end7.i:                                        ; preds = %if.then5.i, %if.then2.i, %if.then.i, %if.then2
  %prev.0.i = phi ptr [ %add.ptr.i.i, %if.then5.i ], [ null, %if.then.i ], [ null, %if.then2 ], [ null, %if.then2.i ]
  %add.i = add i64 %last, 1
  %cmp8.not.i = icmp eq i64 %add.i, 0
  br i1 %cmp8.not.i, label %if.end22.i, label %if.then9.i

if.then9.i:                                       ; preds = %if.end7.i
  %call.i25.i = tail call ptr @interval_tree_iter_first(ptr noundef nonnull @pageflags_root, i64 noundef %add.i, i64 noundef %add.i) #16
  %tobool.not.i26.i = icmp eq ptr %call.i25.i, null
  %add.ptr.i27.i = getelementptr i8, ptr %call.i25.i, i64 -16
  %tobool13.not31.i = icmp eq ptr %add.ptr.i27.i, null
  %tobool13.not.i = or i1 %tobool.not.i26.i, %tobool13.not31.i
  br i1 %tobool13.not.i, label %if.end22.i, label %if.then14.i

if.then14.i:                                      ; preds = %if.then9.i
  %flags15.i = getelementptr i8, ptr %call.i25.i, i64 48
  %1 = load i32, ptr %flags15.i, align 8
  %cmp16.i = icmp eq i32 %1, %set_flags
  br i1 %cmp16.i, label %if.then17.i, label %if.end22.i

if.then17.i:                                      ; preds = %if.then14.i
  tail call void @interval_tree_remove(ptr noundef nonnull %call.i25.i, ptr noundef nonnull @pageflags_root) #16
  br label %if.end22.i

if.end22.i:                                       ; preds = %if.then17.i, %if.then14.i, %if.then9.i, %if.end7.i
  %next.0.i = phi ptr [ %add.ptr.i27.i, %if.then17.i ], [ null, %if.then9.i ], [ null, %if.end7.i ], [ null, %if.then14.i ]
  %tobool23.not.i = icmp eq ptr %prev.0.i, null
  %tobool37.not.i = icmp eq ptr %next.0.i, null
  br i1 %tobool23.not.i, label %if.else36.i, label %if.then24.i

if.then24.i:                                      ; preds = %if.end22.i
  br i1 %tobool37.not.i, label %if.else31.i, label %if.then26.i

if.then26.i:                                      ; preds = %if.then24.i
  %last28.i = getelementptr inbounds %struct.PageFlagsNode, ptr %next.0.i, i64 0, i32 1, i32 2
  %2 = load i64, ptr %last28.i, align 8
  %last30.i = getelementptr inbounds %struct.PageFlagsNode, ptr %prev.0.i, i64 0, i32 1, i32 2
  store i64 %2, ptr %last30.i, align 8
  tail call void @call_rcu1(ptr noundef nonnull %next.0.i, ptr noundef nonnull @g_free) #16
  br label %if.end34.i

if.else31.i:                                      ; preds = %if.then24.i
  %last33.i = getelementptr inbounds %struct.PageFlagsNode, ptr %prev.0.i, i64 0, i32 1, i32 2
  store i64 %last, ptr %last33.i, align 8
  br label %if.end34.i

if.end34.i:                                       ; preds = %if.else31.i, %if.then26.i
  %itree35.i = getelementptr inbounds %struct.PageFlagsNode, ptr %prev.0.i, i64 0, i32 1
  br label %pageflags_create_merge.exit

if.else36.i:                                      ; preds = %if.end22.i
  br i1 %tobool37.not.i, label %if.else42.i, label %if.then38.i

if.then38.i:                                      ; preds = %if.else36.i
  %itree39.i = getelementptr inbounds %struct.PageFlagsNode, ptr %next.0.i, i64 0, i32 1
  %start40.i = getelementptr inbounds %struct.PageFlagsNode, ptr %next.0.i, i64 0, i32 1, i32 1
  store i64 %start.addr.0.ph.lcssa194, ptr %start40.i, align 8
  br label %pageflags_create_merge.exit

if.else42.i:                                      ; preds = %if.else36.i
  %call.i29.i = tail call noalias dereferenceable_or_null(72) ptr @g_malloc_n(i64 noundef 1, i64 noundef 72) #18
  %itree.i.i = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i29.i, i64 0, i32 1
  %start1.i.i = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i29.i, i64 0, i32 1, i32 1
  store i64 %start.addr.0.ph.lcssa194, ptr %start1.i.i, align 8
  %last3.i.i = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i29.i, i64 0, i32 1, i32 2
  store i64 %last, ptr %last3.i.i, align 8
  %flags4.i.i = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i29.i, i64 0, i32 2
  store i32 %set_flags, ptr %flags4.i.i, align 8
  br label %pageflags_create_merge.exit

pageflags_create_merge.exit:                      ; preds = %if.end34.i, %if.then38.i, %if.else42.i
  %itree39.sink.i = phi ptr [ %itree39.i, %if.then38.i ], [ %itree.i.i, %if.else42.i ], [ %itree35.i, %if.end34.i ]
  tail call void @interval_tree_insert(ptr noundef nonnull %itree39.sink.i, ptr noundef nonnull @pageflags_root) #16
  br label %done

if.end3:                                          ; preds = %if.end3.lr.ph, %restart.backedge
  %add.ptr.i210 = phi ptr [ %add.ptr.i205222, %if.end3.lr.ph ], [ %add.ptr.i, %restart.backedge ]
  %call.i209 = phi ptr [ %call.i203221, %if.end3.lr.ph ], [ %call.i, %restart.backedge ]
  %inval_tb.0208 = phi i8 [ %inval_tb.0.ph219, %if.end3.lr.ph ], [ %inval_tb.1, %restart.backedge ]
  %start4 = getelementptr i8, ptr %call.i209, i64 24
  %3 = load i64, ptr %start4, align 8
  %last6 = getelementptr i8, ptr %call.i209, i64 32
  %4 = load i64, ptr %last6, align 8
  %flags = getelementptr i8, ptr %call.i209, i64 48
  %5 = load i32, ptr %flags, align 8
  %and = and i32 %5, %not
  %or = or i32 %and, %set_flags
  %and7 = and i32 %5, 4
  %tobool8.not = icmp eq i32 %and7, 0
  br i1 %tobool8.not, label %if.end16, label %land.lhs.true

land.lhs.true:                                    ; preds = %if.end3
  %and9 = and i32 %or, 4
  %tobool10.not = icmp eq i32 %and9, 0
  br i1 %tobool10.not, label %if.then15, label %lor.lhs.false

lor.lhs.false:                                    ; preds = %land.lhs.true
  %not11 = xor i32 %5, -1
  %and13 = and i32 %and12, %not11
  %tobool14.not = icmp eq i32 %and13, 0
  br i1 %tobool14.not, label %if.end16, label %if.then15

if.then15:                                        ; preds = %lor.lhs.false, %land.lhs.true
  br label %if.end16

if.end16:                                         ; preds = %if.then15, %lor.lhs.false, %if.end3
  %inval_tb.1 = phi i8 [ 1, %if.then15 ], [ %inval_tb.0208, %lor.lhs.false ], [ %inval_tb.0208, %if.end3 ]
  %cmp = icmp eq i64 %start.addr.0.ph220, %3
  %cmp18 = icmp eq i64 %4, %last
  %or.cond102 = select i1 %cmp, i1 %cmp18, i1 false
  br i1 %or.cond102, label %if.then19, label %if.end25

if.then19:                                        ; preds = %if.end16
  %tobool20.not = icmp eq i32 %or, 0
  br i1 %tobool20.not, label %if.else, label %if.then21

if.then21:                                        ; preds = %if.then19
  %flags.le355 = getelementptr i8, ptr %call.i209, i64 48
  store i32 %or, ptr %flags.le355, align 8
  br label %done

if.else:                                          ; preds = %if.then19
  tail call void @interval_tree_remove(ptr noundef nonnull %call.i209, ptr noundef nonnull @pageflags_root) #16
  tail call void @call_rcu1(ptr noundef nonnull %add.ptr.i210, ptr noundef nonnull @g_free) #16
  br label %done

if.end25:                                         ; preds = %if.end16
  %cmp26.not = icmp eq i32 %or, %set_flags
  br i1 %cmp26.not, label %if.end81, label %if.then27

if.then27:                                        ; preds = %if.end25
  %flags.le = getelementptr i8, ptr %call.i209, i64 48
  %cmp28 = icmp ult i64 %3, %start.addr.0.ph220
  br i1 %cmp28, label %if.then29, label %if.else48

if.then29:                                        ; preds = %if.then27
  tail call void @interval_tree_remove(ptr noundef nonnull %call.i209, ptr noundef nonnull @pageflags_root) #16
  store i64 %sub99, ptr %last6, align 8
  tail call void @interval_tree_insert(ptr noundef nonnull %call.i209, ptr noundef nonnull @pageflags_root) #16
  %cmp34 = icmp ugt i64 %4, %last
  %tobool36.not = icmp eq i32 %or, 0
  br i1 %cmp34, label %if.then35, label %if.else39

if.then35:                                        ; preds = %if.then29
  br i1 %tobool36.not, label %if.end38, label %if.then37

if.then37:                                        ; preds = %if.then35
  %call.i104 = tail call noalias dereferenceable_or_null(72) ptr @g_malloc_n(i64 noundef 1, i64 noundef 72) #18
  %itree.i = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i104, i64 0, i32 1
  %start1.i = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i104, i64 0, i32 1, i32 1
  store i64 %start.addr.0.ph220, ptr %start1.i, align 8
  %last3.i = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i104, i64 0, i32 1, i32 2
  store i64 %last, ptr %last3.i, align 8
  %flags4.i = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i104, i64 0, i32 2
  store i32 %or, ptr %flags4.i, align 8
  tail call void @interval_tree_insert(ptr noundef nonnull %itree.i, ptr noundef nonnull @pageflags_root) #16
  br label %if.end38

if.end38:                                         ; preds = %if.then37, %if.then35
  %add = add nuw i64 %last, 1
  %call.i105 = tail call noalias dereferenceable_or_null(72) ptr @g_malloc_n(i64 noundef 1, i64 noundef 72) #18
  %itree.i106 = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i105, i64 0, i32 1
  %start1.i107 = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i105, i64 0, i32 1, i32 1
  store i64 %add, ptr %start1.i107, align 8
  %last3.i108 = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i105, i64 0, i32 1, i32 2
  store i64 %4, ptr %last3.i108, align 8
  %flags4.i109 = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i105, i64 0, i32 2
  store i32 %5, ptr %flags4.i109, align 8
  tail call void @interval_tree_insert(ptr noundef nonnull %itree.i106, ptr noundef nonnull @pageflags_root) #16
  br label %done

if.else39:                                        ; preds = %if.then29
  br i1 %tobool36.not, label %if.end42, label %if.then41

if.then41:                                        ; preds = %if.else39
  %call.i110 = tail call noalias dereferenceable_or_null(72) ptr @g_malloc_n(i64 noundef 1, i64 noundef 72) #18
  %itree.i111 = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i110, i64 0, i32 1
  %start1.i112 = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i110, i64 0, i32 1, i32 1
  store i64 %start.addr.0.ph220, ptr %start1.i112, align 8
  %last3.i113 = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i110, i64 0, i32 1, i32 2
  store i64 %4, ptr %last3.i113, align 8
  %flags4.i114 = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i110, i64 0, i32 2
  store i32 %or, ptr %flags4.i114, align 8
  tail call void @interval_tree_insert(ptr noundef nonnull %itree.i111, ptr noundef nonnull @pageflags_root) #16
  br label %if.end42

if.end42:                                         ; preds = %if.then41, %if.else39
  %cmp43 = icmp ult i64 %4, %last
  br i1 %cmp43, label %restart.outer.backedge, label %done

restart.outer.backedge:                           ; preds = %if.end90, %if.end74, %if.end42
  %start.addr.0.ph.be = add nuw i64 %4, 1
  %call.i203 = tail call ptr @interval_tree_iter_first(ptr noundef nonnull @pageflags_root, i64 noundef %start.addr.0.ph.be, i64 noundef %last) #16
  %tobool.not.i204 = icmp eq ptr %call.i203, null
  %add.ptr.i205 = getelementptr i8, ptr %call.i203, i64 -16
  %tobool.not130206 = icmp eq ptr %add.ptr.i205, null
  %tobool.not207 = or i1 %tobool.not.i204, %tobool.not130206
  br i1 %tobool.not207, label %if.then, label %if.end3.lr.ph

if.else48:                                        ; preds = %if.then27
  %cmp49 = icmp ult i64 %start.addr.0.ph220, %3
  %or.cond = and i1 %tobool51, %cmp49
  br i1 %or.cond, label %if.then52, label %if.end54

if.then52:                                        ; preds = %if.else48
  %sub53 = add i64 %3, -1
  %call.i115 = tail call noalias dereferenceable_or_null(72) ptr @g_malloc_n(i64 noundef 1, i64 noundef 72) #18
  %itree.i116 = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i115, i64 0, i32 1
  %start1.i117 = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i115, i64 0, i32 1, i32 1
  store i64 %start.addr.0.ph220, ptr %start1.i117, align 8
  %last3.i118 = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i115, i64 0, i32 1, i32 2
  store i64 %sub53, ptr %last3.i118, align 8
  %flags4.i119 = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i115, i64 0, i32 2
  store i32 %set_flags, ptr %flags4.i119, align 8
  tail call void @interval_tree_insert(ptr noundef nonnull %itree.i116, ptr noundef nonnull @pageflags_root) #16
  br label %if.end54

if.end54:                                         ; preds = %if.then52, %if.else48
  %cmp55 = icmp ugt i64 %4, %last
  br i1 %cmp55, label %if.then56, label %if.else65

if.then56:                                        ; preds = %if.end54
  %start4.le360.le = getelementptr i8, ptr %call.i209, i64 24
  tail call void @interval_tree_remove(ptr noundef nonnull %call.i209, ptr noundef nonnull @pageflags_root) #16
  %add58 = add nuw i64 %last, 1
  store i64 %add58, ptr %start4.le360.le, align 8
  tail call void @interval_tree_insert(ptr noundef nonnull %call.i209, ptr noundef nonnull @pageflags_root) #16
  %tobool62.not = icmp eq i32 %or, 0
  br i1 %tobool62.not, label %done, label %if.then63

if.then63:                                        ; preds = %if.then56
  %call.i120 = tail call noalias dereferenceable_or_null(72) ptr @g_malloc_n(i64 noundef 1, i64 noundef 72) #18
  %itree.i121 = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i120, i64 0, i32 1
  %start1.i122 = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i120, i64 0, i32 1, i32 1
  store i64 %start.addr.0.ph220, ptr %start1.i122, align 8
  %last3.i123 = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i120, i64 0, i32 1, i32 2
  store i64 %last, ptr %last3.i123, align 8
  %flags4.i124 = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i120, i64 0, i32 2
  store i32 %or, ptr %flags4.i124, align 8
  tail call void @interval_tree_insert(ptr noundef nonnull %itree.i121, ptr noundef nonnull @pageflags_root) #16
  br label %done

if.else65:                                        ; preds = %if.end54
  %tobool66.not = icmp eq i32 %or, 0
  br i1 %tobool66.not, label %if.else69, label %if.then67

if.then67:                                        ; preds = %if.else65
  store i32 %or, ptr %flags.le, align 8
  br label %if.end74

if.else69:                                        ; preds = %if.else65
  tail call void @interval_tree_remove(ptr noundef nonnull %call.i209, ptr noundef nonnull @pageflags_root) #16
  tail call void @call_rcu1(ptr noundef nonnull %add.ptr.i210, ptr noundef nonnull @g_free) #16
  br label %if.end74

if.end74:                                         ; preds = %if.else69, %if.then67
  %cmp75 = icmp ult i64 %4, %last
  br i1 %cmp75, label %restart.outer.backedge, label %done

if.end81:                                         ; preds = %if.end25
  %cmp82 = icmp eq i32 %5, %set_flags
  br i1 %cmp82, label %if.then83, label %if.end95

if.then83:                                        ; preds = %if.end81
  %cmp84 = icmp ult i64 %start.addr.0.ph220, %3
  br i1 %cmp84, label %if.then85, label %if.end90

if.then85:                                        ; preds = %if.then83
  %start4.le358 = getelementptr i8, ptr %call.i209, i64 24
  tail call void @interval_tree_remove(ptr noundef nonnull %call.i209, ptr noundef nonnull @pageflags_root) #16
  store i64 %start.addr.0.ph220, ptr %start4.le358, align 8
  tail call void @interval_tree_insert(ptr noundef nonnull %call.i209, ptr noundef nonnull @pageflags_root) #16
  br label %if.end90

if.end90:                                         ; preds = %if.then85, %if.then83
  %cmp91 = icmp ult i64 %4, %last
  br i1 %cmp91, label %restart.outer.backedge, label %done

if.end95:                                         ; preds = %if.end81
  tail call void @interval_tree_remove(ptr noundef nonnull %call.i209, ptr noundef nonnull @pageflags_root) #16
  %cmp97 = icmp ult i64 %3, %start.addr.0.ph220
  br i1 %cmp97, label %if.then98, label %if.else110

if.then98:                                        ; preds = %if.end95
  store i64 %sub99, ptr %last6, align 8
  tail call void @interval_tree_insert(ptr noundef nonnull %call.i209, ptr noundef nonnull @pageflags_root) #16
  %cmp103 = icmp ult i64 %4, %last
  br i1 %cmp103, label %restart.backedge, label %if.end105

if.end105:                                        ; preds = %if.then98
  %cmp106 = icmp ugt i64 %4, %last
  br i1 %cmp106, label %if.then107, label %if.end122

if.then107:                                       ; preds = %if.end105
  %add108 = add nuw i64 %last, 1
  tail call fastcc void @pageflags_create(i64 noundef %add108, i64 noundef %4, i32 noundef %5)
  br label %if.end122

if.else110:                                       ; preds = %if.end95
  %cmp111 = icmp ugt i64 %4, %last
  br i1 %cmp111, label %if.then112, label %if.else117

if.then112:                                       ; preds = %if.else110
  %start4.le = getelementptr i8, ptr %call.i209, i64 24
  %add113 = add nuw i64 %last, 1
  store i64 %add113, ptr %start4.le, align 8
  tail call void @interval_tree_insert(ptr noundef nonnull %call.i209, ptr noundef nonnull @pageflags_root) #16
  br label %if.end122

if.else117:                                       ; preds = %if.else110
  tail call void @call_rcu1(ptr noundef nonnull %add.ptr.i210, ptr noundef nonnull @g_free) #16
  br label %restart.backedge

restart.backedge:                                 ; preds = %if.else117, %if.then98
  %call.i = tail call ptr @interval_tree_iter_first(ptr noundef nonnull @pageflags_root, i64 noundef %start.addr.0.ph220, i64 noundef %last) #16
  %tobool.not.i = icmp eq ptr %call.i, null
  %add.ptr.i = getelementptr i8, ptr %call.i, i64 -16
  %tobool.not130 = icmp eq ptr %add.ptr.i, null
  %tobool.not = or i1 %tobool.not.i, %tobool.not130
  br i1 %tobool.not, label %if.then, label %if.end3

if.end122:                                        ; preds = %if.end105, %if.then107, %if.then112
  %tobool123.not = icmp eq i32 %set_flags, 0
  br i1 %tobool123.not, label %done, label %if.then124

if.then124:                                       ; preds = %if.end122
  %call.i125 = tail call noalias dereferenceable_or_null(72) ptr @g_malloc_n(i64 noundef 1, i64 noundef 72) #18
  %itree.i126 = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i125, i64 0, i32 1
  %start1.i127 = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i125, i64 0, i32 1, i32 1
  store i64 %start.addr.0.ph220, ptr %start1.i127, align 8
  %last3.i128 = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i125, i64 0, i32 1, i32 2
  store i64 %last, ptr %last3.i128, align 8
  %flags4.i129 = getelementptr inbounds %struct.PageFlagsNode, ptr %call.i125, i64 0, i32 2
  store i32 %set_flags, ptr %flags4.i129, align 8
  tail call void @interval_tree_insert(ptr noundef nonnull %itree.i126, ptr noundef nonnull @pageflags_root) #16
  br label %done

done:                                             ; preds = %if.end90, %if.end42, %if.end74, %if.end122, %if.then124, %if.end38, %if.then56, %if.then63, %if.then21, %if.else, %if.then, %pageflags_create_merge.exit
  %inval_tb.2 = phi i8 [ %inval_tb.1, %if.then21 ], [ %inval_tb.1, %if.else ], [ %inval_tb.1, %if.end38 ], [ %inval_tb.1, %if.then63 ], [ %inval_tb.1, %if.then56 ], [ %inval_tb.1, %if.then124 ], [ %inval_tb.1, %if.end122 ], [ %inval_tb.0.lcssa, %pageflags_create_merge.exit ], [ %inval_tb.0.lcssa, %if.then ], [ %inval_tb.1, %if.end74 ], [ %inval_tb.1, %if.end42 ], [ %inval_tb.1, %if.end90 ]
  %6 = and i8 %inval_tb.2, 1
  %tobool126 = icmp ne i8 %6, 0
  ret i1 %tobool126
}

declare void @tb_invalidate_phys_range(i64 noundef, i64 noundef) local_unnamed_addr #5

; Function Attrs: nounwind sspstrong uwtable
define dso_local zeroext i1 @page_check_range(i64 noundef %start, i64 noundef %len, i32 noundef %flags) local_unnamed_addr #2 {
entry:
  %cmp = icmp eq i64 %len, 0
  br i1 %cmp, label %return, label %if.end

if.end:                                           ; preds = %entry
  %add = add i64 %start, -1
  %sub = add i64 %add, %len
  %cmp1 = icmp ult i64 %sub, %start
  br i1 %cmp1, label %return, label %if.end3

if.end3:                                          ; preds = %if.end
  %call = tail call zeroext i1 @have_mmap_lock() #16
  %conv = zext i1 %call to i32
  br label %while.body

while.body:                                       ; preds = %while.body.backedge, %if.end3
  %start.addr.0 = phi i64 [ %start, %if.end3 ], [ %start.addr.0.be, %while.body.backedge ]
  %locked.0 = phi i32 [ %conv, %if.end3 ], [ %locked.2, %while.body.backedge ]
  %call.i = tail call ptr @interval_tree_iter_first(ptr noundef nonnull @pageflags_root, i64 noundef %start.addr.0, i64 noundef %sub) #16
  %tobool.not.i = icmp eq ptr %call.i, null
  %add.ptr.i = getelementptr i8, ptr %call.i, i64 -16
  %tobool.not36 = icmp eq ptr %add.ptr.i, null
  %tobool.not = or i1 %tobool.not.i, %tobool.not36
  br i1 %tobool.not, label %if.then5, label %if.end13

if.then5:                                         ; preds = %while.body
  %tobool6.not = icmp eq i32 %locked.0, 0
  br i1 %tobool6.not, label %if.then7, label %while.end

if.then7:                                         ; preds = %if.then5
  tail call void @mmap_lock() #16
  %call.i21 = tail call ptr @interval_tree_iter_first(ptr noundef nonnull @pageflags_root, i64 noundef %start.addr.0, i64 noundef %sub) #16
  %tobool.not.i22 = icmp eq ptr %call.i21, null
  %add.ptr.i23 = getelementptr i8, ptr %call.i21, i64 -16
  %tobool10.not = icmp eq ptr %add.ptr.i23, null
  %or.cond = or i1 %tobool.not.i22, %tobool10.not
  br i1 %or.cond, label %if.then54, label %if.end13

if.end13:                                         ; preds = %if.then7, %while.body
  %locked.2 = phi i32 [ %locked.0, %while.body ], [ -1, %if.then7 ]
  %p.1 = phi ptr [ %add.ptr.i, %while.body ], [ %add.ptr.i23, %if.then7 ]
  %start14 = getelementptr inbounds %struct.PageFlagsNode, ptr %p.1, i64 0, i32 1, i32 1
  %0 = load i64, ptr %start14, align 8
  %cmp15 = icmp ult i64 %start.addr.0, %0
  br i1 %cmp15, label %while.end, label %if.end18

if.end18:                                         ; preds = %if.end13
  %flags19 = getelementptr inbounds %struct.PageFlagsNode, ptr %p.1, i64 0, i32 2
  %1 = load i32, ptr %flags19, align 8
  %not = xor i32 %1, -1
  %and = and i32 %not, %flags
  %and20 = and i32 %and, -3
  %tobool21.not = icmp eq i32 %and20, 0
  br i1 %tobool21.not, label %if.end23, label %while.end

if.end23:                                         ; preds = %if.end18
  %tobool25.not = icmp eq i32 %and, 0
  br i1 %tobool25.not, label %if.end42, label %if.then26

if.then26:                                        ; preds = %if.end23
  %and28 = and i32 %1, 16
  %tobool29.not = icmp eq i32 %and28, 0
  br i1 %tobool29.not, label %while.end, label %if.end31

if.end31:                                         ; preds = %if.then26
  %call32 = tail call i32 @page_unprotect(i64 noundef %start.addr.0, i64 noundef 0), !range !5
  %tobool33.not = icmp eq i32 %call32, 0
  br i1 %tobool33.not, label %while.end, label %if.end35

if.end35:                                         ; preds = %if.end31
  %sub36 = sub i64 %sub, %start.addr.0
  %cmp37 = icmp ult i64 %sub36, 4096
  br i1 %cmp37, label %while.end, label %if.end40

if.end40:                                         ; preds = %if.end35
  %add41 = add i64 %start.addr.0, 4096
  br label %while.body.backedge

while.body.backedge:                              ; preds = %if.end40, %if.end48
  %start.addr.0.be = phi i64 [ %add41, %if.end40 ], [ %add51, %if.end48 ]
  br label %while.body

if.end42:                                         ; preds = %if.end23
  %last44 = getelementptr inbounds %struct.PageFlagsNode, ptr %p.1, i64 0, i32 1, i32 2
  %2 = load i64, ptr %last44, align 8
  %cmp45.not = icmp ugt i64 %sub, %2
  br i1 %cmp45.not, label %if.end48, label %while.end

if.end48:                                         ; preds = %if.end42
  %add51 = add nuw i64 %2, 1
  br label %while.body.backedge

while.end:                                        ; preds = %if.then5, %if.end42, %if.end35, %if.end31, %if.then26, %if.end18, %if.end13
  %locked.3 = phi i32 [ %locked.2, %if.end13 ], [ %locked.2, %if.end18 ], [ %locked.2, %if.then26 ], [ %locked.2, %if.end31 ], [ %locked.2, %if.end35 ], [ %locked.2, %if.end42 ], [ %locked.0, %if.then5 ]
  %ret.0 = phi i1 [ false, %if.end13 ], [ false, %if.end18 ], [ false, %if.then26 ], [ false, %if.end31 ], [ true, %if.end35 ], [ true, %if.end42 ], [ false, %if.then5 ]
  %cmp52 = icmp slt i32 %locked.3, 0
  br i1 %cmp52, label %if.then54, label %return

if.then54:                                        ; preds = %if.then7, %while.end
  %ret.031 = phi i1 [ %ret.0, %while.end ], [ false, %if.then7 ]
  tail call void @mmap_unlock() #16
  br label %return

return:                                           ; preds = %while.end, %if.then54, %if.end, %entry
  %retval.0 = phi i1 [ true, %entry ], [ false, %if.end ], [ %ret.031, %if.then54 ], [ %ret.0, %while.end ]
  ret i1 %retval.0
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local zeroext i1 @page_check_range_empty(i64 noundef %start, i64 noundef %last) local_unnamed_addr #2 {
entry:
  %cmp.not = icmp ult i64 %last, %start
  br i1 %cmp.not, label %if.else, label %do.body

if.else:                                          ; preds = %entry
  tail call void @__assert_fail(ptr noundef nonnull @.str.9, ptr noundef nonnull @.str, i32 noundef 604, ptr noundef nonnull @__PRETTY_FUNCTION__.page_check_range_empty) #17
  unreachable

do.body:                                          ; preds = %entry
  %call = tail call zeroext i1 @have_mmap_lock() #16
  tail call void @llvm.assume(i1 %call)
  %call.i = tail call ptr @interval_tree_iter_first(ptr noundef nonnull @pageflags_root, i64 noundef %start, i64 noundef %last) #16
  %tobool.not.i = icmp eq ptr %call.i, null
  %add.ptr.i = getelementptr i8, ptr %call.i, i64 -16
  %cmp43 = icmp eq ptr %add.ptr.i, null
  %cmp4 = or i1 %tobool.not.i, %cmp43
  ret i1 %cmp4
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @page_find_range_empty(i64 noundef %min, i64 noundef %max, i64 noundef %len, i64 noundef %align) local_unnamed_addr #2 {
entry:
  %cmp.not = icmp ugt i64 %min, %max
  br i1 %cmp.not, label %if.else, label %if.end

if.else:                                          ; preds = %entry
  tail call void @__assert_fail(ptr noundef nonnull @.str.10, ptr noundef nonnull @.str, i32 noundef 614, ptr noundef nonnull @__PRETTY_FUNCTION__.page_find_range_empty) #17
  unreachable

if.end:                                           ; preds = %entry
  %0 = load i64, ptr @reserved_va, align 8
  %tobool.not = icmp ne i64 %0, 0
  %cmp1.not17 = icmp ult i64 %0, %max
  %cmp1.not = and i1 %tobool.not, %cmp1.not17
  br i1 %cmp1.not, label %if.else3, label %if.end4

if.else3:                                         ; preds = %if.end
  tail call void @__assert_fail(ptr noundef nonnull @.str.11, ptr noundef nonnull @.str, i32 noundef 615, ptr noundef nonnull @__PRETTY_FUNCTION__.page_find_range_empty) #17
  unreachable

if.end4:                                          ; preds = %if.end
  %cmp5.not = icmp eq i64 %len, 0
  br i1 %cmp5.not, label %if.else7, label %if.end8

if.else7:                                         ; preds = %if.end4
  tail call void @__assert_fail(ptr noundef nonnull @.str.12, ptr noundef nonnull @.str, i32 noundef 616, ptr noundef nonnull @__PRETTY_FUNCTION__.page_find_range_empty) #17
  unreachable

if.end8:                                          ; preds = %if.end4
  %1 = tail call i64 @llvm.ctpop.i64(i64 %align), !range !9
  %or.cond20 = icmp eq i64 %1, 1
  br i1 %or.cond20, label %do.body, label %if.else10

if.else10:                                        ; preds = %if.end8
  tail call void @__assert_fail(ptr noundef nonnull @.str.13, ptr noundef nonnull @.str, i32 noundef 617, ptr noundef nonnull @__PRETTY_FUNCTION__.page_find_range_empty) #17
  unreachable

do.body:                                          ; preds = %if.end8
  %call12 = tail call zeroext i1 @have_mmap_lock() #16
  tail call void @llvm.assume(i1 %call12)
  %sub = add i64 %len, -1
  %sub15 = add i64 %align, -1
  %not = sub i64 0, %align
  %add22 = add i64 %sub15, %min
  %and23 = and i64 %add22, %not
  %cmp1624 = icmp ugt i64 %and23, %max
  %sub1925 = sub i64 %max, %and23
  %cmp2026 = icmp ugt i64 %sub, %sub1925
  %or.cond27 = or i1 %cmp1624, %cmp2026
  br i1 %or.cond27, label %return, label %if.end22

while.body:                                       ; preds = %if.end27
  %add = add i64 %2, %align
  %and = and i64 %add, %not
  %cmp16 = icmp ugt i64 %and, %max
  %sub19 = sub i64 %max, %and
  %cmp20 = icmp ugt i64 %sub, %sub19
  %or.cond = or i1 %cmp16, %cmp20
  br i1 %or.cond, label %return, label %if.end22

if.end22:                                         ; preds = %do.body, %while.body
  %and28 = phi i64 [ %and, %while.body ], [ %and23, %do.body ]
  %add23 = add i64 %and28, %sub
  %call.i = tail call ptr @interval_tree_iter_first(ptr noundef nonnull @pageflags_root, i64 noundef %and28, i64 noundef %add23) #16
  %tobool.not.i18 = icmp eq ptr %call.i, null
  %add.ptr.i = getelementptr i8, ptr %call.i, i64 -16
  %cmp2521 = icmp eq ptr %add.ptr.i, null
  %cmp25 = or i1 %tobool.not.i18, %cmp2521
  br i1 %cmp25, label %return, label %if.end27

if.end27:                                         ; preds = %if.end22
  %last = getelementptr i8, ptr %call.i, i64 32
  %2 = load i64, ptr %last, align 8
  %cmp28.not = icmp ult i64 %2, %max
  br i1 %cmp28.not, label %while.body, label %return

return:                                           ; preds = %while.body, %if.end22, %if.end27, %do.body
  %retval.0 = phi i64 [ -1, %do.body ], [ -1, %if.end27 ], [ %and28, %if.end22 ], [ -1, %while.body ]
  ret i64 %retval.0
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @page_protect(i64 noundef %address) local_unnamed_addr #2 {
entry:
  %call = tail call zeroext i1 @have_mmap_lock() #16
  tail call void @llvm.assume(i1 %call)
  %0 = load i64, ptr @qemu_host_page_size, align 8
  %cmp = icmp ult i64 %0, 4097
  br i1 %cmp, label %if.then1, label %if.else

if.then1:                                         ; preds = %entry
  %and = and i64 %address, -4096
  %sub = or i64 %address, 4095
  br label %if.end5

if.else:                                          ; preds = %entry
  %1 = load i64, ptr @qemu_host_page_mask, align 8
  %and2 = and i64 %1, %address
  %add3 = add i64 %0, -1
  %sub4 = add i64 %add3, %and2
  br label %if.end5

if.end5:                                          ; preds = %if.else, %if.then1
  %start.0 = phi i64 [ %and, %if.then1 ], [ %and2, %if.else ]
  %last.0 = phi i64 [ %sub, %if.then1 ], [ %sub4, %if.else ]
  %call.i = tail call ptr @interval_tree_iter_first(ptr noundef nonnull @pageflags_root, i64 noundef %start.0, i64 noundef %last.0) #16
  %tobool.not.i = icmp eq ptr %call.i, null
  %add.ptr.i = getelementptr i8, ptr %call.i, i64 -16
  %tobool.not20 = icmp eq ptr %add.ptr.i, null
  %tobool.not = or i1 %tobool.not.i, %tobool.not20
  br i1 %tobool.not, label %if.end32, label %if.end8

if.end8:                                          ; preds = %if.end5
  %flags = getelementptr i8, ptr %call.i, i64 48
  %2 = load i32, ptr %flags, align 8
  %last9 = getelementptr i8, ptr %call.i, i64 32
  %3 = load i64, ptr %last9, align 8
  %cmp10 = icmp ult i64 %3, %last.0
  br i1 %cmp10, label %if.then13, label %if.end23

if.then13:                                        ; preds = %if.end8
  %4 = load i64, ptr @qemu_host_page_size, align 8
  %cmp14 = icmp ugt i64 %4, 4096
  br i1 %cmp14, label %while.cond.preheader, label %if.else17

while.cond.preheader:                             ; preds = %if.then13
  %call.i1622 = tail call ptr @interval_tree_iter_next(ptr noundef nonnull %call.i, i64 noundef %start.0, i64 noundef %last.0) #16
  %tobool.not.i1723 = icmp eq ptr %call.i1622, null
  %add.ptr.i1824 = getelementptr i8, ptr %call.i1622, i64 -16
  %cmp20.not2125 = icmp eq ptr %add.ptr.i1824, null
  %cmp20.not26 = or i1 %tobool.not.i1723, %cmp20.not2125
  br i1 %cmp20.not26, label %if.end23, label %while.body

if.else17:                                        ; preds = %if.then13
  tail call void @__assert_fail(ptr noundef nonnull @.str.14, ptr noundef nonnull @.str, i32 noundef 674, ptr noundef nonnull @__PRETTY_FUNCTION__.page_protect) #17
  unreachable

while.body:                                       ; preds = %while.cond.preheader, %while.body
  %call.i1628 = phi ptr [ %call.i16, %while.body ], [ %call.i1622, %while.cond.preheader ]
  %prot.027 = phi i32 [ %or, %while.body ], [ %2, %while.cond.preheader ]
  %flags22 = getelementptr i8, ptr %call.i1628, i64 48
  %5 = load i32, ptr %flags22, align 8
  %or = or i32 %5, %prot.027
  %call.i16 = tail call ptr @interval_tree_iter_next(ptr noundef nonnull %call.i1628, i64 noundef %start.0, i64 noundef %last.0) #16
  %tobool.not.i17 = icmp eq ptr %call.i16, null
  %add.ptr.i18 = getelementptr i8, ptr %call.i16, i64 -16
  %cmp20.not21 = icmp eq ptr %add.ptr.i18, null
  %cmp20.not = or i1 %tobool.not.i17, %cmp20.not21
  br i1 %cmp20.not, label %if.end23, label %while.body, !llvm.loop !10

if.end23:                                         ; preds = %while.body, %while.cond.preheader, %if.end8
  %prot.1 = phi i32 [ %2, %if.end8 ], [ %2, %while.cond.preheader ], [ %or, %while.body ]
  %and24 = and i32 %prot.1, 2
  %tobool25.not = icmp eq i32 %and24, 0
  br i1 %tobool25.not, label %if.end32, label %if.then26

if.then26:                                        ; preds = %if.end23
  %call27 = tail call fastcc zeroext i1 @pageflags_set_clear(i64 noundef %start.0, i64 noundef %last.0, i32 noundef 0, i32 noundef 2)
  %6 = load i64, ptr @guest_base, align 8
  %add.i = add i64 %6, %start.0
  %7 = inttoptr i64 %add.i to ptr
  %8 = load i64, ptr @qemu_host_page_size, align 8
  %and29 = and i32 %prot.1, 5
  %tobool30.not = icmp ne i32 %and29, 0
  %cond = zext i1 %tobool30.not to i32
  %call31 = tail call i32 @mprotect(ptr noundef %7, i64 noundef %8, i32 noundef %cond) #16
  br label %if.end32

if.end32:                                         ; preds = %if.end5, %if.then26, %if.end23
  ret void
}

; Function Attrs: nounwind
declare i32 @mprotect(ptr noundef, i64 noundef, i32 noundef) local_unnamed_addr #3

declare zeroext i1 @tb_invalidate_phys_page_unwind(i64 noundef, i64 noundef) local_unnamed_addr #5

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @probe_access_flags(ptr noundef %env, i64 noundef %addr, i32 noundef %size, i32 noundef %access_type, i32 noundef %mmu_idx, i1 noundef zeroext %nonfault, ptr nocapture noundef writeonly %phost, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %or = or i64 %addr, -4096
  %sub = sub nsw i64 0, %or
  %conv = sext i32 %size to i64
  %cmp.not = icmp ult i64 %sub, %conv
  br i1 %cmp.not, label %if.else, label %do.end

if.else:                                          ; preds = %entry
  tail call void @g_assertion_message_expr(ptr noundef null, ptr noundef nonnull @.str, i32 noundef 821, ptr noundef nonnull @__func__.probe_access_flags, ptr noundef nonnull @.str.15) #17
  unreachable

do.end:                                           ; preds = %entry
  %call = tail call fastcc i32 @probe_access_internal(ptr noundef %env, i64 noundef %addr, i32 noundef %access_type, i1 noundef zeroext %nonfault, i64 noundef %ra)
  %tobool2.not = icmp ult i32 %call, 2048
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i to ptr
  %cond = select i1 %tobool2.not, ptr %1, ptr null
  store ptr %cond, ptr %phost, align 8
  ret i32 %call
}

; Function Attrs: nounwind sspstrong uwtable
define internal fastcc i32 @probe_access_internal(ptr noundef %env, i64 noundef %addr, i32 noundef %access_type, i1 noundef zeroext %nonfault, i64 noundef %ra) unnamed_addr #2 {
entry:
  switch i32 %access_type, label %do.body [
    i32 1, label %sw.epilog
    i32 0, label %sw.bb1
    i32 2, label %sw.bb2
  ]

sw.bb1:                                           ; preds = %entry
  br label %sw.epilog

sw.bb2:                                           ; preds = %entry
  br label %sw.epilog

do.body:                                          ; preds = %entry
  tail call void @g_assertion_message_expr(ptr noundef null, ptr noundef nonnull @.str, i32 noundef 791, ptr noundef nonnull @__func__.probe_access_internal, ptr noundef null) #17
  unreachable

sw.epilog:                                        ; preds = %entry, %sw.bb2, %sw.bb1
  %cmp = phi i1 [ false, %sw.bb2 ], [ true, %sw.bb1 ], [ false, %entry ]
  %acc_flag.0 = phi i32 [ 4, %sw.bb2 ], [ 1, %sw.bb1 ], [ 16, %entry ]
  %0 = load i64, ptr @reserved_va, align 8
  %tobool.not.i = icmp eq i64 %0, 0
  %cmp1.i = icmp uge i64 %0, %addr
  %cmp.i = or i1 %tobool.not.i, %cmp1.i
  br i1 %cmp.i, label %if.then, label %if.end13

if.then:                                          ; preds = %sw.epilog
  %call.i.i = tail call ptr @interval_tree_iter_first(ptr noundef nonnull @pageflags_root, i64 noundef %addr, i64 noundef %addr) #16
  %tobool.not.i.i = icmp eq ptr %call.i.i, null
  %add.ptr.i.i = getelementptr i8, ptr %call.i.i, i64 -16
  %tobool.not11.i = icmp eq ptr %add.ptr.i.i, null
  %tobool.not.i8 = or i1 %tobool.not.i.i, %tobool.not11.i
  br i1 %tobool.not.i8, label %if.end.i, label %return.sink.split.i

if.end.i:                                         ; preds = %if.then
  %call1.i = tail call zeroext i1 @have_mmap_lock() #16
  br i1 %call1.i, label %page_get_flags.exit, label %if.end3.i

if.end3.i:                                        ; preds = %if.end.i
  tail call void @mmap_lock() #16
  %call.i7.i = tail call ptr @interval_tree_iter_first(ptr noundef nonnull @pageflags_root, i64 noundef %addr, i64 noundef %addr) #16
  %tobool.not.i8.i = icmp eq ptr %call.i7.i, null
  %add.ptr.i9.i = getelementptr i8, ptr %call.i7.i, i64 -16
  tail call void @mmap_unlock() #16
  %tobool5.not12.i = icmp eq ptr %add.ptr.i9.i, null
  %tobool5.not.i = or i1 %tobool.not.i8.i, %tobool5.not12.i
  br i1 %tobool5.not.i, label %page_get_flags.exit, label %return.sink.split.i

return.sink.split.i:                              ; preds = %if.end3.i, %if.then
  %call.i7.sink.i = phi ptr [ %call.i.i, %if.then ], [ %call.i7.i, %if.end3.i ]
  %flags6.i = getelementptr i8, ptr %call.i7.sink.i, i64 48
  %1 = load i32, ptr %flags6.i, align 8
  br label %page_get_flags.exit

page_get_flags.exit:                              ; preds = %if.end.i, %if.end3.i, %return.sink.split.i
  %retval.0.i = phi i32 [ 0, %if.end.i ], [ 0, %if.end3.i ], [ %1, %return.sink.split.i ]
  %and = and i32 %retval.0.i, %acc_flag.0
  %tobool.not = icmp eq i32 %and, 0
  br i1 %tobool.not, label %if.end9, label %if.then4

if.then4:                                         ; preds = %page_get_flags.exit
  br i1 %cmp, label %land.lhs.true, label %if.end

land.lhs.true:                                    ; preds = %if.then4
  %2 = getelementptr i8, ptr %env, i64 -9472
  %call6.val = load ptr, ptr %2, align 16
  %tobool.i.not = icmp eq ptr %call6.val, null
  br i1 %tobool.i.not, label %if.end, label %return

if.end:                                           ; preds = %if.then4, %land.lhs.true
  br label %return

if.end9:                                          ; preds = %page_get_flags.exit
  %and10 = and i32 %retval.0.i, 8
  %tobool11.not = icmp eq i32 %and10, 0
  br label %if.end13

if.end13:                                         ; preds = %sw.epilog, %if.end9
  %maperr.0 = phi i1 [ %tobool11.not, %if.end9 ], [ true, %sw.epilog ]
  br i1 %nonfault, label %return, label %if.end16

if.end16:                                         ; preds = %if.end13
  %add.ptr.i9 = getelementptr i8, ptr %env, i64 -10176
  tail call void @cpu_loop_exit_sigsegv(ptr noundef %add.ptr.i9, i64 noundef %addr, i32 noundef %access_type, i1 noundef zeroext %maperr.0, i64 noundef %ra) #17
  unreachable

return:                                           ; preds = %if.end13, %land.lhs.true, %if.end
  %retval.0 = phi i32 [ 0, %if.end ], [ 1024, %land.lhs.true ], [ 2048, %if.end13 ]
  ret i32 %retval.0
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local ptr @probe_access(ptr noundef %env, i64 noundef %addr, i32 noundef %size, i32 noundef %access_type, i32 noundef %mmu_idx, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %or = or i64 %addr, -4096
  %sub = sub nsw i64 0, %or
  %conv = sext i32 %size to i64
  %cmp.not = icmp ult i64 %sub, %conv
  br i1 %cmp.not, label %if.else, label %do.end

if.else:                                          ; preds = %entry
  tail call void @g_assertion_message_expr(ptr noundef null, ptr noundef nonnull @.str, i32 noundef 832, ptr noundef nonnull @__func__.probe_access, ptr noundef nonnull @.str.15) #17
  unreachable

do.end:                                           ; preds = %entry
  %call = tail call fastcc i32 @probe_access_internal(ptr noundef %env, i64 noundef %addr, i32 noundef %access_type, i1 noundef zeroext false, i64 noundef %ra)
  %and = and i32 %call, 3071
  %cmp3 = icmp eq i32 %and, 0
  br i1 %cmp3, label %do.end8, label %if.else6

if.else6:                                         ; preds = %do.end
  tail call void @g_assertion_message_expr(ptr noundef null, ptr noundef nonnull @.str, i32 noundef 834, ptr noundef nonnull @__func__.probe_access, ptr noundef nonnull @.str.16) #17
  unreachable

do.end8:                                          ; preds = %do.end
  %tobool.not = icmp eq i32 %size, 0
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i to ptr
  %cond = select i1 %tobool.not, ptr null, ptr %1
  ret ptr %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @get_page_addr_code_hostp(ptr noundef %env, i64 noundef returned %addr, ptr noundef writeonly %hostp) local_unnamed_addr #2 {
entry:
  %call = tail call fastcc i32 @probe_access_internal(ptr noundef %env, i64 noundef %addr, i32 noundef 2, i1 noundef zeroext false, i64 noundef 0)
  %cmp = icmp eq i32 %call, 0
  br i1 %cmp, label %do.end, label %if.else

if.else:                                          ; preds = %entry
  tail call void @g_assertion_message_expr(ptr noundef null, ptr noundef nonnull @.str, i32 noundef 845, ptr noundef nonnull @__func__.get_page_addr_code_hostp, ptr noundef nonnull @.str.17) #17
  unreachable

do.end:                                           ; preds = %entry
  %tobool.not = icmp eq ptr %hostp, null
  br i1 %tobool.not, label %if.end3, label %if.then1

if.then1:                                         ; preds = %do.end
  %0 = load i64, ptr @guest_base, align 8
  %add.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i to ptr
  store ptr %1, ptr %hostp, align 8
  br label %if.end3

if.end3:                                          ; preds = %if.then1, %do.end
  ret i64 %addr
}

; Function Attrs: mustprogress nofree nosync nounwind sspstrong willreturn uwtable
define dso_local i32 @cpu_ldub_code(ptr nocapture noundef readnone %env, i64 noundef %ptr) local_unnamed_addr #9 {
entry:
  %0 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 1, ptr %0, align 8
  fence syncscope("singlethread") seq_cst
  %1 = load i64, ptr @guest_base, align 8
  %add.i = add i64 %1, %ptr
  %2 = inttoptr i64 %add.i to ptr
  %call.val = load i8, ptr %2, align 1
  %conv.i = zext i8 %call.val to i32
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %0, align 8
  ret i32 %conv.i
}

; Function Attrs: mustprogress nofree nosync nounwind sspstrong willreturn uwtable
define dso_local i32 @cpu_lduw_code(ptr nocapture noundef readnone %env, i64 noundef %ptr) local_unnamed_addr #9 {
entry:
  %0 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 1, ptr %0, align 8
  fence syncscope("singlethread") seq_cst
  %1 = load i64, ptr @guest_base, align 8
  %add.i = add i64 %1, %ptr
  %2 = inttoptr i64 %add.i to ptr
  %call.val = load i16, ptr %2, align 1
  %conv.i.i = zext i16 %call.val to i32
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %0, align 8
  ret i32 %conv.i.i
}

; Function Attrs: mustprogress nofree nosync nounwind sspstrong willreturn uwtable
define dso_local i32 @cpu_ldl_code(ptr nocapture noundef readnone %env, i64 noundef %ptr) local_unnamed_addr #9 {
entry:
  %0 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 1, ptr %0, align 8
  fence syncscope("singlethread") seq_cst
  %1 = load i64, ptr @guest_base, align 8
  %add.i = add i64 %1, %ptr
  %2 = inttoptr i64 %add.i to ptr
  %call.val = load i32, ptr %2, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %0, align 8
  ret i32 %call.val
}

; Function Attrs: mustprogress nofree nosync nounwind sspstrong willreturn uwtable
define dso_local i64 @cpu_ldq_code(ptr nocapture noundef readnone %env, i64 noundef %ptr) local_unnamed_addr #9 {
entry:
  %0 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 1, ptr %0, align 8
  fence syncscope("singlethread") seq_cst
  %1 = load i64, ptr @guest_base, align 8
  %add.i = add i64 %1, %ptr
  %2 = inttoptr i64 %add.i to ptr
  %call.val = load i64, ptr %2, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %0, align 8
  ret i64 %call.val
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local zeroext i8 @cpu_ldb_code_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %oi, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %and.i.i = and i32 %oi, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %oi, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %cpu_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 2, i64 noundef %ra) #17
  unreachable

cpu_mmu_lookup.exit:                              ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %ra, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %call1.val = load i8, ptr %1, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  ret i8 %call1.val
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local zeroext i16 @cpu_ldw_code_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %oi, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %and.i.i = and i32 %oi, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %oi, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %cpu_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 2, i64 noundef %ra) #17
  unreachable

cpu_mmu_lookup.exit:                              ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %ra, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %call1.val = load i16, ptr %1, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  %3 = and i32 %oi, 256
  %tobool.not = icmp eq i32 %3, 0
  %4 = tail call i16 @llvm.bswap.i16(i16 %call1.val)
  %spec.select = select i1 %tobool.not, i16 %call1.val, i16 %4
  ret i16 %spec.select
}

; Function Attrs: mustprogress nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare i16 @llvm.bswap.i16(i16) #1

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_ldl_code_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %oi, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %and.i.i = and i32 %oi, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %oi, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %cpu_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 2, i64 noundef %ra) #17
  unreachable

cpu_mmu_lookup.exit:                              ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %ra, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %call1.val = load i32, ptr %1, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  %3 = and i32 %oi, 256
  %tobool.not = icmp eq i32 %3, 0
  %4 = tail call i32 @llvm.bswap.i32(i32 %call1.val)
  %spec.select = select i1 %tobool.not, i32 %call1.val, i32 %4
  ret i32 %spec.select
}

; Function Attrs: mustprogress nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare i32 @llvm.bswap.i32(i32) #1

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_ldq_code_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %oi, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %and.i.i = and i32 %oi, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %oi, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %cpu_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 0, i64 noundef %ra) #17
  unreachable

cpu_mmu_lookup.exit:                              ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %ra, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %call1.val = load i64, ptr %1, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  %3 = and i32 %oi, 256
  %tobool.not = icmp eq i32 %3, 0
  %4 = tail call i64 @llvm.bswap.i64(i64 %call1.val)
  %spec.select = select i1 %tobool.not, i64 %call1.val, i64 %4
  ret i64 %spec.select
}

; Function Attrs: mustprogress nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare i64 @llvm.bswap.i64(i64) #1

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_ldub_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %shr.i = lshr i32 %oi, 4
  %0 = and i32 %oi, 112
  %cmp = icmp eq i32 %0, 0
  tail call void @llvm.assume(i1 %cmp)
  %and.i.i.i = and i32 %shr.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %get_alignment_bits.exit.i.i
  ]

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %entry, %if.else4.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %shr.i.i.i, %if.else4.i.i.i ], [ 0, %entry ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %do_ld1_mmu.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 0, i64 noundef %retaddr) #17
  unreachable

do_ld1_mmu.exit:                                  ; preds = %get_alignment_bits.exit.i.i
  %1 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %1, %addr
  %2 = inttoptr i64 %add.i.i.i.i to ptr
  %3 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %3, align 8
  fence syncscope("singlethread") seq_cst
  %call1.val.i = load i8, ptr %2, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %3, align 8
  %conv = zext i8 %call1.val.i to i64
  ret i64 %conv
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_lduw_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %0 = and i32 %oi, 112
  %cmp = icmp eq i32 %0, 16
  tail call void @llvm.assume(i1 %cmp)
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %call2 = tail call fastcc zeroext i16 @do_ld2_mmu(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i64 noundef %retaddr)
  %conv = zext i16 %call2 to i64
  ret i64 %conv
}

; Function Attrs: nounwind sspstrong uwtable
define internal fastcc zeroext i16 @do_ld2_mmu(ptr noundef %cpu, i64 noundef %addr, i32 noundef %oi, i64 noundef %ra) unnamed_addr #10 {
entry:
  %shr.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %cpu_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %cpu, i64 noundef %addr, i32 noundef 0, i64 noundef %ra) #17
  unreachable

cpu_mmu_lookup.exit:                              ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %ra, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %and.i6 = and i64 %add.i.i.i, 1
  %cmp.i = icmp eq i64 %and.i6, 0
  br i1 %cmp.i, label %if.then.i9, label %if.end.i

if.then.i9:                                       ; preds = %cpu_mmu_lookup.exit
  call void @llvm.assume(i1 true) [ "align"(ptr %1, i64 2) ]
  %3 = load atomic i16, ptr %1 monotonic, align 2
  br label %load_atom_2.exit

if.end.i:                                         ; preds = %cpu_mmu_lookup.exit
  %4 = load i32, ptr @cpuinfo, align 4
  %and2.i = and i32 %4, 65536
  %tobool3.not.i = icmp ne i32 %and2.i, 0
  %or.i = or i64 %add.i.i.i, -4096
  %cmp11.i = icmp ult i64 %or.i, -8
  %or.cond.i = and i1 %cmp11.i, %tobool3.not.i
  br i1 %or.cond.i, label %if.then19.i, label %if.end23.i

if.then19.i:                                      ; preds = %if.end.i
  %and.i.i7 = and i64 %add.i.i.i, -8
  %5 = inttoptr i64 %and.i.i7 to ptr
  %and2.i.i = and i32 %4, 131072
  %tobool.not.i.i = icmp eq i32 %and2.i.i, 0
  %and3.i.i8 = and i64 %add.i.i.i, 8
  %tobool4.not.i.i = icmp eq i64 %and3.i.i8, 0
  %or.cond.i.i = and i1 %tobool4.not.i.i, %tobool.not.i.i
  br i1 %or.cond.i.i, label %if.else.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.then19.i
  %6 = tail call <2 x i64> asm "vmovdqu $1, $0", "=x,*m,~{dirflag},~{fpsr},~{flags}"(ptr elementtype(i128) %5) #19, !srcloc !11
  br label %load_atom_extract_al16_or_al8.exit.i

if.else.i.i:                                      ; preds = %if.then19.i
  %7 = tail call <2 x i64> asm "vmovdqa $1, $0", "=x,*m,~{dirflag},~{fpsr},~{flags}"(ptr elementtype(i128) %5) #19, !srcloc !12
  br label %load_atom_extract_al16_or_al8.exit.i

load_atom_extract_al16_or_al8.exit.i:             ; preds = %if.else.i.i, %if.then.i.i
  %r.sroa.0.0.i.i = phi <2 x i64> [ %6, %if.then.i.i ], [ %7, %if.else.i.i ]
  %.tr.i.i = trunc i64 %add.i.i.i to i32
  %8 = shl i32 %.tr.i.i, 3
  %conv.i.i = and i32 %8, 56
  %coerce.sroa.0.0.extract.trunc.i.i = extractelement <2 x i64> %r.sroa.0.0.i.i, i64 0
  %coerce.sroa.2.0.extract.trunc.i.i = extractelement <2 x i64> %r.sroa.0.0.i.i, i64 1
  %a.sroa.2.0.insert.ext.i.i.i = zext i64 %coerce.sroa.2.0.extract.trunc.i.i to i128
  %a.sroa.2.0.insert.shift.i.i.i = shl nuw i128 %a.sroa.2.0.insert.ext.i.i.i, 64
  %a.sroa.0.0.insert.ext.i.i.i = zext i64 %coerce.sroa.0.0.extract.trunc.i.i to i128
  %a.sroa.0.0.insert.insert.i.i.i = or disjoint i128 %a.sroa.2.0.insert.shift.i.i.i, %a.sroa.0.0.insert.ext.i.i.i
  %sh_prom.i.i.i = zext nneg i32 %conv.i.i to i128
  %shr.i.i.i = lshr i128 %a.sroa.0.0.insert.insert.i.i.i, %sh_prom.i.i.i
  %conv21.i = trunc i128 %shr.i.i.i to i16
  br label %load_atom_2.exit

if.end23.i:                                       ; preds = %if.end.i
  %and1.i.i = and i32 %shr.i, 7
  %cond.i.i = tail call i32 @llvm.usub.sat.i32(i32 %and1.i.i, i32 1)
  %and.i12.i = lshr i32 %oi, 12
  %9 = and i32 %and.i12.i, 7
  switch i32 %9, label %do.body.i.i [
    i32 5, label %sw.epilog.i.i
    i32 1, label %sw.bb2.i.i
    i32 0, label %sw.bb3.i.i
    i32 2, label %sw.bb11.i.i
    i32 3, label %sw.bb20.i.i
    i32 4, label %sw.epilog.i.i
  ]

sw.bb2.i.i:                                       ; preds = %if.end23.i
  br label %sw.bb3.i.i

sw.bb3.i.i:                                       ; preds = %sw.bb2.i.i, %if.end23.i
  %size.0.i.i = phi i32 [ %and1.i.i, %if.end23.i ], [ %cond.i.i, %sw.bb2.i.i ]
  %notmask.i.i = shl nsw i32 -1, %size.0.i.i
  %sub4.i.i = xor i32 %notmask.i.i, -1
  %conv.i14.i = zext nneg i32 %sub4.i.i to i64
  %and5.i.i = and i64 %add.i.i.i, %conv.i14.i
  %tobool6.not.i.i = icmp eq i64 %and5.i.i, 0
  %cond10.i.i = select i1 %tobool6.not.i.i, i32 %size.0.i.i, i32 0
  br label %sw.epilog.i.i

sw.bb11.i.i:                                      ; preds = %if.end23.i
  %10 = trunc i64 %add.i.i.i to i32
  %conv13.i.i = and i32 %10, 15
  %shl14.i.i = shl nuw nsw i32 1, %and1.i.i
  %add.i.i = add nuw nsw i32 %conv13.i.i, %shl14.i.i
  %cmp.i.i = icmp ult i32 %add.i.i, 17
  %cond19.i.i = select i1 %cmp.i.i, i32 %and1.i.i, i32 0
  br label %sw.epilog.i.i

sw.bb20.i.i:                                      ; preds = %if.end23.i
  %11 = trunc i64 %add.i.i.i to i32
  %conv22.i.i = and i32 %11, 15
  %shl23.i.i = shl nuw nsw i32 1, %and1.i.i
  %add24.i.i = add nuw nsw i32 %conv22.i.i, %shl23.i.i
  %cmp25.i.i = icmp ult i32 %add24.i.i, 17
  br i1 %cmp25.i.i, label %sw.epilog.i.i, label %if.else.i13.i

if.else.i13.i:                                    ; preds = %sw.bb20.i.i
  %shl27.i.i = shl nuw nsw i32 1, %cond.i.i
  %add28.i.i = add nuw nsw i32 %conv22.i.i, %shl27.i.i
  %cmp29.i.i = icmp eq i32 %add28.i.i, 16
  %sub33.i.i = sub nsw i32 0, %cond.i.i
  %spec.select.i.i = select i1 %cmp29.i.i, i32 %cond.i.i, i32 %sub33.i.i
  br label %sw.epilog.i.i

do.body.i.i:                                      ; preds = %if.end23.i
  tail call void @g_assertion_message_expr(ptr noundef null, ptr noundef nonnull @.str.20, i32 noundef 87, ptr noundef nonnull @__func__.required_atomicity, ptr noundef null) #17
  unreachable

sw.epilog.i.i:                                    ; preds = %if.else.i13.i, %sw.bb20.i.i, %sw.bb11.i.i, %sw.bb3.i.i, %if.end23.i, %if.end23.i
  %atmax.0.i.i = phi i32 [ %cond19.i.i, %sw.bb11.i.i ], [ %cond10.i.i, %sw.bb3.i.i ], [ 0, %if.end23.i ], [ %and1.i.i, %sw.bb20.i.i ], [ %spec.select.i.i, %if.else.i13.i ], [ 0, %if.end23.i ]
  %tcg_cflags.i.i.i = getelementptr inbounds %struct.CPUState, ptr %cpu, i64 0, i32 53
  %12 = load i32, ptr %tcg_cflags.i.i.i, align 16
  %and.i.i.i = and i32 %12, 32768
  %tobool.not.i.i.i = icmp eq i32 %and.i.i.i, 0
  br i1 %tobool.not.i.i.i, label %sw.bb.i, label %cpu_in_serial_context.exit.i.i

cpu_in_serial_context.exit.i.i:                   ; preds = %sw.epilog.i.i
  %13 = getelementptr i8, ptr %cpu, i64 208
  %cs.val.i.i.i = load i32, ptr %13, align 16
  %cs.val.i.fr.i.i = freeze i32 %cs.val.i.i.i
  %tobool.i.i.not.i.i = icmp eq i32 %cs.val.i.fr.i.i, 0
  br i1 %tobool.i.i.not.i.i, label %required_atomicity.exit.i, label %sw.bb.i

required_atomicity.exit.i:                        ; preds = %cpu_in_serial_context.exit.i.i
  switch i32 %atmax.0.i.i, label %do.body.i [
    i32 0, label %sw.bb.i
    i32 1, label %sw.bb27.i
  ]

sw.bb.i:                                          ; preds = %required_atomicity.exit.i, %cpu_in_serial_context.exit.i.i, %sw.epilog.i.i
  %pv.val.i = load i16, ptr %1, align 1
  br label %load_atom_2.exit

sw.bb27.i:                                        ; preds = %required_atomicity.exit.i
  %and28.i = and i64 %add.i.i.i, 15
  %cmp29.not.i = icmp eq i64 %and28.i, 7
  br i1 %cmp29.not.i, label %if.end34.i, label %if.then31.i

if.then31.i:                                      ; preds = %sw.bb27.i
  %conv.i16.i = shl i64 %add.i.i.i, 3
  %mul.i.i = and i64 %conv.i16.i, 56
  %and1.i17.i = and i64 %add.i.i.i, -8
  %14 = inttoptr i64 %and1.i17.i to ptr
  call void @llvm.assume(i1 true) [ "align"(ptr %14, i64 8) ]
  %15 = load atomic i64, ptr %14 monotonic, align 8
  %shr2.i.i = lshr i64 %15, %mul.i.i
  %conv33.i = trunc i64 %shr2.i.i to i16
  br label %load_atom_2.exit

if.end34.i:                                       ; preds = %sw.bb27.i
  %and1.i18.i = and i64 %add.i.i.i, -16
  %16 = inttoptr i64 %and1.i18.i to ptr
  call void @llvm.assume(i1 true) [ "align"(ptr %16, i64 16) ]
  %tobool.not.i.i20.i = icmp eq i32 %and2.i, 0
  br i1 %tobool.not.i.i20.i, label %if.end.i.i.i, label %if.then.i.i.i

if.then.i.i.i:                                    ; preds = %if.end34.i
  %17 = tail call <2 x i64> asm "vmovdqa $1, $0", "=x,*m,~{dirflag},~{fpsr},~{flags}"(ptr elementtype(i128) %16) #19, !srcloc !13
  br label %load_atom_extract_al16_or_exit.exit.i

if.end.i.i.i:                                     ; preds = %if.end34.i
  tail call void @mmap_lock() #16
  %18 = load i64, ptr @guest_base, align 8
  %sub.i.i.i = sub i64 %and1.i18.i, %18
  %19 = load i64, ptr @reserved_va, align 8
  %tobool4.not.i.i.i = icmp ne i64 %19, 0
  %cmp5.not11.i.i.i = icmp ult i64 %19, %sub.i.i.i
  %cmp5.not.i.i.i = select i1 %tobool4.not.i.i.i, i1 %cmp5.not11.i.i.i, i1 false
  br i1 %cmp5.not.i.i.i, label %if.else.i.i.i, label %if.end8.i.i.i

if.else.i.i.i:                                    ; preds = %if.end.i.i.i
  tail call void @__assert_fail(ptr noundef nonnull @.str.21, ptr noundef nonnull @.str.20, i32 noundef 205, ptr noundef nonnull @__PRETTY_FUNCTION__.load_atomic16_or_exit) #17
  unreachable

if.end8.i.i.i:                                    ; preds = %if.end.i.i.i
  %call11.i.i.i = tail call zeroext i1 @page_check_range(i64 noundef %sub.i.i.i, i64 noundef 16, i32 noundef 16)
  br i1 %call11.i.i.i, label %if.end13.i.i.i, label %if.then12.i.i.i

if.then12.i.i.i:                                  ; preds = %if.end8.i.i.i
  %20 = load i128, ptr %16, align 16
  %retval.sroa.4.0.extract.shift7.i.i.i = lshr i128 %20, 64
  %21 = insertelement <2 x i128> poison, i128 %20, i64 0
  %22 = insertelement <2 x i128> %21, i128 %retval.sroa.4.0.extract.shift7.i.i.i, i64 1
  %23 = trunc <2 x i128> %22 to <2 x i64>
  br label %cleanup.i.i.i

if.end13.i.i.i:                                   ; preds = %if.end8.i.i.i
  call void @llvm.assume(i1 true) [ "align"(ptr %16, i64 16) ]
  %24 = load i32, ptr @cpuinfo, align 4
  %and.i12.i.i.i = and i32 %24, 65536
  %tobool.not.i.i.i.i = icmp eq i32 %and.i12.i.i.i, 0
  br i1 %tobool.not.i.i.i.i, label %if.else.i.i.i.i, label %if.then.i.i.i.i

if.then.i.i.i.i:                                  ; preds = %if.end13.i.i.i
  %25 = tail call <2 x i64> asm "vmovdqa $1, $0", "=x,*m,~{dirflag},~{fpsr},~{flags}"(ptr elementtype(i128) %16) #19, !srcloc !14
  br label %cleanup.i.i.i

if.else.i.i.i.i:                                  ; preds = %if.end13.i.i.i
  %26 = cmpxchg ptr %16, i128 0, i128 0 seq_cst seq_cst, align 16
  %27 = extractvalue { i128, i1 } %26, 0
  %extract4.i.i.i.i = lshr i128 %27, 64
  %28 = insertelement <2 x i128> poison, i128 %27, i64 0
  %29 = insertelement <2 x i128> %28, i128 %extract4.i.i.i.i, i64 1
  %30 = trunc <2 x i128> %29 to <2 x i64>
  br label %cleanup.i.i.i

cleanup.i.i.i:                                    ; preds = %if.else.i.i.i.i, %if.then.i.i.i.i, %if.then12.i.i.i
  %31 = phi <2 x i64> [ %23, %if.then12.i.i.i ], [ %25, %if.then.i.i.i.i ], [ %30, %if.else.i.i.i.i ]
  tail call void @mmap_unlock() #16
  br label %load_atom_extract_al16_or_exit.exit.i

load_atom_extract_al16_or_exit.exit.i:            ; preds = %cleanup.i.i.i, %if.then.i.i.i
  %32 = phi <2 x i64> [ %17, %if.then.i.i.i ], [ %31, %cleanup.i.i.i ]
  %33 = extractelement <2 x i64> %32, i64 1
  %a.sroa.2.0.insert.ext.i.i23.i = zext i64 %33 to i128
  %a.sroa.2.0.insert.shift.i.i24.i = shl nuw i128 %a.sroa.2.0.insert.ext.i.i23.i, 64
  %34 = extractelement <2 x i64> %32, i64 0
  %a.sroa.0.0.insert.ext.i.i25.i = zext i64 %34 to i128
  %a.sroa.0.0.insert.insert.i.i26.i = or disjoint i128 %a.sroa.2.0.insert.shift.i.i24.i, %a.sroa.0.0.insert.ext.i.i25.i
  %shr.i.i28.i = lshr i128 %a.sroa.0.0.insert.insert.i.i26.i, 56
  %conv36.i = trunc i128 %shr.i.i28.i to i16
  br label %load_atom_2.exit

do.body.i:                                        ; preds = %required_atomicity.exit.i
  tail call void @g_assertion_message_expr(ptr noundef null, ptr noundef nonnull @.str.20, i32 noundef 428, ptr noundef nonnull @__func__.load_atom_2, ptr noundef null) #17
  unreachable

load_atom_2.exit:                                 ; preds = %if.then.i9, %load_atom_extract_al16_or_al8.exit.i, %sw.bb.i, %if.then31.i, %load_atom_extract_al16_or_exit.exit.i
  %retval.0.i = phi i16 [ %3, %if.then.i9 ], [ %conv21.i, %load_atom_extract_al16_or_al8.exit.i ], [ %conv33.i, %if.then31.i ], [ %conv36.i, %load_atom_extract_al16_or_exit.exit.i ], [ %pv.val.i, %sw.bb.i ]
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  %35 = and i32 %oi, 256
  %tobool.not = icmp eq i32 %35, 0
  %36 = tail call i16 @llvm.bswap.i16(i16 %retval.0.i)
  %spec.select = select i1 %tobool.not, i16 %retval.0.i, i16 %36
  ret i16 %spec.select
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_ldul_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %0 = and i32 %oi, 112
  %cmp = icmp eq i32 %0, 32
  tail call void @llvm.assume(i1 %cmp)
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %call2 = tail call fastcc i32 @do_ld4_mmu(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i64 noundef %retaddr)
  %conv = zext i32 %call2 to i64
  ret i64 %conv
}

; Function Attrs: nounwind sspstrong uwtable
define internal fastcc i32 @do_ld4_mmu(ptr noundef %cpu, i64 noundef %addr, i32 noundef %oi, i64 noundef %ra) unnamed_addr #10 {
entry:
  %shr.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %cpu_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %cpu, i64 noundef %addr, i32 noundef 0, i64 noundef %ra) #17
  unreachable

cpu_mmu_lookup.exit:                              ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %ra, ptr %1, align 8
  fence syncscope("singlethread") seq_cst
  %and.i6 = and i64 %add.i.i.i, 3
  %cmp.i = icmp eq i64 %and.i6, 0
  br i1 %cmp.i, label %if.then.i10, label %if.end.i

if.then.i10:                                      ; preds = %cpu_mmu_lookup.exit
  %2 = inttoptr i64 %add.i.i.i to ptr
  call void @llvm.assume(i1 true) [ "align"(ptr %2, i64 4) ]
  %3 = load atomic i32, ptr %2 monotonic, align 4
  br label %load_atom_4.exit

if.end.i:                                         ; preds = %cpu_mmu_lookup.exit
  %4 = load i32, ptr @cpuinfo, align 4
  %and2.i = and i32 %4, 65536
  %tobool3.not.i = icmp ne i32 %and2.i, 0
  %or.i = or i64 %add.i.i.i, -4096
  %cmp11.i = icmp ult i64 %or.i, -8
  %or.cond.i = and i1 %cmp11.i, %tobool3.not.i
  br i1 %or.cond.i, label %if.then19.i, label %if.end23.i

if.then19.i:                                      ; preds = %if.end.i
  %and.i.i8 = and i64 %add.i.i.i, -8
  %5 = inttoptr i64 %and.i.i8 to ptr
  %and2.i.i = and i32 %4, 131072
  %tobool.not.i.i = icmp eq i32 %and2.i.i, 0
  %and3.i.i9 = and i64 %add.i.i.i, 8
  %tobool4.not.i.i = icmp eq i64 %and3.i.i9, 0
  %or.cond.i.i = and i1 %tobool4.not.i.i, %tobool.not.i.i
  br i1 %or.cond.i.i, label %if.else.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.then19.i
  %6 = tail call <2 x i64> asm "vmovdqu $1, $0", "=x,*m,~{dirflag},~{fpsr},~{flags}"(ptr elementtype(i128) %5) #19, !srcloc !11
  br label %load_atom_extract_al16_or_al8.exit.i

if.else.i.i:                                      ; preds = %if.then19.i
  %7 = tail call <2 x i64> asm "vmovdqa $1, $0", "=x,*m,~{dirflag},~{fpsr},~{flags}"(ptr elementtype(i128) %5) #19, !srcloc !12
  br label %load_atom_extract_al16_or_al8.exit.i

load_atom_extract_al16_or_al8.exit.i:             ; preds = %if.else.i.i, %if.then.i.i
  %r.sroa.0.0.i.i = phi <2 x i64> [ %6, %if.then.i.i ], [ %7, %if.else.i.i ]
  %.tr.i.i = trunc i64 %add.i.i.i to i32
  %8 = shl i32 %.tr.i.i, 3
  %conv.i.i = and i32 %8, 56
  %coerce.sroa.0.0.extract.trunc.i.i = extractelement <2 x i64> %r.sroa.0.0.i.i, i64 0
  %coerce.sroa.2.0.extract.trunc.i.i = extractelement <2 x i64> %r.sroa.0.0.i.i, i64 1
  %a.sroa.2.0.insert.ext.i.i.i = zext i64 %coerce.sroa.2.0.extract.trunc.i.i to i128
  %a.sroa.2.0.insert.shift.i.i.i = shl nuw i128 %a.sroa.2.0.insert.ext.i.i.i, 64
  %a.sroa.0.0.insert.ext.i.i.i = zext i64 %coerce.sroa.0.0.extract.trunc.i.i to i128
  %a.sroa.0.0.insert.insert.i.i.i = or disjoint i128 %a.sroa.2.0.insert.shift.i.i.i, %a.sroa.0.0.insert.ext.i.i.i
  %sh_prom.i.i.i = zext nneg i32 %conv.i.i to i128
  %shr.i.i.i = lshr i128 %a.sroa.0.0.insert.insert.i.i.i, %sh_prom.i.i.i
  %conv21.i = trunc i128 %shr.i.i.i to i32
  br label %load_atom_4.exit

if.end23.i:                                       ; preds = %if.end.i
  %and1.i.i = and i32 %shr.i, 7
  %cond.i.i = tail call i32 @llvm.usub.sat.i32(i32 %and1.i.i, i32 1)
  %and.i12.i = lshr i32 %oi, 12
  %9 = and i32 %and.i12.i, 7
  switch i32 %9, label %do.body.i.i [
    i32 5, label %sw.epilog.i.i
    i32 1, label %sw.bb2.i.i
    i32 0, label %sw.bb3.i.i
    i32 2, label %sw.bb11.i.i
    i32 3, label %sw.bb20.i.i
    i32 4, label %sw.bb35.i.i
  ]

sw.bb2.i.i:                                       ; preds = %if.end23.i
  br label %sw.bb3.i.i

sw.bb3.i.i:                                       ; preds = %sw.bb2.i.i, %if.end23.i
  %size.0.i.i = phi i32 [ %and1.i.i, %if.end23.i ], [ %cond.i.i, %sw.bb2.i.i ]
  %notmask.i.i = shl nsw i32 -1, %size.0.i.i
  %sub4.i.i = xor i32 %notmask.i.i, -1
  %conv.i14.i = zext nneg i32 %sub4.i.i to i64
  %and5.i.i = and i64 %add.i.i.i, %conv.i14.i
  %tobool6.not.i.i = icmp eq i64 %and5.i.i, 0
  %cond10.i.i = select i1 %tobool6.not.i.i, i32 %size.0.i.i, i32 0
  br label %sw.epilog.i.i

sw.bb11.i.i:                                      ; preds = %if.end23.i
  %10 = trunc i64 %add.i.i.i to i32
  %conv13.i.i = and i32 %10, 15
  %shl14.i.i = shl nuw nsw i32 1, %and1.i.i
  %add.i.i = add nuw nsw i32 %conv13.i.i, %shl14.i.i
  %cmp.i.i = icmp ult i32 %add.i.i, 17
  %cond19.i.i = select i1 %cmp.i.i, i32 %and1.i.i, i32 0
  br label %sw.epilog.i.i

sw.bb20.i.i:                                      ; preds = %if.end23.i
  %11 = trunc i64 %add.i.i.i to i32
  %conv22.i.i = and i32 %11, 15
  %shl23.i.i = shl nuw nsw i32 1, %and1.i.i
  %add24.i.i = add nuw nsw i32 %conv22.i.i, %shl23.i.i
  %cmp25.i.i = icmp ult i32 %add24.i.i, 17
  br i1 %cmp25.i.i, label %sw.epilog.i.i, label %if.else.i13.i

if.else.i13.i:                                    ; preds = %sw.bb20.i.i
  %shl27.i.i = shl nuw nsw i32 1, %cond.i.i
  %add28.i.i = add nuw nsw i32 %conv22.i.i, %shl27.i.i
  %cmp29.i.i = icmp eq i32 %add28.i.i, 16
  %sub33.i.i = sub nsw i32 0, %cond.i.i
  %spec.select.i.i = select i1 %cmp29.i.i, i32 %cond.i.i, i32 %sub33.i.i
  br label %sw.epilog.i.i

sw.bb35.i.i:                                      ; preds = %if.end23.i
  %conv36.i.i = trunc i64 %add.i.i.i to i32
  %12 = tail call i32 @llvm.cttz.i32(i32 %conv36.i.i, i1 false), !range !15
  %cond43.i.i = tail call i32 @llvm.umin.i32(i32 %and1.i.i, i32 %12)
  br label %sw.epilog.i.i

do.body.i.i:                                      ; preds = %if.end23.i
  tail call void @g_assertion_message_expr(ptr noundef null, ptr noundef nonnull @.str.20, i32 noundef 87, ptr noundef nonnull @__func__.required_atomicity, ptr noundef null) #17
  unreachable

sw.epilog.i.i:                                    ; preds = %sw.bb35.i.i, %if.else.i13.i, %sw.bb20.i.i, %sw.bb11.i.i, %sw.bb3.i.i, %if.end23.i
  %atmax.0.i.i = phi i32 [ %cond43.i.i, %sw.bb35.i.i ], [ %cond19.i.i, %sw.bb11.i.i ], [ %cond10.i.i, %sw.bb3.i.i ], [ 0, %if.end23.i ], [ %and1.i.i, %sw.bb20.i.i ], [ %spec.select.i.i, %if.else.i13.i ]
  %tcg_cflags.i.i.i = getelementptr inbounds %struct.CPUState, ptr %cpu, i64 0, i32 53
  %13 = load i32, ptr %tcg_cflags.i.i.i, align 16
  %and.i.i.i = and i32 %13, 32768
  %tobool.not.i.i.i = icmp eq i32 %and.i.i.i, 0
  br i1 %tobool.not.i.i.i, label %sw.bb.i, label %cpu_in_serial_context.exit.i.i

cpu_in_serial_context.exit.i.i:                   ; preds = %sw.epilog.i.i
  %14 = getelementptr i8, ptr %cpu, i64 208
  %cs.val.i.i.i = load i32, ptr %14, align 16
  %cs.val.i.fr.i.i = freeze i32 %cs.val.i.i.i
  %tobool.i.i.not.i.i = icmp eq i32 %cs.val.i.fr.i.i, 0
  br i1 %tobool.i.i.not.i.i, label %required_atomicity.exit.i, label %sw.bb.i

required_atomicity.exit.i:                        ; preds = %cpu_in_serial_context.exit.i.i
  switch i32 %atmax.0.i.i, label %do.body.i [
    i32 0, label %sw.bb.i
    i32 1, label %sw.bb.i
    i32 -1, label %sw.bb.i
    i32 2, label %sw.bb26.i
  ]

sw.bb.i:                                          ; preds = %required_atomicity.exit.i, %required_atomicity.exit.i, %required_atomicity.exit.i, %cpu_in_serial_context.exit.i.i, %sw.epilog.i.i
  %.tr.i15.i = trunc i64 %add.i.i.i to i32
  %15 = shl i32 %.tr.i15.i, 3
  %conv.i16.i = and i32 %15, 24
  %and1.i17.i = and i64 %add.i.i.i, -4
  %16 = inttoptr i64 %and1.i17.i to ptr
  call void @llvm.assume(i1 true) [ "align"(ptr %16, i64 4) ]
  %17 = load atomic i32, ptr %16 monotonic, align 4
  %add.ptr.i.i = getelementptr i8, ptr %16, i64 4
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i.i, i64 4) ]
  %18 = load atomic i32, ptr %add.ptr.i.i monotonic, align 4
  %shr.i.i7 = lshr i32 %17, %conv.i16.i
  %sub.i.i = sub i32 0, %15
  %and3.i18.i = and i32 %sub.i.i, 24
  %shl.i.i = shl i32 %18, %and3.i18.i
  %or.i.i = or i32 %shl.i.i, %shr.i.i7
  br label %load_atom_4.exit

sw.bb26.i:                                        ; preds = %required_atomicity.exit.i
  %and27.i = and i64 %add.i.i.i, 4
  %tobool28.not.i = icmp eq i64 %and27.i, 0
  br i1 %tobool28.not.i, label %if.then29.i, label %if.end31.i

if.then29.i:                                      ; preds = %sw.bb26.i
  %conv.i19.i = shl i64 %add.i.i.i, 3
  %mul.i.i = and i64 %conv.i19.i, 24
  %and1.i20.i = and i64 %add.i.i.i, -8
  %19 = inttoptr i64 %and1.i20.i to ptr
  call void @llvm.assume(i1 true) [ "align"(ptr %19, i64 8) ]
  %20 = load atomic i64, ptr %19 monotonic, align 8
  %shr2.i.i = lshr i64 %20, %mul.i.i
  %conv3.i.i = trunc i64 %shr2.i.i to i32
  br label %load_atom_4.exit

if.end31.i:                                       ; preds = %sw.bb26.i
  %and1.i21.i = and i64 %add.i.i.i, -8
  %21 = inttoptr i64 %and1.i21.i to ptr
  call void @llvm.assume(i1 true) [ "align"(ptr %21, i64 16) ]
  %tobool.not.i.i23.i = icmp eq i32 %and2.i, 0
  br i1 %tobool.not.i.i23.i, label %if.end.i.i.i, label %if.then.i.i.i

if.then.i.i.i:                                    ; preds = %if.end31.i
  %22 = tail call <2 x i64> asm "vmovdqa $1, $0", "=x,*m,~{dirflag},~{fpsr},~{flags}"(ptr elementtype(i128) %21) #19, !srcloc !13
  br label %load_atom_extract_al16_or_exit.exit.i

if.end.i.i.i:                                     ; preds = %if.end31.i
  tail call void @mmap_lock() #16
  %23 = load i64, ptr @guest_base, align 8
  %sub.i.i.i = sub i64 %and1.i21.i, %23
  %24 = load i64, ptr @reserved_va, align 8
  %tobool4.not.i.i.i = icmp ne i64 %24, 0
  %cmp5.not11.i.i.i = icmp ult i64 %24, %sub.i.i.i
  %cmp5.not.i.i.i = select i1 %tobool4.not.i.i.i, i1 %cmp5.not11.i.i.i, i1 false
  br i1 %cmp5.not.i.i.i, label %if.else.i.i.i, label %if.end8.i.i.i

if.else.i.i.i:                                    ; preds = %if.end.i.i.i
  tail call void @__assert_fail(ptr noundef nonnull @.str.21, ptr noundef nonnull @.str.20, i32 noundef 205, ptr noundef nonnull @__PRETTY_FUNCTION__.load_atomic16_or_exit) #17
  unreachable

if.end8.i.i.i:                                    ; preds = %if.end.i.i.i
  %call11.i.i.i = tail call zeroext i1 @page_check_range(i64 noundef %sub.i.i.i, i64 noundef 16, i32 noundef 16)
  br i1 %call11.i.i.i, label %if.end13.i.i.i, label %if.then12.i.i.i

if.then12.i.i.i:                                  ; preds = %if.end8.i.i.i
  %25 = load i128, ptr %21, align 16
  %retval.sroa.4.0.extract.shift7.i.i.i = lshr i128 %25, 64
  %26 = insertelement <2 x i128> poison, i128 %25, i64 0
  %27 = insertelement <2 x i128> %26, i128 %retval.sroa.4.0.extract.shift7.i.i.i, i64 1
  %28 = trunc <2 x i128> %27 to <2 x i64>
  br label %cleanup.i.i.i

if.end13.i.i.i:                                   ; preds = %if.end8.i.i.i
  call void @llvm.assume(i1 true) [ "align"(ptr %21, i64 16) ]
  %29 = load i32, ptr @cpuinfo, align 4
  %and.i12.i.i.i = and i32 %29, 65536
  %tobool.not.i.i.i.i = icmp eq i32 %and.i12.i.i.i, 0
  br i1 %tobool.not.i.i.i.i, label %if.else.i.i.i.i, label %if.then.i.i.i.i

if.then.i.i.i.i:                                  ; preds = %if.end13.i.i.i
  %30 = tail call <2 x i64> asm "vmovdqa $1, $0", "=x,*m,~{dirflag},~{fpsr},~{flags}"(ptr elementtype(i128) %21) #19, !srcloc !14
  br label %cleanup.i.i.i

if.else.i.i.i.i:                                  ; preds = %if.end13.i.i.i
  %31 = cmpxchg ptr %21, i128 0, i128 0 seq_cst seq_cst, align 16
  %32 = extractvalue { i128, i1 } %31, 0
  %extract4.i.i.i.i = lshr i128 %32, 64
  %33 = insertelement <2 x i128> poison, i128 %32, i64 0
  %34 = insertelement <2 x i128> %33, i128 %extract4.i.i.i.i, i64 1
  %35 = trunc <2 x i128> %34 to <2 x i64>
  br label %cleanup.i.i.i

cleanup.i.i.i:                                    ; preds = %if.else.i.i.i.i, %if.then.i.i.i.i, %if.then12.i.i.i
  %36 = phi <2 x i64> [ %28, %if.then12.i.i.i ], [ %30, %if.then.i.i.i.i ], [ %35, %if.else.i.i.i.i ]
  tail call void @mmap_unlock() #16
  br label %load_atom_extract_al16_or_exit.exit.i

load_atom_extract_al16_or_exit.exit.i:            ; preds = %cleanup.i.i.i, %if.then.i.i.i
  %37 = phi <2 x i64> [ %22, %if.then.i.i.i ], [ %36, %cleanup.i.i.i ]
  %38 = trunc i64 %add.i.i.i to i32
  %conv.i24.i = shl i32 %38, 3
  %mul.i25.i = and i32 %conv.i24.i, 56
  %39 = extractelement <2 x i64> %37, i64 1
  %a.sroa.2.0.insert.ext.i.i26.i = zext i64 %39 to i128
  %a.sroa.2.0.insert.shift.i.i27.i = shl nuw i128 %a.sroa.2.0.insert.ext.i.i26.i, 64
  %40 = extractelement <2 x i64> %37, i64 0
  %a.sroa.0.0.insert.ext.i.i28.i = zext i64 %40 to i128
  %a.sroa.0.0.insert.insert.i.i29.i = or disjoint i128 %a.sroa.2.0.insert.shift.i.i27.i, %a.sroa.0.0.insert.ext.i.i28.i
  %sh_prom.i.i30.i = zext nneg i32 %mul.i25.i to i128
  %shr.i.i31.i = lshr i128 %a.sroa.0.0.insert.insert.i.i29.i, %sh_prom.i.i30.i
  %conv33.i = trunc i128 %shr.i.i31.i to i32
  br label %load_atom_4.exit

do.body.i:                                        ; preds = %required_atomicity.exit.i
  tail call void @g_assertion_message_expr(ptr noundef null, ptr noundef nonnull @.str.20, i32 noundef 473, ptr noundef nonnull @__func__.load_atom_4, ptr noundef null) #17
  unreachable

load_atom_4.exit:                                 ; preds = %if.then.i10, %load_atom_extract_al16_or_al8.exit.i, %sw.bb.i, %if.then29.i, %load_atom_extract_al16_or_exit.exit.i
  %retval.0.i = phi i32 [ %3, %if.then.i10 ], [ %conv21.i, %load_atom_extract_al16_or_al8.exit.i ], [ %conv33.i, %load_atom_extract_al16_or_exit.exit.i ], [ %conv3.i.i, %if.then29.i ], [ %or.i.i, %sw.bb.i ]
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %1, align 8
  %41 = and i32 %oi, 256
  %tobool.not = icmp eq i32 %41, 0
  %42 = tail call i32 @llvm.bswap.i32(i32 %retval.0.i)
  %spec.select = select i1 %tobool.not, i32 %retval.0.i, i32 %42
  ret i32 %spec.select
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_ldq_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %0 = and i32 %oi, 112
  %cmp = icmp eq i32 %0, 48
  tail call void @llvm.assume(i1 %cmp)
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %call2 = tail call fastcc i64 @do_ld8_mmu(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i64 noundef %retaddr)
  ret i64 %call2
}

; Function Attrs: nounwind sspstrong uwtable
define internal fastcc i64 @do_ld8_mmu(ptr noundef %cpu, i64 noundef %addr, i32 noundef %oi, i64 noundef %ra) unnamed_addr #10 {
entry:
  %shr.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %cpu_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %cpu, i64 noundef %addr, i32 noundef 0, i64 noundef %ra) #17
  unreachable

cpu_mmu_lookup.exit:                              ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %ra, ptr %1, align 8
  fence syncscope("singlethread") seq_cst
  %and.i6 = and i64 %add.i.i.i, 7
  %cmp.i = icmp eq i64 %and.i6, 0
  br i1 %cmp.i, label %if.then.i10, label %if.end.i

if.then.i10:                                      ; preds = %cpu_mmu_lookup.exit
  %2 = inttoptr i64 %add.i.i.i to ptr
  call void @llvm.assume(i1 true) [ "align"(ptr %2, i64 8) ]
  %3 = load atomic i64, ptr %2 monotonic, align 8
  br label %load_atom_8.exit

if.end.i:                                         ; preds = %cpu_mmu_lookup.exit
  %4 = load i32, ptr @cpuinfo, align 4
  %and2.i = and i32 %4, 65536
  %tobool3.not.i = icmp eq i32 %and2.i, 0
  br i1 %tobool3.not.i, label %if.end12.i, label %if.then10.i

if.then10.i:                                      ; preds = %if.end.i
  %and.i.i7 = and i64 %add.i.i.i, -8
  %5 = inttoptr i64 %and.i.i7 to ptr
  %and2.i.i = and i32 %4, 131072
  %tobool.not.i.i = icmp eq i32 %and2.i.i, 0
  %and3.i.i8 = and i64 %add.i.i.i, 8
  %tobool4.not.i.i = icmp eq i64 %and3.i.i8, 0
  %or.cond.i.i = and i1 %tobool4.not.i.i, %tobool.not.i.i
  br i1 %or.cond.i.i, label %if.else.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.then10.i
  %6 = tail call <2 x i64> asm "vmovdqu $1, $0", "=x,*m,~{dirflag},~{fpsr},~{flags}"(ptr elementtype(i128) %5) #19, !srcloc !11
  br label %load_atom_extract_al16_or_al8.exit.i

if.else.i.i:                                      ; preds = %if.then10.i
  %7 = tail call <2 x i64> asm "vmovdqa $1, $0", "=x,*m,~{dirflag},~{fpsr},~{flags}"(ptr elementtype(i128) %5) #19, !srcloc !12
  br label %load_atom_extract_al16_or_al8.exit.i

load_atom_extract_al16_or_al8.exit.i:             ; preds = %if.else.i.i, %if.then.i.i
  %r.sroa.0.0.i.i = phi <2 x i64> [ %6, %if.then.i.i ], [ %7, %if.else.i.i ]
  %.tr.i.i = trunc i64 %add.i.i.i to i32
  %8 = shl i32 %.tr.i.i, 3
  %conv.i.i = and i32 %8, 56
  %coerce.sroa.0.0.extract.trunc.i.i = extractelement <2 x i64> %r.sroa.0.0.i.i, i64 0
  %coerce.sroa.2.0.extract.trunc.i.i = extractelement <2 x i64> %r.sroa.0.0.i.i, i64 1
  %a.sroa.2.0.insert.ext.i.i.i = zext i64 %coerce.sroa.2.0.extract.trunc.i.i to i128
  %a.sroa.2.0.insert.shift.i.i.i = shl nuw i128 %a.sroa.2.0.insert.ext.i.i.i, 64
  %a.sroa.0.0.insert.ext.i.i.i = zext i64 %coerce.sroa.0.0.extract.trunc.i.i to i128
  %a.sroa.0.0.insert.insert.i.i.i = or disjoint i128 %a.sroa.2.0.insert.shift.i.i.i, %a.sroa.0.0.insert.ext.i.i.i
  %sh_prom.i.i.i = zext nneg i32 %conv.i.i to i128
  %shr.i.i.i = lshr i128 %a.sroa.0.0.insert.insert.i.i.i, %sh_prom.i.i.i
  %retval.sroa.0.0.extract.trunc.i.i.i = trunc i128 %shr.i.i.i to i64
  br label %load_atom_8.exit

if.end12.i:                                       ; preds = %if.end.i
  %and1.i.i = and i32 %shr.i, 7
  %cond.i.i = tail call i32 @llvm.usub.sat.i32(i32 %and1.i.i, i32 1)
  %and.i7.i = lshr i32 %oi, 12
  %9 = and i32 %and.i7.i, 7
  switch i32 %9, label %do.body.i.i [
    i32 5, label %sw.epilog.i.i
    i32 1, label %sw.bb2.i.i
    i32 0, label %sw.bb3.i.i
    i32 2, label %sw.bb11.i.i
    i32 3, label %sw.bb20.i.i
    i32 4, label %sw.bb35.i.i
  ]

sw.bb2.i.i:                                       ; preds = %if.end12.i
  br label %sw.bb3.i.i

sw.bb3.i.i:                                       ; preds = %sw.bb2.i.i, %if.end12.i
  %size.0.i.i = phi i32 [ %and1.i.i, %if.end12.i ], [ %cond.i.i, %sw.bb2.i.i ]
  %notmask.i.i = shl nsw i32 -1, %size.0.i.i
  %sub4.i.i = xor i32 %notmask.i.i, -1
  %conv.i9.i = zext nneg i32 %sub4.i.i to i64
  %and5.i.i = and i64 %add.i.i.i, %conv.i9.i
  %tobool6.not.i.i = icmp eq i64 %and5.i.i, 0
  %cond10.i.i = select i1 %tobool6.not.i.i, i32 %size.0.i.i, i32 0
  br label %sw.epilog.i.i

sw.bb11.i.i:                                      ; preds = %if.end12.i
  %10 = trunc i64 %add.i.i.i to i32
  %conv13.i.i = and i32 %10, 15
  %shl14.i.i = shl nuw nsw i32 1, %and1.i.i
  %add.i.i = add nuw nsw i32 %conv13.i.i, %shl14.i.i
  %cmp.i.i = icmp ult i32 %add.i.i, 17
  %cond19.i.i = select i1 %cmp.i.i, i32 %and1.i.i, i32 0
  br label %sw.epilog.i.i

sw.bb20.i.i:                                      ; preds = %if.end12.i
  %11 = trunc i64 %add.i.i.i to i32
  %conv22.i.i = and i32 %11, 15
  %shl23.i.i = shl nuw nsw i32 1, %and1.i.i
  %add24.i.i = add nuw nsw i32 %conv22.i.i, %shl23.i.i
  %cmp25.i.i = icmp ult i32 %add24.i.i, 17
  br i1 %cmp25.i.i, label %sw.epilog.i.i, label %if.else.i8.i

if.else.i8.i:                                     ; preds = %sw.bb20.i.i
  %shl27.i.i = shl nuw nsw i32 1, %cond.i.i
  %add28.i.i = add nuw nsw i32 %conv22.i.i, %shl27.i.i
  %cmp29.i.i = icmp eq i32 %add28.i.i, 16
  %sub33.i.i = sub nsw i32 0, %cond.i.i
  %spec.select.i.i = select i1 %cmp29.i.i, i32 %cond.i.i, i32 %sub33.i.i
  br label %sw.epilog.i.i

sw.bb35.i.i:                                      ; preds = %if.end12.i
  %conv36.i.i = trunc i64 %add.i.i.i to i32
  %12 = tail call i32 @llvm.cttz.i32(i32 %conv36.i.i, i1 false), !range !15
  %cond43.i.i = tail call i32 @llvm.umin.i32(i32 %and1.i.i, i32 %12)
  br label %sw.epilog.i.i

do.body.i.i:                                      ; preds = %if.end12.i
  tail call void @g_assertion_message_expr(ptr noundef null, ptr noundef nonnull @.str.20, i32 noundef 87, ptr noundef nonnull @__func__.required_atomicity, ptr noundef null) #17
  unreachable

sw.epilog.i.i:                                    ; preds = %sw.bb35.i.i, %if.else.i8.i, %sw.bb20.i.i, %sw.bb11.i.i, %sw.bb3.i.i, %if.end12.i
  %atmax.0.i.i = phi i32 [ %cond43.i.i, %sw.bb35.i.i ], [ %cond19.i.i, %sw.bb11.i.i ], [ %cond10.i.i, %sw.bb3.i.i ], [ 0, %if.end12.i ], [ %and1.i.i, %sw.bb20.i.i ], [ %spec.select.i.i, %if.else.i8.i ]
  %tcg_cflags.i.i.i = getelementptr inbounds %struct.CPUState, ptr %cpu, i64 0, i32 53
  %13 = load i32, ptr %tcg_cflags.i.i.i, align 16
  %and.i.i.i = and i32 %13, 32768
  %tobool.not.i.i.i = icmp eq i32 %and.i.i.i, 0
  br i1 %tobool.not.i.i.i, label %if.end18.i, label %cpu_in_serial_context.exit.i.i

cpu_in_serial_context.exit.i.i:                   ; preds = %sw.epilog.i.i
  %14 = getelementptr i8, ptr %cpu, i64 208
  %cs.val.i.i.i = load i32, ptr %14, align 16
  %cs.val.i.fr.i.i = freeze i32 %cs.val.i.i.i
  %tobool.i.i.not.i.i = icmp eq i32 %cs.val.i.fr.i.i, 0
  %cmp14.i = icmp eq i32 %atmax.0.i.i, 3
  %or.cond.i = select i1 %tobool.i.i.not.i.i, i1 %cmp14.i, i1 false
  br i1 %or.cond.i, label %if.end.i.i.i, label %if.end18.i

if.end.i.i.i:                                     ; preds = %cpu_in_serial_context.exit.i.i
  %and1.i10.i = and i64 %add.i.i.i, -8
  %15 = inttoptr i64 %and1.i10.i to ptr
  call void @llvm.assume(i1 true) [ "align"(ptr %15, i64 16) ]
  tail call void @mmap_lock() #16
  %16 = load i64, ptr @guest_base, align 8
  %sub.i.i.i = sub i64 %and1.i10.i, %16
  %17 = load i64, ptr @reserved_va, align 8
  %tobool4.not.i.i.i = icmp ne i64 %17, 0
  %cmp5.not11.i.i.i = icmp ult i64 %17, %sub.i.i.i
  %cmp5.not.i.i.i = select i1 %tobool4.not.i.i.i, i1 %cmp5.not11.i.i.i, i1 false
  br i1 %cmp5.not.i.i.i, label %if.else.i.i.i, label %if.end8.i.i.i

if.else.i.i.i:                                    ; preds = %if.end.i.i.i
  tail call void @__assert_fail(ptr noundef nonnull @.str.21, ptr noundef nonnull @.str.20, i32 noundef 205, ptr noundef nonnull @__PRETTY_FUNCTION__.load_atomic16_or_exit) #17
  unreachable

if.end8.i.i.i:                                    ; preds = %if.end.i.i.i
  %call11.i.i.i = tail call zeroext i1 @page_check_range(i64 noundef %sub.i.i.i, i64 noundef 16, i32 noundef 16)
  br i1 %call11.i.i.i, label %if.end13.i.i.i, label %if.then12.i.i.i

if.then12.i.i.i:                                  ; preds = %if.end8.i.i.i
  %18 = load i128, ptr %15, align 16
  %retval.sroa.0.0.extract.trunc5.i.i.i = trunc i128 %18 to i64
  %retval.sroa.4.0.extract.shift7.i.i.i = lshr i128 %18, 64
  %retval.sroa.4.0.extract.trunc8.i.i.i = trunc i128 %retval.sroa.4.0.extract.shift7.i.i.i to i64
  br label %load_atom_extract_al16_or_exit.exit.i

if.end13.i.i.i:                                   ; preds = %if.end8.i.i.i
  call void @llvm.assume(i1 true) [ "align"(ptr %15, i64 16) ]
  %19 = load i32, ptr @cpuinfo, align 4
  %and.i12.i.i.i = and i32 %19, 65536
  %tobool.not.i.i.i.i = icmp eq i32 %and.i12.i.i.i, 0
  br i1 %tobool.not.i.i.i.i, label %if.else.i.i.i.i, label %if.then.i.i.i.i

if.then.i.i.i.i:                                  ; preds = %if.end13.i.i.i
  %20 = tail call <2 x i64> asm "vmovdqa $1, $0", "=x,*m,~{dirflag},~{fpsr},~{flags}"(ptr elementtype(i128) %15) #19, !srcloc !14
  %extract.t.i.i.i.i = extractelement <2 x i64> %20, i64 0
  %extract.t3.i.i.i.i = extractelement <2 x i64> %20, i64 1
  br label %load_atom_extract_al16_or_exit.exit.i

if.else.i.i.i.i:                                  ; preds = %if.end13.i.i.i
  %21 = cmpxchg ptr %15, i128 0, i128 0 seq_cst seq_cst, align 16
  %22 = extractvalue { i128, i1 } %21, 0
  %extract.t2.i.i.i.i = trunc i128 %22 to i64
  %extract4.i.i.i.i = lshr i128 %22, 64
  %extract.t5.i.i.i.i = trunc i128 %extract4.i.i.i.i to i64
  br label %load_atom_extract_al16_or_exit.exit.i

load_atom_extract_al16_or_exit.exit.i:            ; preds = %if.else.i.i.i.i, %if.then.i.i.i.i, %if.then12.i.i.i
  %retval.sroa.0.0.i.i.i = phi i64 [ %retval.sroa.0.0.extract.trunc5.i.i.i, %if.then12.i.i.i ], [ %extract.t.i.i.i.i, %if.then.i.i.i.i ], [ %extract.t2.i.i.i.i, %if.else.i.i.i.i ]
  %retval.sroa.4.0.i.i.i = phi i64 [ %retval.sroa.4.0.extract.trunc8.i.i.i, %if.then12.i.i.i ], [ %extract.t3.i.i.i.i, %if.then.i.i.i.i ], [ %extract.t5.i.i.i.i, %if.else.i.i.i.i ]
  tail call void @mmap_unlock() #16
  %23 = trunc i64 %add.i.i.i to i32
  %conv.i13.i = shl i32 %23, 3
  %mul.i.i = and i32 %conv.i13.i, 56
  %a.sroa.2.0.insert.ext.i.i14.i = zext i64 %retval.sroa.4.0.i.i.i to i128
  %a.sroa.2.0.insert.shift.i.i15.i = shl nuw i128 %a.sroa.2.0.insert.ext.i.i14.i, 64
  %a.sroa.0.0.insert.ext.i.i16.i = zext i64 %retval.sroa.0.0.i.i.i to i128
  %a.sroa.0.0.insert.insert.i.i17.i = or disjoint i128 %a.sroa.2.0.insert.shift.i.i15.i, %a.sroa.0.0.insert.ext.i.i16.i
  %sh_prom.i.i18.i = zext nneg i32 %mul.i.i to i128
  %shr.i.i19.i = lshr i128 %a.sroa.0.0.insert.insert.i.i17.i, %sh_prom.i.i18.i
  %retval.sroa.0.0.extract.trunc.i.i20.i = trunc i128 %shr.i.i19.i to i64
  br label %load_atom_8.exit

if.end18.i:                                       ; preds = %cpu_in_serial_context.exit.i.i, %sw.epilog.i.i
  %.tr.i21.i = trunc i64 %add.i.i.i to i32
  %24 = shl i32 %.tr.i21.i, 3
  %conv.i22.i = and i32 %24, 56
  %and1.i23.i = and i64 %add.i.i.i, -8
  %25 = inttoptr i64 %and1.i23.i to ptr
  call void @llvm.assume(i1 true) [ "align"(ptr %25, i64 8) ]
  %26 = load atomic i64, ptr %25 monotonic, align 8
  %add.ptr.i.i = getelementptr i8, ptr %25, i64 8
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i.i, i64 8) ]
  %27 = load atomic i64, ptr %add.ptr.i.i monotonic, align 8
  %sh_prom.i.i = zext nneg i32 %conv.i22.i to i64
  %shr.i.i9 = lshr i64 %26, %sh_prom.i.i
  %sub.i.i = sub i32 0, %24
  %and3.i24.i = and i32 %sub.i.i, 56
  %sh_prom4.i.i = zext nneg i32 %and3.i24.i to i64
  %shl.i.i = shl i64 %27, %sh_prom4.i.i
  %or.i.i = or i64 %shl.i.i, %shr.i.i9
  br label %load_atom_8.exit

load_atom_8.exit:                                 ; preds = %if.then.i10, %load_atom_extract_al16_or_al8.exit.i, %load_atom_extract_al16_or_exit.exit.i, %if.end18.i
  %retval.0.i = phi i64 [ %3, %if.then.i10 ], [ %retval.sroa.0.0.extract.trunc.i.i.i, %load_atom_extract_al16_or_al8.exit.i ], [ %retval.sroa.0.0.extract.trunc.i.i20.i, %load_atom_extract_al16_or_exit.exit.i ], [ %or.i.i, %if.end18.i ]
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %1, align 8
  %28 = and i32 %oi, 256
  %tobool.not = icmp eq i32 %28, 0
  %29 = tail call i64 @llvm.bswap.i64(i64 %retval.0.i)
  %spec.select = select i1 %tobool.not, i64 %retval.0.i, i64 %29
  ret i64 %spec.select
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_ldsb_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %shr.i.i = lshr i32 %oi, 4
  %0 = and i32 %oi, 112
  %cmp.i = icmp eq i32 %0, 0
  tail call void @llvm.assume(i1 %cmp.i)
  %and.i.i.i.i = and i32 %shr.i.i, 224
  %trunc.i.i.i.i = trunc i32 %and.i.i.i.i to i8
  switch i8 %trunc.i.i.i.i, label %if.else4.i.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i.i
    i8 -32, label %get_alignment_bits.exit.i.i.i
  ]

if.else4.i.i.i.i:                                 ; preds = %entry
  %shr.i.i.i.i = lshr exact i32 %and.i.i.i.i, 5
  br label %get_alignment_bits.exit.i.i.i

get_alignment_bits.exit.i.i.i:                    ; preds = %if.else4.i.i.i.i, %entry, %entry
  %a.0.i.i.i.i = phi i32 [ %shr.i.i.i.i, %if.else4.i.i.i.i ], [ 0, %entry ], [ 0, %entry ]
  %notmask.i.i.i = shl nsw i32 -1, %a.0.i.i.i.i
  %sub.i.i.i = xor i32 %notmask.i.i.i, -1
  %conv.i.i.i = zext nneg i32 %sub.i.i.i to i64
  %and.i.i.i = and i64 %conv.i.i.i, %addr
  %tobool.not.i.i.i = icmp eq i64 %and.i.i.i, 0
  br i1 %tobool.not.i.i.i, label %helper_ldub_mmu.exit, label %if.then.i.i.i

if.then.i.i.i:                                    ; preds = %get_alignment_bits.exit.i.i.i
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 0, i64 noundef %retaddr) #17
  unreachable

helper_ldub_mmu.exit:                             ; preds = %get_alignment_bits.exit.i.i.i
  %1 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i.i = add i64 %1, %addr
  %2 = inttoptr i64 %add.i.i.i.i.i to ptr
  %3 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %3, align 8
  fence syncscope("singlethread") seq_cst
  %call1.val.i.i = load i8, ptr %2, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %3, align 8
  %conv1 = sext i8 %call1.val.i.i to i64
  ret i64 %conv1
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_ldsw_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %0 = and i32 %oi, 112
  %cmp.i = icmp eq i32 %0, 16
  tail call void @llvm.assume(i1 %cmp.i)
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i = tail call fastcc zeroext i16 @do_ld2_mmu(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i64 noundef %retaddr)
  %conv1 = sext i16 %call2.i to i64
  ret i64 %conv1
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_ldsl_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %0 = and i32 %oi, 112
  %cmp.i = icmp eq i32 %0, 32
  tail call void @llvm.assume(i1 %cmp.i)
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i = tail call fastcc i32 @do_ld4_mmu(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i64 noundef %retaddr)
  %conv1 = sext i32 %call2.i to i64
  ret i64 %conv1
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local { i64, i64 } @helper_ld16_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %0 = and i32 %oi, 112
  %cmp = icmp eq i32 %0, 64
  tail call void @llvm.assume(i1 %cmp)
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %call2 = tail call fastcc { i64, i64 } @do_ld16_mmu(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i64 noundef %retaddr)
  ret { i64, i64 } %call2
}

; Function Attrs: nounwind sspstrong uwtable
define internal fastcc { i64, i64 } @do_ld16_mmu(ptr noundef %cpu, i64 noundef %addr, i32 noundef %oi, i64 noundef %ra) unnamed_addr #10 {
entry:
  %shr.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ 4, %if.then2.i.i ], [ %shr.i.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %cpu_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %cpu, i64 noundef %addr, i32 noundef 0, i64 noundef %ra) #17
  unreachable

cpu_mmu_lookup.exit:                              ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %ra, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = load i32, ptr @cpuinfo, align 4
  %and.i11 = and i32 %3, 65536
  %tobool.not.i12 = icmp ne i32 %and.i11, 0
  %and3.i = and i64 %add.i.i.i, 15
  %cmp.i = icmp eq i64 %and3.i, 0
  %or.cond.i = and i1 %cmp.i, %tobool.not.i12
  br i1 %or.cond.i, label %if.then.i15, label %if.end.i

if.then.i15:                                      ; preds = %cpu_mmu_lookup.exit
  %4 = tail call <2 x i64> asm "vmovdqa $1, $0", "=x,*m,~{dirflag},~{fpsr},~{flags}"(ptr elementtype(i128) %1) #19, !srcloc !13
  %retval.sroa.0.0.extract.trunc.i.i = extractelement <2 x i64> %4, i64 0
  %retval.sroa.2.0.extract.trunc.i.i = extractelement <2 x i64> %4, i64 1
  br label %load_atom_16.exit

if.end.i:                                         ; preds = %cpu_mmu_lookup.exit
  %and.i23.i = lshr i32 %oi, 12
  %5 = and i32 %and.i23.i, 7
  switch i32 %5, label %do.body.i.i [
    i32 5, label %sw.epilog.i.i
    i32 1, label %sw.bb2.i.i
    i32 0, label %sw.bb3.i.i
    i32 2, label %sw.bb11.i.i
    i32 3, label %sw.bb20.i.i
    i32 4, label %sw.bb35.i.i
  ]

sw.bb2.i.i:                                       ; preds = %if.end.i
  br label %sw.bb3.i.i

sw.bb3.i.i:                                       ; preds = %sw.bb2.i.i, %if.end.i
  %size.0.i.i = phi i32 [ 4, %if.end.i ], [ 3, %sw.bb2.i.i ]
  %notmask.i.i = shl nsw i32 -1, %size.0.i.i
  %sub4.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub4.i.i to i64
  %and5.i.i = and i64 %add.i.i.i, %conv.i.i
  %tobool6.not.i.i = icmp eq i64 %and5.i.i, 0
  %cond10.i.i = select i1 %tobool6.not.i.i, i32 %size.0.i.i, i32 0
  br label %sw.epilog.i.i

sw.bb11.i.i:                                      ; preds = %if.end.i
  %cond19.i.i = select i1 %cmp.i, i32 4, i32 0
  br label %sw.epilog.i.i

sw.bb20.i.i:                                      ; preds = %if.end.i
  %6 = trunc i64 %add.i.i.i to i32
  %conv22.i.i = and i32 %6, 15
  %cmp25.i.i = icmp eq i32 %conv22.i.i, 0
  br i1 %cmp25.i.i, label %sw.epilog.i.i, label %if.else.i.i

if.else.i.i:                                      ; preds = %sw.bb20.i.i
  %cmp29.i.i = icmp eq i32 %conv22.i.i, 8
  %spec.select.i.i = select i1 %cmp29.i.i, i32 3, i32 -3
  br label %sw.epilog.i.i

sw.bb35.i.i:                                      ; preds = %if.end.i
  %conv36.i.i = trunc i64 %add.i.i.i to i32
  %7 = tail call i32 @llvm.cttz.i32(i32 %conv36.i.i, i1 false), !range !15
  %cond43.i.i = tail call i32 @llvm.umin.i32(i32 %7, i32 4)
  br label %sw.epilog.i.i

do.body.i.i:                                      ; preds = %if.end.i
  tail call void @g_assertion_message_expr(ptr noundef null, ptr noundef nonnull @.str.20, i32 noundef 87, ptr noundef nonnull @__func__.required_atomicity, ptr noundef null) #17
  unreachable

sw.epilog.i.i:                                    ; preds = %sw.bb35.i.i, %if.else.i.i, %sw.bb20.i.i, %sw.bb11.i.i, %sw.bb3.i.i, %if.end.i
  %atmax.0.i.i = phi i32 [ %cond43.i.i, %sw.bb35.i.i ], [ %cond19.i.i, %sw.bb11.i.i ], [ %cond10.i.i, %sw.bb3.i.i ], [ 0, %if.end.i ], [ 4, %sw.bb20.i.i ], [ %spec.select.i.i, %if.else.i.i ]
  %tcg_cflags.i.i.i = getelementptr inbounds %struct.CPUState, ptr %cpu, i64 0, i32 53
  %8 = load i32, ptr %tcg_cflags.i.i.i, align 16
  %and.i.i.i = and i32 %8, 32768
  %tobool.not.i.i.i = icmp eq i32 %and.i.i.i, 0
  br i1 %tobool.not.i.i.i, label %sw.bb.i, label %cpu_in_serial_context.exit.i.i

cpu_in_serial_context.exit.i.i:                   ; preds = %sw.epilog.i.i
  %9 = getelementptr i8, ptr %cpu, i64 208
  %cs.val.i.i.i = load i32, ptr %9, align 16
  %cs.val.i.fr.i.i = freeze i32 %cs.val.i.i.i
  %tobool.i.i.not.i.i = icmp eq i32 %cs.val.i.fr.i.i, 0
  br i1 %tobool.i.i.not.i.i, label %required_atomicity.exit.i, label %sw.bb.i

required_atomicity.exit.i:                        ; preds = %cpu_in_serial_context.exit.i.i
  switch i32 %atmax.0.i.i, label %do.body.i [
    i32 0, label %sw.bb.i
    i32 1, label %sw.bb12.i
    i32 2, label %sw.bb15.i
    i32 3, label %sw.bb19.i
    i32 -3, label %sw.bb23.i
    i32 4, label %sw.bb27.i
  ]

sw.bb.i:                                          ; preds = %required_atomicity.exit.i, %cpu_in_serial_context.exit.i.i, %sw.epilog.i.i
  %r.0.copyload.i = load i128, ptr %1, align 1
  %retval.sroa.0.0.extract.trunc14.i = trunc i128 %r.0.copyload.i to i64
  %retval.sroa.5.0.extract.shift17.i = lshr i128 %r.0.copyload.i, 64
  %retval.sroa.5.0.extract.trunc18.i = trunc i128 %retval.sroa.5.0.extract.shift17.i to i64
  br label %load_atom_16.exit

sw.bb12.i:                                        ; preds = %required_atomicity.exit.i
  call void @llvm.assume(i1 true) [ "align"(ptr %1, i64 2) ]
  %10 = load atomic i16, ptr %1 monotonic, align 2
  %conv.i.i.i = zext i16 %10 to i64
  %add.ptr.i.i.i = getelementptr i8, ptr %1, i64 2
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i.i.i, i64 2) ]
  %11 = load atomic i16, ptr %add.ptr.i.i.i monotonic, align 2
  %conv2.i.i.i = zext i16 %11 to i64
  %shl.i.i.i = shl nuw nsw i64 %conv2.i.i.i, 16
  %add.ptr.i.i = getelementptr i8, ptr %1, i64 4
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i.i, i64 2) ]
  %12 = load atomic i16, ptr %add.ptr.i.i monotonic, align 2
  %conv.i2.i.i = zext i16 %12 to i64
  %add.ptr.i3.i.i = getelementptr i8, ptr %1, i64 6
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i3.i.i, i64 2) ]
  %13 = load atomic i16, ptr %add.ptr.i3.i.i monotonic, align 2
  %conv2.i4.i.i = zext i16 %13 to i64
  %14 = shl nuw i64 %conv2.i4.i.i, 48
  %15 = shl nuw nsw i64 %conv.i2.i.i, 32
  %shl.i.i = or disjoint i64 %shl.i.i.i, %conv.i.i.i
  %or.i.i.i = or disjoint i64 %shl.i.i, %15
  %or.i.i = or disjoint i64 %or.i.i.i, %14
  %add.ptr.i = getelementptr i8, ptr %1, i64 8
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i, i64 2) ]
  %16 = load atomic i16, ptr %add.ptr.i monotonic, align 2
  %conv.i.i24.i = zext i16 %16 to i64
  %add.ptr.i.i25.i = getelementptr i8, ptr %1, i64 10
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i.i25.i, i64 2) ]
  %17 = load atomic i16, ptr %add.ptr.i.i25.i monotonic, align 2
  %conv2.i.i26.i = zext i16 %17 to i64
  %shl.i.i27.i = shl nuw nsw i64 %conv2.i.i26.i, 16
  %add.ptr.i28.i = getelementptr i8, ptr %1, i64 12
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i28.i, i64 2) ]
  %18 = load atomic i16, ptr %add.ptr.i28.i monotonic, align 2
  %conv.i2.i29.i = zext i16 %18 to i64
  %add.ptr.i3.i30.i = getelementptr i8, ptr %1, i64 14
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i3.i30.i, i64 2) ]
  %19 = load atomic i16, ptr %add.ptr.i3.i30.i monotonic, align 2
  %conv2.i4.i31.i = zext i16 %19 to i64
  %20 = shl nuw i64 %conv2.i4.i31.i, 48
  %21 = shl nuw nsw i64 %conv.i2.i29.i, 32
  %shl.i32.i = or disjoint i64 %shl.i.i27.i, %conv.i.i24.i
  %or.i.i33.i = or disjoint i64 %shl.i32.i, %21
  %or.i34.i = or disjoint i64 %or.i.i33.i, %20
  br label %load_atom_16.exit

sw.bb15.i:                                        ; preds = %required_atomicity.exit.i
  call void @llvm.assume(i1 true) [ "align"(ptr %1, i64 4) ]
  %22 = load atomic i32, ptr %1 monotonic, align 4
  %add.ptr.i35.i = getelementptr i8, ptr %1, i64 4
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i35.i, i64 4) ]
  %23 = load atomic i32, ptr %add.ptr.i35.i monotonic, align 4
  %conv.i36.i = zext i32 %23 to i64
  %shl.i37.i = shl nuw i64 %conv.i36.i, 32
  %conv2.i.i = zext i32 %22 to i64
  %or.i38.i = or disjoint i64 %shl.i37.i, %conv2.i.i
  %add.ptr17.i = getelementptr i8, ptr %1, i64 8
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr17.i, i64 4) ]
  %24 = load atomic i32, ptr %add.ptr17.i monotonic, align 4
  %add.ptr.i39.i = getelementptr i8, ptr %1, i64 12
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i39.i, i64 4) ]
  %25 = load atomic i32, ptr %add.ptr.i39.i monotonic, align 4
  %conv.i40.i = zext i32 %25 to i64
  %shl.i41.i = shl nuw i64 %conv.i40.i, 32
  %conv2.i42.i = zext i32 %24 to i64
  %or.i43.i = or disjoint i64 %shl.i41.i, %conv2.i42.i
  br label %load_atom_16.exit

sw.bb19.i:                                        ; preds = %required_atomicity.exit.i
  call void @llvm.assume(i1 true) [ "align"(ptr %1, i64 8) ]
  %26 = load atomic i64, ptr %1 monotonic, align 8
  %add.ptr21.i = getelementptr i8, ptr %1, i64 8
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr21.i, i64 8) ]
  %27 = load atomic i64, ptr %add.ptr21.i monotonic, align 8
  br label %load_atom_16.exit

sw.bb23.i:                                        ; preds = %required_atomicity.exit.i
  %.tr.i.i = trunc i64 %add.i.i.i to i32
  %28 = shl i32 %.tr.i.i, 3
  %conv.i44.i = and i32 %28, 56
  %and1.i45.i = and i64 %add.i.i.i, -8
  %29 = inttoptr i64 %and1.i45.i to ptr
  call void @llvm.assume(i1 true) [ "align"(ptr %29, i64 8) ]
  %30 = load atomic i64, ptr %29 monotonic, align 8
  %add.ptr.i46.i = getelementptr i8, ptr %29, i64 8
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i46.i, i64 8) ]
  %31 = load atomic i64, ptr %add.ptr.i46.i monotonic, align 8
  %sh_prom.i.i = zext nneg i32 %conv.i44.i to i64
  %shr.i.i13 = lshr i64 %30, %sh_prom.i.i
  %sub.i.i = sub i32 0, %28
  %and3.i.i14 = and i32 %sub.i.i, 56
  %sh_prom4.i.i = zext nneg i32 %and3.i.i14 to i64
  %shl.i47.i = shl i64 %31, %sh_prom4.i.i
  %or.i48.i = or i64 %shl.i47.i, %shr.i.i13
  %add.ptr25.i = getelementptr i8, ptr %1, i64 8
  %32 = ptrtoint ptr %add.ptr25.i to i64
  %.tr.i49.i = trunc i64 %32 to i32
  %33 = shl i32 %.tr.i49.i, 3
  %conv.i50.i = and i32 %33, 56
  %and1.i51.i = and i64 %32, -8
  %34 = inttoptr i64 %and1.i51.i to ptr
  call void @llvm.assume(i1 true) [ "align"(ptr %34, i64 8) ]
  %35 = load atomic i64, ptr %34 monotonic, align 8
  %add.ptr.i52.i = getelementptr i8, ptr %34, i64 8
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i52.i, i64 8) ]
  %36 = load atomic i64, ptr %add.ptr.i52.i monotonic, align 8
  %sh_prom.i53.i = zext nneg i32 %conv.i50.i to i64
  %shr.i54.i = lshr i64 %35, %sh_prom.i53.i
  %sub.i55.i = sub i32 0, %33
  %and3.i56.i = and i32 %sub.i55.i, 56
  %sh_prom4.i57.i = zext nneg i32 %and3.i56.i to i64
  %shl.i58.i = shl i64 %36, %sh_prom4.i57.i
  %or.i59.i = or i64 %shl.i58.i, %shr.i54.i
  br label %load_atom_16.exit

sw.bb27.i:                                        ; preds = %required_atomicity.exit.i
  call void @llvm.assume(i1 true) [ "align"(ptr %1, i64 16) ]
  %tobool.not.i.i = icmp eq i32 %and.i11, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %sw.bb27.i
  %37 = tail call <2 x i64> asm "vmovdqa $1, $0", "=x,*m,~{dirflag},~{fpsr},~{flags}"(ptr elementtype(i128) %1) #19, !srcloc !13
  %retval.sroa.0.0.extract.trunc.i.i.i = extractelement <2 x i64> %37, i64 0
  %retval.sroa.2.0.extract.trunc.i.i.i = extractelement <2 x i64> %37, i64 1
  br label %load_atom_16.exit

if.end.i.i:                                       ; preds = %sw.bb27.i
  tail call void @mmap_lock() #16
  %38 = load i64, ptr @guest_base, align 8
  %sub.i63.i = sub i64 %add.i.i.i, %38
  %39 = load i64, ptr @reserved_va, align 8
  %tobool4.not.i.i = icmp ne i64 %39, 0
  %cmp5.not11.i.i = icmp ult i64 %39, %sub.i63.i
  %cmp5.not.i.i = select i1 %tobool4.not.i.i, i1 %cmp5.not11.i.i, i1 false
  br i1 %cmp5.not.i.i, label %if.else.i65.i, label %if.end8.i.i

if.else.i65.i:                                    ; preds = %if.end.i.i
  tail call void @__assert_fail(ptr noundef nonnull @.str.21, ptr noundef nonnull @.str.20, i32 noundef 205, ptr noundef nonnull @__PRETTY_FUNCTION__.load_atomic16_or_exit) #17
  unreachable

if.end8.i.i:                                      ; preds = %if.end.i.i
  %call11.i.i = tail call zeroext i1 @page_check_range(i64 noundef %sub.i63.i, i64 noundef 16, i32 noundef 16)
  br i1 %call11.i.i, label %if.end13.i.i, label %if.then12.i.i

if.then12.i.i:                                    ; preds = %if.end8.i.i
  %40 = load i128, ptr %1, align 16
  %retval.sroa.0.0.extract.trunc5.i.i = trunc i128 %40 to i64
  %retval.sroa.4.0.extract.shift7.i.i = lshr i128 %40, 64
  %retval.sroa.4.0.extract.trunc8.i.i = trunc i128 %retval.sroa.4.0.extract.shift7.i.i to i64
  br label %cleanup.i.i

if.end13.i.i:                                     ; preds = %if.end8.i.i
  call void @llvm.assume(i1 true) [ "align"(ptr %1, i64 16) ]
  %41 = load i32, ptr @cpuinfo, align 4
  %and.i12.i.i = and i32 %41, 65536
  %tobool.not.i.i64.i = icmp eq i32 %and.i12.i.i, 0
  br i1 %tobool.not.i.i64.i, label %if.else.i.i.i, label %if.then.i.i.i

if.then.i.i.i:                                    ; preds = %if.end13.i.i
  %42 = tail call <2 x i64> asm "vmovdqa $1, $0", "=x,*m,~{dirflag},~{fpsr},~{flags}"(ptr elementtype(i128) %1) #19, !srcloc !14
  %extract.t.i.i.i = extractelement <2 x i64> %42, i64 0
  %extract.t3.i.i.i = extractelement <2 x i64> %42, i64 1
  br label %cleanup.i.i

if.else.i.i.i:                                    ; preds = %if.end13.i.i
  %43 = cmpxchg ptr %1, i128 0, i128 0 seq_cst seq_cst, align 16
  %44 = extractvalue { i128, i1 } %43, 0
  %extract.t2.i.i.i = trunc i128 %44 to i64
  %extract4.i.i.i = lshr i128 %44, 64
  %extract.t5.i.i.i = trunc i128 %extract4.i.i.i to i64
  br label %cleanup.i.i

cleanup.i.i:                                      ; preds = %if.else.i.i.i, %if.then.i.i.i, %if.then12.i.i
  %retval.sroa.0.0.i.i = phi i64 [ %retval.sroa.0.0.extract.trunc5.i.i, %if.then12.i.i ], [ %extract.t.i.i.i, %if.then.i.i.i ], [ %extract.t2.i.i.i, %if.else.i.i.i ]
  %retval.sroa.4.0.i.i = phi i64 [ %retval.sroa.4.0.extract.trunc8.i.i, %if.then12.i.i ], [ %extract.t3.i.i.i, %if.then.i.i.i ], [ %extract.t5.i.i.i, %if.else.i.i.i ]
  tail call void @mmap_unlock() #16
  br label %load_atom_16.exit

do.body.i:                                        ; preds = %required_atomicity.exit.i
  tail call void @g_assertion_message_expr(ptr noundef null, ptr noundef nonnull @.str.20, i32 noundef 581, ptr noundef nonnull @__func__.load_atom_16, ptr noundef null) #17
  unreachable

load_atom_16.exit:                                ; preds = %if.then.i15, %sw.bb.i, %sw.bb12.i, %sw.bb15.i, %sw.bb19.i, %sw.bb23.i, %if.then.i.i, %cleanup.i.i
  %retval.sroa.0.0.i = phi i64 [ %retval.sroa.0.0.extract.trunc.i.i, %if.then.i15 ], [ %retval.sroa.0.0.extract.trunc14.i, %sw.bb.i ], [ %retval.sroa.0.0.extract.trunc.i.i.i, %if.then.i.i ], [ %retval.sroa.0.0.i.i, %cleanup.i.i ], [ %or.i48.i, %sw.bb23.i ], [ %26, %sw.bb19.i ], [ %or.i38.i, %sw.bb15.i ], [ %or.i.i, %sw.bb12.i ]
  %retval.sroa.5.0.i = phi i64 [ %retval.sroa.2.0.extract.trunc.i.i, %if.then.i15 ], [ %retval.sroa.5.0.extract.trunc18.i, %sw.bb.i ], [ %retval.sroa.2.0.extract.trunc.i.i.i, %if.then.i.i ], [ %retval.sroa.4.0.i.i, %cleanup.i.i ], [ %or.i59.i, %sw.bb23.i ], [ %27, %sw.bb19.i ], [ %or.i43.i, %sw.bb15.i ], [ %or.i34.i, %sw.bb12.i ]
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  %45 = and i32 %oi, 256
  %tobool.not = icmp eq i32 %45, 0
  %46 = tail call i64 @llvm.bswap.i64(i64 %retval.sroa.5.0.i)
  %47 = tail call i64 @llvm.bswap.i64(i64 %retval.sroa.0.0.i)
  %.pn19 = select i1 %tobool.not, i64 %retval.sroa.0.0.i, i64 %46
  %.pn = select i1 %tobool.not, i64 %retval.sroa.5.0.i, i64 %47
  %.fca.0.insert.i.i.pn = insertvalue { i64, i64 } undef, i64 %.pn19, 0
  %.fca.1.insert.merged = insertvalue { i64, i64 } %.fca.0.insert.i.i.pn, i64 %.pn, 1
  ret { i64, i64 } %.fca.1.insert.merged
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local { i64, i64 } @helper_ld_i128(ptr noundef %env, i64 noundef %addr, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %2 = and i32 %oi, 112
  %cmp.i = icmp eq i32 %2, 64
  tail call void @llvm.assume(i1 %cmp.i)
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i = tail call fastcc { i64, i64 } @do_ld16_mmu(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i64 noundef %1)
  ret { i64, i64 } %call2.i
}

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn memory(none)
declare ptr @llvm.returnaddress(i32 immarg) #11

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @helper_stb_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %shr.i = lshr i32 %oi, 4
  %0 = and i32 %oi, 112
  %cmp = icmp eq i32 %0, 0
  tail call void @llvm.assume(i1 %cmp)
  %and.i.i.i = and i32 %shr.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %get_alignment_bits.exit.i.i
  ]

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %entry, %if.else4.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %shr.i.i.i, %if.else4.i.i.i ], [ 0, %entry ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %do_st1_mmu.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %ra) #17
  unreachable

do_st1_mmu.exit:                                  ; preds = %get_alignment_bits.exit.i.i
  %conv = trunc i32 %val to i8
  %1 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %1, %addr
  %2 = inttoptr i64 %add.i.i.i.i to ptr
  %3 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %ra, ptr %3, align 8
  fence syncscope("singlethread") seq_cst
  store i8 %conv, ptr %2, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %3, align 8
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @helper_stw_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %0 = and i32 %oi, 112
  %cmp = icmp eq i32 %0, 16
  tail call void @llvm.assume(i1 %cmp)
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %conv = trunc i32 %val to i16
  tail call fastcc void @do_st2_mmu(ptr noundef %add.ptr.i, i64 noundef %addr, i16 noundef zeroext %conv, i32 noundef %oi, i64 noundef %retaddr)
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define internal fastcc void @do_st2_mmu(ptr noundef %cpu, i64 noundef %addr, i16 noundef zeroext %val, i32 noundef %oi, i64 noundef %ra) unnamed_addr #2 {
entry:
  %shr.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %cpu_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %cpu, i64 noundef %addr, i32 noundef 1, i64 noundef %ra) #17
  unreachable

cpu_mmu_lookup.exit:                              ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %ra, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = and i32 %oi, 256
  %tobool.not = icmp eq i32 %3, 0
  %4 = tail call i16 @llvm.bswap.i16(i16 %val)
  %spec.select = select i1 %tobool.not, i16 %val, i16 %4
  %and.i6 = and i64 %add.i.i.i, 1
  %cmp.i = icmp eq i64 %and.i6, 0
  br i1 %cmp.i, label %if.then.i8, label %if.end.i

if.then.i8:                                       ; preds = %cpu_mmu_lookup.exit
  call void @llvm.assume(i1 true) [ "align"(ptr %1, i64 2) ]
  store atomic i16 %spec.select, ptr %1 monotonic, align 2
  br label %store_atom_2.exit

if.end.i:                                         ; preds = %cpu_mmu_lookup.exit
  %and1.i.i = and i32 %shr.i, 7
  %cond.i.i = tail call i32 @llvm.usub.sat.i32(i32 %and1.i.i, i32 1)
  %and.i.i7 = lshr i32 %oi, 12
  %5 = and i32 %and.i.i7, 7
  switch i32 %5, label %do.body.i.i [
    i32 5, label %sw.epilog.i.i
    i32 1, label %sw.bb2.i.i
    i32 0, label %sw.bb3.i.i
    i32 2, label %sw.bb11.i.i
    i32 3, label %sw.bb20.i.i
    i32 4, label %sw.epilog.i.i
  ]

sw.bb2.i.i:                                       ; preds = %if.end.i
  br label %sw.bb3.i.i

sw.bb3.i.i:                                       ; preds = %sw.bb2.i.i, %if.end.i
  %size.0.i.i = phi i32 [ %and1.i.i, %if.end.i ], [ %cond.i.i, %sw.bb2.i.i ]
  %notmask.i.i = shl nsw i32 -1, %size.0.i.i
  %sub4.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub4.i.i to i64
  %and5.i.i = and i64 %add.i.i.i, %conv.i.i
  %tobool6.not.i.i = icmp eq i64 %and5.i.i, 0
  %cond10.i.i = select i1 %tobool6.not.i.i, i32 %size.0.i.i, i32 0
  br label %sw.epilog.i.i

sw.bb11.i.i:                                      ; preds = %if.end.i
  %6 = trunc i64 %add.i.i.i to i32
  %conv13.i.i = and i32 %6, 15
  %shl14.i.i = shl nuw nsw i32 1, %and1.i.i
  %add.i.i = add nuw nsw i32 %conv13.i.i, %shl14.i.i
  %cmp.i.i = icmp ult i32 %add.i.i, 17
  %cond19.i.i = select i1 %cmp.i.i, i32 %and1.i.i, i32 0
  br label %sw.epilog.i.i

sw.bb20.i.i:                                      ; preds = %if.end.i
  %7 = trunc i64 %add.i.i.i to i32
  %conv22.i.i = and i32 %7, 15
  %shl23.i.i = shl nuw nsw i32 1, %and1.i.i
  %add24.i.i = add nuw nsw i32 %conv22.i.i, %shl23.i.i
  %cmp25.i.i = icmp ult i32 %add24.i.i, 17
  br i1 %cmp25.i.i, label %sw.epilog.i.i, label %if.else.i.i

if.else.i.i:                                      ; preds = %sw.bb20.i.i
  %shl27.i.i = shl nuw nsw i32 1, %cond.i.i
  %add28.i.i = add nuw nsw i32 %conv22.i.i, %shl27.i.i
  %cmp29.i.i = icmp eq i32 %add28.i.i, 16
  %sub33.i.i = sub nsw i32 0, %cond.i.i
  %spec.select.i.i = select i1 %cmp29.i.i, i32 %cond.i.i, i32 %sub33.i.i
  br label %sw.epilog.i.i

do.body.i.i:                                      ; preds = %if.end.i
  tail call void @g_assertion_message_expr(ptr noundef null, ptr noundef nonnull @.str.20, i32 noundef 87, ptr noundef nonnull @__func__.required_atomicity, ptr noundef null) #17
  unreachable

sw.epilog.i.i:                                    ; preds = %if.else.i.i, %sw.bb20.i.i, %sw.bb11.i.i, %sw.bb3.i.i, %if.end.i, %if.end.i
  %atmax.0.i.i = phi i32 [ %cond19.i.i, %sw.bb11.i.i ], [ %cond10.i.i, %sw.bb3.i.i ], [ 0, %if.end.i ], [ %and1.i.i, %sw.bb20.i.i ], [ %spec.select.i.i, %if.else.i.i ], [ 0, %if.end.i ]
  %tcg_cflags.i.i.i = getelementptr inbounds %struct.CPUState, ptr %cpu, i64 0, i32 53
  %8 = load i32, ptr %tcg_cflags.i.i.i, align 16
  %and.i.i.i = and i32 %8, 32768
  %tobool.not.i.i.i = icmp eq i32 %and.i.i.i, 0
  br i1 %tobool.not.i.i.i, label %if.then4.i, label %cpu_in_serial_context.exit.i.i

cpu_in_serial_context.exit.i.i:                   ; preds = %sw.epilog.i.i
  %9 = getelementptr i8, ptr %cpu, i64 208
  %cs.val.i.i.i = load i32, ptr %9, align 16
  %cs.val.i.fr.i.i = freeze i32 %cs.val.i.i.i
  %tobool.i.i.not.i.i = icmp ne i32 %cs.val.i.fr.i.i, 0
  %cmp2.i = icmp eq i32 %atmax.0.i.i, 0
  %or.cond.i = select i1 %tobool.i.i.not.i.i, i1 true, i1 %cmp2.i
  br i1 %or.cond.i, label %if.then4.i, label %if.end5.i

if.then4.i:                                       ; preds = %cpu_in_serial_context.exit.i.i, %sw.epilog.i.i
  store i16 %spec.select, ptr %1, align 1
  br label %store_atom_2.exit

if.end5.i:                                        ; preds = %cpu_in_serial_context.exit.i.i
  %and6.i = and i64 %add.i.i.i, 3
  %cmp7.i = icmp eq i64 %and6.i, 1
  br i1 %cmp7.i, label %if.then9.i, label %if.else.i

if.then9.i:                                       ; preds = %if.end5.i
  %add.ptr.i = getelementptr i8, ptr %1, i64 -1
  %conv10.i = zext i16 %spec.select to i32
  %shl.i = shl nuw nsw i32 %conv10.i, 8
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i, i64 4) ]
  %10 = load atomic i32, ptr %add.ptr.i monotonic, align 4
  br label %do.body1.i.i

do.body1.i.i:                                     ; preds = %do.body1.i.i, %if.then9.i
  %old.0.i.i = phi i32 [ %10, %if.then9.i ], [ %13, %do.body1.i.i ]
  %and.i14.i = and i32 %old.0.i.i, -16776961
  %or.i.i = or disjoint i32 %and.i14.i, %shl.i
  %11 = cmpxchg weak ptr %add.ptr.i, i32 %old.0.i.i, i32 %or.i.i monotonic monotonic, align 4
  %12 = extractvalue { i32, i1 } %11, 1
  %13 = extractvalue { i32, i1 } %11, 0
  br i1 %12, label %store_atom_2.exit, label %do.body1.i.i, !llvm.loop !16

if.else.i:                                        ; preds = %if.end5.i
  %and11.i = and i64 %add.i.i.i, 7
  %cmp12.i = icmp eq i64 %and11.i, 3
  br i1 %cmp12.i, label %if.then14.i, label %if.else18.i

if.then14.i:                                      ; preds = %if.else.i
  %add.ptr15.i = getelementptr i8, ptr %1, i64 -3
  %conv16.i = zext i16 %spec.select to i64
  %shl17.i = shl nuw nsw i64 %conv16.i, 24
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr15.i, i64 8) ]
  %14 = load atomic i64, ptr %add.ptr15.i monotonic, align 8
  br label %do.body1.i15.i

do.body1.i15.i:                                   ; preds = %do.body1.i15.i, %if.then14.i
  %old.0.i16.i = phi i64 [ %14, %if.then14.i ], [ %17, %do.body1.i15.i ]
  %and.i17.i = and i64 %old.0.i16.i, -1099494850561
  %or.i18.i = or disjoint i64 %and.i17.i, %shl17.i
  %15 = cmpxchg weak ptr %add.ptr15.i, i64 %old.0.i16.i, i64 %or.i18.i monotonic monotonic, align 8
  %16 = extractvalue { i64, i1 } %15, 1
  %17 = extractvalue { i64, i1 } %15, 0
  br i1 %16, label %store_atom_2.exit, label %do.body1.i15.i, !llvm.loop !17

if.else18.i:                                      ; preds = %if.else.i
  %and19.i = and i64 %add.i.i.i, 15
  %cmp20.i = icmp eq i64 %and19.i, 7
  br i1 %cmp20.i, label %if.then22.i, label %do.body.i

if.then22.i:                                      ; preds = %if.else18.i
  %a.sroa.0.0.insert.ext.i.tr.i = zext i16 %spec.select to i64
  %retval.sroa.0.0.extract.trunc.i.i = shl i64 %a.sroa.0.0.insert.ext.i.tr.i, 56
  %18 = lshr i16 %spec.select, 8
  %add.ptr33.i = getelementptr i8, ptr %1, i64 -7
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr33.i, i64 16) ]
  %19 = load i128, ptr %add.ptr33.i, align 16
  %b.sroa.2.0.insert.ext.i18.i.i = zext nneg i16 %18 to i128
  %b.sroa.2.0.insert.shift.i19.i.i = shl nuw nsw i128 %b.sroa.2.0.insert.ext.i18.i.i, 64
  %b.sroa.0.0.insert.ext.i20.i.i = zext i64 %retval.sroa.0.0.extract.trunc.i.i to i128
  %b.sroa.0.0.insert.insert.i21.i.i = or disjoint i128 %b.sroa.2.0.insert.shift.i19.i.i, %b.sroa.0.0.insert.ext.i20.i.i
  br label %do.body.i28.i

do.body.i28.i:                                    ; preds = %do.body.i28.i, %if.then22.i
  %old.sroa.0.0.i.i = phi i128 [ %19, %if.then22.i ], [ %22, %do.body.i28.i ]
  %and.i.i29.i = and i128 %old.sroa.0.0.i.i, -4722294425275607285761
  %or.i.i.i = or disjoint i128 %and.i.i29.i, %b.sroa.0.0.insert.insert.i21.i.i
  %20 = cmpxchg weak ptr %add.ptr33.i, i128 %old.sroa.0.0.i.i, i128 %or.i.i.i monotonic monotonic, align 16
  %21 = extractvalue { i128, i1 } %20, 1
  %22 = extractvalue { i128, i1 } %20, 0
  br i1 %21, label %store_atom_2.exit, label %do.body.i28.i, !llvm.loop !18

do.body.i:                                        ; preds = %if.else18.i
  tail call void @g_assertion_message_expr(ptr noundef null, ptr noundef nonnull @.str.20, i32 noundef 897, ptr noundef nonnull @__func__.store_atom_2, ptr noundef null) #17
  unreachable

store_atom_2.exit:                                ; preds = %do.body.i28.i, %do.body1.i15.i, %do.body1.i.i, %if.then.i8, %if.then4.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @helper_stl_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %0 = and i32 %oi, 112
  %cmp = icmp eq i32 %0, 32
  tail call void @llvm.assume(i1 %cmp)
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  tail call fastcc void @do_st4_mmu(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr)
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define internal fastcc void @do_st4_mmu(ptr noundef %cpu, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %ra) unnamed_addr #2 {
entry:
  %shr.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %cpu_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %cpu, i64 noundef %addr, i32 noundef 1, i64 noundef %ra) #17
  unreachable

cpu_mmu_lookup.exit:                              ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %ra, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = and i32 %oi, 256
  %tobool.not = icmp eq i32 %3, 0
  %4 = tail call i32 @llvm.bswap.i32(i32 %val)
  %spec.select = select i1 %tobool.not, i32 %val, i32 %4
  %and.i6 = and i64 %add.i.i.i, 3
  %cmp.i = icmp eq i64 %and.i6, 0
  br i1 %cmp.i, label %if.then.i9, label %if.end.i

if.then.i9:                                       ; preds = %cpu_mmu_lookup.exit
  call void @llvm.assume(i1 true) [ "align"(ptr %1, i64 4) ]
  store atomic i32 %spec.select, ptr %1 monotonic, align 4
  br label %store_atom_4.exit

if.end.i:                                         ; preds = %cpu_mmu_lookup.exit
  %and1.i.i = and i32 %shr.i, 7
  %cond.i.i = tail call i32 @llvm.usub.sat.i32(i32 %and1.i.i, i32 1)
  %and.i.i7 = lshr i32 %oi, 12
  %5 = and i32 %and.i.i7, 7
  switch i32 %5, label %do.body.i.i [
    i32 5, label %sw.epilog.i.i
    i32 1, label %sw.bb2.i.i
    i32 0, label %sw.bb3.i.i
    i32 2, label %sw.bb11.i.i
    i32 3, label %sw.bb20.i.i
    i32 4, label %sw.bb35.i.i
  ]

sw.bb2.i.i:                                       ; preds = %if.end.i
  br label %sw.bb3.i.i

sw.bb3.i.i:                                       ; preds = %sw.bb2.i.i, %if.end.i
  %size.0.i.i = phi i32 [ %and1.i.i, %if.end.i ], [ %cond.i.i, %sw.bb2.i.i ]
  %notmask.i.i = shl nsw i32 -1, %size.0.i.i
  %sub4.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub4.i.i to i64
  %and5.i.i = and i64 %add.i.i.i, %conv.i.i
  %tobool6.not.i.i = icmp eq i64 %and5.i.i, 0
  %cond10.i.i = select i1 %tobool6.not.i.i, i32 %size.0.i.i, i32 0
  br label %sw.epilog.i.i

sw.bb11.i.i:                                      ; preds = %if.end.i
  %6 = trunc i64 %add.i.i.i to i32
  %conv13.i.i = and i32 %6, 15
  %shl14.i.i = shl nuw nsw i32 1, %and1.i.i
  %add.i.i = add nuw nsw i32 %conv13.i.i, %shl14.i.i
  %cmp.i.i = icmp ult i32 %add.i.i, 17
  %cond19.i.i = select i1 %cmp.i.i, i32 %and1.i.i, i32 0
  br label %sw.epilog.i.i

sw.bb20.i.i:                                      ; preds = %if.end.i
  %7 = trunc i64 %add.i.i.i to i32
  %conv22.i.i = and i32 %7, 15
  %shl23.i.i = shl nuw nsw i32 1, %and1.i.i
  %add24.i.i = add nuw nsw i32 %conv22.i.i, %shl23.i.i
  %cmp25.i.i = icmp ult i32 %add24.i.i, 17
  br i1 %cmp25.i.i, label %sw.epilog.i.i, label %if.else.i.i

if.else.i.i:                                      ; preds = %sw.bb20.i.i
  %shl27.i.i = shl nuw nsw i32 1, %cond.i.i
  %add28.i.i = add nuw nsw i32 %conv22.i.i, %shl27.i.i
  %cmp29.i.i = icmp eq i32 %add28.i.i, 16
  %sub33.i.i = sub nsw i32 0, %cond.i.i
  %spec.select.i.i = select i1 %cmp29.i.i, i32 %cond.i.i, i32 %sub33.i.i
  br label %sw.epilog.i.i

sw.bb35.i.i:                                      ; preds = %if.end.i
  %conv36.i.i = trunc i64 %add.i.i.i to i32
  %8 = tail call i32 @llvm.cttz.i32(i32 %conv36.i.i, i1 false), !range !15
  %cond43.i.i = tail call i32 @llvm.umin.i32(i32 %and1.i.i, i32 %8)
  br label %sw.epilog.i.i

do.body.i.i:                                      ; preds = %if.end.i
  tail call void @g_assertion_message_expr(ptr noundef null, ptr noundef nonnull @.str.20, i32 noundef 87, ptr noundef nonnull @__func__.required_atomicity, ptr noundef null) #17
  unreachable

sw.epilog.i.i:                                    ; preds = %sw.bb35.i.i, %if.else.i.i, %sw.bb20.i.i, %sw.bb11.i.i, %sw.bb3.i.i, %if.end.i
  %atmax.0.i.i = phi i32 [ %cond43.i.i, %sw.bb35.i.i ], [ %cond19.i.i, %sw.bb11.i.i ], [ %cond10.i.i, %sw.bb3.i.i ], [ 0, %if.end.i ], [ %and1.i.i, %sw.bb20.i.i ], [ %spec.select.i.i, %if.else.i.i ]
  %tcg_cflags.i.i.i = getelementptr inbounds %struct.CPUState, ptr %cpu, i64 0, i32 53
  %9 = load i32, ptr %tcg_cflags.i.i.i, align 16
  %and.i.i.i = and i32 %9, 32768
  %tobool.not.i.i.i = icmp eq i32 %and.i.i.i, 0
  br i1 %tobool.not.i.i.i, label %sw.bb.i, label %cpu_in_serial_context.exit.i.i

cpu_in_serial_context.exit.i.i:                   ; preds = %sw.epilog.i.i
  %10 = getelementptr i8, ptr %cpu, i64 208
  %cs.val.i.i.i = load i32, ptr %10, align 16
  %cs.val.i.fr.i.i = freeze i32 %cs.val.i.i.i
  %tobool.i.i.not.i.i = icmp eq i32 %cs.val.i.fr.i.i, 0
  br i1 %tobool.i.i.not.i.i, label %required_atomicity.exit.i, label %sw.bb.i

required_atomicity.exit.i:                        ; preds = %cpu_in_serial_context.exit.i.i
  switch i32 %atmax.0.i.i, label %do.body32.i [
    i32 0, label %sw.bb.i
    i32 1, label %sw.bb2.i
    i32 -1, label %sw.bb3.i
    i32 2, label %sw.bb18.i
  ]

sw.bb.i:                                          ; preds = %required_atomicity.exit.i, %cpu_in_serial_context.exit.i.i, %sw.epilog.i.i
  store i32 %spec.select, ptr %1, align 1
  br label %store_atom_4.exit

sw.bb2.i:                                         ; preds = %required_atomicity.exit.i
  %conv.i23.i = trunc i32 %spec.select to i16
  call void @llvm.assume(i1 true) [ "align"(ptr %1, i64 2) ]
  store atomic i16 %conv.i23.i, ptr %1 monotonic, align 2
  %add.ptr.i.i = getelementptr i8, ptr %1, i64 2
  %shr1.i.i = lshr i32 %spec.select, 16
  %conv2.i.i = trunc i32 %shr1.i.i to i16
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i.i, i64 2) ]
  store atomic i16 %conv2.i.i, ptr %add.ptr.i.i monotonic, align 2
  br label %store_atom_4.exit

sw.bb3.i:                                         ; preds = %required_atomicity.exit.i
  %conv6.i = trunc i64 %and.i6 to i32
  switch i32 %conv6.i, label %do.body.i [
    i32 1, label %sw.bb7.i
    i32 3, label %sw.bb12.i
  ]

sw.bb7.i:                                         ; preds = %sw.bb3.i
  %11 = trunc i64 %add.i.i.i to i32
  %conv.i24.i = shl i32 %11, 3
  %mul1.i.i = and i32 %conv.i24.i, 24
  %shl4.i.i = shl i32 %spec.select, %mul1.i.i
  %shl6.i.i = shl i32 16777215, %mul1.i.i
  %idx.neg.i.i = sub nsw i64 0, %and.i6
  %add.ptr.i26.i = getelementptr i8, ptr %1, i64 %idx.neg.i.i
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i26.i, i64 4) ]
  %12 = load atomic i32, ptr %add.ptr.i26.i monotonic, align 4
  %not.i.i.i = xor i32 %shl6.i.i, -1
  br label %do.body1.i.i.i

do.body1.i.i.i:                                   ; preds = %do.body1.i.i.i, %sw.bb7.i
  %old.0.i.i.i = phi i32 [ %12, %sw.bb7.i ], [ %15, %do.body1.i.i.i ]
  %and.i.i27.i = and i32 %old.0.i.i.i, %not.i.i.i
  %or.i.i.i = or i32 %and.i.i27.i, %shl4.i.i
  %13 = cmpxchg weak ptr %add.ptr.i26.i, i32 %old.0.i.i.i, i32 %or.i.i.i monotonic monotonic, align 4
  %14 = extractvalue { i32, i1 } %13, 1
  %15 = extractvalue { i32, i1 } %13, 0
  br i1 %14, label %store_whole_le4.exit.i, label %do.body1.i.i.i, !llvm.loop !16

store_whole_le4.exit.i:                           ; preds = %do.body1.i.i.i
  %16 = lshr i32 %spec.select, 24
  %conv11.i = trunc i32 %16 to i8
  %add.ptr.i = getelementptr i8, ptr %1, i64 3
  store i8 %conv11.i, ptr %add.ptr.i, align 1
  br label %store_atom_4.exit

sw.bb12.i:                                        ; preds = %sw.bb3.i
  %conv13.i = trunc i32 %spec.select to i8
  store i8 %conv13.i, ptr %1, align 1
  %add.ptr14.i = getelementptr i8, ptr %1, i64 1
  %shr.i8 = lshr i32 %spec.select, 8
  %17 = ptrtoint ptr %add.ptr14.i to i64
  %18 = trunc i64 %17 to i32
  %conv.i28.i = shl i32 %18, 3
  %mul1.i29.i = and i32 %conv.i28.i, 24
  %shl4.i31.i = shl i32 %shr.i8, %mul1.i29.i
  %shl6.i33.i = shl i32 16777215, %mul1.i29.i
  %idx.ext.i34.i = and i64 %17, 3
  %idx.neg.i35.i = sub nsw i64 0, %idx.ext.i34.i
  %add.ptr.i36.i = getelementptr i8, ptr %add.ptr14.i, i64 %idx.neg.i35.i
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i36.i, i64 4) ]
  %19 = load atomic i32, ptr %add.ptr.i36.i monotonic, align 4
  %not.i.i37.i = xor i32 %shl6.i33.i, -1
  br label %do.body1.i.i38.i

do.body1.i.i38.i:                                 ; preds = %do.body1.i.i38.i, %sw.bb12.i
  %old.0.i.i39.i = phi i32 [ %19, %sw.bb12.i ], [ %22, %do.body1.i.i38.i ]
  %and.i.i40.i = and i32 %old.0.i.i39.i, %not.i.i37.i
  %or.i.i41.i = or i32 %and.i.i40.i, %shl4.i31.i
  %20 = cmpxchg weak ptr %add.ptr.i36.i, i32 %old.0.i.i39.i, i32 %or.i.i41.i monotonic monotonic, align 4
  %21 = extractvalue { i32, i1 } %20, 1
  %22 = extractvalue { i32, i1 } %20, 0
  br i1 %21, label %store_atom_4.exit, label %do.body1.i.i38.i, !llvm.loop !16

do.body.i:                                        ; preds = %sw.bb3.i
  tail call void @g_assertion_message_expr(ptr noundef null, ptr noundef nonnull @.str.20, i32 noundef 948, ptr noundef nonnull @__func__.store_atom_4, ptr noundef null) #17
  unreachable

sw.bb18.i:                                        ; preds = %required_atomicity.exit.i
  %and19.i = and i64 %add.i.i.i, 4
  %cmp20.not.not.i = icmp eq i64 %and19.i, 0
  br i1 %cmp20.not.not.i, label %if.then22.i, label %if.else.i

if.then22.i:                                      ; preds = %sw.bb18.i
  %conv24.i = zext i32 %spec.select to i64
  %conv.i44.i = shl i64 %add.i.i.i, 3
  %mul1.i45.i = and i64 %conv.i44.i, 24
  %shl3.i.i = shl nuw nsw i64 %conv24.i, %mul1.i45.i
  %shl5.i.i = shl nuw nsw i64 4294967295, %mul1.i45.i
  %idx.neg.i47.i = sub nsw i64 0, %and.i6
  %add.ptr.i48.i = getelementptr i8, ptr %1, i64 %idx.neg.i47.i
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i48.i, i64 8) ]
  %23 = load atomic i64, ptr %add.ptr.i48.i monotonic, align 8
  %not.i.i49.i = xor i64 %shl5.i.i, -1
  br label %do.body1.i.i50.i

do.body1.i.i50.i:                                 ; preds = %do.body1.i.i50.i, %if.then22.i
  %old.0.i.i51.i = phi i64 [ %23, %if.then22.i ], [ %26, %do.body1.i.i50.i ]
  %and.i.i52.i = and i64 %old.0.i.i51.i, %not.i.i49.i
  %or.i.i53.i = or i64 %and.i.i52.i, %shl3.i.i
  %24 = cmpxchg weak ptr %add.ptr.i48.i, i64 %old.0.i.i51.i, i64 %or.i.i53.i monotonic monotonic, align 8
  %25 = extractvalue { i64, i1 } %24, 1
  %26 = extractvalue { i64, i1 } %24, 0
  br i1 %25, label %store_atom_4.exit, label %do.body1.i.i50.i, !llvm.loop !17

if.else.i:                                        ; preds = %sw.bb18.i
  %27 = trunc i64 %add.i.i.i to i32
  %conv.i54.i = shl i32 %27, 3
  %mul2.i.i = and i32 %conv.i54.i, 120
  %a.sroa.0.0.insert.ext.i.i.i = zext i32 %spec.select to i128
  %sh_prom.i.i.i = zext nneg i32 %mul2.i.i to i128
  %shl.i.i.i = shl i128 %a.sroa.0.0.insert.ext.i.i.i, %sh_prom.i.i.i
  %shl.i22.i.i = shl i128 4294967295, %sh_prom.i.i.i
  %retval.sroa.0.0.extract.trunc.i23.i.i = trunc i128 %shl.i22.i.i to i64
  %retval.sroa.2.0.extract.shift.i24.i.i = and i128 %shl.i22.i.i, -18446744073709551616
  %idx.ext.i55.i = and i64 %add.i.i.i, 15
  %idx.neg.i56.i = sub nsw i64 0, %idx.ext.i55.i
  %add.ptr.i57.i = getelementptr i8, ptr %1, i64 %idx.neg.i56.i
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i57.i, i64 16) ]
  %28 = load i128, ptr %add.ptr.i57.i, align 16
  %not.i.i.i.i = xor i64 %retval.sroa.0.0.extract.trunc.i23.i.i, -1
  %b.sroa.0.0.insert.ext.i.i.i.i = zext i64 %not.i.i.i.i to i128
  %29 = or disjoint i128 %retval.sroa.2.0.extract.shift.i24.i.i, %b.sroa.0.0.insert.ext.i.i.i.i
  %b.sroa.0.0.insert.insert.i.i.i.i = xor i128 %29, -18446744073709551616
  br label %do.body.i.i.i

do.body.i.i.i:                                    ; preds = %do.body.i.i.i, %if.else.i
  %old.sroa.0.0.i.i.i = phi i128 [ %28, %if.else.i ], [ %32, %do.body.i.i.i ]
  %and.i.i.i.i = and i128 %old.sroa.0.0.i.i.i, %b.sroa.0.0.insert.insert.i.i.i.i
  %or.i.i.i.i = or i128 %and.i.i.i.i, %shl.i.i.i
  %30 = cmpxchg weak ptr %add.ptr.i57.i, i128 %old.sroa.0.0.i.i.i, i128 %or.i.i.i.i monotonic monotonic, align 16
  %31 = extractvalue { i128, i1 } %30, 1
  %32 = extractvalue { i128, i1 } %30, 0
  br i1 %31, label %store_atom_4.exit, label %do.body.i.i.i, !llvm.loop !18

do.body32.i:                                      ; preds = %required_atomicity.exit.i
  tail call void @g_assertion_message_expr(ptr noundef null, ptr noundef nonnull @.str.20, i32 noundef 966, ptr noundef nonnull @__func__.store_atom_4, ptr noundef null) #17
  unreachable

store_atom_4.exit:                                ; preds = %do.body.i.i.i, %do.body1.i.i50.i, %do.body1.i.i38.i, %if.then.i9, %sw.bb.i, %sw.bb2.i, %store_whole_le4.exit.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @helper_stq_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %0 = and i32 %oi, 112
  %cmp = icmp eq i32 %0, 48
  tail call void @llvm.assume(i1 %cmp)
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  tail call fastcc void @do_st8_mmu(ptr noundef %add.ptr.i, i64 noundef %addr, i64 noundef %val, i32 noundef %oi, i64 noundef %retaddr)
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define internal fastcc void @do_st8_mmu(ptr noundef %cpu, i64 noundef %addr, i64 noundef %val, i32 noundef %oi, i64 noundef %ra) unnamed_addr #2 {
entry:
  %shr.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %cpu_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %cpu, i64 noundef %addr, i32 noundef 1, i64 noundef %ra) #17
  unreachable

cpu_mmu_lookup.exit:                              ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %ra, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = and i32 %oi, 256
  %tobool.not = icmp eq i32 %3, 0
  %4 = tail call i64 @llvm.bswap.i64(i64 %val)
  %spec.select = select i1 %tobool.not, i64 %val, i64 %4
  %and.i6 = and i64 %add.i.i.i, 7
  %cmp.i = icmp eq i64 %and.i6, 0
  br i1 %cmp.i, label %if.then.i10, label %if.end.i

if.then.i10:                                      ; preds = %cpu_mmu_lookup.exit
  call void @llvm.assume(i1 true) [ "align"(ptr %1, i64 8) ]
  store atomic i64 %spec.select, ptr %1 monotonic, align 8
  br label %store_atom_8.exit

if.end.i:                                         ; preds = %cpu_mmu_lookup.exit
  %and1.i.i = and i32 %shr.i, 7
  %cond.i.i = tail call i32 @llvm.usub.sat.i32(i32 %and1.i.i, i32 1)
  %and.i.i7 = lshr i32 %oi, 12
  %5 = and i32 %and.i.i7, 7
  switch i32 %5, label %do.body.i.i [
    i32 5, label %sw.epilog.i.i
    i32 1, label %sw.bb2.i.i
    i32 0, label %sw.bb3.i.i
    i32 2, label %sw.bb11.i.i
    i32 3, label %sw.bb20.i.i
    i32 4, label %sw.bb35.i.i
  ]

sw.bb2.i.i:                                       ; preds = %if.end.i
  br label %sw.bb3.i.i

sw.bb3.i.i:                                       ; preds = %sw.bb2.i.i, %if.end.i
  %size.0.i.i = phi i32 [ %and1.i.i, %if.end.i ], [ %cond.i.i, %sw.bb2.i.i ]
  %notmask.i.i = shl nsw i32 -1, %size.0.i.i
  %sub4.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub4.i.i to i64
  %and5.i.i = and i64 %add.i.i.i, %conv.i.i
  %tobool6.not.i.i = icmp eq i64 %and5.i.i, 0
  %cond10.i.i = select i1 %tobool6.not.i.i, i32 %size.0.i.i, i32 0
  br label %sw.epilog.i.i

sw.bb11.i.i:                                      ; preds = %if.end.i
  %6 = trunc i64 %add.i.i.i to i32
  %conv13.i.i = and i32 %6, 15
  %shl14.i.i = shl nuw nsw i32 1, %and1.i.i
  %add.i.i = add nuw nsw i32 %conv13.i.i, %shl14.i.i
  %cmp.i.i = icmp ult i32 %add.i.i, 17
  %cond19.i.i = select i1 %cmp.i.i, i32 %and1.i.i, i32 0
  br label %sw.epilog.i.i

sw.bb20.i.i:                                      ; preds = %if.end.i
  %7 = trunc i64 %add.i.i.i to i32
  %conv22.i.i = and i32 %7, 15
  %shl23.i.i = shl nuw nsw i32 1, %and1.i.i
  %add24.i.i = add nuw nsw i32 %conv22.i.i, %shl23.i.i
  %cmp25.i.i = icmp ult i32 %add24.i.i, 17
  br i1 %cmp25.i.i, label %sw.epilog.i.i, label %if.else.i.i

if.else.i.i:                                      ; preds = %sw.bb20.i.i
  %shl27.i.i = shl nuw nsw i32 1, %cond.i.i
  %add28.i.i = add nuw nsw i32 %conv22.i.i, %shl27.i.i
  %cmp29.i.i = icmp eq i32 %add28.i.i, 16
  %sub33.i.i = sub nsw i32 0, %cond.i.i
  %spec.select.i.i = select i1 %cmp29.i.i, i32 %cond.i.i, i32 %sub33.i.i
  br label %sw.epilog.i.i

sw.bb35.i.i:                                      ; preds = %if.end.i
  %conv36.i.i = trunc i64 %add.i.i.i to i32
  %8 = tail call i32 @llvm.cttz.i32(i32 %conv36.i.i, i1 false), !range !15
  %cond43.i.i = tail call i32 @llvm.umin.i32(i32 %and1.i.i, i32 %8)
  br label %sw.epilog.i.i

do.body.i.i:                                      ; preds = %if.end.i
  tail call void @g_assertion_message_expr(ptr noundef null, ptr noundef nonnull @.str.20, i32 noundef 87, ptr noundef nonnull @__func__.required_atomicity, ptr noundef null) #17
  unreachable

sw.epilog.i.i:                                    ; preds = %sw.bb35.i.i, %if.else.i.i, %sw.bb20.i.i, %sw.bb11.i.i, %sw.bb3.i.i, %if.end.i
  %atmax.0.i.i = phi i32 [ %cond43.i.i, %sw.bb35.i.i ], [ %cond19.i.i, %sw.bb11.i.i ], [ %cond10.i.i, %sw.bb3.i.i ], [ 0, %if.end.i ], [ %and1.i.i, %sw.bb20.i.i ], [ %spec.select.i.i, %if.else.i.i ]
  %tcg_cflags.i.i.i = getelementptr inbounds %struct.CPUState, ptr %cpu, i64 0, i32 53
  %9 = load i32, ptr %tcg_cflags.i.i.i, align 16
  %and.i.i.i = and i32 %9, 32768
  %tobool.not.i.i.i = icmp eq i32 %and.i.i.i, 0
  br i1 %tobool.not.i.i.i, label %sw.bb.i, label %cpu_in_serial_context.exit.i.i

cpu_in_serial_context.exit.i.i:                   ; preds = %sw.epilog.i.i
  %10 = getelementptr i8, ptr %cpu, i64 208
  %cs.val.i.i.i = load i32, ptr %10, align 16
  %cs.val.i.fr.i.i = freeze i32 %cs.val.i.i.i
  %tobool.i.i.not.i.i = icmp eq i32 %cs.val.i.fr.i.i, 0
  br i1 %tobool.i.i.not.i.i, label %required_atomicity.exit.i, label %sw.bb.i

required_atomicity.exit.i:                        ; preds = %cpu_in_serial_context.exit.i.i
  switch i32 %atmax.0.i.i, label %do.body23.i [
    i32 0, label %sw.bb.i
    i32 1, label %sw.bb2.i
    i32 2, label %sw.bb3.i
    i32 -2, label %sw.bb4.i
    i32 3, label %sw.bb17.i
  ]

sw.bb.i:                                          ; preds = %required_atomicity.exit.i, %cpu_in_serial_context.exit.i.i, %sw.epilog.i.i
  store i64 %spec.select, ptr %1, align 1
  br label %store_atom_8.exit

sw.bb2.i:                                         ; preds = %required_atomicity.exit.i
  %conv.i.i.i = trunc i64 %spec.select to i16
  call void @llvm.assume(i1 true) [ "align"(ptr %1, i64 2) ]
  store atomic i16 %conv.i.i.i, ptr %1 monotonic, align 2
  %add.ptr.i.i.i = getelementptr i8, ptr %1, i64 2
  %shr1.i7.i.i = lshr i64 %spec.select, 16
  %conv2.i.i.i = trunc i64 %shr1.i7.i.i to i16
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i.i.i, i64 2) ]
  store atomic i16 %conv2.i.i.i, ptr %add.ptr.i.i.i monotonic, align 2
  %add.ptr.i.i = getelementptr i8, ptr %1, i64 4
  %shr1.i.i = lshr i64 %spec.select, 32
  %conv.i3.i.i = trunc i64 %shr1.i.i to i16
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i.i, i64 2) ]
  store atomic i16 %conv.i3.i.i, ptr %add.ptr.i.i monotonic, align 2
  %add.ptr.i4.i.i = getelementptr i8, ptr %1, i64 6
  %sum.shift.i.i = lshr i64 %spec.select, 48
  %conv2.i6.i.i = trunc i64 %sum.shift.i.i to i16
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i4.i.i, i64 2) ]
  store atomic i16 %conv2.i6.i.i, ptr %add.ptr.i4.i.i monotonic, align 2
  br label %store_atom_8.exit

sw.bb3.i:                                         ; preds = %required_atomicity.exit.i
  %conv.i26.i = trunc i64 %spec.select to i32
  call void @llvm.assume(i1 true) [ "align"(ptr %1, i64 4) ]
  store atomic i32 %conv.i26.i, ptr %1 monotonic, align 4
  %add.ptr.i27.i = getelementptr i8, ptr %1, i64 4
  %shr1.i28.i = lshr i64 %spec.select, 32
  %conv2.i.i = trunc i64 %shr1.i28.i to i32
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i27.i, i64 4) ]
  store atomic i32 %conv2.i.i, ptr %add.ptr.i27.i monotonic, align 4
  br label %store_atom_8.exit

sw.bb4.i:                                         ; preds = %required_atomicity.exit.i
  %conv7.i = trunc i64 %and.i6 to i32
  %sub.i8 = sub nuw nsw i32 8, %conv7.i
  switch i32 %conv7.i, label %do.body.i [
    i32 1, label %sw.bb8.i
    i32 2, label %sw.bb8.i
    i32 3, label %sw.bb8.i
    i32 5, label %sw.bb11.i
    i32 6, label %sw.bb11.i
    i32 7, label %sw.bb11.i
  ]

sw.bb8.i:                                         ; preds = %sw.bb4.i, %sw.bb4.i, %sw.bb4.i
  %mul.i.i = shl nuw nsw i32 %sub.i8, 3
  %conv.i29.i = shl i64 %add.i.i.i, 3
  %mul1.i.i = and i64 %conv.i29.i, 56
  %sub.i.i = sub nuw nsw i32 64, %mul.i.i
  %sh_prom.i.i = zext nneg i32 %sub.i.i to i64
  %shr.i.i9 = lshr i64 -1, %sh_prom.i.i
  %shl3.i.i = shl i64 %spec.select, %mul1.i.i
  %shl5.i.i = shl i64 %shr.i.i9, %mul1.i.i
  %idx.neg.i.i = sub nsw i64 0, %and.i6
  %add.ptr.i30.i = getelementptr i8, ptr %1, i64 %idx.neg.i.i
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i30.i, i64 8) ]
  %11 = load atomic i64, ptr %add.ptr.i30.i monotonic, align 8
  %not.i.i.i = xor i64 %shl5.i.i, -1
  br label %do.body1.i.i.i

do.body1.i.i.i:                                   ; preds = %do.body1.i.i.i, %sw.bb8.i
  %old.0.i.i.i = phi i64 [ %11, %sw.bb8.i ], [ %14, %do.body1.i.i.i ]
  %and.i.i31.i = and i64 %old.0.i.i.i, %not.i.i.i
  %or.i.i.i = or i64 %and.i.i31.i, %shl3.i.i
  %12 = cmpxchg weak ptr %add.ptr.i30.i, i64 %old.0.i.i.i, i64 %or.i.i.i monotonic monotonic, align 8
  %13 = extractvalue { i64, i1 } %12, 1
  %14 = extractvalue { i64, i1 } %12, 0
  br i1 %13, label %store_whole_le8.exit.i, label %do.body1.i.i.i, !llvm.loop !17

store_whole_le8.exit.i:                           ; preds = %do.body1.i.i.i
  %idx.ext.i = zext nneg i32 %sub.i8 to i64
  %add.ptr.i = getelementptr i8, ptr %1, i64 %idx.ext.i
  %cmp5.i.not.i = icmp eq i32 %conv7.i, 0
  br i1 %cmp5.i.not.i, label %store_atom_8.exit, label %for.body.preheader.i.i

for.body.preheader.i.i:                           ; preds = %store_whole_le8.exit.i
  %sh_prom6.i.i = zext nneg i32 %mul.i.i to i64
  %shr7.i.i = lshr i64 %spec.select, %sh_prom6.i.i
  br label %for.body.i.i

for.body.i.i:                                     ; preds = %for.body.i.i, %for.body.preheader.i.i
  %indvars.iv.i.i = phi i64 [ 0, %for.body.preheader.i.i ], [ %indvars.iv.next.i.i, %for.body.i.i ]
  %val_le.addr.06.i.i = phi i64 [ %shr7.i.i, %for.body.preheader.i.i ], [ %shr.i33.i, %for.body.i.i ]
  %conv.i32.i = trunc i64 %val_le.addr.06.i.i to i8
  %arrayidx.i.i = getelementptr i8, ptr %add.ptr.i, i64 %indvars.iv.i.i
  store i8 %conv.i32.i, ptr %arrayidx.i.i, align 1
  %indvars.iv.next.i.i = add nuw nsw i64 %indvars.iv.i.i, 1
  %shr.i33.i = lshr i64 %val_le.addr.06.i.i, 8
  %exitcond.not.i.i = icmp eq i64 %indvars.iv.next.i.i, %and.i6
  br i1 %exitcond.not.i.i, label %store_atom_8.exit, label %for.body.i.i, !llvm.loop !19

sw.bb11.i:                                        ; preds = %sw.bb4.i, %sw.bb4.i, %sw.bb4.i
  %wide.trip.count.i35.i = zext nneg i32 %sub.i8 to i64
  br label %for.body.i36.i

for.body.i36.i:                                   ; preds = %for.body.i36.i, %sw.bb11.i
  %indvars.iv.i37.i = phi i64 [ 0, %sw.bb11.i ], [ %indvars.iv.next.i41.i, %for.body.i36.i ]
  %val_le.addr.06.i38.i = phi i64 [ %spec.select, %sw.bb11.i ], [ %shr.i42.i, %for.body.i36.i ]
  %conv.i39.i = trunc i64 %val_le.addr.06.i38.i to i8
  %arrayidx.i40.i = getelementptr i8, ptr %1, i64 %indvars.iv.i37.i
  store i8 %conv.i39.i, ptr %arrayidx.i40.i, align 1
  %indvars.iv.next.i41.i = add nuw nsw i64 %indvars.iv.i37.i, 1
  %shr.i42.i = lshr i64 %val_le.addr.06.i38.i, 8
  %exitcond.not.i43.i = icmp eq i64 %indvars.iv.next.i41.i, %wide.trip.count.i35.i
  br i1 %exitcond.not.i43.i, label %store_bytes_leN.exit45.i, label %for.body.i36.i, !llvm.loop !19

store_bytes_leN.exit45.i:                         ; preds = %for.body.i36.i
  %add.ptr14.i = getelementptr i8, ptr %1, i64 %wide.trip.count.i35.i
  %mul.i46.i = shl nuw nsw i64 %and.i6, 3
  %15 = ptrtoint ptr %add.ptr14.i to i64
  %conv.i47.i = shl i64 %15, 3
  %mul1.i48.i = and i64 %conv.i47.i, 56
  %sub.i49.i = sub nuw nsw i64 64, %mul.i46.i
  %shr.i51.i = lshr i64 -1, %sub.i49.i
  %shl3.i52.i = shl i64 %shr.i42.i, %mul1.i48.i
  %shl5.i53.i = shl i64 %shr.i51.i, %mul1.i48.i
  %idx.ext.i54.i = and i64 %15, 7
  %idx.neg.i55.i = sub nsw i64 0, %idx.ext.i54.i
  %add.ptr.i56.i = getelementptr i8, ptr %add.ptr14.i, i64 %idx.neg.i55.i
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i56.i, i64 8) ]
  %16 = load atomic i64, ptr %add.ptr.i56.i monotonic, align 8
  %not.i.i57.i = xor i64 %shl5.i53.i, -1
  br label %do.body1.i.i58.i

do.body1.i.i58.i:                                 ; preds = %do.body1.i.i58.i, %store_bytes_leN.exit45.i
  %old.0.i.i59.i = phi i64 [ %16, %store_bytes_leN.exit45.i ], [ %19, %do.body1.i.i58.i ]
  %and.i.i60.i = and i64 %old.0.i.i59.i, %not.i.i57.i
  %or.i.i61.i = or i64 %and.i.i60.i, %shl3.i52.i
  %17 = cmpxchg weak ptr %add.ptr.i56.i, i64 %old.0.i.i59.i, i64 %or.i.i61.i monotonic monotonic, align 8
  %18 = extractvalue { i64, i1 } %17, 1
  %19 = extractvalue { i64, i1 } %17, 0
  br i1 %18, label %store_atom_8.exit, label %do.body1.i.i58.i, !llvm.loop !17

do.body.i:                                        ; preds = %sw.bb4.i
  tail call void @g_assertion_message_expr(ptr noundef null, ptr noundef nonnull @.str.20, i32 noundef 1018, ptr noundef nonnull @__func__.store_atom_8, ptr noundef null) #17
  unreachable

sw.bb17.i:                                        ; preds = %required_atomicity.exit.i
  %20 = trunc i64 %add.i.i.i to i32
  %conv.i65.i = shl i32 %20, 3
  %mul2.i.i = and i32 %conv.i65.i, 120
  %a.sroa.0.0.insert.ext.i.i.i = zext i64 %spec.select to i128
  %sh_prom.i.i.i = zext nneg i32 %mul2.i.i to i128
  %shl.i.i.i = shl i128 %a.sroa.0.0.insert.ext.i.i.i, %sh_prom.i.i.i
  %shl.i22.i.i = shl i128 18446744073709551615, %sh_prom.i.i.i
  %retval.sroa.0.0.extract.trunc.i23.i.i = trunc i128 %shl.i22.i.i to i64
  %retval.sroa.2.0.extract.shift.i24.i.i = and i128 %shl.i22.i.i, -18446744073709551616
  %idx.ext.i66.i = and i64 %add.i.i.i, 15
  %idx.neg.i67.i = sub nsw i64 0, %idx.ext.i66.i
  %add.ptr.i68.i = getelementptr i8, ptr %1, i64 %idx.neg.i67.i
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i68.i, i64 16) ]
  %21 = load i128, ptr %add.ptr.i68.i, align 16
  %not.i.i.i.i = xor i64 %retval.sroa.0.0.extract.trunc.i23.i.i, -1
  %b.sroa.0.0.insert.ext.i.i.i.i = zext i64 %not.i.i.i.i to i128
  %22 = or disjoint i128 %retval.sroa.2.0.extract.shift.i24.i.i, %b.sroa.0.0.insert.ext.i.i.i.i
  %b.sroa.0.0.insert.insert.i.i.i.i = xor i128 %22, -18446744073709551616
  br label %do.body.i.i.i

do.body.i.i.i:                                    ; preds = %do.body.i.i.i, %sw.bb17.i
  %old.sroa.0.0.i.i.i = phi i128 [ %21, %sw.bb17.i ], [ %25, %do.body.i.i.i ]
  %and.i.i.i.i = and i128 %old.sroa.0.0.i.i.i, %b.sroa.0.0.insert.insert.i.i.i.i
  %or.i.i.i.i = or i128 %and.i.i.i.i, %shl.i.i.i
  %23 = cmpxchg weak ptr %add.ptr.i68.i, i128 %old.sroa.0.0.i.i.i, i128 %or.i.i.i.i monotonic monotonic, align 16
  %24 = extractvalue { i128, i1 } %23, 1
  %25 = extractvalue { i128, i1 } %23, 0
  br i1 %24, label %store_atom_8.exit, label %do.body.i.i.i, !llvm.loop !18

do.body23.i:                                      ; preds = %required_atomicity.exit.i
  tail call void @g_assertion_message_expr(ptr noundef null, ptr noundef nonnull @.str.20, i32 noundef 1030, ptr noundef nonnull @__func__.store_atom_8, ptr noundef null) #17
  unreachable

store_atom_8.exit:                                ; preds = %do.body.i.i.i, %do.body1.i.i58.i, %for.body.i.i, %if.then.i10, %sw.bb.i, %sw.bb2.i, %sw.bb3.i, %store_whole_le8.exit.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @helper_st16_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %val.coerce0, i64 noundef %val.coerce1, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %0 = and i32 %oi, 112
  %cmp = icmp eq i32 %0, 64
  tail call void @llvm.assume(i1 %cmp)
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  tail call fastcc void @do_st16_mmu(ptr noundef %add.ptr.i, i64 noundef %addr, i64 noundef %val.coerce0, i64 noundef %val.coerce1, i32 noundef %oi, i64 noundef %retaddr)
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define internal fastcc void @do_st16_mmu(ptr noundef %cpu, i64 noundef %addr, i64 noundef %val.coerce0, i64 noundef %val.coerce1, i32 noundef %oi, i64 noundef %ra) unnamed_addr #10 {
entry:
  %shr.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %cpu_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %cpu, i64 noundef %addr, i32 noundef 1, i64 noundef %ra) #17
  unreachable

cpu_mmu_lookup.exit:                              ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %ra, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = and i32 %oi, 256
  %tobool.not = icmp eq i32 %3, 0
  %4 = tail call i64 @llvm.bswap.i64(i64 %val.coerce1)
  %5 = tail call i64 @llvm.bswap.i64(i64 %val.coerce0)
  %val.addr.0.off0 = select i1 %tobool.not, i64 %val.coerce0, i64 %4
  %val.addr.0.off64 = select i1 %tobool.not, i64 %val.coerce1, i64 %5
  %val.sroa.2.0.insert.ext.i = zext i64 %val.addr.0.off64 to i128
  %val.sroa.2.0.insert.shift.i = shl nuw i128 %val.sroa.2.0.insert.ext.i, 64
  %val.sroa.0.0.insert.ext.i = zext i64 %val.addr.0.off0 to i128
  %val.sroa.0.0.insert.insert.i = or disjoint i128 %val.sroa.2.0.insert.shift.i, %val.sroa.0.0.insert.ext.i
  %and.i10 = and i64 %add.i.i.i, 15
  %cmp.i = icmp eq i64 %and.i10, 0
  br i1 %cmp.i, label %if.then.i12, label %if.end.i

if.then.i12:                                      ; preds = %cpu_mmu_lookup.exit
  call void @llvm.assume(i1 true) [ "align"(ptr %1, i64 16) ]
  %6 = load i32, ptr @cpuinfo, align 4
  %and.i.i13 = and i32 %6, 65536
  %tobool.not.i.i = icmp eq i32 %and.i.i13, 0
  br i1 %tobool.not.i.i, label %do.body.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.then.i12
  %7 = bitcast i128 %val.sroa.0.0.insert.insert.i to <2 x i64>
  tail call void asm "vmovdqa $1, $0", "=*m,x,~{dirflag},~{fpsr},~{flags}"(ptr elementtype(i128) %1, <2 x i64> %7) #16, !srcloc !20
  br label %store_atom_16.exit

do.body.i.i:                                      ; preds = %if.then.i12, %do.body.i.i
  %8 = load i128, ptr %1, align 16
  %9 = cmpxchg ptr %1, i128 %8, i128 %val.sroa.0.0.insert.insert.i seq_cst seq_cst, align 16
  %10 = extractvalue { i128, i1 } %9, 1
  br i1 %10, label %store_atom_16.exit, label %do.body.i.i, !llvm.loop !21

if.end.i:                                         ; preds = %cpu_mmu_lookup.exit
  %and1.i.i = and i32 %shr.i, 7
  %cond.i.i = tail call i32 @llvm.usub.sat.i32(i32 %and1.i.i, i32 1)
  %and.i33.i = lshr i32 %oi, 12
  %11 = and i32 %and.i33.i, 7
  switch i32 %11, label %do.body.i34.i [
    i32 5, label %sw.epilog.i.i
    i32 1, label %sw.bb2.i.i
    i32 0, label %sw.bb3.i.i
    i32 2, label %sw.bb11.i.i
    i32 3, label %sw.bb20.i.i
    i32 4, label %sw.bb35.i.i
  ]

sw.bb2.i.i:                                       ; preds = %if.end.i
  br label %sw.bb3.i.i

sw.bb3.i.i:                                       ; preds = %sw.bb2.i.i, %if.end.i
  %size.0.i.i = phi i32 [ %and1.i.i, %if.end.i ], [ %cond.i.i, %sw.bb2.i.i ]
  %notmask.i.i = shl nsw i32 -1, %size.0.i.i
  %sub4.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub4.i.i to i64
  %and5.i.i = and i64 %add.i.i.i, %conv.i.i
  %tobool6.not.i.i = icmp eq i64 %and5.i.i, 0
  %cond10.i.i = select i1 %tobool6.not.i.i, i32 %size.0.i.i, i32 0
  br label %sw.epilog.i.i

sw.bb11.i.i:                                      ; preds = %if.end.i
  %12 = trunc i64 %add.i.i.i to i32
  %conv13.i.i = and i32 %12, 15
  %shl14.i.i = shl nuw nsw i32 1, %and1.i.i
  %add.i.i = add nuw nsw i32 %conv13.i.i, %shl14.i.i
  %cmp.i.i = icmp ult i32 %add.i.i, 17
  %cond19.i.i = select i1 %cmp.i.i, i32 %and1.i.i, i32 0
  br label %sw.epilog.i.i

sw.bb20.i.i:                                      ; preds = %if.end.i
  %13 = trunc i64 %add.i.i.i to i32
  %conv22.i.i = and i32 %13, 15
  %shl23.i.i = shl nuw nsw i32 1, %and1.i.i
  %add24.i.i = add nuw nsw i32 %conv22.i.i, %shl23.i.i
  %cmp25.i.i = icmp ult i32 %add24.i.i, 17
  br i1 %cmp25.i.i, label %sw.epilog.i.i, label %if.else.i.i

if.else.i.i:                                      ; preds = %sw.bb20.i.i
  %shl27.i.i = shl nuw nsw i32 1, %cond.i.i
  %add28.i.i = add nuw nsw i32 %conv22.i.i, %shl27.i.i
  %cmp29.i.i = icmp eq i32 %add28.i.i, 16
  %sub33.i.i = sub nsw i32 0, %cond.i.i
  %spec.select.i.i = select i1 %cmp29.i.i, i32 %cond.i.i, i32 %sub33.i.i
  br label %sw.epilog.i.i

sw.bb35.i.i:                                      ; preds = %if.end.i
  %conv36.i.i = trunc i64 %add.i.i.i to i32
  %14 = tail call i32 @llvm.cttz.i32(i32 %conv36.i.i, i1 false), !range !15
  %cond43.i.i = tail call i32 @llvm.umin.i32(i32 %and1.i.i, i32 %14)
  br label %sw.epilog.i.i

do.body.i34.i:                                    ; preds = %if.end.i
  tail call void @g_assertion_message_expr(ptr noundef null, ptr noundef nonnull @.str.20, i32 noundef 87, ptr noundef nonnull @__func__.required_atomicity, ptr noundef null) #17
  unreachable

sw.epilog.i.i:                                    ; preds = %sw.bb35.i.i, %if.else.i.i, %sw.bb20.i.i, %sw.bb11.i.i, %sw.bb3.i.i, %if.end.i
  %atmax.0.i.i = phi i32 [ %cond43.i.i, %sw.bb35.i.i ], [ %cond19.i.i, %sw.bb11.i.i ], [ %cond10.i.i, %sw.bb3.i.i ], [ 0, %if.end.i ], [ %and1.i.i, %sw.bb20.i.i ], [ %spec.select.i.i, %if.else.i.i ]
  %tcg_cflags.i.i.i = getelementptr inbounds %struct.CPUState, ptr %cpu, i64 0, i32 53
  %15 = load i32, ptr %tcg_cflags.i.i.i, align 16
  %and.i.i.i = and i32 %15, 32768
  %tobool.not.i.i.i = icmp eq i32 %and.i.i.i, 0
  br i1 %tobool.not.i.i.i, label %sw.bb.i, label %cpu_in_serial_context.exit.i.i

cpu_in_serial_context.exit.i.i:                   ; preds = %sw.epilog.i.i
  %16 = getelementptr i8, ptr %cpu, i64 208
  %cs.val.i.i.i = load i32, ptr %16, align 16
  %cs.val.i.fr.i.i = freeze i32 %cs.val.i.i.i
  %tobool.i.i.not.i.i = icmp eq i32 %cs.val.i.fr.i.i, 0
  br i1 %tobool.i.i.not.i.i, label %required_atomicity.exit.i, label %sw.bb.i

required_atomicity.exit.i:                        ; preds = %cpu_in_serial_context.exit.i.i
  switch i32 %atmax.0.i.i, label %do.body34.i [
    i32 0, label %sw.bb.i
    i32 1, label %sw.bb7.i
    i32 2, label %sw.bb8.i
    i32 3, label %sw.bb10.i
    i32 -3, label %sw.bb12.i
    i32 4, label %sw.epilog36.i
  ]

sw.bb.i:                                          ; preds = %required_atomicity.exit.i, %cpu_in_serial_context.exit.i.i, %sw.epilog.i.i
  store i128 %val.sroa.0.0.insert.insert.i, ptr %1, align 1
  br label %store_atom_16.exit

sw.bb7.i:                                         ; preds = %required_atomicity.exit.i
  %conv.i.i.i = trunc i64 %val.addr.0.off0 to i16
  call void @llvm.assume(i1 true) [ "align"(ptr %1, i64 2) ]
  store atomic i16 %conv.i.i.i, ptr %1 monotonic, align 2
  %add.ptr.i.i.i = getelementptr i8, ptr %1, i64 2
  %shr1.i7.i.i = lshr i64 %val.addr.0.off0, 16
  %conv2.i.i.i = trunc i64 %shr1.i7.i.i to i16
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i.i.i, i64 2) ]
  store atomic i16 %conv2.i.i.i, ptr %add.ptr.i.i.i monotonic, align 2
  %add.ptr.i.i = getelementptr i8, ptr %1, i64 4
  %shr1.i.i = lshr i64 %val.addr.0.off0, 32
  %conv.i3.i.i = trunc i64 %shr1.i.i to i16
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i.i, i64 2) ]
  store atomic i16 %conv.i3.i.i, ptr %add.ptr.i.i monotonic, align 2
  %add.ptr.i4.i.i = getelementptr i8, ptr %1, i64 6
  %sum.shift.i.i = lshr i64 %val.addr.0.off0, 48
  %conv2.i6.i.i = trunc i64 %sum.shift.i.i to i16
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i4.i.i, i64 2) ]
  store atomic i16 %conv2.i6.i.i, ptr %add.ptr.i4.i.i monotonic, align 2
  %add.ptr.i = getelementptr i8, ptr %1, i64 8
  %conv.i.i35.i = trunc i64 %val.addr.0.off64 to i16
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i, i64 2) ]
  store atomic i16 %conv.i.i35.i, ptr %add.ptr.i monotonic, align 2
  %add.ptr.i.i36.i = getelementptr i8, ptr %1, i64 10
  %shr1.i7.i37.i = lshr i64 %val.addr.0.off64, 16
  %conv2.i.i38.i = trunc i64 %shr1.i7.i37.i to i16
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i.i36.i, i64 2) ]
  store atomic i16 %conv2.i.i38.i, ptr %add.ptr.i.i36.i monotonic, align 2
  %add.ptr.i39.i = getelementptr i8, ptr %1, i64 12
  %shr1.i40.i = lshr i64 %val.addr.0.off64, 32
  %conv.i3.i41.i = trunc i64 %shr1.i40.i to i16
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i39.i, i64 2) ]
  store atomic i16 %conv.i3.i41.i, ptr %add.ptr.i39.i monotonic, align 2
  %add.ptr.i4.i42.i = getelementptr i8, ptr %1, i64 14
  %sum.shift.i43.i = lshr i64 %val.addr.0.off64, 48
  %conv2.i6.i44.i = trunc i64 %sum.shift.i43.i to i16
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i4.i42.i, i64 2) ]
  store atomic i16 %conv2.i6.i44.i, ptr %add.ptr.i4.i42.i monotonic, align 2
  br label %store_atom_16.exit

sw.bb8.i:                                         ; preds = %required_atomicity.exit.i
  %conv.i45.i = trunc i64 %val.addr.0.off0 to i32
  call void @llvm.assume(i1 true) [ "align"(ptr %1, i64 4) ]
  store atomic i32 %conv.i45.i, ptr %1 monotonic, align 4
  %add.ptr.i46.i = getelementptr i8, ptr %1, i64 4
  %shr1.i47.i = lshr i64 %val.addr.0.off0, 32
  %conv2.i.i = trunc i64 %shr1.i47.i to i32
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i46.i, i64 4) ]
  store atomic i32 %conv2.i.i, ptr %add.ptr.i46.i monotonic, align 4
  %add.ptr9.i = getelementptr i8, ptr %1, i64 8
  %conv.i48.i = trunc i64 %val.addr.0.off64 to i32
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr9.i, i64 4) ]
  store atomic i32 %conv.i48.i, ptr %add.ptr9.i monotonic, align 4
  %add.ptr.i49.i = getelementptr i8, ptr %1, i64 12
  %shr1.i50.i = lshr i64 %val.addr.0.off64, 32
  %conv2.i51.i = trunc i64 %shr1.i50.i to i32
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i49.i, i64 4) ]
  store atomic i32 %conv2.i51.i, ptr %add.ptr.i49.i monotonic, align 4
  br label %store_atom_16.exit

sw.bb10.i:                                        ; preds = %required_atomicity.exit.i
  call void @llvm.assume(i1 true) [ "align"(ptr %1, i64 8) ]
  store atomic i64 %val.addr.0.off0, ptr %1 monotonic, align 8
  %add.ptr11.i = getelementptr i8, ptr %1, i64 8
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr11.i, i64 8) ]
  store atomic i64 %val.addr.0.off64, ptr %add.ptr11.i monotonic, align 8
  br label %store_atom_16.exit

sw.bb12.i:                                        ; preds = %required_atomicity.exit.i
  %conv14.i = trunc i64 %and.i10 to i32
  %sub.i11 = sub nuw nsw i32 16, %conv14.i
  switch i32 %conv14.i, label %do.body.i [
    i32 1, label %sw.bb15.i
    i32 2, label %sw.bb15.i
    i32 3, label %sw.bb15.i
    i32 4, label %sw.bb15.i
    i32 5, label %sw.bb15.i
    i32 6, label %sw.bb15.i
    i32 7, label %sw.bb15.i
    i32 9, label %sw.bb20.i
    i32 10, label %sw.bb20.i
    i32 11, label %sw.bb20.i
    i32 12, label %sw.bb20.i
    i32 13, label %sw.bb20.i
    i32 14, label %sw.bb20.i
    i32 15, label %sw.bb20.i
  ]

sw.bb15.i:                                        ; preds = %sw.bb12.i, %sw.bb12.i, %sw.bb12.i, %sw.bb12.i, %sw.bb12.i, %sw.bb12.i, %sw.bb12.i
  %mul.i.i = shl nuw nsw i32 %sub.i11, 3
  %17 = trunc i64 %add.i.i.i to i32
  %conv.i52.i = shl i32 %17, 3
  %mul2.i.i = and i32 %conv.i52.i, 120
  %sub5.i.i = sub nuw nsw i32 128, %mul.i.i
  %sh_prom6.i.i = zext nneg i32 %sub5.i.i to i64
  %shr7.i.i = lshr i64 -1, %sh_prom6.i.i
  %18 = zext nneg i64 %shr7.i.i to i128
  %sh_prom.i.i.i = zext nneg i32 %mul2.i.i to i128
  %shl.i.i.i = shl i128 %val.sroa.0.0.insert.insert.i, %sh_prom.i.i.i
  %19 = shl nuw nsw i128 %18, 64
  %20 = or disjoint i128 %19, 18446744073709551615
  %shl.i22.i.i = shl i128 %20, %sh_prom.i.i.i
  %retval.sroa.0.0.extract.trunc.i23.i.i = trunc i128 %shl.i22.i.i to i64
  %retval.sroa.2.0.extract.shift.i24.i.i = and i128 %shl.i22.i.i, -18446744073709551616
  %idx.neg.i.i = sub nsw i64 0, %and.i10
  %add.ptr.i54.i = getelementptr i8, ptr %1, i64 %idx.neg.i.i
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i54.i, i64 16) ]
  %21 = load i128, ptr %add.ptr.i54.i, align 16
  %not.i.i.i.i = xor i64 %retval.sroa.0.0.extract.trunc.i23.i.i, -1
  %b.sroa.0.0.insert.ext.i.i.i.i = zext i64 %not.i.i.i.i to i128
  %22 = or disjoint i128 %retval.sroa.2.0.extract.shift.i24.i.i, %b.sroa.0.0.insert.ext.i.i.i.i
  %b.sroa.0.0.insert.insert.i.i.i.i = xor i128 %22, -18446744073709551616
  br label %do.body.i.i.i

do.body.i.i.i:                                    ; preds = %do.body.i.i.i, %sw.bb15.i
  %old.sroa.0.0.i.i.i = phi i128 [ %21, %sw.bb15.i ], [ %25, %do.body.i.i.i ]
  %and.i.i.i.i = and i128 %old.sroa.0.0.i.i.i, %b.sroa.0.0.insert.insert.i.i.i.i
  %or.i.i.i.i = or i128 %and.i.i.i.i, %shl.i.i.i
  %23 = cmpxchg weak ptr %add.ptr.i54.i, i128 %old.sroa.0.0.i.i.i, i128 %or.i.i.i.i monotonic monotonic, align 16
  %24 = extractvalue { i128, i1 } %23, 1
  %25 = extractvalue { i128, i1 } %23, 0
  br i1 %24, label %store_whole_le16.exit.i, label %do.body.i.i.i, !llvm.loop !18

store_whole_le16.exit.i:                          ; preds = %do.body.i.i.i
  %idx.ext.i = zext nneg i32 %sub.i11 to i64
  %add.ptr18.i = getelementptr i8, ptr %1, i64 %idx.ext.i
  %cmp5.i.not.i = icmp eq i32 %conv14.i, 0
  br i1 %cmp5.i.not.i, label %store_atom_16.exit, label %for.body.preheader.i.i

for.body.preheader.i.i:                           ; preds = %store_whole_le16.exit.i
  %sub25.i.i = add nsw i32 %mul.i.i, -64
  %sh_prom26.i.i = zext nneg i32 %sub25.i.i to i64
  %shr27.i.i = ashr i64 %val.addr.0.off64, %sh_prom26.i.i
  br label %for.body.i.i

for.body.i.i:                                     ; preds = %for.body.i.i, %for.body.preheader.i.i
  %indvars.iv.i.i = phi i64 [ 0, %for.body.preheader.i.i ], [ %indvars.iv.next.i.i, %for.body.i.i ]
  %val_le.addr.06.i.i = phi i64 [ %shr27.i.i, %for.body.preheader.i.i ], [ %shr.i56.i, %for.body.i.i ]
  %conv.i55.i = trunc i64 %val_le.addr.06.i.i to i8
  %arrayidx.i.i = getelementptr i8, ptr %add.ptr18.i, i64 %indvars.iv.i.i
  store i8 %conv.i55.i, ptr %arrayidx.i.i, align 1
  %indvars.iv.next.i.i = add nuw nsw i64 %indvars.iv.i.i, 1
  %shr.i56.i = lshr i64 %val_le.addr.06.i.i, 8
  %exitcond.not.i.i = icmp eq i64 %indvars.iv.next.i.i, %and.i10
  br i1 %exitcond.not.i.i, label %store_atom_16.exit, label %for.body.i.i, !llvm.loop !19

sw.bb20.i:                                        ; preds = %sw.bb12.i, %sw.bb12.i, %sw.bb12.i, %sw.bb12.i, %sw.bb12.i, %sw.bb12.i, %sw.bb12.i
  %wide.trip.count.i58.i = zext nneg i32 %sub.i11 to i64
  br label %for.body.i59.i

for.body.i59.i:                                   ; preds = %for.body.i59.i, %sw.bb20.i
  %indvars.iv.i60.i = phi i64 [ 0, %sw.bb20.i ], [ %indvars.iv.next.i64.i, %for.body.i59.i ]
  %val_le.addr.06.i61.i = phi i64 [ %val.addr.0.off0, %sw.bb20.i ], [ %shr.i65.i, %for.body.i59.i ]
  %conv.i62.i = trunc i64 %val_le.addr.06.i61.i to i8
  %arrayidx.i63.i = getelementptr i8, ptr %1, i64 %indvars.iv.i60.i
  store i8 %conv.i62.i, ptr %arrayidx.i63.i, align 1
  %indvars.iv.next.i64.i = add nuw nsw i64 %indvars.iv.i60.i, 1
  %shr.i65.i = lshr i64 %val_le.addr.06.i61.i, 8
  %exitcond.not.i66.i = icmp eq i64 %indvars.iv.next.i64.i, %wide.trip.count.i58.i
  br i1 %exitcond.not.i66.i, label %store_bytes_leN.exit68.i, label %for.body.i59.i, !llvm.loop !19

store_bytes_leN.exit68.i:                         ; preds = %for.body.i59.i
  %mul.i = shl nuw nsw i32 %sub.i11, 3
  %sh_prom.i69.i = zext nneg i32 %mul.i to i128
  %shr.i70.i = lshr i128 %val.sroa.0.0.insert.insert.i, %sh_prom.i69.i
  %add.ptr28.i = getelementptr i8, ptr %1, i64 %wide.trip.count.i58.i
  %mul.i71.i = shl nuw nsw i32 %conv14.i, 3
  %26 = ptrtoint ptr %add.ptr28.i to i64
  %27 = trunc i64 %26 to i32
  %conv.i72.i = shl i32 %27, 3
  %mul2.i73.i = and i32 %conv.i72.i, 120
  %cmp.i74.i = icmp ult i32 %conv14.i, 9
  %sub.i75.i = sub nsw i32 64, %mul.i71.i
  %sh_prom.i76.i = zext nneg i32 %sub.i75.i to i64
  %shr.i77.i = lshr i64 -1, %sh_prom.i76.i
  %sub5.i78.i = sub nuw nsw i32 128, %mul.i71.i
  %sh_prom6.i79.i = zext nneg i32 %sub5.i78.i to i64
  %shr7.i80.i = lshr i64 -1, %sh_prom6.i79.i
  %28 = zext i64 %shr.i77.i to i128
  %29 = zext nneg i64 %shr7.i80.i to i128
  %sh_prom.i.i85.i = zext nneg i32 %mul2.i73.i to i128
  %shl.i.i86.i = shl i128 %shr.i70.i, %sh_prom.i.i85.i
  %30 = shl nuw nsw i128 %29, 64
  %31 = or disjoint i128 %30, 18446744073709551615
  %a.sroa.0.0.insert.insert.i20.i87.i = select i1 %cmp.i74.i, i128 %28, i128 %31
  %shl.i22.i88.i = shl i128 %a.sroa.0.0.insert.insert.i20.i87.i, %sh_prom.i.i85.i
  %retval.sroa.0.0.extract.trunc.i23.i89.i = trunc i128 %shl.i22.i88.i to i64
  %retval.sroa.2.0.extract.shift.i24.i90.i = and i128 %shl.i22.i88.i, -18446744073709551616
  %idx.ext.i91.i = and i64 %26, 15
  %idx.neg.i92.i = sub nsw i64 0, %idx.ext.i91.i
  %add.ptr.i93.i = getelementptr i8, ptr %add.ptr28.i, i64 %idx.neg.i92.i
  call void @llvm.assume(i1 true) [ "align"(ptr %add.ptr.i93.i, i64 16) ]
  %32 = load i128, ptr %add.ptr.i93.i, align 16
  %not.i.i.i94.i = xor i64 %retval.sroa.0.0.extract.trunc.i23.i89.i, -1
  %b.sroa.0.0.insert.ext.i.i.i95.i = zext i64 %not.i.i.i94.i to i128
  %33 = or disjoint i128 %retval.sroa.2.0.extract.shift.i24.i90.i, %b.sroa.0.0.insert.ext.i.i.i95.i
  %b.sroa.0.0.insert.insert.i.i.i96.i = xor i128 %33, -18446744073709551616
  br label %do.body.i.i97.i

do.body.i.i97.i:                                  ; preds = %do.body.i.i97.i, %store_bytes_leN.exit68.i
  %old.sroa.0.0.i.i98.i = phi i128 [ %32, %store_bytes_leN.exit68.i ], [ %36, %do.body.i.i97.i ]
  %and.i.i.i99.i = and i128 %old.sroa.0.0.i.i98.i, %b.sroa.0.0.insert.insert.i.i.i96.i
  %or.i.i.i100.i = or i128 %and.i.i.i99.i, %shl.i.i86.i
  %34 = cmpxchg weak ptr %add.ptr.i93.i, i128 %old.sroa.0.0.i.i98.i, i128 %or.i.i.i100.i monotonic monotonic, align 16
  %35 = extractvalue { i128, i1 } %34, 1
  %36 = extractvalue { i128, i1 } %34, 0
  br i1 %35, label %store_atom_16.exit, label %do.body.i.i97.i, !llvm.loop !18

do.body.i:                                        ; preds = %sw.bb12.i
  tail call void @g_assertion_message_expr(ptr noundef null, ptr noundef nonnull @.str.20, i32 noundef 1100, ptr noundef nonnull @__func__.store_atom_16, ptr noundef null) #17
  unreachable

do.body34.i:                                      ; preds = %required_atomicity.exit.i
  tail call void @g_assertion_message_expr(ptr noundef null, ptr noundef nonnull @.str.20, i32 noundef 1108, ptr noundef nonnull @__func__.store_atom_16, ptr noundef null) #17
  unreachable

sw.epilog36.i:                                    ; preds = %required_atomicity.exit.i
  tail call void @cpu_loop_exit_atomic(ptr noundef nonnull %cpu, i64 noundef %ra) #17
  unreachable

store_atom_16.exit:                               ; preds = %do.body.i.i97.i, %for.body.i.i, %do.body.i.i, %if.then.i.i, %sw.bb.i, %sw.bb7.i, %sw.bb8.i, %sw.bb10.i, %store_whole_le16.exit.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @helper_st_i128(ptr noundef %env, i64 noundef %addr, i64 noundef %val.coerce0, i64 noundef %val.coerce1, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %2 = and i32 %oi, 112
  %cmp.i = icmp eq i32 %2, 64
  tail call void @llvm.assume(i1 %cmp.i)
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  tail call fastcc void @do_st16_mmu(ptr noundef %add.ptr.i.i, i64 noundef %addr, i64 noundef %val.coerce0, i64 noundef %val.coerce1, i32 noundef %oi, i64 noundef %1)
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local zeroext i8 @cpu_ldb_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %oi, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %shr.i = lshr i32 %oi, 4
  %0 = and i32 %oi, 112
  %cmp = icmp eq i32 %0, 0
  tail call void @llvm.assume(i1 %cmp)
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %and.i.i.i = and i32 %shr.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %get_alignment_bits.exit.i.i
  ]

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %entry, %if.else4.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %shr.i.i.i, %if.else4.i.i.i ], [ 0, %entry ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %do_ld1_mmu.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 0, i64 noundef %ra) #17
  unreachable

do_ld1_mmu.exit:                                  ; preds = %get_alignment_bits.exit.i.i
  %1 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %1, %addr
  %2 = inttoptr i64 %add.i.i.i.i to ptr
  %3 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %ra, ptr %3, align 8
  fence syncscope("singlethread") seq_cst
  %call1.val.i = load i8, ptr %2, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %3, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 1) #16
  ret i8 %call1.val.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local zeroext i16 @cpu_ldw_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %oi, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %0 = and i32 %oi, 112
  %cmp = icmp eq i32 %0, 16
  tail call void @llvm.assume(i1 %cmp)
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %call2 = tail call fastcc zeroext i16 @do_ld2_mmu(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 1) #16
  ret i16 %call2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_ldl_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %oi, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %0 = and i32 %oi, 112
  %cmp = icmp eq i32 %0, 32
  tail call void @llvm.assume(i1 %cmp)
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %call2 = tail call fastcc i32 @do_ld4_mmu(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 1) #16
  ret i32 %call2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_ldq_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %oi, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %0 = and i32 %oi, 112
  %cmp = icmp eq i32 %0, 48
  tail call void @llvm.assume(i1 %cmp)
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %call2 = tail call fastcc i64 @do_ld8_mmu(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 1) #16
  ret i64 %call2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local { i64, i64 } @cpu_ld16_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %oi, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %0 = and i32 %oi, 112
  %cmp = icmp eq i32 %0, 64
  tail call void @llvm.assume(i1 %cmp)
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %call2 = tail call fastcc { i64, i64 } @do_ld16_mmu(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 1) #16
  ret { i64, i64 } %call2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @cpu_stb_mmu(ptr noundef %env, i64 noundef %addr, i8 noundef zeroext %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %shr.i.i = lshr i32 %oi, 4
  %0 = and i32 %oi, 112
  %cmp.i = icmp eq i32 %0, 0
  tail call void @llvm.assume(i1 %cmp.i)
  %and.i.i.i.i = and i32 %shr.i.i, 224
  %trunc.i.i.i.i = trunc i32 %and.i.i.i.i to i8
  switch i8 %trunc.i.i.i.i, label %if.else4.i.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i.i
    i8 -32, label %get_alignment_bits.exit.i.i.i
  ]

if.else4.i.i.i.i:                                 ; preds = %entry
  %shr.i.i.i.i = lshr exact i32 %and.i.i.i.i, 5
  br label %get_alignment_bits.exit.i.i.i

get_alignment_bits.exit.i.i.i:                    ; preds = %if.else4.i.i.i.i, %entry, %entry
  %a.0.i.i.i.i = phi i32 [ %shr.i.i.i.i, %if.else4.i.i.i.i ], [ 0, %entry ], [ 0, %entry ]
  %notmask.i.i.i = shl nsw i32 -1, %a.0.i.i.i.i
  %sub.i.i.i = xor i32 %notmask.i.i.i, -1
  %conv.i.i.i = zext nneg i32 %sub.i.i.i to i64
  %and.i.i.i = and i64 %conv.i.i.i, %addr
  %tobool.not.i.i.i = icmp eq i64 %and.i.i.i, 0
  br i1 %tobool.not.i.i.i, label %helper_stb_mmu.exit, label %if.then.i.i.i

if.then.i.i.i:                                    ; preds = %get_alignment_bits.exit.i.i.i
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

helper_stb_mmu.exit:                              ; preds = %get_alignment_bits.exit.i.i.i
  %1 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i.i = add i64 %1, %addr
  %2 = inttoptr i64 %add.i.i.i.i.i to ptr
  %3 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %3, align 8
  fence syncscope("singlethread") seq_cst
  store i8 %val, ptr %2, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %3, align 8
  %add.ptr.i.i4 = getelementptr i8, ptr %env, i64 -10176
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i4, i64 noundef %addr, i32 noundef %oi, i32 noundef 2) #16
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @cpu_stw_mmu(ptr noundef %env, i64 noundef %addr, i16 noundef zeroext %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %0 = and i32 %oi, 112
  %cmp = icmp eq i32 %0, 16
  tail call void @llvm.assume(i1 %cmp)
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  tail call fastcc void @do_st2_mmu(ptr noundef %add.ptr.i, i64 noundef %addr, i16 noundef zeroext %val, i32 noundef %oi, i64 noundef %retaddr)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 2) #16
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @cpu_stl_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %0 = and i32 %oi, 112
  %cmp = icmp eq i32 %0, 32
  tail call void @llvm.assume(i1 %cmp)
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  tail call fastcc void @do_st4_mmu(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 2) #16
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @cpu_stq_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %0 = and i32 %oi, 112
  %cmp = icmp eq i32 %0, 48
  tail call void @llvm.assume(i1 %cmp)
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  tail call fastcc void @do_st8_mmu(ptr noundef %add.ptr.i, i64 noundef %addr, i64 noundef %val, i32 noundef %oi, i64 noundef %retaddr)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 2) #16
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @cpu_st16_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %val.coerce0, i64 noundef %val.coerce1, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %0 = and i32 %oi, 112
  %cmp = icmp eq i32 %0, 64
  tail call void @llvm.assume(i1 %cmp)
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  tail call fastcc void @do_st16_mmu(ptr noundef %add.ptr.i, i64 noundef %addr, i64 noundef %val.coerce0, i64 noundef %val.coerce1, i32 noundef %oi, i64 noundef %retaddr)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 2) #16
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_ldub_mmuidx_ra(ptr noundef %env, i64 noundef %addr, i32 noundef %mmu_idx, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %shr.i.i = lshr i32 %mmu_idx, 4
  %0 = and i32 %mmu_idx, 112
  %cmp.i = icmp eq i32 %0, 0
  tail call void @llvm.assume(i1 %cmp.i)
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %and.i.i.i.i = and i32 %shr.i.i, 224
  %trunc.i.i.i.i = trunc i32 %and.i.i.i.i to i8
  switch i8 %trunc.i.i.i.i, label %if.else4.i.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i.i
    i8 -32, label %get_alignment_bits.exit.i.i.i
  ]

if.else4.i.i.i.i:                                 ; preds = %entry
  %shr.i.i.i.i = lshr exact i32 %and.i.i.i.i, 5
  br label %get_alignment_bits.exit.i.i.i

get_alignment_bits.exit.i.i.i:                    ; preds = %if.else4.i.i.i.i, %entry, %entry
  %a.0.i.i.i.i = phi i32 [ %shr.i.i.i.i, %if.else4.i.i.i.i ], [ 0, %entry ], [ 0, %entry ]
  %notmask.i.i.i = shl nsw i32 -1, %a.0.i.i.i.i
  %sub.i.i.i = xor i32 %notmask.i.i.i, -1
  %conv.i.i.i = zext nneg i32 %sub.i.i.i to i64
  %and.i.i.i = and i64 %conv.i.i.i, %addr
  %tobool.not.i.i.i = icmp eq i64 %and.i.i.i, 0
  br i1 %tobool.not.i.i.i, label %cpu_ldb_mmu.exit, label %if.then.i.i.i

if.then.i.i.i:                                    ; preds = %get_alignment_bits.exit.i.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 0, i64 noundef %ra) #17
  unreachable

cpu_ldb_mmu.exit:                                 ; preds = %get_alignment_bits.exit.i.i.i
  %1 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i.i = add i64 %1, %addr
  %2 = inttoptr i64 %add.i.i.i.i.i to ptr
  %3 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %ra, ptr %3, align 8
  fence syncscope("singlethread") seq_cst
  %call1.val.i.i = load i8, ptr %2, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %3, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %mmu_idx, i32 noundef 1) #16
  %conv = zext i8 %call1.val.i.i to i32
  ret i32 %conv
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_ldsb_mmuidx_ra(ptr noundef %env, i64 noundef %addr, i32 noundef %mmu_idx, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %shr.i.i.i = lshr i32 %mmu_idx, 4
  %0 = and i32 %mmu_idx, 112
  %cmp.i.i = icmp eq i32 %0, 0
  tail call void @llvm.assume(i1 %cmp.i.i)
  %add.ptr.i.i.i = getelementptr i8, ptr %env, i64 -10176
  %and.i.i.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i.i.i = trunc i32 %and.i.i.i.i.i to i8
  switch i8 %trunc.i.i.i.i.i, label %if.else4.i.i.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i.i.i
    i8 -32, label %get_alignment_bits.exit.i.i.i.i
  ]

if.else4.i.i.i.i.i:                               ; preds = %entry
  %shr.i.i.i.i.i = lshr exact i32 %and.i.i.i.i.i, 5
  br label %get_alignment_bits.exit.i.i.i.i

get_alignment_bits.exit.i.i.i.i:                  ; preds = %if.else4.i.i.i.i.i, %entry, %entry
  %a.0.i.i.i.i.i = phi i32 [ %shr.i.i.i.i.i, %if.else4.i.i.i.i.i ], [ 0, %entry ], [ 0, %entry ]
  %notmask.i.i.i.i = shl nsw i32 -1, %a.0.i.i.i.i.i
  %sub.i.i.i.i = xor i32 %notmask.i.i.i.i, -1
  %conv.i.i.i.i = zext nneg i32 %sub.i.i.i.i to i64
  %and.i.i.i.i = and i64 %conv.i.i.i.i, %addr
  %tobool.not.i.i.i.i = icmp eq i64 %and.i.i.i.i, 0
  br i1 %tobool.not.i.i.i.i, label %cpu_ldub_mmuidx_ra.exit, label %if.then.i.i.i.i

if.then.i.i.i.i:                                  ; preds = %get_alignment_bits.exit.i.i.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef 0, i64 noundef %ra) #17
  unreachable

cpu_ldub_mmuidx_ra.exit:                          ; preds = %get_alignment_bits.exit.i.i.i.i
  %1 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i.i.i = add i64 %1, %addr
  %2 = inttoptr i64 %add.i.i.i.i.i.i to ptr
  %3 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %ra, ptr %3, align 8
  fence syncscope("singlethread") seq_cst
  %call1.val.i.i.i = load i8, ptr %2, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %3, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef %mmu_idx, i32 noundef 1) #16
  %conv1 = sext i8 %call1.val.i.i.i to i32
  ret i32 %conv1
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_lduw_be_mmuidx_ra(ptr noundef %env, i64 noundef %addr, i32 noundef %mmu_idx, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %or.i = or i32 %mmu_idx, 272
  %0 = and i32 %or.i, 112
  %cmp.i = icmp eq i32 %0, 16
  tail call void @llvm.assume(i1 %cmp.i)
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i = tail call fastcc zeroext i16 @do_ld2_mmu(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %or.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %or.i, i32 noundef 1) #16
  %conv = zext i16 %call2.i to i32
  ret i32 %conv
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_ldsw_be_mmuidx_ra(ptr noundef %env, i64 noundef %addr, i32 noundef %mmu_idx, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %or.i.i = or i32 %mmu_idx, 272
  %0 = and i32 %or.i.i, 112
  %cmp.i.i = icmp eq i32 %0, 16
  tail call void @llvm.assume(i1 %cmp.i.i)
  %add.ptr.i.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i.i = tail call fastcc zeroext i16 @do_ld2_mmu(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef %or.i.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef %or.i.i, i32 noundef 1) #16
  %conv1 = sext i16 %call2.i.i to i32
  ret i32 %conv1
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_ldl_be_mmuidx_ra(ptr noundef %env, i64 noundef %addr, i32 noundef %mmu_idx, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %or.i = or i32 %mmu_idx, 288
  %0 = and i32 %or.i, 112
  %cmp.i = icmp eq i32 %0, 32
  tail call void @llvm.assume(i1 %cmp.i)
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i = tail call fastcc i32 @do_ld4_mmu(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %or.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %or.i, i32 noundef 1) #16
  ret i32 %call2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_ldq_be_mmuidx_ra(ptr noundef %env, i64 noundef %addr, i32 noundef %mmu_idx, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %or.i = or i32 %mmu_idx, 304
  %0 = and i32 %or.i, 112
  %cmp.i = icmp eq i32 %0, 48
  tail call void @llvm.assume(i1 %cmp.i)
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i = tail call fastcc i64 @do_ld8_mmu(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %or.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %or.i, i32 noundef 1) #16
  ret i64 %call2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_lduw_le_mmuidx_ra(ptr noundef %env, i64 noundef %addr, i32 noundef %mmu_idx, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %or.i = or i32 %mmu_idx, 16
  %0 = and i32 %or.i, 112
  %cmp.i = icmp eq i32 %0, 16
  tail call void @llvm.assume(i1 %cmp.i)
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i = tail call fastcc zeroext i16 @do_ld2_mmu(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %or.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %or.i, i32 noundef 1) #16
  %conv = zext i16 %call2.i to i32
  ret i32 %conv
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_ldsw_le_mmuidx_ra(ptr noundef %env, i64 noundef %addr, i32 noundef %mmu_idx, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %or.i.i = or i32 %mmu_idx, 16
  %0 = and i32 %or.i.i, 112
  %cmp.i.i = icmp eq i32 %0, 16
  tail call void @llvm.assume(i1 %cmp.i.i)
  %add.ptr.i.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i.i = tail call fastcc zeroext i16 @do_ld2_mmu(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef %or.i.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef %or.i.i, i32 noundef 1) #16
  %conv1 = sext i16 %call2.i.i to i32
  ret i32 %conv1
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_ldl_le_mmuidx_ra(ptr noundef %env, i64 noundef %addr, i32 noundef %mmu_idx, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %or.i = or i32 %mmu_idx, 32
  %0 = and i32 %or.i, 112
  %cmp.i = icmp eq i32 %0, 32
  tail call void @llvm.assume(i1 %cmp.i)
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i = tail call fastcc i32 @do_ld4_mmu(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %or.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %or.i, i32 noundef 1) #16
  ret i32 %call2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_ldq_le_mmuidx_ra(ptr noundef %env, i64 noundef %addr, i32 noundef %mmu_idx, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %or.i = or i32 %mmu_idx, 48
  %0 = and i32 %or.i, 112
  %cmp.i = icmp eq i32 %0, 48
  tail call void @llvm.assume(i1 %cmp.i)
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i = tail call fastcc i64 @do_ld8_mmu(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %or.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %or.i, i32 noundef 1) #16
  ret i64 %call2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @cpu_stb_mmuidx_ra(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %mmu_idx, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %shr.i.i.i = lshr i32 %mmu_idx, 4
  %0 = and i32 %mmu_idx, 112
  %cmp.i.i = icmp eq i32 %0, 0
  tail call void @llvm.assume(i1 %cmp.i.i)
  %and.i.i.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i.i.i = trunc i32 %and.i.i.i.i.i to i8
  switch i8 %trunc.i.i.i.i.i, label %if.else4.i.i.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i.i.i
    i8 -32, label %get_alignment_bits.exit.i.i.i.i
  ]

if.else4.i.i.i.i.i:                               ; preds = %entry
  %shr.i.i.i.i.i = lshr exact i32 %and.i.i.i.i.i, 5
  br label %get_alignment_bits.exit.i.i.i.i

get_alignment_bits.exit.i.i.i.i:                  ; preds = %if.else4.i.i.i.i.i, %entry, %entry
  %a.0.i.i.i.i.i = phi i32 [ %shr.i.i.i.i.i, %if.else4.i.i.i.i.i ], [ 0, %entry ], [ 0, %entry ]
  %notmask.i.i.i.i = shl nsw i32 -1, %a.0.i.i.i.i.i
  %sub.i.i.i.i = xor i32 %notmask.i.i.i.i, -1
  %conv.i.i.i.i = zext nneg i32 %sub.i.i.i.i to i64
  %and.i.i.i.i = and i64 %conv.i.i.i.i, %addr
  %tobool.not.i.i.i.i = icmp eq i64 %and.i.i.i.i, 0
  br i1 %tobool.not.i.i.i.i, label %cpu_stb_mmu.exit, label %if.then.i.i.i.i

if.then.i.i.i.i:                                  ; preds = %get_alignment_bits.exit.i.i.i.i
  %add.ptr.i.i.i = getelementptr i8, ptr %env, i64 -10176
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %ra) #17
  unreachable

cpu_stb_mmu.exit:                                 ; preds = %get_alignment_bits.exit.i.i.i.i
  %conv = trunc i32 %val to i8
  %1 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i.i.i = add i64 %1, %addr
  %2 = inttoptr i64 %add.i.i.i.i.i.i to ptr
  %3 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %ra, ptr %3, align 8
  fence syncscope("singlethread") seq_cst
  store i8 %conv, ptr %2, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %3, align 8
  %add.ptr.i.i4.i = getelementptr i8, ptr %env, i64 -10176
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i4.i, i64 noundef %addr, i32 noundef %mmu_idx, i32 noundef 2) #16
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @cpu_stw_be_mmuidx_ra(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %mmu_idx, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %or.i = or i32 %mmu_idx, 272
  %conv = trunc i32 %val to i16
  %0 = and i32 %or.i, 112
  %cmp.i = icmp eq i32 %0, 16
  tail call void @llvm.assume(i1 %cmp.i)
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  tail call fastcc void @do_st2_mmu(ptr noundef %add.ptr.i.i, i64 noundef %addr, i16 noundef zeroext %conv, i32 noundef %or.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %or.i, i32 noundef 2) #16
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @cpu_stl_be_mmuidx_ra(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %mmu_idx, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %or.i = or i32 %mmu_idx, 288
  %0 = and i32 %or.i, 112
  %cmp.i = icmp eq i32 %0, 32
  tail call void @llvm.assume(i1 %cmp.i)
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  tail call fastcc void @do_st4_mmu(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %val, i32 noundef %or.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %or.i, i32 noundef 2) #16
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @cpu_stq_be_mmuidx_ra(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %mmu_idx, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %or.i = or i32 %mmu_idx, 304
  %0 = and i32 %or.i, 112
  %cmp.i = icmp eq i32 %0, 48
  tail call void @llvm.assume(i1 %cmp.i)
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  tail call fastcc void @do_st8_mmu(ptr noundef %add.ptr.i.i, i64 noundef %addr, i64 noundef %val, i32 noundef %or.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %or.i, i32 noundef 2) #16
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @cpu_stw_le_mmuidx_ra(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %mmu_idx, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %or.i = or i32 %mmu_idx, 16
  %conv = trunc i32 %val to i16
  %0 = and i32 %or.i, 112
  %cmp.i = icmp eq i32 %0, 16
  tail call void @llvm.assume(i1 %cmp.i)
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  tail call fastcc void @do_st2_mmu(ptr noundef %add.ptr.i.i, i64 noundef %addr, i16 noundef zeroext %conv, i32 noundef %or.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %or.i, i32 noundef 2) #16
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @cpu_stl_le_mmuidx_ra(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %mmu_idx, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %or.i = or i32 %mmu_idx, 32
  %0 = and i32 %or.i, 112
  %cmp.i = icmp eq i32 %0, 32
  tail call void @llvm.assume(i1 %cmp.i)
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  tail call fastcc void @do_st4_mmu(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %val, i32 noundef %or.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %or.i, i32 noundef 2) #16
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @cpu_stq_le_mmuidx_ra(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %mmu_idx, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %or.i = or i32 %mmu_idx, 48
  %0 = and i32 %or.i, 112
  %cmp.i = icmp eq i32 %0, 48
  tail call void @llvm.assume(i1 %cmp.i)
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  tail call fastcc void @do_st8_mmu(ptr noundef %add.ptr.i.i, i64 noundef %addr, i64 noundef %val, i32 noundef %or.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %or.i, i32 noundef 2) #16
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_ldub_data_ra(ptr noundef %env, i64 noundef %addr, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %call = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %shr.i.i.i = lshr i32 %call, 4
  %0 = and i32 %call, 112
  %cmp.i.i = icmp eq i32 %0, 0
  tail call void @llvm.assume(i1 %cmp.i.i)
  %add.ptr.i.i.i = getelementptr i8, ptr %env, i64 -10176
  %and.i.i.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i.i.i = trunc i32 %and.i.i.i.i.i to i8
  switch i8 %trunc.i.i.i.i.i, label %if.else4.i.i.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i.i.i
    i8 -32, label %get_alignment_bits.exit.i.i.i.i
  ]

if.else4.i.i.i.i.i:                               ; preds = %entry
  %shr.i.i.i.i.i = lshr exact i32 %and.i.i.i.i.i, 5
  br label %get_alignment_bits.exit.i.i.i.i

get_alignment_bits.exit.i.i.i.i:                  ; preds = %if.else4.i.i.i.i.i, %entry, %entry
  %a.0.i.i.i.i.i = phi i32 [ %shr.i.i.i.i.i, %if.else4.i.i.i.i.i ], [ 0, %entry ], [ 0, %entry ]
  %notmask.i.i.i.i = shl nsw i32 -1, %a.0.i.i.i.i.i
  %sub.i.i.i.i = xor i32 %notmask.i.i.i.i, -1
  %conv.i.i.i.i = zext nneg i32 %sub.i.i.i.i to i64
  %and.i.i.i.i = and i64 %conv.i.i.i.i, %addr
  %tobool.not.i.i.i.i = icmp eq i64 %and.i.i.i.i, 0
  br i1 %tobool.not.i.i.i.i, label %cpu_ldub_mmuidx_ra.exit, label %if.then.i.i.i.i

if.then.i.i.i.i:                                  ; preds = %get_alignment_bits.exit.i.i.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef 0, i64 noundef %ra) #17
  unreachable

cpu_ldub_mmuidx_ra.exit:                          ; preds = %get_alignment_bits.exit.i.i.i.i
  %1 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i.i.i = add i64 %1, %addr
  %2 = inttoptr i64 %add.i.i.i.i.i.i to ptr
  %3 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %ra, ptr %3, align 8
  fence syncscope("singlethread") seq_cst
  %call1.val.i.i.i = load i8, ptr %2, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %3, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef %call, i32 noundef 1) #16
  %conv.i = zext i8 %call1.val.i.i.i to i32
  ret i32 %conv.i
}

declare i32 @riscv_cpu_mmu_index(ptr noundef, i1 noundef zeroext) local_unnamed_addr #5

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_ldsb_data_ra(ptr noundef %env, i64 noundef %addr, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %call.i = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %shr.i.i.i.i = lshr i32 %call.i, 4
  %0 = and i32 %call.i, 112
  %cmp.i.i.i = icmp eq i32 %0, 0
  tail call void @llvm.assume(i1 %cmp.i.i.i)
  %add.ptr.i.i.i.i = getelementptr i8, ptr %env, i64 -10176
  %and.i.i.i.i.i.i = and i32 %shr.i.i.i.i, 224
  %trunc.i.i.i.i.i.i = trunc i32 %and.i.i.i.i.i.i to i8
  switch i8 %trunc.i.i.i.i.i.i, label %if.else4.i.i.i.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i.i.i.i
    i8 -32, label %get_alignment_bits.exit.i.i.i.i.i
  ]

if.else4.i.i.i.i.i.i:                             ; preds = %entry
  %shr.i.i.i.i.i.i = lshr exact i32 %and.i.i.i.i.i.i, 5
  br label %get_alignment_bits.exit.i.i.i.i.i

get_alignment_bits.exit.i.i.i.i.i:                ; preds = %if.else4.i.i.i.i.i.i, %entry, %entry
  %a.0.i.i.i.i.i.i = phi i32 [ %shr.i.i.i.i.i.i, %if.else4.i.i.i.i.i.i ], [ 0, %entry ], [ 0, %entry ]
  %notmask.i.i.i.i.i = shl nsw i32 -1, %a.0.i.i.i.i.i.i
  %sub.i.i.i.i.i = xor i32 %notmask.i.i.i.i.i, -1
  %conv.i.i.i.i.i = zext nneg i32 %sub.i.i.i.i.i to i64
  %and.i.i.i.i.i = and i64 %conv.i.i.i.i.i, %addr
  %tobool.not.i.i.i.i.i = icmp eq i64 %and.i.i.i.i.i, 0
  br i1 %tobool.not.i.i.i.i.i, label %cpu_ldub_data_ra.exit, label %if.then.i.i.i.i.i

if.then.i.i.i.i.i:                                ; preds = %get_alignment_bits.exit.i.i.i.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef 0, i64 noundef %ra) #17
  unreachable

cpu_ldub_data_ra.exit:                            ; preds = %get_alignment_bits.exit.i.i.i.i.i
  %1 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i.i.i.i = add i64 %1, %addr
  %2 = inttoptr i64 %add.i.i.i.i.i.i.i to ptr
  %3 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %ra, ptr %3, align 8
  fence syncscope("singlethread") seq_cst
  %call1.val.i.i.i.i = load i8, ptr %2, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %3, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef %call.i, i32 noundef 1) #16
  %conv1 = sext i8 %call1.val.i.i.i.i to i32
  ret i32 %conv1
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_lduw_be_data_ra(ptr noundef %env, i64 noundef %addr, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %call = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i = or i32 %call, 272
  %0 = and i32 %or.i.i, 112
  %cmp.i.i = icmp eq i32 %0, 16
  tail call void @llvm.assume(i1 %cmp.i.i)
  %add.ptr.i.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i.i = tail call fastcc zeroext i16 @do_ld2_mmu(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef %or.i.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef %or.i.i, i32 noundef 1) #16
  %conv.i = zext i16 %call2.i.i to i32
  ret i32 %conv.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_ldsw_be_data_ra(ptr noundef %env, i64 noundef %addr, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %call.i = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i.i = or i32 %call.i, 272
  %0 = and i32 %or.i.i.i, 112
  %cmp.i.i.i = icmp eq i32 %0, 16
  tail call void @llvm.assume(i1 %cmp.i.i.i)
  %add.ptr.i.i.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i.i.i = tail call fastcc zeroext i16 @do_ld2_mmu(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef %or.i.i.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef %or.i.i.i, i32 noundef 1) #16
  %conv1 = sext i16 %call2.i.i.i to i32
  ret i32 %conv1
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_ldl_be_data_ra(ptr noundef %env, i64 noundef %addr, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %call = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i = or i32 %call, 288
  %0 = and i32 %or.i.i, 112
  %cmp.i.i = icmp eq i32 %0, 32
  tail call void @llvm.assume(i1 %cmp.i.i)
  %add.ptr.i.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i.i = tail call fastcc i32 @do_ld4_mmu(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef %or.i.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef %or.i.i, i32 noundef 1) #16
  ret i32 %call2.i.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_ldq_be_data_ra(ptr noundef %env, i64 noundef %addr, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %call = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i = or i32 %call, 304
  %0 = and i32 %or.i.i, 112
  %cmp.i.i = icmp eq i32 %0, 48
  tail call void @llvm.assume(i1 %cmp.i.i)
  %add.ptr.i.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i.i = tail call fastcc i64 @do_ld8_mmu(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef %or.i.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef %or.i.i, i32 noundef 1) #16
  ret i64 %call2.i.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_lduw_le_data_ra(ptr noundef %env, i64 noundef %addr, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %call = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i = or i32 %call, 16
  %0 = and i32 %or.i.i, 112
  %cmp.i.i = icmp eq i32 %0, 16
  tail call void @llvm.assume(i1 %cmp.i.i)
  %add.ptr.i.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i.i = tail call fastcc zeroext i16 @do_ld2_mmu(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef %or.i.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef %or.i.i, i32 noundef 1) #16
  %conv.i = zext i16 %call2.i.i to i32
  ret i32 %conv.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_ldsw_le_data_ra(ptr noundef %env, i64 noundef %addr, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %call.i = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i.i = or i32 %call.i, 16
  %0 = and i32 %or.i.i.i, 112
  %cmp.i.i.i = icmp eq i32 %0, 16
  tail call void @llvm.assume(i1 %cmp.i.i.i)
  %add.ptr.i.i.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i.i.i = tail call fastcc zeroext i16 @do_ld2_mmu(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef %or.i.i.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef %or.i.i.i, i32 noundef 1) #16
  %conv1 = sext i16 %call2.i.i.i to i32
  ret i32 %conv1
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_ldl_le_data_ra(ptr noundef %env, i64 noundef %addr, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %call = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i = or i32 %call, 32
  %0 = and i32 %or.i.i, 112
  %cmp.i.i = icmp eq i32 %0, 32
  tail call void @llvm.assume(i1 %cmp.i.i)
  %add.ptr.i.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i.i = tail call fastcc i32 @do_ld4_mmu(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef %or.i.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef %or.i.i, i32 noundef 1) #16
  ret i32 %call2.i.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_ldq_le_data_ra(ptr noundef %env, i64 noundef %addr, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %call = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i = or i32 %call, 48
  %0 = and i32 %or.i.i, 112
  %cmp.i.i = icmp eq i32 %0, 48
  tail call void @llvm.assume(i1 %cmp.i.i)
  %add.ptr.i.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i.i = tail call fastcc i64 @do_ld8_mmu(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef %or.i.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef %or.i.i, i32 noundef 1) #16
  ret i64 %call2.i.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @cpu_stb_data_ra(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %call = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %shr.i.i.i.i = lshr i32 %call, 4
  %0 = and i32 %call, 112
  %cmp.i.i.i = icmp eq i32 %0, 0
  tail call void @llvm.assume(i1 %cmp.i.i.i)
  %and.i.i.i.i.i.i = and i32 %shr.i.i.i.i, 224
  %trunc.i.i.i.i.i.i = trunc i32 %and.i.i.i.i.i.i to i8
  switch i8 %trunc.i.i.i.i.i.i, label %if.else4.i.i.i.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i.i.i.i
    i8 -32, label %get_alignment_bits.exit.i.i.i.i.i
  ]

if.else4.i.i.i.i.i.i:                             ; preds = %entry
  %shr.i.i.i.i.i.i = lshr exact i32 %and.i.i.i.i.i.i, 5
  br label %get_alignment_bits.exit.i.i.i.i.i

get_alignment_bits.exit.i.i.i.i.i:                ; preds = %if.else4.i.i.i.i.i.i, %entry, %entry
  %a.0.i.i.i.i.i.i = phi i32 [ %shr.i.i.i.i.i.i, %if.else4.i.i.i.i.i.i ], [ 0, %entry ], [ 0, %entry ]
  %notmask.i.i.i.i.i = shl nsw i32 -1, %a.0.i.i.i.i.i.i
  %sub.i.i.i.i.i = xor i32 %notmask.i.i.i.i.i, -1
  %conv.i.i.i.i.i = zext nneg i32 %sub.i.i.i.i.i to i64
  %and.i.i.i.i.i = and i64 %conv.i.i.i.i.i, %addr
  %tobool.not.i.i.i.i.i = icmp eq i64 %and.i.i.i.i.i, 0
  br i1 %tobool.not.i.i.i.i.i, label %cpu_stb_mmuidx_ra.exit, label %if.then.i.i.i.i.i

if.then.i.i.i.i.i:                                ; preds = %get_alignment_bits.exit.i.i.i.i.i
  %add.ptr.i.i.i.i = getelementptr i8, ptr %env, i64 -10176
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %ra) #17
  unreachable

cpu_stb_mmuidx_ra.exit:                           ; preds = %get_alignment_bits.exit.i.i.i.i.i
  %conv.i = trunc i32 %val to i8
  %1 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i.i.i.i = add i64 %1, %addr
  %2 = inttoptr i64 %add.i.i.i.i.i.i.i to ptr
  %3 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %ra, ptr %3, align 8
  fence syncscope("singlethread") seq_cst
  store i8 %conv.i, ptr %2, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %3, align 8
  %add.ptr.i.i4.i.i = getelementptr i8, ptr %env, i64 -10176
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i4.i.i, i64 noundef %addr, i32 noundef %call, i32 noundef 2) #16
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @cpu_stw_be_data_ra(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %call = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i = or i32 %call, 272
  %conv.i = trunc i32 %val to i16
  %0 = and i32 %or.i.i, 112
  %cmp.i.i = icmp eq i32 %0, 16
  tail call void @llvm.assume(i1 %cmp.i.i)
  %add.ptr.i.i.i = getelementptr i8, ptr %env, i64 -10176
  tail call fastcc void @do_st2_mmu(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i16 noundef zeroext %conv.i, i32 noundef %or.i.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef %or.i.i, i32 noundef 2) #16
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @cpu_stl_be_data_ra(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %call = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i = or i32 %call, 288
  %0 = and i32 %or.i.i, 112
  %cmp.i.i = icmp eq i32 %0, 32
  tail call void @llvm.assume(i1 %cmp.i.i)
  %add.ptr.i.i.i = getelementptr i8, ptr %env, i64 -10176
  tail call fastcc void @do_st4_mmu(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef %val, i32 noundef %or.i.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef %or.i.i, i32 noundef 2) #16
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @cpu_stq_be_data_ra(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %call = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i = or i32 %call, 304
  %0 = and i32 %or.i.i, 112
  %cmp.i.i = icmp eq i32 %0, 48
  tail call void @llvm.assume(i1 %cmp.i.i)
  %add.ptr.i.i.i = getelementptr i8, ptr %env, i64 -10176
  tail call fastcc void @do_st8_mmu(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i64 noundef %val, i32 noundef %or.i.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef %or.i.i, i32 noundef 2) #16
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @cpu_stw_le_data_ra(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %call = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i = or i32 %call, 16
  %conv.i = trunc i32 %val to i16
  %0 = and i32 %or.i.i, 112
  %cmp.i.i = icmp eq i32 %0, 16
  tail call void @llvm.assume(i1 %cmp.i.i)
  %add.ptr.i.i.i = getelementptr i8, ptr %env, i64 -10176
  tail call fastcc void @do_st2_mmu(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i16 noundef zeroext %conv.i, i32 noundef %or.i.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef %or.i.i, i32 noundef 2) #16
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @cpu_stl_le_data_ra(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %call = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i = or i32 %call, 32
  %0 = and i32 %or.i.i, 112
  %cmp.i.i = icmp eq i32 %0, 32
  tail call void @llvm.assume(i1 %cmp.i.i)
  %add.ptr.i.i.i = getelementptr i8, ptr %env, i64 -10176
  tail call fastcc void @do_st4_mmu(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef %val, i32 noundef %or.i.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef %or.i.i, i32 noundef 2) #16
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @cpu_stq_le_data_ra(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i64 noundef %ra) local_unnamed_addr #2 {
entry:
  %call = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i = or i32 %call, 48
  %0 = and i32 %or.i.i, 112
  %cmp.i.i = icmp eq i32 %0, 48
  tail call void @llvm.assume(i1 %cmp.i.i)
  %add.ptr.i.i.i = getelementptr i8, ptr %env, i64 -10176
  tail call fastcc void @do_st8_mmu(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i64 noundef %val, i32 noundef %or.i.i, i64 noundef %ra)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i, i64 noundef %addr, i32 noundef %or.i.i, i32 noundef 2) #16
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_ldub_data(ptr noundef %env, i64 noundef %addr) local_unnamed_addr #2 {
entry:
  %call.i = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %shr.i.i.i.i = lshr i32 %call.i, 4
  %0 = and i32 %call.i, 112
  %cmp.i.i.i = icmp eq i32 %0, 0
  tail call void @llvm.assume(i1 %cmp.i.i.i)
  %add.ptr.i.i.i.i = getelementptr i8, ptr %env, i64 -10176
  %and.i.i.i.i.i.i = and i32 %shr.i.i.i.i, 224
  %trunc.i.i.i.i.i.i = trunc i32 %and.i.i.i.i.i.i to i8
  switch i8 %trunc.i.i.i.i.i.i, label %if.else4.i.i.i.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i.i.i.i
    i8 -32, label %get_alignment_bits.exit.i.i.i.i.i
  ]

if.else4.i.i.i.i.i.i:                             ; preds = %entry
  %shr.i.i.i.i.i.i = lshr exact i32 %and.i.i.i.i.i.i, 5
  br label %get_alignment_bits.exit.i.i.i.i.i

get_alignment_bits.exit.i.i.i.i.i:                ; preds = %if.else4.i.i.i.i.i.i, %entry, %entry
  %a.0.i.i.i.i.i.i = phi i32 [ %shr.i.i.i.i.i.i, %if.else4.i.i.i.i.i.i ], [ 0, %entry ], [ 0, %entry ]
  %notmask.i.i.i.i.i = shl nsw i32 -1, %a.0.i.i.i.i.i.i
  %sub.i.i.i.i.i = xor i32 %notmask.i.i.i.i.i, -1
  %conv.i.i.i.i.i = zext nneg i32 %sub.i.i.i.i.i to i64
  %and.i.i.i.i.i = and i64 %conv.i.i.i.i.i, %addr
  %tobool.not.i.i.i.i.i = icmp eq i64 %and.i.i.i.i.i, 0
  br i1 %tobool.not.i.i.i.i.i, label %cpu_ldub_data_ra.exit, label %if.then.i.i.i.i.i

if.then.i.i.i.i.i:                                ; preds = %get_alignment_bits.exit.i.i.i.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef 0, i64 noundef 0) #17
  unreachable

cpu_ldub_data_ra.exit:                            ; preds = %get_alignment_bits.exit.i.i.i.i.i
  %1 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i.i.i.i = add i64 %1, %addr
  %2 = inttoptr i64 %add.i.i.i.i.i.i.i to ptr
  %3 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 0, ptr %3, align 8
  fence syncscope("singlethread") seq_cst
  %call1.val.i.i.i.i = load i8, ptr %2, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %3, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef %call.i, i32 noundef 1) #16
  %conv.i.i = zext i8 %call1.val.i.i.i.i to i32
  ret i32 %conv.i.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_ldsb_data(ptr noundef %env, i64 noundef %addr) local_unnamed_addr #2 {
entry:
  %call.i.i = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %shr.i.i.i.i.i = lshr i32 %call.i.i, 4
  %0 = and i32 %call.i.i, 112
  %cmp.i.i.i.i = icmp eq i32 %0, 0
  tail call void @llvm.assume(i1 %cmp.i.i.i.i)
  %add.ptr.i.i.i.i.i = getelementptr i8, ptr %env, i64 -10176
  %and.i.i.i.i.i.i.i = and i32 %shr.i.i.i.i.i, 224
  %trunc.i.i.i.i.i.i.i = trunc i32 %and.i.i.i.i.i.i.i to i8
  switch i8 %trunc.i.i.i.i.i.i.i, label %if.else4.i.i.i.i.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i.i.i.i.i
    i8 -32, label %get_alignment_bits.exit.i.i.i.i.i.i
  ]

if.else4.i.i.i.i.i.i.i:                           ; preds = %entry
  %shr.i.i.i.i.i.i.i = lshr exact i32 %and.i.i.i.i.i.i.i, 5
  br label %get_alignment_bits.exit.i.i.i.i.i.i

get_alignment_bits.exit.i.i.i.i.i.i:              ; preds = %if.else4.i.i.i.i.i.i.i, %entry, %entry
  %a.0.i.i.i.i.i.i.i = phi i32 [ %shr.i.i.i.i.i.i.i, %if.else4.i.i.i.i.i.i.i ], [ 0, %entry ], [ 0, %entry ]
  %notmask.i.i.i.i.i.i = shl nsw i32 -1, %a.0.i.i.i.i.i.i.i
  %sub.i.i.i.i.i.i = xor i32 %notmask.i.i.i.i.i.i, -1
  %conv.i.i.i.i.i.i = zext nneg i32 %sub.i.i.i.i.i.i to i64
  %and.i.i.i.i.i.i = and i64 %conv.i.i.i.i.i.i, %addr
  %tobool.not.i.i.i.i.i.i = icmp eq i64 %and.i.i.i.i.i.i, 0
  br i1 %tobool.not.i.i.i.i.i.i, label %cpu_ldub_data.exit, label %if.then.i.i.i.i.i.i

if.then.i.i.i.i.i.i:                              ; preds = %get_alignment_bits.exit.i.i.i.i.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i.i.i.i, i64 noundef %addr, i32 noundef 0, i64 noundef 0) #17
  unreachable

cpu_ldub_data.exit:                               ; preds = %get_alignment_bits.exit.i.i.i.i.i.i
  %1 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i.i.i.i.i = add i64 %1, %addr
  %2 = inttoptr i64 %add.i.i.i.i.i.i.i.i to ptr
  %3 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 0, ptr %3, align 8
  fence syncscope("singlethread") seq_cst
  %call1.val.i.i.i.i.i = load i8, ptr %2, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %3, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i.i.i, i64 noundef %addr, i32 noundef %call.i.i, i32 noundef 1) #16
  %conv1 = sext i8 %call1.val.i.i.i.i.i to i32
  ret i32 %conv1
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_lduw_be_data(ptr noundef %env, i64 noundef %addr) local_unnamed_addr #2 {
entry:
  %call.i = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i.i = or i32 %call.i, 272
  %0 = and i32 %or.i.i.i, 112
  %cmp.i.i.i = icmp eq i32 %0, 16
  tail call void @llvm.assume(i1 %cmp.i.i.i)
  %add.ptr.i.i.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i.i.i = tail call fastcc zeroext i16 @do_ld2_mmu(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef %or.i.i.i, i64 noundef 0)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef %or.i.i.i, i32 noundef 1) #16
  %conv.i.i = zext i16 %call2.i.i.i to i32
  ret i32 %conv.i.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_ldsw_be_data(ptr noundef %env, i64 noundef %addr) local_unnamed_addr #2 {
entry:
  %call.i.i = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i.i.i = or i32 %call.i.i, 272
  %0 = and i32 %or.i.i.i.i, 112
  %cmp.i.i.i.i = icmp eq i32 %0, 16
  tail call void @llvm.assume(i1 %cmp.i.i.i.i)
  %add.ptr.i.i.i.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i.i.i.i = tail call fastcc zeroext i16 @do_ld2_mmu(ptr noundef %add.ptr.i.i.i.i.i, i64 noundef %addr, i32 noundef %or.i.i.i.i, i64 noundef 0)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i.i.i, i64 noundef %addr, i32 noundef %or.i.i.i.i, i32 noundef 1) #16
  %conv1 = sext i16 %call2.i.i.i.i to i32
  ret i32 %conv1
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_ldl_be_data(ptr noundef %env, i64 noundef %addr) local_unnamed_addr #2 {
entry:
  %call.i = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i.i = or i32 %call.i, 288
  %0 = and i32 %or.i.i.i, 112
  %cmp.i.i.i = icmp eq i32 %0, 32
  tail call void @llvm.assume(i1 %cmp.i.i.i)
  %add.ptr.i.i.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i.i.i = tail call fastcc i32 @do_ld4_mmu(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef %or.i.i.i, i64 noundef 0)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef %or.i.i.i, i32 noundef 1) #16
  ret i32 %call2.i.i.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_ldq_be_data(ptr noundef %env, i64 noundef %addr) local_unnamed_addr #2 {
entry:
  %call.i = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i.i = or i32 %call.i, 304
  %0 = and i32 %or.i.i.i, 112
  %cmp.i.i.i = icmp eq i32 %0, 48
  tail call void @llvm.assume(i1 %cmp.i.i.i)
  %add.ptr.i.i.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i.i.i = tail call fastcc i64 @do_ld8_mmu(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef %or.i.i.i, i64 noundef 0)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef %or.i.i.i, i32 noundef 1) #16
  ret i64 %call2.i.i.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_lduw_le_data(ptr noundef %env, i64 noundef %addr) local_unnamed_addr #2 {
entry:
  %call.i = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i.i = or i32 %call.i, 16
  %0 = and i32 %or.i.i.i, 112
  %cmp.i.i.i = icmp eq i32 %0, 16
  tail call void @llvm.assume(i1 %cmp.i.i.i)
  %add.ptr.i.i.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i.i.i = tail call fastcc zeroext i16 @do_ld2_mmu(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef %or.i.i.i, i64 noundef 0)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef %or.i.i.i, i32 noundef 1) #16
  %conv.i.i = zext i16 %call2.i.i.i to i32
  ret i32 %conv.i.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_ldsw_le_data(ptr noundef %env, i64 noundef %addr) local_unnamed_addr #2 {
entry:
  %call.i.i = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i.i.i = or i32 %call.i.i, 16
  %0 = and i32 %or.i.i.i.i, 112
  %cmp.i.i.i.i = icmp eq i32 %0, 16
  tail call void @llvm.assume(i1 %cmp.i.i.i.i)
  %add.ptr.i.i.i.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i.i.i.i = tail call fastcc zeroext i16 @do_ld2_mmu(ptr noundef %add.ptr.i.i.i.i.i, i64 noundef %addr, i32 noundef %or.i.i.i.i, i64 noundef 0)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i.i.i, i64 noundef %addr, i32 noundef %or.i.i.i.i, i32 noundef 1) #16
  %conv1 = sext i16 %call2.i.i.i.i to i32
  ret i32 %conv1
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_ldl_le_data(ptr noundef %env, i64 noundef %addr) local_unnamed_addr #2 {
entry:
  %call.i = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i.i = or i32 %call.i, 32
  %0 = and i32 %or.i.i.i, 112
  %cmp.i.i.i = icmp eq i32 %0, 32
  tail call void @llvm.assume(i1 %cmp.i.i.i)
  %add.ptr.i.i.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i.i.i = tail call fastcc i32 @do_ld4_mmu(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef %or.i.i.i, i64 noundef 0)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef %or.i.i.i, i32 noundef 1) #16
  ret i32 %call2.i.i.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_ldq_le_data(ptr noundef %env, i64 noundef %addr) local_unnamed_addr #2 {
entry:
  %call.i = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i.i = or i32 %call.i, 48
  %0 = and i32 %or.i.i.i, 112
  %cmp.i.i.i = icmp eq i32 %0, 48
  tail call void @llvm.assume(i1 %cmp.i.i.i)
  %add.ptr.i.i.i.i = getelementptr i8, ptr %env, i64 -10176
  %call2.i.i.i = tail call fastcc i64 @do_ld8_mmu(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef %or.i.i.i, i64 noundef 0)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef %or.i.i.i, i32 noundef 1) #16
  ret i64 %call2.i.i.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @cpu_stb_data(ptr noundef %env, i64 noundef %addr, i32 noundef %val) local_unnamed_addr #2 {
entry:
  %call.i = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %shr.i.i.i.i.i = lshr i32 %call.i, 4
  %0 = and i32 %call.i, 112
  %cmp.i.i.i.i = icmp eq i32 %0, 0
  tail call void @llvm.assume(i1 %cmp.i.i.i.i)
  %and.i.i.i.i.i.i.i = and i32 %shr.i.i.i.i.i, 224
  %trunc.i.i.i.i.i.i.i = trunc i32 %and.i.i.i.i.i.i.i to i8
  switch i8 %trunc.i.i.i.i.i.i.i, label %if.else4.i.i.i.i.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i.i.i.i.i
    i8 -32, label %get_alignment_bits.exit.i.i.i.i.i.i
  ]

if.else4.i.i.i.i.i.i.i:                           ; preds = %entry
  %shr.i.i.i.i.i.i.i = lshr exact i32 %and.i.i.i.i.i.i.i, 5
  br label %get_alignment_bits.exit.i.i.i.i.i.i

get_alignment_bits.exit.i.i.i.i.i.i:              ; preds = %if.else4.i.i.i.i.i.i.i, %entry, %entry
  %a.0.i.i.i.i.i.i.i = phi i32 [ %shr.i.i.i.i.i.i.i, %if.else4.i.i.i.i.i.i.i ], [ 0, %entry ], [ 0, %entry ]
  %notmask.i.i.i.i.i.i = shl nsw i32 -1, %a.0.i.i.i.i.i.i.i
  %sub.i.i.i.i.i.i = xor i32 %notmask.i.i.i.i.i.i, -1
  %conv.i.i.i.i.i.i = zext nneg i32 %sub.i.i.i.i.i.i to i64
  %and.i.i.i.i.i.i = and i64 %conv.i.i.i.i.i.i, %addr
  %tobool.not.i.i.i.i.i.i = icmp eq i64 %and.i.i.i.i.i.i, 0
  br i1 %tobool.not.i.i.i.i.i.i, label %cpu_stb_data_ra.exit, label %if.then.i.i.i.i.i.i

if.then.i.i.i.i.i.i:                              ; preds = %get_alignment_bits.exit.i.i.i.i.i.i
  %add.ptr.i.i.i.i.i = getelementptr i8, ptr %env, i64 -10176
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i.i.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef 0) #17
  unreachable

cpu_stb_data_ra.exit:                             ; preds = %get_alignment_bits.exit.i.i.i.i.i.i
  %conv.i.i = trunc i32 %val to i8
  %1 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i.i.i.i.i = add i64 %1, %addr
  %2 = inttoptr i64 %add.i.i.i.i.i.i.i.i to ptr
  %3 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 0, ptr %3, align 8
  fence syncscope("singlethread") seq_cst
  store i8 %conv.i.i, ptr %2, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %3, align 8
  %add.ptr.i.i4.i.i.i = getelementptr i8, ptr %env, i64 -10176
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i4.i.i.i, i64 noundef %addr, i32 noundef %call.i, i32 noundef 2) #16
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @cpu_stw_be_data(ptr noundef %env, i64 noundef %addr, i32 noundef %val) local_unnamed_addr #2 {
entry:
  %call.i = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i.i = or i32 %call.i, 272
  %conv.i.i = trunc i32 %val to i16
  %0 = and i32 %or.i.i.i, 112
  %cmp.i.i.i = icmp eq i32 %0, 16
  tail call void @llvm.assume(i1 %cmp.i.i.i)
  %add.ptr.i.i.i.i = getelementptr i8, ptr %env, i64 -10176
  tail call fastcc void @do_st2_mmu(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i16 noundef zeroext %conv.i.i, i32 noundef %or.i.i.i, i64 noundef 0)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef %or.i.i.i, i32 noundef 2) #16
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @cpu_stl_be_data(ptr noundef %env, i64 noundef %addr, i32 noundef %val) local_unnamed_addr #2 {
entry:
  %call.i = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i.i = or i32 %call.i, 288
  %0 = and i32 %or.i.i.i, 112
  %cmp.i.i.i = icmp eq i32 %0, 32
  tail call void @llvm.assume(i1 %cmp.i.i.i)
  %add.ptr.i.i.i.i = getelementptr i8, ptr %env, i64 -10176
  tail call fastcc void @do_st4_mmu(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef %val, i32 noundef %or.i.i.i, i64 noundef 0)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef %or.i.i.i, i32 noundef 2) #16
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @cpu_stq_be_data(ptr noundef %env, i64 noundef %addr, i64 noundef %val) local_unnamed_addr #2 {
entry:
  %call.i = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i.i = or i32 %call.i, 304
  %0 = and i32 %or.i.i.i, 112
  %cmp.i.i.i = icmp eq i32 %0, 48
  tail call void @llvm.assume(i1 %cmp.i.i.i)
  %add.ptr.i.i.i.i = getelementptr i8, ptr %env, i64 -10176
  tail call fastcc void @do_st8_mmu(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i64 noundef %val, i32 noundef %or.i.i.i, i64 noundef 0)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef %or.i.i.i, i32 noundef 2) #16
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @cpu_stw_le_data(ptr noundef %env, i64 noundef %addr, i32 noundef %val) local_unnamed_addr #2 {
entry:
  %call.i = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i.i = or i32 %call.i, 16
  %conv.i.i = trunc i32 %val to i16
  %0 = and i32 %or.i.i.i, 112
  %cmp.i.i.i = icmp eq i32 %0, 16
  tail call void @llvm.assume(i1 %cmp.i.i.i)
  %add.ptr.i.i.i.i = getelementptr i8, ptr %env, i64 -10176
  tail call fastcc void @do_st2_mmu(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i16 noundef zeroext %conv.i.i, i32 noundef %or.i.i.i, i64 noundef 0)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef %or.i.i.i, i32 noundef 2) #16
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @cpu_stl_le_data(ptr noundef %env, i64 noundef %addr, i32 noundef %val) local_unnamed_addr #2 {
entry:
  %call.i = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i.i = or i32 %call.i, 32
  %0 = and i32 %or.i.i.i, 112
  %cmp.i.i.i = icmp eq i32 %0, 32
  tail call void @llvm.assume(i1 %cmp.i.i.i)
  %add.ptr.i.i.i.i = getelementptr i8, ptr %env, i64 -10176
  tail call fastcc void @do_st4_mmu(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef %val, i32 noundef %or.i.i.i, i64 noundef 0)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef %or.i.i.i, i32 noundef 2) #16
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @cpu_stq_le_data(ptr noundef %env, i64 noundef %addr, i64 noundef %val) local_unnamed_addr #2 {
entry:
  %call.i = tail call i32 @riscv_cpu_mmu_index(ptr noundef %env, i1 noundef zeroext false) #16
  %or.i.i.i = or i32 %call.i, 48
  %0 = and i32 %or.i.i.i, 112
  %cmp.i.i.i = icmp eq i32 %0, 48
  tail call void @llvm.assume(i1 %cmp.i.i.i)
  %add.ptr.i.i.i.i = getelementptr i8, ptr %env, i64 -10176
  tail call fastcc void @do_st8_mmu(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i64 noundef %val, i32 noundef %or.i.i.i, i64 noundef 0)
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i.i.i, i64 noundef %addr, i32 noundef %or.i.i.i, i32 noundef 2) #16
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_cmpxchgb(ptr noundef %env, i64 noundef %addr, i32 noundef %oldv, i32 noundef %newv, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %cpu_atomic_cmpxchgb_mmu.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

cpu_atomic_cmpxchgb_mmu.exit:                     ; preds = %get_alignment_bits.exit.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %oldv to i8
  %conv2.i = trunc i32 %newv to i8
  %5 = cmpxchg ptr %3, i8 %conv.i, i8 %conv2.i seq_cst seq_cst, align 1
  %6 = extractvalue { i8, i1 } %5, 0
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv3.i = zext i8 %6 to i32
  ret i32 %conv3.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_cmpxchgb_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %cmpv, i32 noundef %newv, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %atomic_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %cmpv to i8
  %conv2 = trunc i32 %newv to i8
  %3 = cmpxchg ptr %1, i8 %conv, i8 %conv2 seq_cst seq_cst, align 1
  %4 = extractvalue { i8, i1 } %3, 0
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv3 = zext i8 %4 to i32
  ret i32 %conv3
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_cmpxchgw_be(ptr noundef %env, i64 noundef %addr, i32 noundef %oldv, i32 noundef %newv, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 1
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_cmpxchgw_be_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_cmpxchgw_be_mmu.exit:                  ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %oldv to i16
  %5 = tail call i16 @llvm.bswap.i16(i16 %conv.i)
  %conv2.i = trunc i32 %newv to i16
  %6 = tail call i16 @llvm.bswap.i16(i16 %conv2.i)
  %7 = cmpxchg ptr %3, i16 %5, i16 %6 seq_cst seq_cst, align 2
  %8 = extractvalue { i16, i1 } %7, 0
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %9 = tail call i16 @llvm.bswap.i16(i16 %8)
  %conv3.i = zext i16 %9 to i32
  ret i32 %conv3.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_cmpxchgw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %cmpv, i32 noundef %newv, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %cmpv to i16
  %3 = tail call i16 @llvm.bswap.i16(i16 %conv)
  %conv2 = trunc i32 %newv to i16
  %4 = tail call i16 @llvm.bswap.i16(i16 %conv2)
  %5 = cmpxchg ptr %1, i16 %3, i16 %4 seq_cst seq_cst, align 2
  %6 = extractvalue { i16, i1 } %5, 0
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %7 = tail call i16 @llvm.bswap.i16(i16 %6)
  %conv3 = zext i16 %7 to i32
  ret i32 %conv3
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_cmpxchgw_le(ptr noundef %env, i64 noundef %addr, i32 noundef %oldv, i32 noundef %newv, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 1
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_cmpxchgw_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_cmpxchgw_le_mmu.exit:                  ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %oldv to i16
  %conv2.i = trunc i32 %newv to i16
  %5 = cmpxchg ptr %3, i16 %conv.i, i16 %conv2.i seq_cst seq_cst, align 2
  %6 = extractvalue { i16, i1 } %5, 0
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv3.i = zext i16 %6 to i32
  ret i32 %conv3.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_cmpxchgw_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %cmpv, i32 noundef %newv, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %cmpv to i16
  %conv2 = trunc i32 %newv to i16
  %3 = cmpxchg ptr %1, i16 %conv, i16 %conv2 seq_cst seq_cst, align 2
  %4 = extractvalue { i16, i1 } %3, 0
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv3 = zext i16 %4 to i32
  ret i32 %conv3
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_cmpxchgl_be(ptr noundef %env, i64 noundef %addr, i32 noundef %oldv, i32 noundef %newv, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_cmpxchgl_be_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_cmpxchgl_be_mmu.exit:                  ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = tail call i32 @llvm.bswap.i32(i32 %oldv)
  %6 = tail call i32 @llvm.bswap.i32(i32 %newv)
  %7 = cmpxchg ptr %3, i32 %5, i32 %6 seq_cst seq_cst, align 4
  %8 = extractvalue { i32, i1 } %7, 0
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %9 = tail call i32 @llvm.bswap.i32(i32 %8)
  ret i32 %9
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_cmpxchgl_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %cmpv, i32 noundef %newv, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = tail call i32 @llvm.bswap.i32(i32 %cmpv)
  %4 = tail call i32 @llvm.bswap.i32(i32 %newv)
  %5 = cmpxchg ptr %1, i32 %3, i32 %4 seq_cst seq_cst, align 4
  %6 = extractvalue { i32, i1 } %5, 0
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %7 = tail call i32 @llvm.bswap.i32(i32 %6)
  ret i32 %7
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_cmpxchgl_le(ptr noundef %env, i64 noundef %addr, i32 noundef %oldv, i32 noundef %newv, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_cmpxchgl_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_cmpxchgl_le_mmu.exit:                  ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = cmpxchg ptr %3, i32 %oldv, i32 %newv seq_cst seq_cst, align 4
  %6 = extractvalue { i32, i1 } %5, 0
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %6
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_cmpxchgl_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %cmpv, i32 noundef %newv, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = cmpxchg ptr %1, i32 %cmpv, i32 %newv seq_cst seq_cst, align 4
  %4 = extractvalue { i32, i1 } %3, 0
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %4
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_cmpxchgq_be(ptr noundef %env, i64 noundef %addr, i64 noundef %oldv, i64 noundef %newv, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_cmpxchgq_be_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_cmpxchgq_be_mmu.exit:                  ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = tail call i64 @llvm.bswap.i64(i64 %oldv)
  %6 = tail call i64 @llvm.bswap.i64(i64 %newv)
  %7 = cmpxchg ptr %3, i64 %5, i64 %6 seq_cst seq_cst, align 8
  %8 = extractvalue { i64, i1 } %7, 0
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %9 = tail call i64 @llvm.bswap.i64(i64 %8)
  ret i64 %9
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_cmpxchgq_be_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %cmpv, i64 noundef %newv, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = tail call i64 @llvm.bswap.i64(i64 %cmpv)
  %4 = tail call i64 @llvm.bswap.i64(i64 %newv)
  %5 = cmpxchg ptr %1, i64 %3, i64 %4 seq_cst seq_cst, align 8
  %6 = extractvalue { i64, i1 } %5, 0
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %7 = tail call i64 @llvm.bswap.i64(i64 %6)
  ret i64 %7
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_cmpxchgq_le(ptr noundef %env, i64 noundef %addr, i64 noundef %oldv, i64 noundef %newv, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_cmpxchgq_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_cmpxchgq_le_mmu.exit:                  ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = cmpxchg ptr %3, i64 %oldv, i64 %newv seq_cst seq_cst, align 8
  %6 = extractvalue { i64, i1 } %5, 0
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %6
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_cmpxchgq_le_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %cmpv, i64 noundef %newv, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = cmpxchg ptr %1, i64 %cmpv, i64 %newv seq_cst seq_cst, align 8
  %4 = extractvalue { i64, i1 } %3, 0
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %4
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local { i64, i64 } @helper_atomic_cmpxchgo_be(ptr noundef %env, i64 noundef %addr, i64 noundef %oldv.coerce0, i64 noundef %oldv.coerce1, i64 noundef %newv.coerce0, i64 noundef %newv.coerce1, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %call = tail call { i64, i64 } @cpu_atomic_cmpxchgo_be_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %oldv.coerce0, i64 noundef %oldv.coerce1, i64 noundef %newv.coerce0, i64 noundef %newv.coerce1, i32 noundef %oi, i64 noundef %1)
  ret { i64, i64 } %call
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local { i64, i64 } @cpu_atomic_cmpxchgo_be_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %cmpv.coerce0, i64 noundef %cmpv.coerce1, i64 noundef %newv.coerce0, i64 noundef %newv.coerce1, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 15
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = tail call i64 @llvm.bswap.i64(i64 %cmpv.coerce1)
  %4 = tail call i64 @llvm.bswap.i64(i64 %cmpv.coerce0)
  %5 = zext i64 %cmpv.coerce0 to i128
  %6 = zext i64 %cmpv.coerce1 to i128
  %7 = shl nuw i128 %6, 64
  %8 = or disjoint i128 %7, %5
  %cmp.sroa.0.0.insert.insert.i = tail call i128 @llvm.bswap.i128(i128 %8)
  %9 = zext i64 %newv.coerce0 to i128
  %10 = zext i64 %newv.coerce1 to i128
  %11 = shl nuw i128 %10, 64
  %12 = or disjoint i128 %11, %9
  %new.sroa.0.0.insert.insert.i = tail call i128 @llvm.bswap.i128(i128 %12)
  call void @llvm.assume(i1 true) [ "align"(ptr %1, i64 16) ]
  %13 = cmpxchg ptr %1, i128 %cmp.sroa.0.0.insert.insert.i, i128 %new.sroa.0.0.insert.insert.i seq_cst seq_cst, align 16
  %14 = extractvalue { i128, i1 } %13, 1
  %15 = extractvalue { i128, i1 } %13, 0
  %extract.t2.i = trunc i128 %15 to i64
  %extract4.i = lshr i128 %15, 64
  %extract.t5.i = trunc i128 %extract4.i to i64
  %_old.0.off0.i = select i1 %14, i64 %3, i64 %extract.t2.i
  %_old.0.off64.i = select i1 %14, i64 %4, i64 %extract.t5.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %16 = tail call i64 @llvm.bswap.i64(i64 %_old.0.off64.i)
  %17 = tail call i64 @llvm.bswap.i64(i64 %_old.0.off0.i)
  %.fca.0.insert.i.i6 = insertvalue { i64, i64 } poison, i64 %16, 0
  %.fca.1.insert.i.i7 = insertvalue { i64, i64 } %.fca.0.insert.i.i6, i64 %17, 1
  ret { i64, i64 } %.fca.1.insert.i.i7
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local { i64, i64 } @helper_atomic_cmpxchgo_le(ptr noundef %env, i64 noundef %addr, i64 noundef %oldv.coerce0, i64 noundef %oldv.coerce1, i64 noundef %newv.coerce0, i64 noundef %newv.coerce1, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %call = tail call { i64, i64 } @cpu_atomic_cmpxchgo_le_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %oldv.coerce0, i64 noundef %oldv.coerce1, i64 noundef %newv.coerce0, i64 noundef %newv.coerce1, i32 noundef %oi, i64 noundef %1)
  ret { i64, i64 } %call
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local { i64, i64 } @cpu_atomic_cmpxchgo_le_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %cmpv.coerce0, i64 noundef %cmpv.coerce1, i64 noundef %newv.coerce0, i64 noundef %newv.coerce1, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 15
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %cmp.sroa.2.0.insert.ext.i = zext i64 %cmpv.coerce1 to i128
  %cmp.sroa.2.0.insert.shift.i = shl nuw i128 %cmp.sroa.2.0.insert.ext.i, 64
  %cmp.sroa.0.0.insert.ext.i = zext i64 %cmpv.coerce0 to i128
  %cmp.sroa.0.0.insert.insert.i = or disjoint i128 %cmp.sroa.2.0.insert.shift.i, %cmp.sroa.0.0.insert.ext.i
  %new.sroa.2.0.insert.ext.i = zext i64 %newv.coerce1 to i128
  %new.sroa.2.0.insert.shift.i = shl nuw i128 %new.sroa.2.0.insert.ext.i, 64
  %new.sroa.0.0.insert.ext.i = zext i64 %newv.coerce0 to i128
  %new.sroa.0.0.insert.insert.i = or disjoint i128 %new.sroa.2.0.insert.shift.i, %new.sroa.0.0.insert.ext.i
  call void @llvm.assume(i1 true) [ "align"(ptr %1, i64 16) ]
  %3 = cmpxchg ptr %1, i128 %cmp.sroa.0.0.insert.insert.i, i128 %new.sroa.0.0.insert.insert.i seq_cst seq_cst, align 16
  %4 = extractvalue { i128, i1 } %3, 1
  %5 = extractvalue { i128, i1 } %3, 0
  %extract.t2.i = trunc i128 %5 to i64
  %extract4.i = lshr i128 %5, 64
  %extract.t5.i = trunc i128 %extract4.i to i64
  %_old.0.off0.i = select i1 %4, i64 %cmpv.coerce0, i64 %extract.t2.i
  %_old.0.off64.i = select i1 %4, i64 %cmpv.coerce1, i64 %extract.t5.i
  %.fca.0.insert.i = insertvalue { i64, i64 } poison, i64 %_old.0.off0.i, 0
  %.fca.1.insert.i = insertvalue { i64, i64 } %.fca.0.insert.i, i64 %_old.0.off64.i, 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret { i64, i64 } %.fca.1.insert.i
}

; Function Attrs: noreturn nounwind sspstrong uwtable
define dso_local { i64, i64 } @helper_nonatomic_cmpxchgo(ptr nocapture noundef readnone %env, i64 noundef %addr, i64 noundef %cmpv.coerce0, i64 noundef %cmpv.coerce1, i64 noundef %newv.coerce0, i64 noundef %newv.coerce1, i32 noundef %oi) local_unnamed_addr #12 {
entry:
  tail call void @g_assertion_message_expr(ptr noundef null, ptr noundef nonnull @.str.18, i32 noundef 67, ptr noundef nonnull @__func__.helper_nonatomic_cmpxchgo, ptr noundef null) #17
  unreachable
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_addb(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %cpu_atomic_fetch_addb_mmu.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

cpu_atomic_fetch_addb_mmu.exit:                   ; preds = %get_alignment_bits.exit.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %val to i8
  %5 = atomicrmw add ptr %3, i8 %conv.i seq_cst, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2.i = zext i8 %5 to i32
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_addb_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %atomic_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %val to i8
  %3 = atomicrmw add ptr %1, i8 %conv seq_cst, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2 = zext i8 %3 to i32
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_addw_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %call = tail call i32 @cpu_atomic_fetch_addw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %1), !range !22
  ret i32 %call
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_addw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !23
  fence seq_cst
  %3 = load atomic i16, ptr %1 monotonic, align 2
  %4 = trunc i32 %xval to i16
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i16 [ %3, %atomic_mmu_lookup.exit ], [ %8, %do.body ]
  %5 = tail call i16 @llvm.bswap.i16(i16 %ldn.0)
  %conv4 = add i16 %5, %4
  %6 = tail call i16 @llvm.bswap.i16(i16 %conv4)
  %7 = cmpxchg ptr %1, i16 %ldn.0, i16 %6 seq_cst seq_cst, align 2
  %8 = extractvalue { i16, i1 } %7, 0
  %cmp.not = icmp eq i16 %ldn.0, %8
  br i1 %cmp.not, label %do.body8, label %do.body, !llvm.loop !24

do.body8:                                         ; preds = %do.body
  %conv2 = zext i16 %5 to i32
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_addw_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 1
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_fetch_addw_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_fetch_addw_le_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %val to i16
  %5 = atomicrmw add ptr %3, i16 %conv.i seq_cst, align 2
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2.i = zext i16 %5 to i32
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_addw_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %val to i16
  %3 = atomicrmw add ptr %1, i16 %conv seq_cst, align 2
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2 = zext i16 %3 to i32
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_addl_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !25
  fence seq_cst
  %5 = load atomic i32, ptr %3 monotonic, align 4
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %ldn.0.i = phi i32 [ %5, %atomic_mmu_lookup.exit.i ], [ %9, %do.body.i ]
  %6 = tail call i32 @llvm.bswap.i32(i32 %ldn.0.i)
  %add.i = add i32 %6, %val
  %7 = tail call i32 @llvm.bswap.i32(i32 %add.i)
  %8 = cmpxchg ptr %3, i32 %ldn.0.i, i32 %7 seq_cst seq_cst, align 4
  %9 = extractvalue { i32, i1 } %8, 0
  %cmp.not.i = icmp eq i32 %ldn.0.i, %9
  br i1 %cmp.not.i, label %cpu_atomic_fetch_addl_be_mmu.exit, label %do.body.i, !llvm.loop !26

cpu_atomic_fetch_addl_be_mmu.exit:                ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %6
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_addl_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !25
  fence seq_cst
  %3 = load atomic i32, ptr %1 monotonic, align 4
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i32 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i32 @llvm.bswap.i32(i32 %ldn.0)
  %add = add i32 %4, %xval
  %5 = tail call i32 @llvm.bswap.i32(i32 %add)
  %6 = cmpxchg ptr %1, i32 %ldn.0, i32 %5 seq_cst seq_cst, align 4
  %7 = extractvalue { i32, i1 } %6, 0
  %cmp.not = icmp eq i32 %ldn.0, %7
  br i1 %cmp.not, label %do.body2, label %do.body, !llvm.loop !26

do.body2:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %4
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_addl_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_fetch_addl_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_fetch_addl_le_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = atomicrmw add ptr %3, i32 %val seq_cst, align 4
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %5
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_addl_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = atomicrmw add ptr %1, i32 %val seq_cst, align 4
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %3
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_fetch_addq_be(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !27
  fence seq_cst
  %5 = load atomic i64, ptr %3 monotonic, align 8
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %ldn.0.i = phi i64 [ %5, %atomic_mmu_lookup.exit.i ], [ %9, %do.body.i ]
  %6 = tail call i64 @llvm.bswap.i64(i64 %ldn.0.i)
  %add.i = add i64 %6, %val
  %7 = tail call i64 @llvm.bswap.i64(i64 %add.i)
  %8 = cmpxchg ptr %3, i64 %ldn.0.i, i64 %7 seq_cst seq_cst, align 8
  %9 = extractvalue { i64, i1 } %8, 0
  %cmp.not.i = icmp eq i64 %ldn.0.i, %9
  br i1 %cmp.not.i, label %cpu_atomic_fetch_addq_be_mmu.exit, label %do.body.i, !llvm.loop !28

cpu_atomic_fetch_addq_be_mmu.exit:                ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %6
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_fetch_addq_be_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !27
  fence seq_cst
  %3 = load atomic i64, ptr %1 monotonic, align 8
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i64 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i64 @llvm.bswap.i64(i64 %ldn.0)
  %add = add i64 %4, %xval
  %5 = tail call i64 @llvm.bswap.i64(i64 %add)
  %6 = cmpxchg ptr %1, i64 %ldn.0, i64 %5 seq_cst seq_cst, align 8
  %7 = extractvalue { i64, i1 } %6, 0
  %cmp.not = icmp eq i64 %ldn.0, %7
  br i1 %cmp.not, label %do.body2, label %do.body, !llvm.loop !28

do.body2:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %4
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_fetch_addq_le(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_fetch_addq_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_fetch_addq_le_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = atomicrmw add ptr %3, i64 %val seq_cst, align 8
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %5
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_fetch_addq_le_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = atomicrmw add ptr %1, i64 %val seq_cst, align 8
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %3
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_andb(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %cpu_atomic_fetch_andb_mmu.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

cpu_atomic_fetch_andb_mmu.exit:                   ; preds = %get_alignment_bits.exit.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %val to i8
  %5 = atomicrmw and ptr %3, i8 %conv.i seq_cst, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2.i = zext i8 %5 to i32
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_andb_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %atomic_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %val to i8
  %3 = atomicrmw and ptr %1, i8 %conv seq_cst, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2 = zext i8 %3 to i32
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_andw_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 1
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_fetch_andw_be_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_fetch_andw_be_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %val to i16
  %5 = tail call i16 @llvm.bswap.i16(i16 %conv.i)
  %6 = atomicrmw and ptr %3, i16 %5 seq_cst, align 2
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %7 = tail call i16 @llvm.bswap.i16(i16 %6)
  %conv2.i = zext i16 %7 to i32
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_andw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %val to i16
  %3 = tail call i16 @llvm.bswap.i16(i16 %conv)
  %4 = atomicrmw and ptr %1, i16 %3 seq_cst, align 2
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %5 = tail call i16 @llvm.bswap.i16(i16 %4)
  %conv2 = zext i16 %5 to i32
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_andw_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 1
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_fetch_andw_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_fetch_andw_le_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %val to i16
  %5 = atomicrmw and ptr %3, i16 %conv.i seq_cst, align 2
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2.i = zext i16 %5 to i32
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_andw_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %val to i16
  %3 = atomicrmw and ptr %1, i16 %conv seq_cst, align 2
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2 = zext i16 %3 to i32
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_andl_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_fetch_andl_be_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_fetch_andl_be_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = tail call i32 @llvm.bswap.i32(i32 %val)
  %6 = atomicrmw and ptr %3, i32 %5 seq_cst, align 4
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %7 = tail call i32 @llvm.bswap.i32(i32 %6)
  ret i32 %7
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_andl_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = tail call i32 @llvm.bswap.i32(i32 %val)
  %4 = atomicrmw and ptr %1, i32 %3 seq_cst, align 4
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %5 = tail call i32 @llvm.bswap.i32(i32 %4)
  ret i32 %5
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_andl_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_fetch_andl_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_fetch_andl_le_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = atomicrmw and ptr %3, i32 %val seq_cst, align 4
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %5
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_andl_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = atomicrmw and ptr %1, i32 %val seq_cst, align 4
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %3
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_fetch_andq_be(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_fetch_andq_be_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_fetch_andq_be_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = tail call i64 @llvm.bswap.i64(i64 %val)
  %6 = atomicrmw and ptr %3, i64 %5 seq_cst, align 8
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %7 = tail call i64 @llvm.bswap.i64(i64 %6)
  ret i64 %7
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_fetch_andq_be_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = tail call i64 @llvm.bswap.i64(i64 %val)
  %4 = atomicrmw and ptr %1, i64 %3 seq_cst, align 8
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %5 = tail call i64 @llvm.bswap.i64(i64 %4)
  ret i64 %5
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_fetch_andq_le(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_fetch_andq_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_fetch_andq_le_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = atomicrmw and ptr %3, i64 %val seq_cst, align 8
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %5
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_fetch_andq_le_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = atomicrmw and ptr %1, i64 %val seq_cst, align 8
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %3
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_orb(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %cpu_atomic_fetch_orb_mmu.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

cpu_atomic_fetch_orb_mmu.exit:                    ; preds = %get_alignment_bits.exit.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %val to i8
  %5 = atomicrmw or ptr %3, i8 %conv.i seq_cst, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2.i = zext i8 %5 to i32
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_orb_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %atomic_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %val to i8
  %3 = atomicrmw or ptr %1, i8 %conv seq_cst, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2 = zext i8 %3 to i32
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_orw_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 1
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_fetch_orw_be_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_fetch_orw_be_mmu.exit:                 ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %val to i16
  %5 = tail call i16 @llvm.bswap.i16(i16 %conv.i)
  %6 = atomicrmw or ptr %3, i16 %5 seq_cst, align 2
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %7 = tail call i16 @llvm.bswap.i16(i16 %6)
  %conv2.i = zext i16 %7 to i32
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_orw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %val to i16
  %3 = tail call i16 @llvm.bswap.i16(i16 %conv)
  %4 = atomicrmw or ptr %1, i16 %3 seq_cst, align 2
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %5 = tail call i16 @llvm.bswap.i16(i16 %4)
  %conv2 = zext i16 %5 to i32
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_orw_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 1
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_fetch_orw_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_fetch_orw_le_mmu.exit:                 ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %val to i16
  %5 = atomicrmw or ptr %3, i16 %conv.i seq_cst, align 2
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2.i = zext i16 %5 to i32
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_orw_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %val to i16
  %3 = atomicrmw or ptr %1, i16 %conv seq_cst, align 2
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2 = zext i16 %3 to i32
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_orl_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_fetch_orl_be_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_fetch_orl_be_mmu.exit:                 ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = tail call i32 @llvm.bswap.i32(i32 %val)
  %6 = atomicrmw or ptr %3, i32 %5 seq_cst, align 4
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %7 = tail call i32 @llvm.bswap.i32(i32 %6)
  ret i32 %7
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_orl_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = tail call i32 @llvm.bswap.i32(i32 %val)
  %4 = atomicrmw or ptr %1, i32 %3 seq_cst, align 4
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %5 = tail call i32 @llvm.bswap.i32(i32 %4)
  ret i32 %5
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_orl_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_fetch_orl_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_fetch_orl_le_mmu.exit:                 ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = atomicrmw or ptr %3, i32 %val seq_cst, align 4
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %5
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_orl_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = atomicrmw or ptr %1, i32 %val seq_cst, align 4
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %3
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_fetch_orq_be(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_fetch_orq_be_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_fetch_orq_be_mmu.exit:                 ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = tail call i64 @llvm.bswap.i64(i64 %val)
  %6 = atomicrmw or ptr %3, i64 %5 seq_cst, align 8
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %7 = tail call i64 @llvm.bswap.i64(i64 %6)
  ret i64 %7
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_fetch_orq_be_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = tail call i64 @llvm.bswap.i64(i64 %val)
  %4 = atomicrmw or ptr %1, i64 %3 seq_cst, align 8
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %5 = tail call i64 @llvm.bswap.i64(i64 %4)
  ret i64 %5
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_fetch_orq_le(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_fetch_orq_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_fetch_orq_le_mmu.exit:                 ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = atomicrmw or ptr %3, i64 %val seq_cst, align 8
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %5
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_fetch_orq_le_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = atomicrmw or ptr %1, i64 %val seq_cst, align 8
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %3
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_xorb(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %cpu_atomic_fetch_xorb_mmu.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

cpu_atomic_fetch_xorb_mmu.exit:                   ; preds = %get_alignment_bits.exit.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %val to i8
  %5 = atomicrmw xor ptr %3, i8 %conv.i seq_cst, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2.i = zext i8 %5 to i32
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_xorb_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %atomic_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %val to i8
  %3 = atomicrmw xor ptr %1, i8 %conv seq_cst, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2 = zext i8 %3 to i32
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_xorw_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 1
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_fetch_xorw_be_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_fetch_xorw_be_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %val to i16
  %5 = tail call i16 @llvm.bswap.i16(i16 %conv.i)
  %6 = atomicrmw xor ptr %3, i16 %5 seq_cst, align 2
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %7 = tail call i16 @llvm.bswap.i16(i16 %6)
  %conv2.i = zext i16 %7 to i32
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_xorw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %val to i16
  %3 = tail call i16 @llvm.bswap.i16(i16 %conv)
  %4 = atomicrmw xor ptr %1, i16 %3 seq_cst, align 2
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %5 = tail call i16 @llvm.bswap.i16(i16 %4)
  %conv2 = zext i16 %5 to i32
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_xorw_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 1
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_fetch_xorw_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_fetch_xorw_le_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %val to i16
  %5 = atomicrmw xor ptr %3, i16 %conv.i seq_cst, align 2
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2.i = zext i16 %5 to i32
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_xorw_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %val to i16
  %3 = atomicrmw xor ptr %1, i16 %conv seq_cst, align 2
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2 = zext i16 %3 to i32
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_xorl_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_fetch_xorl_be_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_fetch_xorl_be_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = tail call i32 @llvm.bswap.i32(i32 %val)
  %6 = atomicrmw xor ptr %3, i32 %5 seq_cst, align 4
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %7 = tail call i32 @llvm.bswap.i32(i32 %6)
  ret i32 %7
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_xorl_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = tail call i32 @llvm.bswap.i32(i32 %val)
  %4 = atomicrmw xor ptr %1, i32 %3 seq_cst, align 4
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %5 = tail call i32 @llvm.bswap.i32(i32 %4)
  ret i32 %5
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_xorl_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_fetch_xorl_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_fetch_xorl_le_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = atomicrmw xor ptr %3, i32 %val seq_cst, align 4
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %5
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_xorl_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = atomicrmw xor ptr %1, i32 %val seq_cst, align 4
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %3
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_fetch_xorq_be(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_fetch_xorq_be_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_fetch_xorq_be_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = tail call i64 @llvm.bswap.i64(i64 %val)
  %6 = atomicrmw xor ptr %3, i64 %5 seq_cst, align 8
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %7 = tail call i64 @llvm.bswap.i64(i64 %6)
  ret i64 %7
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_fetch_xorq_be_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = tail call i64 @llvm.bswap.i64(i64 %val)
  %4 = atomicrmw xor ptr %1, i64 %3 seq_cst, align 8
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %5 = tail call i64 @llvm.bswap.i64(i64 %4)
  ret i64 %5
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_fetch_xorq_le(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_fetch_xorq_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_fetch_xorq_le_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = atomicrmw xor ptr %3, i64 %val seq_cst, align 8
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %5
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_fetch_xorq_le_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = atomicrmw xor ptr %1, i64 %val seq_cst, align 8
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %3
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_sminb(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %get_alignment_bits.exit.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !29
  fence seq_cst
  %5 = load atomic i8, ptr %3 monotonic, align 1
  %sext.i = shl i32 %val, 24
  %conv3.i = ashr exact i32 %sext.i, 24
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i8 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %conv2.i = sext i8 %cmp.0.i to i32
  %cond.i = tail call i32 @llvm.smin.i32(i32 %conv3.i, i32 %conv2.i)
  %conv6.i = trunc i32 %cond.i to i8
  %6 = cmpxchg ptr %3, i8 %cmp.0.i, i8 %conv6.i seq_cst seq_cst, align 1
  %7 = extractvalue { i8, i1 } %6, 0
  %cmp10.not.i = extractvalue { i8, i1 } %6, 1
  br i1 %cmp10.not.i, label %cpu_atomic_fetch_sminb_mmu.exit, label %do.body.i, !llvm.loop !30

cpu_atomic_fetch_sminb_mmu.exit:                  ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_sminb_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %atomic_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !29
  fence seq_cst
  %3 = load atomic i8, ptr %1 monotonic, align 1
  %sext = shl i32 %xval, 24
  %conv3 = ashr exact i32 %sext, 24
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i8 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %conv2 = sext i8 %cmp.0 to i32
  %cond = tail call i32 @llvm.smin.i32(i32 %conv3, i32 %conv2)
  %conv6 = trunc i32 %cond to i8
  %4 = cmpxchg ptr %1, i8 %cmp.0, i8 %conv6 seq_cst seq_cst, align 1
  %5 = extractvalue { i8, i1 } %4, 0
  %cmp10.not = extractvalue { i8, i1 } %4, 1
  br i1 %cmp10.not, label %do.body12, label %do.body, !llvm.loop !30

do.body12:                                        ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_sminw_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %call = tail call i32 @cpu_atomic_fetch_sminw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %1), !range !31
  ret i32 %call
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_sminw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !32
  fence seq_cst
  %3 = load atomic i16, ptr %1 monotonic, align 2
  %sext = shl i32 %xval, 16
  %conv3 = ashr exact i32 %sext, 16
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i16 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i16 @llvm.bswap.i16(i16 %ldn.0)
  %conv2 = sext i16 %4 to i32
  %cond = tail call i32 @llvm.smin.i32(i32 %conv3, i32 %conv2)
  %conv5 = trunc i32 %cond to i16
  %5 = tail call i16 @llvm.bswap.i16(i16 %conv5)
  %6 = cmpxchg ptr %1, i16 %ldn.0, i16 %5 seq_cst seq_cst, align 2
  %7 = extractvalue { i16, i1 } %6, 0
  %cmp9.not = icmp eq i16 %ldn.0, %7
  br i1 %cmp9.not, label %do.body11, label %do.body, !llvm.loop !33

do.body11:                                        ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_sminw_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 1
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !34
  fence seq_cst
  %5 = load atomic i16, ptr %3 monotonic, align 2
  %sext.i = shl i32 %val, 16
  %conv3.i = ashr exact i32 %sext.i, 16
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i16 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %conv2.i = sext i16 %cmp.0.i to i32
  %cond.i = tail call i32 @llvm.smin.i32(i32 %conv3.i, i32 %conv2.i)
  %conv6.i = trunc i32 %cond.i to i16
  %6 = cmpxchg ptr %3, i16 %cmp.0.i, i16 %conv6.i seq_cst seq_cst, align 2
  %7 = extractvalue { i16, i1 } %6, 0
  %cmp10.not.i = extractvalue { i16, i1 } %6, 1
  br i1 %cmp10.not.i, label %cpu_atomic_fetch_sminw_le_mmu.exit, label %do.body.i, !llvm.loop !35

cpu_atomic_fetch_sminw_le_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_sminw_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !34
  fence seq_cst
  %3 = load atomic i16, ptr %1 monotonic, align 2
  %sext = shl i32 %xval, 16
  %conv3 = ashr exact i32 %sext, 16
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i16 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %conv2 = sext i16 %cmp.0 to i32
  %cond = tail call i32 @llvm.smin.i32(i32 %conv3, i32 %conv2)
  %conv6 = trunc i32 %cond to i16
  %4 = cmpxchg ptr %1, i16 %cmp.0, i16 %conv6 seq_cst seq_cst, align 2
  %5 = extractvalue { i16, i1 } %4, 0
  %cmp10.not = extractvalue { i16, i1 } %4, 1
  br i1 %cmp10.not, label %do.body12, label %do.body, !llvm.loop !35

do.body12:                                        ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_sminl_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !36
  fence seq_cst
  %5 = load atomic i32, ptr %3 monotonic, align 4
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %ldn.0.i = phi i32 [ %5, %atomic_mmu_lookup.exit.i ], [ %9, %do.body.i ]
  %6 = tail call i32 @llvm.bswap.i32(i32 %ldn.0.i)
  %cond.i = tail call i32 @llvm.smin.i32(i32 %6, i32 %val)
  %7 = tail call i32 @llvm.bswap.i32(i32 %cond.i)
  %8 = cmpxchg ptr %3, i32 %ldn.0.i, i32 %7 seq_cst seq_cst, align 4
  %9 = extractvalue { i32, i1 } %8, 0
  %cmp3.not.i = icmp eq i32 %ldn.0.i, %9
  br i1 %cmp3.not.i, label %cpu_atomic_fetch_sminl_be_mmu.exit, label %do.body.i, !llvm.loop !37

cpu_atomic_fetch_sminl_be_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %6
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_sminl_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !36
  fence seq_cst
  %3 = load atomic i32, ptr %1 monotonic, align 4
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i32 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i32 @llvm.bswap.i32(i32 %ldn.0)
  %cond = tail call i32 @llvm.smin.i32(i32 %4, i32 %xval)
  %5 = tail call i32 @llvm.bswap.i32(i32 %cond)
  %6 = cmpxchg ptr %1, i32 %ldn.0, i32 %5 seq_cst seq_cst, align 4
  %7 = extractvalue { i32, i1 } %6, 0
  %cmp3.not = icmp eq i32 %ldn.0, %7
  br i1 %cmp3.not, label %do.body4, label %do.body, !llvm.loop !37

do.body4:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %4
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_sminl_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !38
  fence seq_cst
  %5 = load atomic i32, ptr %3 monotonic, align 4
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i32 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %cond.i = tail call i32 @llvm.smin.i32(i32 %cmp.0.i, i32 %val)
  %6 = cmpxchg ptr %3, i32 %cmp.0.i, i32 %cond.i seq_cst seq_cst, align 4
  %7 = extractvalue { i32, i1 } %6, 0
  %cmp4.not.i = extractvalue { i32, i1 } %6, 1
  br i1 %cmp4.not.i, label %cpu_atomic_fetch_sminl_le_mmu.exit, label %do.body.i, !llvm.loop !39

cpu_atomic_fetch_sminl_le_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cmp.0.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_sminl_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !38
  fence seq_cst
  %3 = load atomic i32, ptr %1 monotonic, align 4
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i32 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %cond = tail call i32 @llvm.smin.i32(i32 %cmp.0, i32 %xval)
  %4 = cmpxchg ptr %1, i32 %cmp.0, i32 %cond seq_cst seq_cst, align 4
  %5 = extractvalue { i32, i1 } %4, 0
  %cmp4.not = extractvalue { i32, i1 } %4, 1
  br i1 %cmp4.not, label %do.body5, label %do.body, !llvm.loop !39

do.body5:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cmp.0
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_fetch_sminq_be(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !40
  fence seq_cst
  %5 = load atomic i64, ptr %3 monotonic, align 8
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %ldn.0.i = phi i64 [ %5, %atomic_mmu_lookup.exit.i ], [ %9, %do.body.i ]
  %6 = tail call i64 @llvm.bswap.i64(i64 %ldn.0.i)
  %cond.i = tail call i64 @llvm.smin.i64(i64 %6, i64 %val)
  %7 = tail call i64 @llvm.bswap.i64(i64 %cond.i)
  %8 = cmpxchg ptr %3, i64 %ldn.0.i, i64 %7 seq_cst seq_cst, align 8
  %9 = extractvalue { i64, i1 } %8, 0
  %cmp3.not.i = icmp eq i64 %ldn.0.i, %9
  br i1 %cmp3.not.i, label %cpu_atomic_fetch_sminq_be_mmu.exit, label %do.body.i, !llvm.loop !41

cpu_atomic_fetch_sminq_be_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %6
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_fetch_sminq_be_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !40
  fence seq_cst
  %3 = load atomic i64, ptr %1 monotonic, align 8
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i64 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i64 @llvm.bswap.i64(i64 %ldn.0)
  %cond = tail call i64 @llvm.smin.i64(i64 %4, i64 %xval)
  %5 = tail call i64 @llvm.bswap.i64(i64 %cond)
  %6 = cmpxchg ptr %1, i64 %ldn.0, i64 %5 seq_cst seq_cst, align 8
  %7 = extractvalue { i64, i1 } %6, 0
  %cmp3.not = icmp eq i64 %ldn.0, %7
  br i1 %cmp3.not, label %do.body4, label %do.body, !llvm.loop !41

do.body4:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %4
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_fetch_sminq_le(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !42
  fence seq_cst
  %5 = load atomic i64, ptr %3 monotonic, align 8
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i64 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %cond.i = tail call i64 @llvm.smin.i64(i64 %cmp.0.i, i64 %val)
  %6 = cmpxchg ptr %3, i64 %cmp.0.i, i64 %cond.i seq_cst seq_cst, align 8
  %7 = extractvalue { i64, i1 } %6, 0
  %cmp4.not.i = extractvalue { i64, i1 } %6, 1
  br i1 %cmp4.not.i, label %cpu_atomic_fetch_sminq_le_mmu.exit, label %do.body.i, !llvm.loop !43

cpu_atomic_fetch_sminq_le_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %cmp.0.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_fetch_sminq_le_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !42
  fence seq_cst
  %3 = load atomic i64, ptr %1 monotonic, align 8
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i64 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %cond = tail call i64 @llvm.smin.i64(i64 %cmp.0, i64 %xval)
  %4 = cmpxchg ptr %1, i64 %cmp.0, i64 %cond seq_cst seq_cst, align 8
  %5 = extractvalue { i64, i1 } %4, 0
  %cmp4.not = extractvalue { i64, i1 } %4, 1
  br i1 %cmp4.not, label %do.body5, label %do.body, !llvm.loop !43

do.body5:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %cmp.0
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_uminb(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %get_alignment_bits.exit.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !44
  fence seq_cst
  %5 = load atomic i8, ptr %3 monotonic, align 1
  %conv3.i = and i32 %val, 255
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i8 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %conv2.i = zext i8 %cmp.0.i to i32
  %cond.i = tail call i32 @llvm.umin.i32(i32 %conv3.i, i32 %conv2.i)
  %conv6.i = trunc i32 %cond.i to i8
  %6 = cmpxchg ptr %3, i8 %cmp.0.i, i8 %conv6.i seq_cst seq_cst, align 1
  %7 = extractvalue { i8, i1 } %6, 0
  %cmp10.not.i = extractvalue { i8, i1 } %6, 1
  br i1 %cmp10.not.i, label %cpu_atomic_fetch_uminb_mmu.exit, label %do.body.i, !llvm.loop !45

cpu_atomic_fetch_uminb_mmu.exit:                  ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_uminb_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %atomic_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !44
  fence seq_cst
  %3 = load atomic i8, ptr %1 monotonic, align 1
  %conv3 = and i32 %xval, 255
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i8 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %conv2 = zext i8 %cmp.0 to i32
  %cond = tail call i32 @llvm.umin.i32(i32 %conv3, i32 %conv2)
  %conv6 = trunc i32 %cond to i8
  %4 = cmpxchg ptr %1, i8 %cmp.0, i8 %conv6 seq_cst seq_cst, align 1
  %5 = extractvalue { i8, i1 } %4, 0
  %cmp10.not = extractvalue { i8, i1 } %4, 1
  br i1 %cmp10.not, label %do.body12, label %do.body, !llvm.loop !45

do.body12:                                        ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_uminw_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %call = tail call i32 @cpu_atomic_fetch_uminw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %1), !range !22
  ret i32 %call
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_uminw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !46
  fence seq_cst
  %3 = load atomic i16, ptr %1 monotonic, align 2
  %conv3 = and i32 %xval, 65535
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i16 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i16 @llvm.bswap.i16(i16 %ldn.0)
  %conv2 = zext i16 %4 to i32
  %cond = tail call i32 @llvm.umin.i32(i32 %conv3, i32 %conv2)
  %conv5 = trunc i32 %cond to i16
  %5 = tail call i16 @llvm.bswap.i16(i16 %conv5)
  %6 = cmpxchg ptr %1, i16 %ldn.0, i16 %5 seq_cst seq_cst, align 2
  %7 = extractvalue { i16, i1 } %6, 0
  %cmp9.not = icmp eq i16 %ldn.0, %7
  br i1 %cmp9.not, label %do.body11, label %do.body, !llvm.loop !47

do.body11:                                        ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_uminw_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 1
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !48
  fence seq_cst
  %5 = load atomic i16, ptr %3 monotonic, align 2
  %conv3.i = and i32 %val, 65535
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i16 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %conv2.i = zext i16 %cmp.0.i to i32
  %cond.i = tail call i32 @llvm.umin.i32(i32 %conv3.i, i32 %conv2.i)
  %conv6.i = trunc i32 %cond.i to i16
  %6 = cmpxchg ptr %3, i16 %cmp.0.i, i16 %conv6.i seq_cst seq_cst, align 2
  %7 = extractvalue { i16, i1 } %6, 0
  %cmp10.not.i = extractvalue { i16, i1 } %6, 1
  br i1 %cmp10.not.i, label %cpu_atomic_fetch_uminw_le_mmu.exit, label %do.body.i, !llvm.loop !49

cpu_atomic_fetch_uminw_le_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_uminw_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !48
  fence seq_cst
  %3 = load atomic i16, ptr %1 monotonic, align 2
  %conv3 = and i32 %xval, 65535
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i16 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %conv2 = zext i16 %cmp.0 to i32
  %cond = tail call i32 @llvm.umin.i32(i32 %conv3, i32 %conv2)
  %conv6 = trunc i32 %cond to i16
  %4 = cmpxchg ptr %1, i16 %cmp.0, i16 %conv6 seq_cst seq_cst, align 2
  %5 = extractvalue { i16, i1 } %4, 0
  %cmp10.not = extractvalue { i16, i1 } %4, 1
  br i1 %cmp10.not, label %do.body12, label %do.body, !llvm.loop !49

do.body12:                                        ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_uminl_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !50
  fence seq_cst
  %5 = load atomic i32, ptr %3 monotonic, align 4
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %ldn.0.i = phi i32 [ %5, %atomic_mmu_lookup.exit.i ], [ %9, %do.body.i ]
  %6 = tail call i32 @llvm.bswap.i32(i32 %ldn.0.i)
  %cond.i = tail call i32 @llvm.umin.i32(i32 %6, i32 %val)
  %7 = tail call i32 @llvm.bswap.i32(i32 %cond.i)
  %8 = cmpxchg ptr %3, i32 %ldn.0.i, i32 %7 seq_cst seq_cst, align 4
  %9 = extractvalue { i32, i1 } %8, 0
  %cmp3.not.i = icmp eq i32 %ldn.0.i, %9
  br i1 %cmp3.not.i, label %cpu_atomic_fetch_uminl_be_mmu.exit, label %do.body.i, !llvm.loop !51

cpu_atomic_fetch_uminl_be_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %6
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_uminl_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !50
  fence seq_cst
  %3 = load atomic i32, ptr %1 monotonic, align 4
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i32 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i32 @llvm.bswap.i32(i32 %ldn.0)
  %cond = tail call i32 @llvm.umin.i32(i32 %4, i32 %xval)
  %5 = tail call i32 @llvm.bswap.i32(i32 %cond)
  %6 = cmpxchg ptr %1, i32 %ldn.0, i32 %5 seq_cst seq_cst, align 4
  %7 = extractvalue { i32, i1 } %6, 0
  %cmp3.not = icmp eq i32 %ldn.0, %7
  br i1 %cmp3.not, label %do.body4, label %do.body, !llvm.loop !51

do.body4:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %4
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_uminl_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !52
  fence seq_cst
  %5 = load atomic i32, ptr %3 monotonic, align 4
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i32 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %cond.i = tail call i32 @llvm.umin.i32(i32 %cmp.0.i, i32 %val)
  %6 = cmpxchg ptr %3, i32 %cmp.0.i, i32 %cond.i seq_cst seq_cst, align 4
  %7 = extractvalue { i32, i1 } %6, 0
  %cmp4.not.i = extractvalue { i32, i1 } %6, 1
  br i1 %cmp4.not.i, label %cpu_atomic_fetch_uminl_le_mmu.exit, label %do.body.i, !llvm.loop !53

cpu_atomic_fetch_uminl_le_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cmp.0.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_uminl_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !52
  fence seq_cst
  %3 = load atomic i32, ptr %1 monotonic, align 4
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i32 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %cond = tail call i32 @llvm.umin.i32(i32 %cmp.0, i32 %xval)
  %4 = cmpxchg ptr %1, i32 %cmp.0, i32 %cond seq_cst seq_cst, align 4
  %5 = extractvalue { i32, i1 } %4, 0
  %cmp4.not = extractvalue { i32, i1 } %4, 1
  br i1 %cmp4.not, label %do.body5, label %do.body, !llvm.loop !53

do.body5:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cmp.0
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_fetch_uminq_be(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !54
  fence seq_cst
  %5 = load atomic i64, ptr %3 monotonic, align 8
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %ldn.0.i = phi i64 [ %5, %atomic_mmu_lookup.exit.i ], [ %9, %do.body.i ]
  %6 = tail call i64 @llvm.bswap.i64(i64 %ldn.0.i)
  %cond.i = tail call i64 @llvm.umin.i64(i64 %6, i64 %val)
  %7 = tail call i64 @llvm.bswap.i64(i64 %cond.i)
  %8 = cmpxchg ptr %3, i64 %ldn.0.i, i64 %7 seq_cst seq_cst, align 8
  %9 = extractvalue { i64, i1 } %8, 0
  %cmp3.not.i = icmp eq i64 %ldn.0.i, %9
  br i1 %cmp3.not.i, label %cpu_atomic_fetch_uminq_be_mmu.exit, label %do.body.i, !llvm.loop !55

cpu_atomic_fetch_uminq_be_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %6
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_fetch_uminq_be_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !54
  fence seq_cst
  %3 = load atomic i64, ptr %1 monotonic, align 8
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i64 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i64 @llvm.bswap.i64(i64 %ldn.0)
  %cond = tail call i64 @llvm.umin.i64(i64 %4, i64 %xval)
  %5 = tail call i64 @llvm.bswap.i64(i64 %cond)
  %6 = cmpxchg ptr %1, i64 %ldn.0, i64 %5 seq_cst seq_cst, align 8
  %7 = extractvalue { i64, i1 } %6, 0
  %cmp3.not = icmp eq i64 %ldn.0, %7
  br i1 %cmp3.not, label %do.body4, label %do.body, !llvm.loop !55

do.body4:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %4
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_fetch_uminq_le(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !56
  fence seq_cst
  %5 = load atomic i64, ptr %3 monotonic, align 8
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i64 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %cond.i = tail call i64 @llvm.umin.i64(i64 %cmp.0.i, i64 %val)
  %6 = cmpxchg ptr %3, i64 %cmp.0.i, i64 %cond.i seq_cst seq_cst, align 8
  %7 = extractvalue { i64, i1 } %6, 0
  %cmp4.not.i = extractvalue { i64, i1 } %6, 1
  br i1 %cmp4.not.i, label %cpu_atomic_fetch_uminq_le_mmu.exit, label %do.body.i, !llvm.loop !57

cpu_atomic_fetch_uminq_le_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %cmp.0.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_fetch_uminq_le_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !56
  fence seq_cst
  %3 = load atomic i64, ptr %1 monotonic, align 8
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i64 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %cond = tail call i64 @llvm.umin.i64(i64 %cmp.0, i64 %xval)
  %4 = cmpxchg ptr %1, i64 %cmp.0, i64 %cond seq_cst seq_cst, align 8
  %5 = extractvalue { i64, i1 } %4, 0
  %cmp4.not = extractvalue { i64, i1 } %4, 1
  br i1 %cmp4.not, label %do.body5, label %do.body, !llvm.loop !57

do.body5:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %cmp.0
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_smaxb(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %get_alignment_bits.exit.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !58
  fence seq_cst
  %5 = load atomic i8, ptr %3 monotonic, align 1
  %sext.i = shl i32 %val, 24
  %conv3.i = ashr exact i32 %sext.i, 24
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i8 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %conv2.i = sext i8 %cmp.0.i to i32
  %cond.i = tail call i32 @llvm.smax.i32(i32 %conv3.i, i32 %conv2.i)
  %conv6.i = trunc i32 %cond.i to i8
  %6 = cmpxchg ptr %3, i8 %cmp.0.i, i8 %conv6.i seq_cst seq_cst, align 1
  %7 = extractvalue { i8, i1 } %6, 0
  %cmp10.not.i = extractvalue { i8, i1 } %6, 1
  br i1 %cmp10.not.i, label %cpu_atomic_fetch_smaxb_mmu.exit, label %do.body.i, !llvm.loop !59

cpu_atomic_fetch_smaxb_mmu.exit:                  ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_smaxb_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %atomic_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !58
  fence seq_cst
  %3 = load atomic i8, ptr %1 monotonic, align 1
  %sext = shl i32 %xval, 24
  %conv3 = ashr exact i32 %sext, 24
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i8 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %conv2 = sext i8 %cmp.0 to i32
  %cond = tail call i32 @llvm.smax.i32(i32 %conv3, i32 %conv2)
  %conv6 = trunc i32 %cond to i8
  %4 = cmpxchg ptr %1, i8 %cmp.0, i8 %conv6 seq_cst seq_cst, align 1
  %5 = extractvalue { i8, i1 } %4, 0
  %cmp10.not = extractvalue { i8, i1 } %4, 1
  br i1 %cmp10.not, label %do.body12, label %do.body, !llvm.loop !59

do.body12:                                        ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_smaxw_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %call = tail call i32 @cpu_atomic_fetch_smaxw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %1), !range !31
  ret i32 %call
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_smaxw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !60
  fence seq_cst
  %3 = load atomic i16, ptr %1 monotonic, align 2
  %sext = shl i32 %xval, 16
  %conv3 = ashr exact i32 %sext, 16
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i16 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i16 @llvm.bswap.i16(i16 %ldn.0)
  %conv2 = sext i16 %4 to i32
  %cond = tail call i32 @llvm.smax.i32(i32 %conv3, i32 %conv2)
  %conv5 = trunc i32 %cond to i16
  %5 = tail call i16 @llvm.bswap.i16(i16 %conv5)
  %6 = cmpxchg ptr %1, i16 %ldn.0, i16 %5 seq_cst seq_cst, align 2
  %7 = extractvalue { i16, i1 } %6, 0
  %cmp9.not = icmp eq i16 %ldn.0, %7
  br i1 %cmp9.not, label %do.body11, label %do.body, !llvm.loop !61

do.body11:                                        ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_smaxw_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 1
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !62
  fence seq_cst
  %5 = load atomic i16, ptr %3 monotonic, align 2
  %sext.i = shl i32 %val, 16
  %conv3.i = ashr exact i32 %sext.i, 16
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i16 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %conv2.i = sext i16 %cmp.0.i to i32
  %cond.i = tail call i32 @llvm.smax.i32(i32 %conv3.i, i32 %conv2.i)
  %conv6.i = trunc i32 %cond.i to i16
  %6 = cmpxchg ptr %3, i16 %cmp.0.i, i16 %conv6.i seq_cst seq_cst, align 2
  %7 = extractvalue { i16, i1 } %6, 0
  %cmp10.not.i = extractvalue { i16, i1 } %6, 1
  br i1 %cmp10.not.i, label %cpu_atomic_fetch_smaxw_le_mmu.exit, label %do.body.i, !llvm.loop !63

cpu_atomic_fetch_smaxw_le_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_smaxw_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !62
  fence seq_cst
  %3 = load atomic i16, ptr %1 monotonic, align 2
  %sext = shl i32 %xval, 16
  %conv3 = ashr exact i32 %sext, 16
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i16 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %conv2 = sext i16 %cmp.0 to i32
  %cond = tail call i32 @llvm.smax.i32(i32 %conv3, i32 %conv2)
  %conv6 = trunc i32 %cond to i16
  %4 = cmpxchg ptr %1, i16 %cmp.0, i16 %conv6 seq_cst seq_cst, align 2
  %5 = extractvalue { i16, i1 } %4, 0
  %cmp10.not = extractvalue { i16, i1 } %4, 1
  br i1 %cmp10.not, label %do.body12, label %do.body, !llvm.loop !63

do.body12:                                        ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_smaxl_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !64
  fence seq_cst
  %5 = load atomic i32, ptr %3 monotonic, align 4
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %ldn.0.i = phi i32 [ %5, %atomic_mmu_lookup.exit.i ], [ %9, %do.body.i ]
  %6 = tail call i32 @llvm.bswap.i32(i32 %ldn.0.i)
  %cond.i = tail call i32 @llvm.smax.i32(i32 %6, i32 %val)
  %7 = tail call i32 @llvm.bswap.i32(i32 %cond.i)
  %8 = cmpxchg ptr %3, i32 %ldn.0.i, i32 %7 seq_cst seq_cst, align 4
  %9 = extractvalue { i32, i1 } %8, 0
  %cmp3.not.i = icmp eq i32 %ldn.0.i, %9
  br i1 %cmp3.not.i, label %cpu_atomic_fetch_smaxl_be_mmu.exit, label %do.body.i, !llvm.loop !65

cpu_atomic_fetch_smaxl_be_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %6
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_smaxl_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !64
  fence seq_cst
  %3 = load atomic i32, ptr %1 monotonic, align 4
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i32 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i32 @llvm.bswap.i32(i32 %ldn.0)
  %cond = tail call i32 @llvm.smax.i32(i32 %4, i32 %xval)
  %5 = tail call i32 @llvm.bswap.i32(i32 %cond)
  %6 = cmpxchg ptr %1, i32 %ldn.0, i32 %5 seq_cst seq_cst, align 4
  %7 = extractvalue { i32, i1 } %6, 0
  %cmp3.not = icmp eq i32 %ldn.0, %7
  br i1 %cmp3.not, label %do.body4, label %do.body, !llvm.loop !65

do.body4:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %4
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_smaxl_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !66
  fence seq_cst
  %5 = load atomic i32, ptr %3 monotonic, align 4
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i32 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %cond.i = tail call i32 @llvm.smax.i32(i32 %cmp.0.i, i32 %val)
  %6 = cmpxchg ptr %3, i32 %cmp.0.i, i32 %cond.i seq_cst seq_cst, align 4
  %7 = extractvalue { i32, i1 } %6, 0
  %cmp4.not.i = extractvalue { i32, i1 } %6, 1
  br i1 %cmp4.not.i, label %cpu_atomic_fetch_smaxl_le_mmu.exit, label %do.body.i, !llvm.loop !67

cpu_atomic_fetch_smaxl_le_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cmp.0.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_smaxl_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !66
  fence seq_cst
  %3 = load atomic i32, ptr %1 monotonic, align 4
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i32 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %cond = tail call i32 @llvm.smax.i32(i32 %cmp.0, i32 %xval)
  %4 = cmpxchg ptr %1, i32 %cmp.0, i32 %cond seq_cst seq_cst, align 4
  %5 = extractvalue { i32, i1 } %4, 0
  %cmp4.not = extractvalue { i32, i1 } %4, 1
  br i1 %cmp4.not, label %do.body5, label %do.body, !llvm.loop !67

do.body5:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cmp.0
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_fetch_smaxq_be(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !68
  fence seq_cst
  %5 = load atomic i64, ptr %3 monotonic, align 8
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %ldn.0.i = phi i64 [ %5, %atomic_mmu_lookup.exit.i ], [ %9, %do.body.i ]
  %6 = tail call i64 @llvm.bswap.i64(i64 %ldn.0.i)
  %cond.i = tail call i64 @llvm.smax.i64(i64 %6, i64 %val)
  %7 = tail call i64 @llvm.bswap.i64(i64 %cond.i)
  %8 = cmpxchg ptr %3, i64 %ldn.0.i, i64 %7 seq_cst seq_cst, align 8
  %9 = extractvalue { i64, i1 } %8, 0
  %cmp3.not.i = icmp eq i64 %ldn.0.i, %9
  br i1 %cmp3.not.i, label %cpu_atomic_fetch_smaxq_be_mmu.exit, label %do.body.i, !llvm.loop !69

cpu_atomic_fetch_smaxq_be_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %6
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_fetch_smaxq_be_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !68
  fence seq_cst
  %3 = load atomic i64, ptr %1 monotonic, align 8
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i64 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i64 @llvm.bswap.i64(i64 %ldn.0)
  %cond = tail call i64 @llvm.smax.i64(i64 %4, i64 %xval)
  %5 = tail call i64 @llvm.bswap.i64(i64 %cond)
  %6 = cmpxchg ptr %1, i64 %ldn.0, i64 %5 seq_cst seq_cst, align 8
  %7 = extractvalue { i64, i1 } %6, 0
  %cmp3.not = icmp eq i64 %ldn.0, %7
  br i1 %cmp3.not, label %do.body4, label %do.body, !llvm.loop !69

do.body4:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %4
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_fetch_smaxq_le(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !70
  fence seq_cst
  %5 = load atomic i64, ptr %3 monotonic, align 8
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i64 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %cond.i = tail call i64 @llvm.smax.i64(i64 %cmp.0.i, i64 %val)
  %6 = cmpxchg ptr %3, i64 %cmp.0.i, i64 %cond.i seq_cst seq_cst, align 8
  %7 = extractvalue { i64, i1 } %6, 0
  %cmp4.not.i = extractvalue { i64, i1 } %6, 1
  br i1 %cmp4.not.i, label %cpu_atomic_fetch_smaxq_le_mmu.exit, label %do.body.i, !llvm.loop !71

cpu_atomic_fetch_smaxq_le_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %cmp.0.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_fetch_smaxq_le_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !70
  fence seq_cst
  %3 = load atomic i64, ptr %1 monotonic, align 8
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i64 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %cond = tail call i64 @llvm.smax.i64(i64 %cmp.0, i64 %xval)
  %4 = cmpxchg ptr %1, i64 %cmp.0, i64 %cond seq_cst seq_cst, align 8
  %5 = extractvalue { i64, i1 } %4, 0
  %cmp4.not = extractvalue { i64, i1 } %4, 1
  br i1 %cmp4.not, label %do.body5, label %do.body, !llvm.loop !71

do.body5:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %cmp.0
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_umaxb(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %get_alignment_bits.exit.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !72
  fence seq_cst
  %5 = load atomic i8, ptr %3 monotonic, align 1
  %conv3.i = and i32 %val, 255
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i8 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %conv2.i = zext i8 %cmp.0.i to i32
  %cond.i = tail call i32 @llvm.umax.i32(i32 %conv3.i, i32 %conv2.i)
  %conv6.i = trunc i32 %cond.i to i8
  %6 = cmpxchg ptr %3, i8 %cmp.0.i, i8 %conv6.i seq_cst seq_cst, align 1
  %7 = extractvalue { i8, i1 } %6, 0
  %cmp10.not.i = extractvalue { i8, i1 } %6, 1
  br i1 %cmp10.not.i, label %cpu_atomic_fetch_umaxb_mmu.exit, label %do.body.i, !llvm.loop !73

cpu_atomic_fetch_umaxb_mmu.exit:                  ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_umaxb_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %atomic_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !72
  fence seq_cst
  %3 = load atomic i8, ptr %1 monotonic, align 1
  %conv3 = and i32 %xval, 255
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i8 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %conv2 = zext i8 %cmp.0 to i32
  %cond = tail call i32 @llvm.umax.i32(i32 %conv3, i32 %conv2)
  %conv6 = trunc i32 %cond to i8
  %4 = cmpxchg ptr %1, i8 %cmp.0, i8 %conv6 seq_cst seq_cst, align 1
  %5 = extractvalue { i8, i1 } %4, 0
  %cmp10.not = extractvalue { i8, i1 } %4, 1
  br i1 %cmp10.not, label %do.body12, label %do.body, !llvm.loop !73

do.body12:                                        ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_umaxw_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %call = tail call i32 @cpu_atomic_fetch_umaxw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %1), !range !22
  ret i32 %call
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_umaxw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !74
  fence seq_cst
  %3 = load atomic i16, ptr %1 monotonic, align 2
  %conv3 = and i32 %xval, 65535
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i16 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i16 @llvm.bswap.i16(i16 %ldn.0)
  %conv2 = zext i16 %4 to i32
  %cond = tail call i32 @llvm.umax.i32(i32 %conv3, i32 %conv2)
  %conv5 = trunc i32 %cond to i16
  %5 = tail call i16 @llvm.bswap.i16(i16 %conv5)
  %6 = cmpxchg ptr %1, i16 %ldn.0, i16 %5 seq_cst seq_cst, align 2
  %7 = extractvalue { i16, i1 } %6, 0
  %cmp9.not = icmp eq i16 %ldn.0, %7
  br i1 %cmp9.not, label %do.body11, label %do.body, !llvm.loop !75

do.body11:                                        ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_umaxw_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 1
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !76
  fence seq_cst
  %5 = load atomic i16, ptr %3 monotonic, align 2
  %conv3.i = and i32 %val, 65535
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i16 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %conv2.i = zext i16 %cmp.0.i to i32
  %cond.i = tail call i32 @llvm.umax.i32(i32 %conv3.i, i32 %conv2.i)
  %conv6.i = trunc i32 %cond.i to i16
  %6 = cmpxchg ptr %3, i16 %cmp.0.i, i16 %conv6.i seq_cst seq_cst, align 2
  %7 = extractvalue { i16, i1 } %6, 0
  %cmp10.not.i = extractvalue { i16, i1 } %6, 1
  br i1 %cmp10.not.i, label %cpu_atomic_fetch_umaxw_le_mmu.exit, label %do.body.i, !llvm.loop !77

cpu_atomic_fetch_umaxw_le_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_umaxw_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !76
  fence seq_cst
  %3 = load atomic i16, ptr %1 monotonic, align 2
  %conv3 = and i32 %xval, 65535
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i16 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %conv2 = zext i16 %cmp.0 to i32
  %cond = tail call i32 @llvm.umax.i32(i32 %conv3, i32 %conv2)
  %conv6 = trunc i32 %cond to i16
  %4 = cmpxchg ptr %1, i16 %cmp.0, i16 %conv6 seq_cst seq_cst, align 2
  %5 = extractvalue { i16, i1 } %4, 0
  %cmp10.not = extractvalue { i16, i1 } %4, 1
  br i1 %cmp10.not, label %do.body12, label %do.body, !llvm.loop !77

do.body12:                                        ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_umaxl_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !78
  fence seq_cst
  %5 = load atomic i32, ptr %3 monotonic, align 4
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %ldn.0.i = phi i32 [ %5, %atomic_mmu_lookup.exit.i ], [ %9, %do.body.i ]
  %6 = tail call i32 @llvm.bswap.i32(i32 %ldn.0.i)
  %cond.i = tail call i32 @llvm.umax.i32(i32 %6, i32 %val)
  %7 = tail call i32 @llvm.bswap.i32(i32 %cond.i)
  %8 = cmpxchg ptr %3, i32 %ldn.0.i, i32 %7 seq_cst seq_cst, align 4
  %9 = extractvalue { i32, i1 } %8, 0
  %cmp3.not.i = icmp eq i32 %ldn.0.i, %9
  br i1 %cmp3.not.i, label %cpu_atomic_fetch_umaxl_be_mmu.exit, label %do.body.i, !llvm.loop !79

cpu_atomic_fetch_umaxl_be_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %6
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_umaxl_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !78
  fence seq_cst
  %3 = load atomic i32, ptr %1 monotonic, align 4
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i32 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i32 @llvm.bswap.i32(i32 %ldn.0)
  %cond = tail call i32 @llvm.umax.i32(i32 %4, i32 %xval)
  %5 = tail call i32 @llvm.bswap.i32(i32 %cond)
  %6 = cmpxchg ptr %1, i32 %ldn.0, i32 %5 seq_cst seq_cst, align 4
  %7 = extractvalue { i32, i1 } %6, 0
  %cmp3.not = icmp eq i32 %ldn.0, %7
  br i1 %cmp3.not, label %do.body4, label %do.body, !llvm.loop !79

do.body4:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %4
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_fetch_umaxl_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !80
  fence seq_cst
  %5 = load atomic i32, ptr %3 monotonic, align 4
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i32 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %cond.i = tail call i32 @llvm.umax.i32(i32 %cmp.0.i, i32 %val)
  %6 = cmpxchg ptr %3, i32 %cmp.0.i, i32 %cond.i seq_cst seq_cst, align 4
  %7 = extractvalue { i32, i1 } %6, 0
  %cmp4.not.i = extractvalue { i32, i1 } %6, 1
  br i1 %cmp4.not.i, label %cpu_atomic_fetch_umaxl_le_mmu.exit, label %do.body.i, !llvm.loop !81

cpu_atomic_fetch_umaxl_le_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cmp.0.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_fetch_umaxl_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !80
  fence seq_cst
  %3 = load atomic i32, ptr %1 monotonic, align 4
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i32 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %cond = tail call i32 @llvm.umax.i32(i32 %cmp.0, i32 %xval)
  %4 = cmpxchg ptr %1, i32 %cmp.0, i32 %cond seq_cst seq_cst, align 4
  %5 = extractvalue { i32, i1 } %4, 0
  %cmp4.not = extractvalue { i32, i1 } %4, 1
  br i1 %cmp4.not, label %do.body5, label %do.body, !llvm.loop !81

do.body5:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cmp.0
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_fetch_umaxq_be(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !82
  fence seq_cst
  %5 = load atomic i64, ptr %3 monotonic, align 8
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %ldn.0.i = phi i64 [ %5, %atomic_mmu_lookup.exit.i ], [ %9, %do.body.i ]
  %6 = tail call i64 @llvm.bswap.i64(i64 %ldn.0.i)
  %cond.i = tail call i64 @llvm.umax.i64(i64 %6, i64 %val)
  %7 = tail call i64 @llvm.bswap.i64(i64 %cond.i)
  %8 = cmpxchg ptr %3, i64 %ldn.0.i, i64 %7 seq_cst seq_cst, align 8
  %9 = extractvalue { i64, i1 } %8, 0
  %cmp3.not.i = icmp eq i64 %ldn.0.i, %9
  br i1 %cmp3.not.i, label %cpu_atomic_fetch_umaxq_be_mmu.exit, label %do.body.i, !llvm.loop !83

cpu_atomic_fetch_umaxq_be_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %6
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_fetch_umaxq_be_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !82
  fence seq_cst
  %3 = load atomic i64, ptr %1 monotonic, align 8
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i64 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i64 @llvm.bswap.i64(i64 %ldn.0)
  %cond = tail call i64 @llvm.umax.i64(i64 %4, i64 %xval)
  %5 = tail call i64 @llvm.bswap.i64(i64 %cond)
  %6 = cmpxchg ptr %1, i64 %ldn.0, i64 %5 seq_cst seq_cst, align 8
  %7 = extractvalue { i64, i1 } %6, 0
  %cmp3.not = icmp eq i64 %ldn.0, %7
  br i1 %cmp3.not, label %do.body4, label %do.body, !llvm.loop !83

do.body4:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %4
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_fetch_umaxq_le(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !84
  fence seq_cst
  %5 = load atomic i64, ptr %3 monotonic, align 8
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i64 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %cond.i = tail call i64 @llvm.umax.i64(i64 %cmp.0.i, i64 %val)
  %6 = cmpxchg ptr %3, i64 %cmp.0.i, i64 %cond.i seq_cst seq_cst, align 8
  %7 = extractvalue { i64, i1 } %6, 0
  %cmp4.not.i = extractvalue { i64, i1 } %6, 1
  br i1 %cmp4.not.i, label %cpu_atomic_fetch_umaxq_le_mmu.exit, label %do.body.i, !llvm.loop !85

cpu_atomic_fetch_umaxq_le_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %cmp.0.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_fetch_umaxq_le_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !84
  fence seq_cst
  %3 = load atomic i64, ptr %1 monotonic, align 8
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i64 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %cond = tail call i64 @llvm.umax.i64(i64 %cmp.0, i64 %xval)
  %4 = cmpxchg ptr %1, i64 %cmp.0, i64 %cond seq_cst seq_cst, align 8
  %5 = extractvalue { i64, i1 } %4, 0
  %cmp4.not = extractvalue { i64, i1 } %4, 1
  br i1 %cmp4.not, label %do.body5, label %do.body, !llvm.loop !85

do.body5:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %cmp.0
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_add_fetchb(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %cpu_atomic_add_fetchb_mmu.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

cpu_atomic_add_fetchb_mmu.exit:                   ; preds = %get_alignment_bits.exit.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %val to i8
  %5 = atomicrmw add ptr %3, i8 %conv.i seq_cst, align 1
  %6 = add i8 %5, %conv.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2.i = zext i8 %6 to i32
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_add_fetchb_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %atomic_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %val to i8
  %3 = atomicrmw add ptr %1, i8 %conv seq_cst, align 1
  %4 = add i8 %3, %conv
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2 = zext i8 %4 to i32
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_add_fetchw_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %call = tail call i32 @cpu_atomic_add_fetchw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %1), !range !22
  ret i32 %call
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_add_fetchw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !86
  fence seq_cst
  %3 = load atomic i16, ptr %1 monotonic, align 2
  %4 = trunc i32 %xval to i16
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i16 [ %3, %atomic_mmu_lookup.exit ], [ %8, %do.body ]
  %5 = tail call i16 @llvm.bswap.i16(i16 %ldn.0)
  %conv4 = add i16 %5, %4
  %6 = tail call i16 @llvm.bswap.i16(i16 %conv4)
  %7 = cmpxchg ptr %1, i16 %ldn.0, i16 %6 seq_cst seq_cst, align 2
  %8 = extractvalue { i16, i1 } %7, 0
  %cmp.not = icmp eq i16 %ldn.0, %8
  br i1 %cmp.not, label %do.body8, label %do.body, !llvm.loop !87

do.body8:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv11 = zext i16 %conv4 to i32
  ret i32 %conv11
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_add_fetchw_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 1
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_add_fetchw_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_add_fetchw_le_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %val to i16
  %5 = atomicrmw add ptr %3, i16 %conv.i seq_cst, align 2
  %6 = add i16 %5, %conv.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2.i = zext i16 %6 to i32
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_add_fetchw_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %val to i16
  %3 = atomicrmw add ptr %1, i16 %conv seq_cst, align 2
  %4 = add i16 %3, %conv
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2 = zext i16 %4 to i32
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_add_fetchl_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !88
  fence seq_cst
  %5 = load atomic i32, ptr %3 monotonic, align 4
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %ldn.0.i = phi i32 [ %5, %atomic_mmu_lookup.exit.i ], [ %9, %do.body.i ]
  %6 = tail call i32 @llvm.bswap.i32(i32 %ldn.0.i)
  %add.i = add i32 %6, %val
  %7 = tail call i32 @llvm.bswap.i32(i32 %add.i)
  %8 = cmpxchg ptr %3, i32 %ldn.0.i, i32 %7 seq_cst seq_cst, align 4
  %9 = extractvalue { i32, i1 } %8, 0
  %cmp.not.i = icmp eq i32 %ldn.0.i, %9
  br i1 %cmp.not.i, label %cpu_atomic_add_fetchl_be_mmu.exit, label %do.body.i, !llvm.loop !89

cpu_atomic_add_fetchl_be_mmu.exit:                ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %add.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_add_fetchl_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !88
  fence seq_cst
  %3 = load atomic i32, ptr %1 monotonic, align 4
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i32 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i32 @llvm.bswap.i32(i32 %ldn.0)
  %add = add i32 %4, %xval
  %5 = tail call i32 @llvm.bswap.i32(i32 %add)
  %6 = cmpxchg ptr %1, i32 %ldn.0, i32 %5 seq_cst seq_cst, align 4
  %7 = extractvalue { i32, i1 } %6, 0
  %cmp.not = icmp eq i32 %ldn.0, %7
  br i1 %cmp.not, label %do.body2, label %do.body, !llvm.loop !89

do.body2:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %add
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_add_fetchl_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_add_fetchl_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_add_fetchl_le_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = atomicrmw add ptr %3, i32 %val seq_cst, align 4
  %6 = add i32 %5, %val
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %6
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_add_fetchl_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = atomicrmw add ptr %1, i32 %val seq_cst, align 4
  %4 = add i32 %3, %val
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %4
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_add_fetchq_be(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !90
  fence seq_cst
  %5 = load atomic i64, ptr %3 monotonic, align 8
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %ldn.0.i = phi i64 [ %5, %atomic_mmu_lookup.exit.i ], [ %9, %do.body.i ]
  %6 = tail call i64 @llvm.bswap.i64(i64 %ldn.0.i)
  %add.i = add i64 %6, %val
  %7 = tail call i64 @llvm.bswap.i64(i64 %add.i)
  %8 = cmpxchg ptr %3, i64 %ldn.0.i, i64 %7 seq_cst seq_cst, align 8
  %9 = extractvalue { i64, i1 } %8, 0
  %cmp.not.i = icmp eq i64 %ldn.0.i, %9
  br i1 %cmp.not.i, label %cpu_atomic_add_fetchq_be_mmu.exit, label %do.body.i, !llvm.loop !91

cpu_atomic_add_fetchq_be_mmu.exit:                ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %add.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_add_fetchq_be_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !90
  fence seq_cst
  %3 = load atomic i64, ptr %1 monotonic, align 8
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i64 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i64 @llvm.bswap.i64(i64 %ldn.0)
  %add = add i64 %4, %xval
  %5 = tail call i64 @llvm.bswap.i64(i64 %add)
  %6 = cmpxchg ptr %1, i64 %ldn.0, i64 %5 seq_cst seq_cst, align 8
  %7 = extractvalue { i64, i1 } %6, 0
  %cmp.not = icmp eq i64 %ldn.0, %7
  br i1 %cmp.not, label %do.body2, label %do.body, !llvm.loop !91

do.body2:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %add
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_add_fetchq_le(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_add_fetchq_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_add_fetchq_le_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = atomicrmw add ptr %3, i64 %val seq_cst, align 8
  %6 = add i64 %5, %val
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %6
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_add_fetchq_le_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = atomicrmw add ptr %1, i64 %val seq_cst, align 8
  %4 = add i64 %3, %val
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %4
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_and_fetchb(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %cpu_atomic_and_fetchb_mmu.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

cpu_atomic_and_fetchb_mmu.exit:                   ; preds = %get_alignment_bits.exit.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %val to i8
  %5 = atomicrmw and ptr %3, i8 %conv.i seq_cst, align 1
  %6 = and i8 %5, %conv.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2.i = zext i8 %6 to i32
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_and_fetchb_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %atomic_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %val to i8
  %3 = atomicrmw and ptr %1, i8 %conv seq_cst, align 1
  %4 = and i8 %3, %conv
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2 = zext i8 %4 to i32
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_and_fetchw_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 1
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_and_fetchw_be_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_and_fetchw_be_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %val to i16
  %5 = tail call i16 @llvm.bswap.i16(i16 %conv.i)
  %6 = atomicrmw and ptr %3, i16 %5 seq_cst, align 2
  %7 = and i16 %6, %5
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %8 = tail call i16 @llvm.bswap.i16(i16 %7)
  %conv2.i = zext i16 %8 to i32
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_and_fetchw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %val to i16
  %3 = tail call i16 @llvm.bswap.i16(i16 %conv)
  %4 = atomicrmw and ptr %1, i16 %3 seq_cst, align 2
  %5 = and i16 %4, %3
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %6 = tail call i16 @llvm.bswap.i16(i16 %5)
  %conv2 = zext i16 %6 to i32
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_and_fetchw_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 1
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_and_fetchw_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_and_fetchw_le_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %val to i16
  %5 = atomicrmw and ptr %3, i16 %conv.i seq_cst, align 2
  %6 = and i16 %5, %conv.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2.i = zext i16 %6 to i32
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_and_fetchw_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %val to i16
  %3 = atomicrmw and ptr %1, i16 %conv seq_cst, align 2
  %4 = and i16 %3, %conv
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2 = zext i16 %4 to i32
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_and_fetchl_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_and_fetchl_be_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_and_fetchl_be_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = tail call i32 @llvm.bswap.i32(i32 %val)
  %6 = atomicrmw and ptr %3, i32 %5 seq_cst, align 4
  %7 = and i32 %6, %5
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %8 = tail call i32 @llvm.bswap.i32(i32 %7)
  ret i32 %8
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_and_fetchl_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = tail call i32 @llvm.bswap.i32(i32 %val)
  %4 = atomicrmw and ptr %1, i32 %3 seq_cst, align 4
  %5 = and i32 %4, %3
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %6 = tail call i32 @llvm.bswap.i32(i32 %5)
  ret i32 %6
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_and_fetchl_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_and_fetchl_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_and_fetchl_le_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = atomicrmw and ptr %3, i32 %val seq_cst, align 4
  %6 = and i32 %5, %val
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %6
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_and_fetchl_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = atomicrmw and ptr %1, i32 %val seq_cst, align 4
  %4 = and i32 %3, %val
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %4
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_and_fetchq_be(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_and_fetchq_be_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_and_fetchq_be_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = tail call i64 @llvm.bswap.i64(i64 %val)
  %6 = atomicrmw and ptr %3, i64 %5 seq_cst, align 8
  %7 = and i64 %6, %5
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %8 = tail call i64 @llvm.bswap.i64(i64 %7)
  ret i64 %8
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_and_fetchq_be_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = tail call i64 @llvm.bswap.i64(i64 %val)
  %4 = atomicrmw and ptr %1, i64 %3 seq_cst, align 8
  %5 = and i64 %4, %3
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %6 = tail call i64 @llvm.bswap.i64(i64 %5)
  ret i64 %6
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_and_fetchq_le(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_and_fetchq_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_and_fetchq_le_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = atomicrmw and ptr %3, i64 %val seq_cst, align 8
  %6 = and i64 %5, %val
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %6
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_and_fetchq_le_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = atomicrmw and ptr %1, i64 %val seq_cst, align 8
  %4 = and i64 %3, %val
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %4
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_or_fetchb(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %cpu_atomic_or_fetchb_mmu.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

cpu_atomic_or_fetchb_mmu.exit:                    ; preds = %get_alignment_bits.exit.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %val to i8
  %5 = atomicrmw or ptr %3, i8 %conv.i seq_cst, align 1
  %6 = or i8 %5, %conv.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2.i = zext i8 %6 to i32
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_or_fetchb_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %atomic_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %val to i8
  %3 = atomicrmw or ptr %1, i8 %conv seq_cst, align 1
  %4 = or i8 %3, %conv
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2 = zext i8 %4 to i32
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_or_fetchw_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 1
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_or_fetchw_be_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_or_fetchw_be_mmu.exit:                 ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %val to i16
  %5 = tail call i16 @llvm.bswap.i16(i16 %conv.i)
  %6 = atomicrmw or ptr %3, i16 %5 seq_cst, align 2
  %7 = or i16 %6, %5
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %8 = tail call i16 @llvm.bswap.i16(i16 %7)
  %conv2.i = zext i16 %8 to i32
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_or_fetchw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %val to i16
  %3 = tail call i16 @llvm.bswap.i16(i16 %conv)
  %4 = atomicrmw or ptr %1, i16 %3 seq_cst, align 2
  %5 = or i16 %4, %3
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %6 = tail call i16 @llvm.bswap.i16(i16 %5)
  %conv2 = zext i16 %6 to i32
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_or_fetchw_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 1
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_or_fetchw_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_or_fetchw_le_mmu.exit:                 ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %val to i16
  %5 = atomicrmw or ptr %3, i16 %conv.i seq_cst, align 2
  %6 = or i16 %5, %conv.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2.i = zext i16 %6 to i32
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_or_fetchw_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %val to i16
  %3 = atomicrmw or ptr %1, i16 %conv seq_cst, align 2
  %4 = or i16 %3, %conv
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2 = zext i16 %4 to i32
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_or_fetchl_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_or_fetchl_be_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_or_fetchl_be_mmu.exit:                 ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = tail call i32 @llvm.bswap.i32(i32 %val)
  %6 = atomicrmw or ptr %3, i32 %5 seq_cst, align 4
  %7 = or i32 %6, %5
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %8 = tail call i32 @llvm.bswap.i32(i32 %7)
  ret i32 %8
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_or_fetchl_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = tail call i32 @llvm.bswap.i32(i32 %val)
  %4 = atomicrmw or ptr %1, i32 %3 seq_cst, align 4
  %5 = or i32 %4, %3
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %6 = tail call i32 @llvm.bswap.i32(i32 %5)
  ret i32 %6
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_or_fetchl_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_or_fetchl_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_or_fetchl_le_mmu.exit:                 ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = atomicrmw or ptr %3, i32 %val seq_cst, align 4
  %6 = or i32 %5, %val
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %6
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_or_fetchl_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = atomicrmw or ptr %1, i32 %val seq_cst, align 4
  %4 = or i32 %3, %val
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %4
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_or_fetchq_be(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_or_fetchq_be_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_or_fetchq_be_mmu.exit:                 ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = tail call i64 @llvm.bswap.i64(i64 %val)
  %6 = atomicrmw or ptr %3, i64 %5 seq_cst, align 8
  %7 = or i64 %6, %5
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %8 = tail call i64 @llvm.bswap.i64(i64 %7)
  ret i64 %8
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_or_fetchq_be_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = tail call i64 @llvm.bswap.i64(i64 %val)
  %4 = atomicrmw or ptr %1, i64 %3 seq_cst, align 8
  %5 = or i64 %4, %3
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %6 = tail call i64 @llvm.bswap.i64(i64 %5)
  ret i64 %6
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_or_fetchq_le(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_or_fetchq_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_or_fetchq_le_mmu.exit:                 ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = atomicrmw or ptr %3, i64 %val seq_cst, align 8
  %6 = or i64 %5, %val
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %6
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_or_fetchq_le_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = atomicrmw or ptr %1, i64 %val seq_cst, align 8
  %4 = or i64 %3, %val
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %4
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_xor_fetchb(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %cpu_atomic_xor_fetchb_mmu.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

cpu_atomic_xor_fetchb_mmu.exit:                   ; preds = %get_alignment_bits.exit.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %val to i8
  %5 = atomicrmw xor ptr %3, i8 %conv.i seq_cst, align 1
  %6 = xor i8 %5, %conv.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2.i = zext i8 %6 to i32
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_xor_fetchb_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %atomic_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %val to i8
  %3 = atomicrmw xor ptr %1, i8 %conv seq_cst, align 1
  %4 = xor i8 %3, %conv
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2 = zext i8 %4 to i32
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_xor_fetchw_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 1
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_xor_fetchw_be_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_xor_fetchw_be_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %val to i16
  %5 = tail call i16 @llvm.bswap.i16(i16 %conv.i)
  %6 = atomicrmw xor ptr %3, i16 %5 seq_cst, align 2
  %7 = xor i16 %6, %5
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %8 = tail call i16 @llvm.bswap.i16(i16 %7)
  %conv2.i = zext i16 %8 to i32
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_xor_fetchw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %val to i16
  %3 = tail call i16 @llvm.bswap.i16(i16 %conv)
  %4 = atomicrmw xor ptr %1, i16 %3 seq_cst, align 2
  %5 = xor i16 %4, %3
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %6 = tail call i16 @llvm.bswap.i16(i16 %5)
  %conv2 = zext i16 %6 to i32
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_xor_fetchw_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 1
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_xor_fetchw_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_xor_fetchw_le_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %val to i16
  %5 = atomicrmw xor ptr %3, i16 %conv.i seq_cst, align 2
  %6 = xor i16 %5, %conv.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2.i = zext i16 %6 to i32
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_xor_fetchw_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %val to i16
  %3 = atomicrmw xor ptr %1, i16 %conv seq_cst, align 2
  %4 = xor i16 %3, %conv
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2 = zext i16 %4 to i32
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_xor_fetchl_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_xor_fetchl_be_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_xor_fetchl_be_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = tail call i32 @llvm.bswap.i32(i32 %val)
  %6 = atomicrmw xor ptr %3, i32 %5 seq_cst, align 4
  %7 = xor i32 %6, %5
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %8 = tail call i32 @llvm.bswap.i32(i32 %7)
  ret i32 %8
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_xor_fetchl_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = tail call i32 @llvm.bswap.i32(i32 %val)
  %4 = atomicrmw xor ptr %1, i32 %3 seq_cst, align 4
  %5 = xor i32 %4, %3
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %6 = tail call i32 @llvm.bswap.i32(i32 %5)
  ret i32 %6
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_xor_fetchl_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_xor_fetchl_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_xor_fetchl_le_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = atomicrmw xor ptr %3, i32 %val seq_cst, align 4
  %6 = xor i32 %5, %val
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %6
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_xor_fetchl_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = atomicrmw xor ptr %1, i32 %val seq_cst, align 4
  %4 = xor i32 %3, %val
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %4
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_xor_fetchq_be(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_xor_fetchq_be_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_xor_fetchq_be_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = tail call i64 @llvm.bswap.i64(i64 %val)
  %6 = atomicrmw xor ptr %3, i64 %5 seq_cst, align 8
  %7 = xor i64 %6, %5
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %8 = tail call i64 @llvm.bswap.i64(i64 %7)
  ret i64 %8
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_xor_fetchq_be_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = tail call i64 @llvm.bswap.i64(i64 %val)
  %4 = atomicrmw xor ptr %1, i64 %3 seq_cst, align 8
  %5 = xor i64 %4, %3
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %6 = tail call i64 @llvm.bswap.i64(i64 %5)
  ret i64 %6
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_xor_fetchq_le(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_xor_fetchq_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_xor_fetchq_le_mmu.exit:                ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = atomicrmw xor ptr %3, i64 %val seq_cst, align 8
  %6 = xor i64 %5, %val
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %6
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_xor_fetchq_le_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = atomicrmw xor ptr %1, i64 %val seq_cst, align 8
  %4 = xor i64 %3, %val
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %4
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_smin_fetchb(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %get_alignment_bits.exit.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !92
  fence seq_cst
  %5 = load atomic i8, ptr %3 monotonic, align 1
  %sext.i = shl i32 %val, 24
  %conv3.i = ashr exact i32 %sext.i, 24
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i8 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %conv2.i = sext i8 %cmp.0.i to i32
  %cond.i = tail call i32 @llvm.smin.i32(i32 %conv3.i, i32 %conv2.i)
  %conv6.i = trunc i32 %cond.i to i8
  %6 = cmpxchg ptr %3, i8 %cmp.0.i, i8 %conv6.i seq_cst seq_cst, align 1
  %7 = extractvalue { i8, i1 } %6, 0
  %cmp10.not.i = extractvalue { i8, i1 } %6, 1
  br i1 %cmp10.not.i, label %cpu_atomic_smin_fetchb_mmu.exit, label %do.body.i, !llvm.loop !93

cpu_atomic_smin_fetchb_mmu.exit:                  ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_smin_fetchb_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %atomic_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !92
  fence seq_cst
  %3 = load atomic i8, ptr %1 monotonic, align 1
  %sext = shl i32 %xval, 24
  %conv3 = ashr exact i32 %sext, 24
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i8 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %conv2 = sext i8 %cmp.0 to i32
  %cond = tail call i32 @llvm.smin.i32(i32 %conv3, i32 %conv2)
  %conv6 = trunc i32 %cond to i8
  %4 = cmpxchg ptr %1, i8 %cmp.0, i8 %conv6 seq_cst seq_cst, align 1
  %5 = extractvalue { i8, i1 } %4, 0
  %cmp10.not = extractvalue { i8, i1 } %4, 1
  br i1 %cmp10.not, label %do.body12, label %do.body, !llvm.loop !93

do.body12:                                        ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_smin_fetchw_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %call = tail call i32 @cpu_atomic_smin_fetchw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %1), !range !31
  ret i32 %call
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_smin_fetchw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !94
  fence seq_cst
  %3 = load atomic i16, ptr %1 monotonic, align 2
  %sext = shl i32 %xval, 16
  %conv3 = ashr exact i32 %sext, 16
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i16 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i16 @llvm.bswap.i16(i16 %ldn.0)
  %conv2 = sext i16 %4 to i32
  %cond = tail call i32 @llvm.smin.i32(i32 %conv3, i32 %conv2)
  %conv5 = trunc i32 %cond to i16
  %5 = tail call i16 @llvm.bswap.i16(i16 %conv5)
  %6 = cmpxchg ptr %1, i16 %ldn.0, i16 %5 seq_cst seq_cst, align 2
  %7 = extractvalue { i16, i1 } %6, 0
  %cmp9.not = icmp eq i16 %ldn.0, %7
  br i1 %cmp9.not, label %do.body11, label %do.body, !llvm.loop !95

do.body11:                                        ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_smin_fetchw_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 1
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !96
  fence seq_cst
  %5 = load atomic i16, ptr %3 monotonic, align 2
  %sext.i = shl i32 %val, 16
  %conv3.i = ashr exact i32 %sext.i, 16
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i16 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %conv2.i = sext i16 %cmp.0.i to i32
  %cond.i = tail call i32 @llvm.smin.i32(i32 %conv3.i, i32 %conv2.i)
  %conv6.i = trunc i32 %cond.i to i16
  %6 = cmpxchg ptr %3, i16 %cmp.0.i, i16 %conv6.i seq_cst seq_cst, align 2
  %7 = extractvalue { i16, i1 } %6, 0
  %cmp10.not.i = extractvalue { i16, i1 } %6, 1
  br i1 %cmp10.not.i, label %cpu_atomic_smin_fetchw_le_mmu.exit, label %do.body.i, !llvm.loop !97

cpu_atomic_smin_fetchw_le_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_smin_fetchw_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !96
  fence seq_cst
  %3 = load atomic i16, ptr %1 monotonic, align 2
  %sext = shl i32 %xval, 16
  %conv3 = ashr exact i32 %sext, 16
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i16 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %conv2 = sext i16 %cmp.0 to i32
  %cond = tail call i32 @llvm.smin.i32(i32 %conv3, i32 %conv2)
  %conv6 = trunc i32 %cond to i16
  %4 = cmpxchg ptr %1, i16 %cmp.0, i16 %conv6 seq_cst seq_cst, align 2
  %5 = extractvalue { i16, i1 } %4, 0
  %cmp10.not = extractvalue { i16, i1 } %4, 1
  br i1 %cmp10.not, label %do.body12, label %do.body, !llvm.loop !97

do.body12:                                        ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_smin_fetchl_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !98
  fence seq_cst
  %5 = load atomic i32, ptr %3 monotonic, align 4
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %ldn.0.i = phi i32 [ %5, %atomic_mmu_lookup.exit.i ], [ %9, %do.body.i ]
  %6 = tail call i32 @llvm.bswap.i32(i32 %ldn.0.i)
  %cond.i = tail call i32 @llvm.smin.i32(i32 %6, i32 %val)
  %7 = tail call i32 @llvm.bswap.i32(i32 %cond.i)
  %8 = cmpxchg ptr %3, i32 %ldn.0.i, i32 %7 seq_cst seq_cst, align 4
  %9 = extractvalue { i32, i1 } %8, 0
  %cmp3.not.i = icmp eq i32 %ldn.0.i, %9
  br i1 %cmp3.not.i, label %cpu_atomic_smin_fetchl_be_mmu.exit, label %do.body.i, !llvm.loop !99

cpu_atomic_smin_fetchl_be_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_smin_fetchl_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !98
  fence seq_cst
  %3 = load atomic i32, ptr %1 monotonic, align 4
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i32 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i32 @llvm.bswap.i32(i32 %ldn.0)
  %cond = tail call i32 @llvm.smin.i32(i32 %4, i32 %xval)
  %5 = tail call i32 @llvm.bswap.i32(i32 %cond)
  %6 = cmpxchg ptr %1, i32 %ldn.0, i32 %5 seq_cst seq_cst, align 4
  %7 = extractvalue { i32, i1 } %6, 0
  %cmp3.not = icmp eq i32 %ldn.0, %7
  br i1 %cmp3.not, label %do.body4, label %do.body, !llvm.loop !99

do.body4:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_smin_fetchl_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !100
  fence seq_cst
  %5 = load atomic i32, ptr %3 monotonic, align 4
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i32 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %cond.i = tail call i32 @llvm.smin.i32(i32 %cmp.0.i, i32 %val)
  %6 = cmpxchg ptr %3, i32 %cmp.0.i, i32 %cond.i seq_cst seq_cst, align 4
  %7 = extractvalue { i32, i1 } %6, 0
  %cmp4.not.i = extractvalue { i32, i1 } %6, 1
  br i1 %cmp4.not.i, label %cpu_atomic_smin_fetchl_le_mmu.exit, label %do.body.i, !llvm.loop !101

cpu_atomic_smin_fetchl_le_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_smin_fetchl_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !100
  fence seq_cst
  %3 = load atomic i32, ptr %1 monotonic, align 4
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i32 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %cond = tail call i32 @llvm.smin.i32(i32 %cmp.0, i32 %xval)
  %4 = cmpxchg ptr %1, i32 %cmp.0, i32 %cond seq_cst seq_cst, align 4
  %5 = extractvalue { i32, i1 } %4, 0
  %cmp4.not = extractvalue { i32, i1 } %4, 1
  br i1 %cmp4.not, label %do.body5, label %do.body, !llvm.loop !101

do.body5:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_smin_fetchq_be(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !102
  fence seq_cst
  %5 = load atomic i64, ptr %3 monotonic, align 8
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %ldn.0.i = phi i64 [ %5, %atomic_mmu_lookup.exit.i ], [ %9, %do.body.i ]
  %6 = tail call i64 @llvm.bswap.i64(i64 %ldn.0.i)
  %cond.i = tail call i64 @llvm.smin.i64(i64 %6, i64 %val)
  %7 = tail call i64 @llvm.bswap.i64(i64 %cond.i)
  %8 = cmpxchg ptr %3, i64 %ldn.0.i, i64 %7 seq_cst seq_cst, align 8
  %9 = extractvalue { i64, i1 } %8, 0
  %cmp3.not.i = icmp eq i64 %ldn.0.i, %9
  br i1 %cmp3.not.i, label %cpu_atomic_smin_fetchq_be_mmu.exit, label %do.body.i, !llvm.loop !103

cpu_atomic_smin_fetchq_be_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %cond.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_smin_fetchq_be_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !102
  fence seq_cst
  %3 = load atomic i64, ptr %1 monotonic, align 8
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i64 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i64 @llvm.bswap.i64(i64 %ldn.0)
  %cond = tail call i64 @llvm.smin.i64(i64 %4, i64 %xval)
  %5 = tail call i64 @llvm.bswap.i64(i64 %cond)
  %6 = cmpxchg ptr %1, i64 %ldn.0, i64 %5 seq_cst seq_cst, align 8
  %7 = extractvalue { i64, i1 } %6, 0
  %cmp3.not = icmp eq i64 %ldn.0, %7
  br i1 %cmp3.not, label %do.body4, label %do.body, !llvm.loop !103

do.body4:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_smin_fetchq_le(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !104
  fence seq_cst
  %5 = load atomic i64, ptr %3 monotonic, align 8
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i64 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %cond.i = tail call i64 @llvm.smin.i64(i64 %cmp.0.i, i64 %val)
  %6 = cmpxchg ptr %3, i64 %cmp.0.i, i64 %cond.i seq_cst seq_cst, align 8
  %7 = extractvalue { i64, i1 } %6, 0
  %cmp4.not.i = extractvalue { i64, i1 } %6, 1
  br i1 %cmp4.not.i, label %cpu_atomic_smin_fetchq_le_mmu.exit, label %do.body.i, !llvm.loop !105

cpu_atomic_smin_fetchq_le_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %cond.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_smin_fetchq_le_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !104
  fence seq_cst
  %3 = load atomic i64, ptr %1 monotonic, align 8
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i64 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %cond = tail call i64 @llvm.smin.i64(i64 %cmp.0, i64 %xval)
  %4 = cmpxchg ptr %1, i64 %cmp.0, i64 %cond seq_cst seq_cst, align 8
  %5 = extractvalue { i64, i1 } %4, 0
  %cmp4.not = extractvalue { i64, i1 } %4, 1
  br i1 %cmp4.not, label %do.body5, label %do.body, !llvm.loop !105

do.body5:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_umin_fetchb(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %get_alignment_bits.exit.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !106
  fence seq_cst
  %5 = load atomic i8, ptr %3 monotonic, align 1
  %conv3.i = and i32 %val, 255
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i8 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %conv2.i = zext i8 %cmp.0.i to i32
  %cond.i = tail call i32 @llvm.umin.i32(i32 %conv3.i, i32 %conv2.i)
  %conv6.i = trunc i32 %cond.i to i8
  %6 = cmpxchg ptr %3, i8 %cmp.0.i, i8 %conv6.i seq_cst seq_cst, align 1
  %7 = extractvalue { i8, i1 } %6, 0
  %cmp10.not.i = extractvalue { i8, i1 } %6, 1
  br i1 %cmp10.not.i, label %cpu_atomic_umin_fetchb_mmu.exit, label %do.body.i, !llvm.loop !107

cpu_atomic_umin_fetchb_mmu.exit:                  ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_umin_fetchb_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %atomic_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !106
  fence seq_cst
  %3 = load atomic i8, ptr %1 monotonic, align 1
  %conv3 = and i32 %xval, 255
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i8 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %conv2 = zext i8 %cmp.0 to i32
  %cond = tail call i32 @llvm.umin.i32(i32 %conv3, i32 %conv2)
  %conv6 = trunc i32 %cond to i8
  %4 = cmpxchg ptr %1, i8 %cmp.0, i8 %conv6 seq_cst seq_cst, align 1
  %5 = extractvalue { i8, i1 } %4, 0
  %cmp10.not = extractvalue { i8, i1 } %4, 1
  br i1 %cmp10.not, label %do.body12, label %do.body, !llvm.loop !107

do.body12:                                        ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_umin_fetchw_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %call = tail call i32 @cpu_atomic_umin_fetchw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %1), !range !22
  ret i32 %call
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_umin_fetchw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !108
  fence seq_cst
  %3 = load atomic i16, ptr %1 monotonic, align 2
  %conv3 = and i32 %xval, 65535
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i16 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i16 @llvm.bswap.i16(i16 %ldn.0)
  %conv2 = zext i16 %4 to i32
  %cond = tail call i32 @llvm.umin.i32(i32 %conv3, i32 %conv2)
  %conv5 = trunc i32 %cond to i16
  %5 = tail call i16 @llvm.bswap.i16(i16 %conv5)
  %6 = cmpxchg ptr %1, i16 %ldn.0, i16 %5 seq_cst seq_cst, align 2
  %7 = extractvalue { i16, i1 } %6, 0
  %cmp9.not = icmp eq i16 %ldn.0, %7
  br i1 %cmp9.not, label %do.body11, label %do.body, !llvm.loop !109

do.body11:                                        ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_umin_fetchw_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 1
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !110
  fence seq_cst
  %5 = load atomic i16, ptr %3 monotonic, align 2
  %conv3.i = and i32 %val, 65535
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i16 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %conv2.i = zext i16 %cmp.0.i to i32
  %cond.i = tail call i32 @llvm.umin.i32(i32 %conv3.i, i32 %conv2.i)
  %conv6.i = trunc i32 %cond.i to i16
  %6 = cmpxchg ptr %3, i16 %cmp.0.i, i16 %conv6.i seq_cst seq_cst, align 2
  %7 = extractvalue { i16, i1 } %6, 0
  %cmp10.not.i = extractvalue { i16, i1 } %6, 1
  br i1 %cmp10.not.i, label %cpu_atomic_umin_fetchw_le_mmu.exit, label %do.body.i, !llvm.loop !111

cpu_atomic_umin_fetchw_le_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_umin_fetchw_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !110
  fence seq_cst
  %3 = load atomic i16, ptr %1 monotonic, align 2
  %conv3 = and i32 %xval, 65535
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i16 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %conv2 = zext i16 %cmp.0 to i32
  %cond = tail call i32 @llvm.umin.i32(i32 %conv3, i32 %conv2)
  %conv6 = trunc i32 %cond to i16
  %4 = cmpxchg ptr %1, i16 %cmp.0, i16 %conv6 seq_cst seq_cst, align 2
  %5 = extractvalue { i16, i1 } %4, 0
  %cmp10.not = extractvalue { i16, i1 } %4, 1
  br i1 %cmp10.not, label %do.body12, label %do.body, !llvm.loop !111

do.body12:                                        ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_umin_fetchl_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !112
  fence seq_cst
  %5 = load atomic i32, ptr %3 monotonic, align 4
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %ldn.0.i = phi i32 [ %5, %atomic_mmu_lookup.exit.i ], [ %9, %do.body.i ]
  %6 = tail call i32 @llvm.bswap.i32(i32 %ldn.0.i)
  %cond.i = tail call i32 @llvm.umin.i32(i32 %6, i32 %val)
  %7 = tail call i32 @llvm.bswap.i32(i32 %cond.i)
  %8 = cmpxchg ptr %3, i32 %ldn.0.i, i32 %7 seq_cst seq_cst, align 4
  %9 = extractvalue { i32, i1 } %8, 0
  %cmp3.not.i = icmp eq i32 %ldn.0.i, %9
  br i1 %cmp3.not.i, label %cpu_atomic_umin_fetchl_be_mmu.exit, label %do.body.i, !llvm.loop !113

cpu_atomic_umin_fetchl_be_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_umin_fetchl_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !112
  fence seq_cst
  %3 = load atomic i32, ptr %1 monotonic, align 4
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i32 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i32 @llvm.bswap.i32(i32 %ldn.0)
  %cond = tail call i32 @llvm.umin.i32(i32 %4, i32 %xval)
  %5 = tail call i32 @llvm.bswap.i32(i32 %cond)
  %6 = cmpxchg ptr %1, i32 %ldn.0, i32 %5 seq_cst seq_cst, align 4
  %7 = extractvalue { i32, i1 } %6, 0
  %cmp3.not = icmp eq i32 %ldn.0, %7
  br i1 %cmp3.not, label %do.body4, label %do.body, !llvm.loop !113

do.body4:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_umin_fetchl_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !114
  fence seq_cst
  %5 = load atomic i32, ptr %3 monotonic, align 4
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i32 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %cond.i = tail call i32 @llvm.umin.i32(i32 %cmp.0.i, i32 %val)
  %6 = cmpxchg ptr %3, i32 %cmp.0.i, i32 %cond.i seq_cst seq_cst, align 4
  %7 = extractvalue { i32, i1 } %6, 0
  %cmp4.not.i = extractvalue { i32, i1 } %6, 1
  br i1 %cmp4.not.i, label %cpu_atomic_umin_fetchl_le_mmu.exit, label %do.body.i, !llvm.loop !115

cpu_atomic_umin_fetchl_le_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_umin_fetchl_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !114
  fence seq_cst
  %3 = load atomic i32, ptr %1 monotonic, align 4
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i32 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %cond = tail call i32 @llvm.umin.i32(i32 %cmp.0, i32 %xval)
  %4 = cmpxchg ptr %1, i32 %cmp.0, i32 %cond seq_cst seq_cst, align 4
  %5 = extractvalue { i32, i1 } %4, 0
  %cmp4.not = extractvalue { i32, i1 } %4, 1
  br i1 %cmp4.not, label %do.body5, label %do.body, !llvm.loop !115

do.body5:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_umin_fetchq_be(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !116
  fence seq_cst
  %5 = load atomic i64, ptr %3 monotonic, align 8
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %ldn.0.i = phi i64 [ %5, %atomic_mmu_lookup.exit.i ], [ %9, %do.body.i ]
  %6 = tail call i64 @llvm.bswap.i64(i64 %ldn.0.i)
  %cond.i = tail call i64 @llvm.umin.i64(i64 %6, i64 %val)
  %7 = tail call i64 @llvm.bswap.i64(i64 %cond.i)
  %8 = cmpxchg ptr %3, i64 %ldn.0.i, i64 %7 seq_cst seq_cst, align 8
  %9 = extractvalue { i64, i1 } %8, 0
  %cmp3.not.i = icmp eq i64 %ldn.0.i, %9
  br i1 %cmp3.not.i, label %cpu_atomic_umin_fetchq_be_mmu.exit, label %do.body.i, !llvm.loop !117

cpu_atomic_umin_fetchq_be_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %cond.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_umin_fetchq_be_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !116
  fence seq_cst
  %3 = load atomic i64, ptr %1 monotonic, align 8
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i64 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i64 @llvm.bswap.i64(i64 %ldn.0)
  %cond = tail call i64 @llvm.umin.i64(i64 %4, i64 %xval)
  %5 = tail call i64 @llvm.bswap.i64(i64 %cond)
  %6 = cmpxchg ptr %1, i64 %ldn.0, i64 %5 seq_cst seq_cst, align 8
  %7 = extractvalue { i64, i1 } %6, 0
  %cmp3.not = icmp eq i64 %ldn.0, %7
  br i1 %cmp3.not, label %do.body4, label %do.body, !llvm.loop !117

do.body4:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_umin_fetchq_le(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !118
  fence seq_cst
  %5 = load atomic i64, ptr %3 monotonic, align 8
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i64 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %cond.i = tail call i64 @llvm.umin.i64(i64 %cmp.0.i, i64 %val)
  %6 = cmpxchg ptr %3, i64 %cmp.0.i, i64 %cond.i seq_cst seq_cst, align 8
  %7 = extractvalue { i64, i1 } %6, 0
  %cmp4.not.i = extractvalue { i64, i1 } %6, 1
  br i1 %cmp4.not.i, label %cpu_atomic_umin_fetchq_le_mmu.exit, label %do.body.i, !llvm.loop !119

cpu_atomic_umin_fetchq_le_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %cond.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_umin_fetchq_le_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !118
  fence seq_cst
  %3 = load atomic i64, ptr %1 monotonic, align 8
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i64 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %cond = tail call i64 @llvm.umin.i64(i64 %cmp.0, i64 %xval)
  %4 = cmpxchg ptr %1, i64 %cmp.0, i64 %cond seq_cst seq_cst, align 8
  %5 = extractvalue { i64, i1 } %4, 0
  %cmp4.not = extractvalue { i64, i1 } %4, 1
  br i1 %cmp4.not, label %do.body5, label %do.body, !llvm.loop !119

do.body5:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_smax_fetchb(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %get_alignment_bits.exit.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !120
  fence seq_cst
  %5 = load atomic i8, ptr %3 monotonic, align 1
  %sext.i = shl i32 %val, 24
  %conv3.i = ashr exact i32 %sext.i, 24
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i8 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %conv2.i = sext i8 %cmp.0.i to i32
  %cond.i = tail call i32 @llvm.smax.i32(i32 %conv3.i, i32 %conv2.i)
  %conv6.i = trunc i32 %cond.i to i8
  %6 = cmpxchg ptr %3, i8 %cmp.0.i, i8 %conv6.i seq_cst seq_cst, align 1
  %7 = extractvalue { i8, i1 } %6, 0
  %cmp10.not.i = extractvalue { i8, i1 } %6, 1
  br i1 %cmp10.not.i, label %cpu_atomic_smax_fetchb_mmu.exit, label %do.body.i, !llvm.loop !121

cpu_atomic_smax_fetchb_mmu.exit:                  ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_smax_fetchb_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %atomic_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !120
  fence seq_cst
  %3 = load atomic i8, ptr %1 monotonic, align 1
  %sext = shl i32 %xval, 24
  %conv3 = ashr exact i32 %sext, 24
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i8 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %conv2 = sext i8 %cmp.0 to i32
  %cond = tail call i32 @llvm.smax.i32(i32 %conv3, i32 %conv2)
  %conv6 = trunc i32 %cond to i8
  %4 = cmpxchg ptr %1, i8 %cmp.0, i8 %conv6 seq_cst seq_cst, align 1
  %5 = extractvalue { i8, i1 } %4, 0
  %cmp10.not = extractvalue { i8, i1 } %4, 1
  br i1 %cmp10.not, label %do.body12, label %do.body, !llvm.loop !121

do.body12:                                        ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_smax_fetchw_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %call = tail call i32 @cpu_atomic_smax_fetchw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %1), !range !31
  ret i32 %call
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_smax_fetchw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !122
  fence seq_cst
  %3 = load atomic i16, ptr %1 monotonic, align 2
  %sext = shl i32 %xval, 16
  %conv3 = ashr exact i32 %sext, 16
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i16 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i16 @llvm.bswap.i16(i16 %ldn.0)
  %conv2 = sext i16 %4 to i32
  %cond = tail call i32 @llvm.smax.i32(i32 %conv3, i32 %conv2)
  %conv5 = trunc i32 %cond to i16
  %5 = tail call i16 @llvm.bswap.i16(i16 %conv5)
  %6 = cmpxchg ptr %1, i16 %ldn.0, i16 %5 seq_cst seq_cst, align 2
  %7 = extractvalue { i16, i1 } %6, 0
  %cmp9.not = icmp eq i16 %ldn.0, %7
  br i1 %cmp9.not, label %do.body11, label %do.body, !llvm.loop !123

do.body11:                                        ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_smax_fetchw_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 1
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !124
  fence seq_cst
  %5 = load atomic i16, ptr %3 monotonic, align 2
  %sext.i = shl i32 %val, 16
  %conv3.i = ashr exact i32 %sext.i, 16
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i16 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %conv2.i = sext i16 %cmp.0.i to i32
  %cond.i = tail call i32 @llvm.smax.i32(i32 %conv3.i, i32 %conv2.i)
  %conv6.i = trunc i32 %cond.i to i16
  %6 = cmpxchg ptr %3, i16 %cmp.0.i, i16 %conv6.i seq_cst seq_cst, align 2
  %7 = extractvalue { i16, i1 } %6, 0
  %cmp10.not.i = extractvalue { i16, i1 } %6, 1
  br i1 %cmp10.not.i, label %cpu_atomic_smax_fetchw_le_mmu.exit, label %do.body.i, !llvm.loop !125

cpu_atomic_smax_fetchw_le_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_smax_fetchw_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !124
  fence seq_cst
  %3 = load atomic i16, ptr %1 monotonic, align 2
  %sext = shl i32 %xval, 16
  %conv3 = ashr exact i32 %sext, 16
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i16 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %conv2 = sext i16 %cmp.0 to i32
  %cond = tail call i32 @llvm.smax.i32(i32 %conv3, i32 %conv2)
  %conv6 = trunc i32 %cond to i16
  %4 = cmpxchg ptr %1, i16 %cmp.0, i16 %conv6 seq_cst seq_cst, align 2
  %5 = extractvalue { i16, i1 } %4, 0
  %cmp10.not = extractvalue { i16, i1 } %4, 1
  br i1 %cmp10.not, label %do.body12, label %do.body, !llvm.loop !125

do.body12:                                        ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_smax_fetchl_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !126
  fence seq_cst
  %5 = load atomic i32, ptr %3 monotonic, align 4
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %ldn.0.i = phi i32 [ %5, %atomic_mmu_lookup.exit.i ], [ %9, %do.body.i ]
  %6 = tail call i32 @llvm.bswap.i32(i32 %ldn.0.i)
  %cond.i = tail call i32 @llvm.smax.i32(i32 %6, i32 %val)
  %7 = tail call i32 @llvm.bswap.i32(i32 %cond.i)
  %8 = cmpxchg ptr %3, i32 %ldn.0.i, i32 %7 seq_cst seq_cst, align 4
  %9 = extractvalue { i32, i1 } %8, 0
  %cmp3.not.i = icmp eq i32 %ldn.0.i, %9
  br i1 %cmp3.not.i, label %cpu_atomic_smax_fetchl_be_mmu.exit, label %do.body.i, !llvm.loop !127

cpu_atomic_smax_fetchl_be_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_smax_fetchl_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !126
  fence seq_cst
  %3 = load atomic i32, ptr %1 monotonic, align 4
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i32 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i32 @llvm.bswap.i32(i32 %ldn.0)
  %cond = tail call i32 @llvm.smax.i32(i32 %4, i32 %xval)
  %5 = tail call i32 @llvm.bswap.i32(i32 %cond)
  %6 = cmpxchg ptr %1, i32 %ldn.0, i32 %5 seq_cst seq_cst, align 4
  %7 = extractvalue { i32, i1 } %6, 0
  %cmp3.not = icmp eq i32 %ldn.0, %7
  br i1 %cmp3.not, label %do.body4, label %do.body, !llvm.loop !127

do.body4:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_smax_fetchl_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !128
  fence seq_cst
  %5 = load atomic i32, ptr %3 monotonic, align 4
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i32 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %cond.i = tail call i32 @llvm.smax.i32(i32 %cmp.0.i, i32 %val)
  %6 = cmpxchg ptr %3, i32 %cmp.0.i, i32 %cond.i seq_cst seq_cst, align 4
  %7 = extractvalue { i32, i1 } %6, 0
  %cmp4.not.i = extractvalue { i32, i1 } %6, 1
  br i1 %cmp4.not.i, label %cpu_atomic_smax_fetchl_le_mmu.exit, label %do.body.i, !llvm.loop !129

cpu_atomic_smax_fetchl_le_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_smax_fetchl_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !128
  fence seq_cst
  %3 = load atomic i32, ptr %1 monotonic, align 4
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i32 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %cond = tail call i32 @llvm.smax.i32(i32 %cmp.0, i32 %xval)
  %4 = cmpxchg ptr %1, i32 %cmp.0, i32 %cond seq_cst seq_cst, align 4
  %5 = extractvalue { i32, i1 } %4, 0
  %cmp4.not = extractvalue { i32, i1 } %4, 1
  br i1 %cmp4.not, label %do.body5, label %do.body, !llvm.loop !129

do.body5:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_smax_fetchq_be(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !130
  fence seq_cst
  %5 = load atomic i64, ptr %3 monotonic, align 8
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %ldn.0.i = phi i64 [ %5, %atomic_mmu_lookup.exit.i ], [ %9, %do.body.i ]
  %6 = tail call i64 @llvm.bswap.i64(i64 %ldn.0.i)
  %cond.i = tail call i64 @llvm.smax.i64(i64 %6, i64 %val)
  %7 = tail call i64 @llvm.bswap.i64(i64 %cond.i)
  %8 = cmpxchg ptr %3, i64 %ldn.0.i, i64 %7 seq_cst seq_cst, align 8
  %9 = extractvalue { i64, i1 } %8, 0
  %cmp3.not.i = icmp eq i64 %ldn.0.i, %9
  br i1 %cmp3.not.i, label %cpu_atomic_smax_fetchq_be_mmu.exit, label %do.body.i, !llvm.loop !131

cpu_atomic_smax_fetchq_be_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %cond.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_smax_fetchq_be_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !130
  fence seq_cst
  %3 = load atomic i64, ptr %1 monotonic, align 8
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i64 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i64 @llvm.bswap.i64(i64 %ldn.0)
  %cond = tail call i64 @llvm.smax.i64(i64 %4, i64 %xval)
  %5 = tail call i64 @llvm.bswap.i64(i64 %cond)
  %6 = cmpxchg ptr %1, i64 %ldn.0, i64 %5 seq_cst seq_cst, align 8
  %7 = extractvalue { i64, i1 } %6, 0
  %cmp3.not = icmp eq i64 %ldn.0, %7
  br i1 %cmp3.not, label %do.body4, label %do.body, !llvm.loop !131

do.body4:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_smax_fetchq_le(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !132
  fence seq_cst
  %5 = load atomic i64, ptr %3 monotonic, align 8
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i64 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %cond.i = tail call i64 @llvm.smax.i64(i64 %cmp.0.i, i64 %val)
  %6 = cmpxchg ptr %3, i64 %cmp.0.i, i64 %cond.i seq_cst seq_cst, align 8
  %7 = extractvalue { i64, i1 } %6, 0
  %cmp4.not.i = extractvalue { i64, i1 } %6, 1
  br i1 %cmp4.not.i, label %cpu_atomic_smax_fetchq_le_mmu.exit, label %do.body.i, !llvm.loop !133

cpu_atomic_smax_fetchq_le_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %cond.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_smax_fetchq_le_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !132
  fence seq_cst
  %3 = load atomic i64, ptr %1 monotonic, align 8
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i64 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %cond = tail call i64 @llvm.smax.i64(i64 %cmp.0, i64 %xval)
  %4 = cmpxchg ptr %1, i64 %cmp.0, i64 %cond seq_cst seq_cst, align 8
  %5 = extractvalue { i64, i1 } %4, 0
  %cmp4.not = extractvalue { i64, i1 } %4, 1
  br i1 %cmp4.not, label %do.body5, label %do.body, !llvm.loop !133

do.body5:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_umax_fetchb(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %get_alignment_bits.exit.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !134
  fence seq_cst
  %5 = load atomic i8, ptr %3 monotonic, align 1
  %conv3.i = and i32 %val, 255
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i8 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %conv2.i = zext i8 %cmp.0.i to i32
  %cond.i = tail call i32 @llvm.umax.i32(i32 %conv3.i, i32 %conv2.i)
  %conv6.i = trunc i32 %cond.i to i8
  %6 = cmpxchg ptr %3, i8 %cmp.0.i, i8 %conv6.i seq_cst seq_cst, align 1
  %7 = extractvalue { i8, i1 } %6, 0
  %cmp10.not.i = extractvalue { i8, i1 } %6, 1
  br i1 %cmp10.not.i, label %cpu_atomic_umax_fetchb_mmu.exit, label %do.body.i, !llvm.loop !135

cpu_atomic_umax_fetchb_mmu.exit:                  ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_umax_fetchb_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %atomic_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !134
  fence seq_cst
  %3 = load atomic i8, ptr %1 monotonic, align 1
  %conv3 = and i32 %xval, 255
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i8 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %conv2 = zext i8 %cmp.0 to i32
  %cond = tail call i32 @llvm.umax.i32(i32 %conv3, i32 %conv2)
  %conv6 = trunc i32 %cond to i8
  %4 = cmpxchg ptr %1, i8 %cmp.0, i8 %conv6 seq_cst seq_cst, align 1
  %5 = extractvalue { i8, i1 } %4, 0
  %cmp10.not = extractvalue { i8, i1 } %4, 1
  br i1 %cmp10.not, label %do.body12, label %do.body, !llvm.loop !135

do.body12:                                        ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_umax_fetchw_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %call = tail call i32 @cpu_atomic_umax_fetchw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %1), !range !22
  ret i32 %call
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_umax_fetchw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !136
  fence seq_cst
  %3 = load atomic i16, ptr %1 monotonic, align 2
  %conv3 = and i32 %xval, 65535
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i16 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i16 @llvm.bswap.i16(i16 %ldn.0)
  %conv2 = zext i16 %4 to i32
  %cond = tail call i32 @llvm.umax.i32(i32 %conv3, i32 %conv2)
  %conv5 = trunc i32 %cond to i16
  %5 = tail call i16 @llvm.bswap.i16(i16 %conv5)
  %6 = cmpxchg ptr %1, i16 %ldn.0, i16 %5 seq_cst seq_cst, align 2
  %7 = extractvalue { i16, i1 } %6, 0
  %cmp9.not = icmp eq i16 %ldn.0, %7
  br i1 %cmp9.not, label %do.body11, label %do.body, !llvm.loop !137

do.body11:                                        ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_umax_fetchw_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 1
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !138
  fence seq_cst
  %5 = load atomic i16, ptr %3 monotonic, align 2
  %conv3.i = and i32 %val, 65535
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i16 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %conv2.i = zext i16 %cmp.0.i to i32
  %cond.i = tail call i32 @llvm.umax.i32(i32 %conv3.i, i32 %conv2.i)
  %conv6.i = trunc i32 %cond.i to i16
  %6 = cmpxchg ptr %3, i16 %cmp.0.i, i16 %conv6.i seq_cst seq_cst, align 2
  %7 = extractvalue { i16, i1 } %6, 0
  %cmp10.not.i = extractvalue { i16, i1 } %6, 1
  br i1 %cmp10.not.i, label %cpu_atomic_umax_fetchw_le_mmu.exit, label %do.body.i, !llvm.loop !139

cpu_atomic_umax_fetchw_le_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_umax_fetchw_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !138
  fence seq_cst
  %3 = load atomic i16, ptr %1 monotonic, align 2
  %conv3 = and i32 %xval, 65535
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i16 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %conv2 = zext i16 %cmp.0 to i32
  %cond = tail call i32 @llvm.umax.i32(i32 %conv3, i32 %conv2)
  %conv6 = trunc i32 %cond to i16
  %4 = cmpxchg ptr %1, i16 %cmp.0, i16 %conv6 seq_cst seq_cst, align 2
  %5 = extractvalue { i16, i1 } %4, 0
  %cmp10.not = extractvalue { i16, i1 } %4, 1
  br i1 %cmp10.not, label %do.body12, label %do.body, !llvm.loop !139

do.body12:                                        ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_umax_fetchl_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !140
  fence seq_cst
  %5 = load atomic i32, ptr %3 monotonic, align 4
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %ldn.0.i = phi i32 [ %5, %atomic_mmu_lookup.exit.i ], [ %9, %do.body.i ]
  %6 = tail call i32 @llvm.bswap.i32(i32 %ldn.0.i)
  %cond.i = tail call i32 @llvm.umax.i32(i32 %6, i32 %val)
  %7 = tail call i32 @llvm.bswap.i32(i32 %cond.i)
  %8 = cmpxchg ptr %3, i32 %ldn.0.i, i32 %7 seq_cst seq_cst, align 4
  %9 = extractvalue { i32, i1 } %8, 0
  %cmp3.not.i = icmp eq i32 %ldn.0.i, %9
  br i1 %cmp3.not.i, label %cpu_atomic_umax_fetchl_be_mmu.exit, label %do.body.i, !llvm.loop !141

cpu_atomic_umax_fetchl_be_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_umax_fetchl_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !140
  fence seq_cst
  %3 = load atomic i32, ptr %1 monotonic, align 4
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i32 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i32 @llvm.bswap.i32(i32 %ldn.0)
  %cond = tail call i32 @llvm.umax.i32(i32 %4, i32 %xval)
  %5 = tail call i32 @llvm.bswap.i32(i32 %cond)
  %6 = cmpxchg ptr %1, i32 %ldn.0, i32 %5 seq_cst seq_cst, align 4
  %7 = extractvalue { i32, i1 } %6, 0
  %cmp3.not = icmp eq i32 %ldn.0, %7
  br i1 %cmp3.not, label %do.body4, label %do.body, !llvm.loop !141

do.body4:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_umax_fetchl_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !142
  fence seq_cst
  %5 = load atomic i32, ptr %3 monotonic, align 4
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i32 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %cond.i = tail call i32 @llvm.umax.i32(i32 %cmp.0.i, i32 %val)
  %6 = cmpxchg ptr %3, i32 %cmp.0.i, i32 %cond.i seq_cst seq_cst, align 4
  %7 = extractvalue { i32, i1 } %6, 0
  %cmp4.not.i = extractvalue { i32, i1 } %6, 1
  br i1 %cmp4.not.i, label %cpu_atomic_umax_fetchl_le_mmu.exit, label %do.body.i, !llvm.loop !143

cpu_atomic_umax_fetchl_le_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_umax_fetchl_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !142
  fence seq_cst
  %3 = load atomic i32, ptr %1 monotonic, align 4
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i32 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %cond = tail call i32 @llvm.umax.i32(i32 %cmp.0, i32 %xval)
  %4 = cmpxchg ptr %1, i32 %cmp.0, i32 %cond seq_cst seq_cst, align 4
  %5 = extractvalue { i32, i1 } %4, 0
  %cmp4.not = extractvalue { i32, i1 } %4, 1
  br i1 %cmp4.not, label %do.body5, label %do.body, !llvm.loop !143

do.body5:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_umax_fetchq_be(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !144
  fence seq_cst
  %5 = load atomic i64, ptr %3 monotonic, align 8
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %ldn.0.i = phi i64 [ %5, %atomic_mmu_lookup.exit.i ], [ %9, %do.body.i ]
  %6 = tail call i64 @llvm.bswap.i64(i64 %ldn.0.i)
  %cond.i = tail call i64 @llvm.umax.i64(i64 %6, i64 %val)
  %7 = tail call i64 @llvm.bswap.i64(i64 %cond.i)
  %8 = cmpxchg ptr %3, i64 %ldn.0.i, i64 %7 seq_cst seq_cst, align 8
  %9 = extractvalue { i64, i1 } %8, 0
  %cmp3.not.i = icmp eq i64 %ldn.0.i, %9
  br i1 %cmp3.not.i, label %cpu_atomic_umax_fetchq_be_mmu.exit, label %do.body.i, !llvm.loop !145

cpu_atomic_umax_fetchq_be_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %cond.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_umax_fetchq_be_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !144
  fence seq_cst
  %3 = load atomic i64, ptr %1 monotonic, align 8
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %ldn.0 = phi i64 [ %3, %atomic_mmu_lookup.exit ], [ %7, %do.body ]
  %4 = tail call i64 @llvm.bswap.i64(i64 %ldn.0)
  %cond = tail call i64 @llvm.umax.i64(i64 %4, i64 %xval)
  %5 = tail call i64 @llvm.bswap.i64(i64 %cond)
  %6 = cmpxchg ptr %1, i64 %ldn.0, i64 %5 seq_cst seq_cst, align 8
  %7 = extractvalue { i64, i1 } %6, 0
  %cmp3.not = icmp eq i64 %ldn.0, %7
  br i1 %cmp3.not, label %do.body4, label %do.body, !llvm.loop !145

do.body4:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_umax_fetchq_le(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %atomic_mmu_lookup.exit.i, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

atomic_mmu_lookup.exit.i:                         ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !146
  fence seq_cst
  %5 = load atomic i64, ptr %3 monotonic, align 8
  br label %do.body.i

do.body.i:                                        ; preds = %do.body.i, %atomic_mmu_lookup.exit.i
  %cmp.0.i = phi i64 [ %5, %atomic_mmu_lookup.exit.i ], [ %7, %do.body.i ]
  %cond.i = tail call i64 @llvm.umax.i64(i64 %cmp.0.i, i64 %val)
  %6 = cmpxchg ptr %3, i64 %cmp.0.i, i64 %cond.i seq_cst seq_cst, align 8
  %7 = extractvalue { i64, i1 } %6, 0
  %cmp4.not.i = extractvalue { i64, i1 } %6, 1
  br i1 %cmp4.not.i, label %cpu_atomic_umax_fetchq_le_mmu.exit, label %do.body.i, !llvm.loop !147

cpu_atomic_umax_fetchq_le_mmu.exit:               ; preds = %do.body.i
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %cond.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_umax_fetchq_le_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %xval, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  tail call void asm sideeffect "", "~{memory},~{dirflag},~{fpsr},~{flags}"() #16, !srcloc !146
  fence seq_cst
  %3 = load atomic i64, ptr %1 monotonic, align 8
  br label %do.body

do.body:                                          ; preds = %do.body, %atomic_mmu_lookup.exit
  %cmp.0 = phi i64 [ %3, %atomic_mmu_lookup.exit ], [ %5, %do.body ]
  %cond = tail call i64 @llvm.umax.i64(i64 %cmp.0, i64 %xval)
  %4 = cmpxchg ptr %1, i64 %cmp.0, i64 %cond seq_cst seq_cst, align 8
  %5 = extractvalue { i64, i1 } %4, 0
  %cmp4.not = extractvalue { i64, i1 } %4, 1
  br i1 %cmp4.not, label %do.body5, label %do.body, !llvm.loop !147

do.body5:                                         ; preds = %do.body
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %cond
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_xchgb(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %cpu_atomic_xchgb_mmu.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

cpu_atomic_xchgb_mmu.exit:                        ; preds = %get_alignment_bits.exit.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %val to i8
  %5 = atomicrmw xchg ptr %3, i8 %conv.i seq_cst, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2.i = zext i8 %5 to i32
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_xchgb_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %atomic_mmu_lookup.exit, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %get_alignment_bits.exit.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %val to i8
  %3 = atomicrmw xchg ptr %1, i8 %conv seq_cst, align 1
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2 = zext i8 %3 to i32
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_xchgw_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 1
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_xchgw_be_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_xchgw_be_mmu.exit:                     ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %val to i16
  %5 = tail call i16 @llvm.bswap.i16(i16 %conv.i)
  %6 = atomicrmw xchg ptr %3, i16 %5 seq_cst, align 2
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %7 = tail call i16 @llvm.bswap.i16(i16 %6)
  %conv4.i = zext i16 %7 to i32
  ret i32 %conv4.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_xchgw_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %val to i16
  %3 = tail call i16 @llvm.bswap.i16(i16 %conv)
  %4 = atomicrmw xchg ptr %1, i16 %3 seq_cst, align 2
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %5 = tail call i16 @llvm.bswap.i16(i16 %4)
  %conv4 = zext i16 %5 to i32
  ret i32 %conv4
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_xchgw_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 1
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_xchgw_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_xchgw_le_mmu.exit:                     ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %conv.i = trunc i32 %val to i16
  %5 = atomicrmw xchg ptr %3, i16 %conv.i seq_cst, align 2
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2.i = zext i16 %5 to i32
  ret i32 %conv2.i
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_xchgw_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 1
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %conv = trunc i32 %val to i16
  %3 = atomicrmw xchg ptr %1, i16 %conv seq_cst, align 2
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %conv2 = zext i16 %3 to i32
  ret i32 %conv2
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_xchgl_be(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_xchgl_be_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_xchgl_be_mmu.exit:                     ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = tail call i32 @llvm.bswap.i32(i32 %val)
  %6 = atomicrmw xchg ptr %3, i32 %5 seq_cst, align 4
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %7 = tail call i32 @llvm.bswap.i32(i32 %6)
  ret i32 %7
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_xchgl_be_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = tail call i32 @llvm.bswap.i32(i32 %val)
  %4 = atomicrmw xchg ptr %1, i32 %3 seq_cst, align 4
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %5 = tail call i32 @llvm.bswap.i32(i32 %4)
  ret i32 %5
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @helper_atomic_xchgl_le(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 3
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_xchgl_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_xchgl_le_mmu.exit:                     ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = atomicrmw xchg ptr %3, i32 %val seq_cst, align 4
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %5
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @cpu_atomic_xchgl_le_mmu(ptr noundef %env, i64 noundef %addr, i32 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 3
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = atomicrmw xchg ptr %1, i32 %val seq_cst, align 4
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i32 %3
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_xchgq_be(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_xchgq_be_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_xchgq_be_mmu.exit:                     ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = tail call i64 @llvm.bswap.i64(i64 %val)
  %6 = atomicrmw xchg ptr %3, i64 %5 seq_cst, align 8
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %7 = tail call i64 @llvm.bswap.i64(i64 %6)
  ret i64 %7
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_xchgq_be_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = tail call i64 @llvm.bswap.i64(i64 %val)
  %4 = atomicrmw xchg ptr %1, i64 %3 seq_cst, align 8
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  %5 = tail call i64 @llvm.bswap.i64(i64 %4)
  ret i64 %5
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @helper_atomic_xchgq_le(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi) local_unnamed_addr #2 {
entry:
  %0 = tail call ptr @llvm.returnaddress(i32 0)
  %1 = ptrtoint ptr %0 to i64
  %add.ptr.i.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i.i = lshr i32 %oi, 4
  %and.i.i.i = and i32 %shr.i.i.i, 224
  %trunc.i.i.i = trunc i32 %and.i.i.i to i8
  switch i8 %trunc.i.i.i, label %if.else4.i.i.i [
    i8 0, label %get_alignment_bits.exit.i.i
    i8 -32, label %if.then2.i.i.i
  ]

if.then2.i.i.i:                                   ; preds = %entry
  %and3.i.i.i = and i32 %shr.i.i.i, 7
  br label %get_alignment_bits.exit.i.i

if.else4.i.i.i:                                   ; preds = %entry
  %shr.i8.i.i = lshr exact i32 %and.i.i.i, 5
  br label %get_alignment_bits.exit.i.i

get_alignment_bits.exit.i.i:                      ; preds = %if.else4.i.i.i, %if.then2.i.i.i, %entry
  %a.0.i.i.i = phi i32 [ %and3.i.i.i, %if.then2.i.i.i ], [ %shr.i8.i.i, %if.else4.i.i.i ], [ 0, %entry ]
  %notmask.i.i = shl nsw i32 -1, %a.0.i.i.i
  %sub.i.i = xor i32 %notmask.i.i, -1
  %conv.i.i = zext nneg i32 %sub.i.i to i64
  %and.i.i = and i64 %conv.i.i, %addr
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.end.i.i, label %if.then.i.i

if.then.i.i:                                      ; preds = %get_alignment_bits.exit.i.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef 1, i64 noundef %1) #17
  unreachable

if.end.i.i:                                       ; preds = %get_alignment_bits.exit.i.i
  %and7.i.i = and i64 %addr, 7
  %tobool8.not.i.i = icmp eq i64 %and7.i.i, 0
  br i1 %tobool8.not.i.i, label %cpu_atomic_xchgq_le_mmu.exit, label %if.then15.i.i

if.then15.i.i:                                    ; preds = %if.end.i.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i.i, i64 noundef %1) #17
  unreachable

cpu_atomic_xchgq_le_mmu.exit:                     ; preds = %if.end.i.i
  %2 = load i64, ptr @guest_base, align 8
  %add.i.i.i.i = add i64 %2, %addr
  %3 = inttoptr i64 %add.i.i.i.i to ptr
  %4 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %1, ptr %4, align 8
  fence syncscope("singlethread") seq_cst
  %5 = atomicrmw xchg ptr %3, i64 %val seq_cst, align 8
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %4, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %5
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @cpu_atomic_xchgq_le_mmu(ptr noundef %env, i64 noundef %addr, i64 noundef %val, i32 noundef %oi, i64 noundef %retaddr) local_unnamed_addr #2 {
entry:
  %add.ptr.i = getelementptr i8, ptr %env, i64 -10176
  %shr.i.i = lshr i32 %oi, 4
  %and.i.i = and i32 %shr.i.i, 224
  %trunc.i.i = trunc i32 %and.i.i to i8
  switch i8 %trunc.i.i, label %if.else4.i.i [
    i8 0, label %get_alignment_bits.exit.i
    i8 -32, label %if.then2.i.i
  ]

if.then2.i.i:                                     ; preds = %entry
  %and3.i.i = and i32 %shr.i.i, 7
  br label %get_alignment_bits.exit.i

if.else4.i.i:                                     ; preds = %entry
  %shr.i8.i = lshr exact i32 %and.i.i, 5
  br label %get_alignment_bits.exit.i

get_alignment_bits.exit.i:                        ; preds = %if.else4.i.i, %if.then2.i.i, %entry
  %a.0.i.i = phi i32 [ %and3.i.i, %if.then2.i.i ], [ %shr.i8.i, %if.else4.i.i ], [ 0, %entry ]
  %notmask.i = shl nsw i32 -1, %a.0.i.i
  %sub.i = xor i32 %notmask.i, -1
  %conv.i = zext nneg i32 %sub.i to i64
  %and.i = and i64 %conv.i, %addr
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.end.i, label %if.then.i

if.then.i:                                        ; preds = %get_alignment_bits.exit.i
  tail call void @cpu_loop_exit_sigbus(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef 1, i64 noundef %retaddr) #17
  unreachable

if.end.i:                                         ; preds = %get_alignment_bits.exit.i
  %and7.i = and i64 %addr, 7
  %tobool8.not.i = icmp eq i64 %and7.i, 0
  br i1 %tobool8.not.i, label %atomic_mmu_lookup.exit, label %if.then15.i

if.then15.i:                                      ; preds = %if.end.i
  tail call void @cpu_loop_exit_atomic(ptr noundef %add.ptr.i, i64 noundef %retaddr) #17
  unreachable

atomic_mmu_lookup.exit:                           ; preds = %if.end.i
  %0 = load i64, ptr @guest_base, align 8
  %add.i.i.i = add i64 %0, %addr
  %1 = inttoptr i64 %add.i.i.i to ptr
  %2 = tail call align 8 ptr @llvm.threadlocal.address.p0(ptr align 8 @helper_retaddr)
  store i64 %retaddr, ptr %2, align 8
  fence syncscope("singlethread") seq_cst
  %3 = atomicrmw xchg ptr %1, i64 %val seq_cst, align 8
  fence syncscope("singlethread") seq_cst
  store i64 0, ptr %2, align 8
  tail call void @qemu_plugin_vcpu_mem_cb(ptr noundef %add.ptr.i, i64 noundef %addr, i32 noundef %oi, i32 noundef 3) #16
  ret i64 %3
}

declare void @interval_tree_remove(ptr noundef, ptr noundef) local_unnamed_addr #5

declare void @interval_tree_insert(ptr noundef, ptr noundef) local_unnamed_addr #5

; Function Attrs: nounwind sspstrong uwtable
define internal fastcc void @pageflags_create(i64 noundef %start, i64 noundef %last, i32 noundef %flags) unnamed_addr #2 {
entry:
  %call = tail call noalias dereferenceable_or_null(72) ptr @g_malloc_n(i64 noundef 1, i64 noundef 72) #18
  %itree = getelementptr inbounds %struct.PageFlagsNode, ptr %call, i64 0, i32 1
  %start1 = getelementptr inbounds %struct.PageFlagsNode, ptr %call, i64 0, i32 1, i32 1
  store i64 %start, ptr %start1, align 8
  %last3 = getelementptr inbounds %struct.PageFlagsNode, ptr %call, i64 0, i32 1, i32 2
  store i64 %last, ptr %last3, align 8
  %flags4 = getelementptr inbounds %struct.PageFlagsNode, ptr %call, i64 0, i32 2
  store i32 %flags, ptr %flags4, align 8
  tail call void @interval_tree_insert(ptr noundef nonnull %itree, ptr noundef nonnull @pageflags_root) #16
  ret void
}

declare void @call_rcu1(ptr noundef, ptr noundef) local_unnamed_addr #5

declare void @g_free(ptr noundef) #5

; Function Attrs: allocsize(0,1)
declare noalias ptr @g_malloc_n(i64 noundef, i64 noundef) local_unnamed_addr #13

; Function Attrs: noreturn
declare void @cpu_loop_exit_sigsegv(ptr noundef, i64 noundef, i32 noundef, i1 noundef zeroext, i64 noundef) local_unnamed_addr #4

; Function Attrs: noreturn
declare void @cpu_loop_exit_sigbus(ptr noundef, i64 noundef, i32 noundef, i64 noundef) local_unnamed_addr #4

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn memory(inaccessiblemem: write)
declare void @llvm.assume(i1 noundef) #14

; Function Attrs: mustprogress nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare i32 @llvm.cttz.i32(i32, i1 immarg) #1

; Function Attrs: noreturn
declare void @cpu_loop_exit_atomic(ptr noundef, i64 noundef) local_unnamed_addr #4

declare void @qemu_plugin_vcpu_mem_cb(ptr noundef, i64 noundef, i32 noundef, i32 noundef) local_unnamed_addr #5

; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare i64 @llvm.ctpop.i64(i64) #15

; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare i32 @llvm.smin.i32(i32, i32) #15

; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare i64 @llvm.smin.i64(i64, i64) #15

; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare i32 @llvm.umin.i32(i32, i32) #15

; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare i64 @llvm.umin.i64(i64, i64) #15

; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare i32 @llvm.smax.i32(i32, i32) #15

; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare i64 @llvm.smax.i64(i64, i64) #15

; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare i32 @llvm.umax.i32(i32, i32) #15

; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare i64 @llvm.umax.i64(i64, i64) #15

; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare i32 @llvm.usub.sat.i32(i32, i32) #15

; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare i128 @llvm.bswap.i128(i128) #15

attributes #0 = { mustprogress nofree nosync nounwind sspstrong willreturn memory(read, argmem: readwrite, inaccessiblemem: none) uwtable "frame-pointer"="all" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx16,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #1 = { mustprogress nocallback nofree nosync nounwind speculatable willreturn memory(none) }
attributes #2 = { nounwind sspstrong uwtable "frame-pointer"="all" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx16,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #3 = { nounwind "frame-pointer"="all" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx16,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #4 = { noreturn "frame-pointer"="all" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx16,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #5 = { "frame-pointer"="all" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx16,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #6 = { nofree nounwind "frame-pointer"="all" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx16,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #7 = { noreturn nounwind "frame-pointer"="all" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx16,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #8 = { mustprogress nofree norecurse nosync nounwind sspstrong willreturn memory(none) uwtable "frame-pointer"="all" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx16,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #9 = { mustprogress nofree nosync nounwind sspstrong willreturn uwtable "frame-pointer"="all" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx16,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #10 = { nounwind sspstrong uwtable "frame-pointer"="all" "min-legal-vector-width"="128" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx16,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #11 = { mustprogress nocallback nofree nosync nounwind willreturn memory(none) }
attributes #12 = { noreturn nounwind sspstrong uwtable "frame-pointer"="all" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx16,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #13 = { allocsize(0,1) "frame-pointer"="all" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx16,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #14 = { mustprogress nocallback nofree nosync nounwind willreturn memory(inaccessiblemem: write) }
attributes #15 = { nocallback nofree nosync nounwind speculatable willreturn memory(none) }
attributes #16 = { nounwind }
attributes #17 = { noreturn nounwind }
attributes #18 = { nounwind allocsize(0,1) }
attributes #19 = { nounwind memory(read) }

!llvm.module.flags = !{!0, !1, !2, !3, !4}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 8, !"PIC Level", i32 2}
!2 = !{i32 7, !"PIE Level", i32 2}
!3 = !{i32 7, !"uwtable", i32 2}
!4 = !{i32 7, !"frame-pointer", i32 2}
!5 = !{i32 0, i32 3}
!6 = distinct !{!6, !7}
!7 = !{!"llvm.loop.mustprogress"}
!8 = distinct !{!8, !7}
!9 = !{i64 0, i64 65}
!10 = distinct !{!10, !7}
!11 = !{i64 8262804}
!12 = !{i64 8262880}
!13 = !{i64 3399615}
!14 = !{i64 3399860}
!15 = !{i32 0, i32 33}
!16 = distinct !{!16, !7}
!17 = distinct !{!17, !7}
!18 = distinct !{!18, !7}
!19 = distinct !{!19, !7}
!20 = !{i64 3400215}
!21 = distinct !{!21, !7}
!22 = !{i32 0, i32 65536}
!23 = !{i64 2156106479}
!24 = distinct !{!24, !7}
!25 = !{i64 2156242386}
!26 = distinct !{!26, !7}
!27 = !{i64 2156379433}
!28 = distinct !{!28, !7}
!29 = !{i64 2155939325}
!30 = distinct !{!30, !7}
!31 = !{i32 -32768, i32 32768}
!32 = !{i64 2156063529}
!33 = distinct !{!33, !7}
!34 = !{i64 2156005094}
!35 = distinct !{!35, !7}
!36 = !{i64 2156199276}
!37 = distinct !{!37, !7}
!38 = !{i64 2156140501}
!39 = distinct !{!39, !7}
!40 = !{i64 2156335969}
!41 = distinct !{!41, !7}
!42 = !{i64 2156276722}
!43 = distinct !{!43, !7}
!44 = !{i64 2155944544}
!45 = distinct !{!45, !7}
!46 = !{i64 2156068900}
!47 = distinct !{!47, !7}
!48 = !{i64 2156010376}
!49 = distinct !{!49, !7}
!50 = !{i64 2156204667}
!51 = distinct !{!51, !7}
!52 = !{i64 2156145803}
!53 = distinct !{!53, !7}
!54 = !{i64 2156341392}
!55 = distinct !{!55, !7}
!56 = !{i64 2156282056}
!57 = distinct !{!57, !7}
!58 = !{i64 2155949761}
!59 = distinct !{!59, !7}
!60 = !{i64 2156074269}
!61 = distinct !{!61, !7}
!62 = !{i64 2156015656}
!63 = distinct !{!63, !7}
!64 = !{i64 2156210056}
!65 = distinct !{!65, !7}
!66 = !{i64 2156151103}
!67 = distinct !{!67, !7}
!68 = !{i64 2156346827}
!69 = distinct !{!69, !7}
!70 = !{i64 2156287388}
!71 = distinct !{!71, !7}
!72 = !{i64 2155954980}
!73 = distinct !{!73, !7}
!74 = !{i64 2156079640}
!75 = distinct !{!75, !7}
!76 = !{i64 2156020938}
!77 = distinct !{!77, !7}
!78 = !{i64 2156215447}
!79 = distinct !{!79, !7}
!80 = !{i64 2156156405}
!81 = distinct !{!81, !7}
!82 = !{i64 2156352264}
!83 = distinct !{!83, !7}
!84 = !{i64 2156292722}
!85 = distinct !{!85, !7}
!86 = !{i64 2156111412}
!87 = distinct !{!87, !7}
!88 = !{i64 2156247339}
!89 = distinct !{!89, !7}
!90 = !{i64 2156384418}
!91 = distinct !{!91, !7}
!92 = !{i64 2155960197}
!93 = distinct !{!93, !7}
!94 = !{i64 2156085009}
!95 = distinct !{!95, !7}
!96 = !{i64 2156026218}
!97 = distinct !{!97, !7}
!98 = !{i64 2156220836}
!99 = distinct !{!99, !7}
!100 = !{i64 2156161705}
!101 = distinct !{!101, !7}
!102 = !{i64 2156357699}
!103 = distinct !{!103, !7}
!104 = !{i64 2156302115}
!105 = distinct !{!105, !7}
!106 = !{i64 2155965430}
!107 = distinct !{!107, !7}
!108 = !{i64 2156090380}
!109 = distinct !{!109, !7}
!110 = !{i64 2156031500}
!111 = distinct !{!111, !7}
!112 = !{i64 2156226227}
!113 = distinct !{!113, !7}
!114 = !{i64 2156167007}
!115 = distinct !{!115, !7}
!116 = !{i64 2156363136}
!117 = distinct !{!117, !7}
!118 = !{i64 2156307449}
!119 = distinct !{!119, !7}
!120 = !{i64 2155970661}
!121 = distinct !{!121, !7}
!122 = !{i64 2156095749}
!123 = distinct !{!123, !7}
!124 = !{i64 2156036780}
!125 = distinct !{!125, !7}
!126 = !{i64 2156231616}
!127 = distinct !{!127, !7}
!128 = !{i64 2156176368}
!129 = distinct !{!129, !7}
!130 = !{i64 2156368571}
!131 = distinct !{!131, !7}
!132 = !{i64 2156312781}
!133 = distinct !{!133, !7}
!134 = !{i64 2155975894}
!135 = distinct !{!135, !7}
!136 = !{i64 2156101120}
!137 = distinct !{!137, !7}
!138 = !{i64 2156046123}
!139 = distinct !{!139, !7}
!140 = !{i64 2156237007}
!141 = distinct !{!141, !7}
!142 = !{i64 2156181670}
!143 = distinct !{!143, !7}
!144 = !{i64 2156374008}
!145 = distinct !{!145, !7}
!146 = !{i64 2156318115}
!147 = distinct !{!147, !7}
